
======================================================
Classification of text documents using sparse features
======================================================

This is an example showing how scikit-learn can be used to classify documents
by topics using a bag-of-words approach. This example uses a scipy.sparse
matrix to store the features and demonstrates various classifiers that can
efficiently handle sparse matrices.

The dataset used in this example is the 20 newsgroups dataset. It will be
automatically downloaded, then cached.

The bar plot indicates the accuracy, training time (normalized) and test time
(normalized) of each classifier.


Usage: document_classification_20newsgroups.py [options]

Options:
  -h, --help            show this help message and exit
  --report              Print a detailed classification report.
  --chi2_select=SELECT_CHI2
                        Select some number of features using a chi-squared
                        test
  --confusion_matrix    Print the confusion matrix.
  --top10               Print ten most discriminative terms per class for
                        every classifier.
  --all_categories      Whether to use all categories or not.
  --use_hashing         Use a hashing vectorizer.
  --n_features=N_FEATURES
                        n_features when using the hashing vectorizer.
  --filtered            Remove newsgroup information that is easily overfit:
                        headers, signatures, and quoting.

Loading 20 newsgroups dataset for categories:
['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']
crime                             97
politics                          63
demography                        63
video games                       51
trade and business                39
social media                      36
online information                33
sports events                     23
roadways                          21
astronomy                         21
programming                       20
text analysis                     20
accidents                         17
news                              17
student performances              17
airways                           16
real estate                       16
food and nutrition                16
elections                         16
disasters                         16
geographical information          14
stock data                        14
weather and climate               14
diagnostics                       12
commodity prices                  10
zoological                        10
movies                            10
air pollution                     10
books and comics                  10
genomics                           9
                                  ..
universities and colleges          6
online reviews and ratings         5
board games                        5
mail and messaging                 5
mental health                      5
TV shows                           5
botanical                          5
artificial intelligence            5
blockchain and crytocurrencies     5
income                             4
game strategies                    4
visual arts                        4
Written script                     4
speech                             4
pharmaceuticals                    3
renewable energy                   3
sports scores                      3
drugs and addiction                3
energy production                  3
literature                         3
water pollution                    3
cricket                            3
jobs and employment                3
traffic                            2
noise pollution                    2
physical sciences                  1
photos and images                  1
military                           1
energy consumption                 1
traffic                            1
Name: label, Length: 78, dtype: int64
['crime', 'politics', 'demography', 'video games', 'trade and business', 'social media', 'online information', 'sports events', 'roadways', 'astronomy', 'programming', 'text analysis', 'accidents', 'news', 'student performances', 'airways', 'real estate', 'food and nutrition', 'elections', 'disasters', 'geographical information', 'stock data', 'weather and climate', 'diagnostics', 'commodity prices', 'zoological', 'movies', 'air pollution', 'books and comics', 'genomics', 'football', 'chemistry', 'diseases and epidemics', 'banking', 'sports teams and players', 'music', 'Online forums ', 'grammar', 'health infrastructure', 'fitness and personal well being', 'traffic violations', 'Insurance', 'gadgets', 'videos', 'rail and metro', 'online shopping', 'history', 'court cases', 'universities and colleges']
data loaded
Extracting features from the training data using a sparse vectorizer
n_samples: 651, n_features: 13434

Extracting features from the test data using the same vectorizer
n_samples: 217, n_features: 13434

================================================================================
Ridge Classifier
________________________________________________________________________________
Training: 
RidgeClassifier(alpha=1.0, class_weight='balanced', copy_X=True,
        fit_intercept=True, max_iter=None, normalize=False,
        random_state=None, solver='lsqr', tol=0.01)
train time: 49.055s
test time:  0.003s
accuracy:   0.018
dimensionality: 13434
density: 1.000000


================================================================================
Perceptron
________________________________________________________________________________
Training: 
Perceptron(alpha=0.0001, class_weight='balanced', eta0=1.0,
      fit_intercept=True, max_iter=50, n_iter=None, n_jobs=1, penalty=None,
      random_state=0, shuffle=True, tol=None, verbose=0, warm_start=False)
train time: 0.334s
test time:  0.003s
accuracy:   0.590
dimensionality: 13434
density: 0.101410


================================================================================
Passive-Aggressive
________________________________________________________________________________
Training: 
PassiveAggressiveClassifier(C=1.0, average=False, class_weight='balanced',
              fit_intercept=True, loss='hinge', max_iter=50, n_iter=None,
              n_jobs=1, random_state=None, shuffle=True, tol=None,
              verbose=0, warm_start=False)
train time: 0.491s
test time:  0.003s
accuracy:   0.710
dimensionality: 13434
density: 0.519562


================================================================================
kNN
________________________________________________________________________________
Training: 
KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=10, p=2,
           weights='uniform')
train time: 0.001s
test time:  0.013s
accuracy:   0.641

================================================================================
Random forest
________________________________________________________________________________
Training: 
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,
            verbose=0, warm_start=False)
train time: 1.009s
test time:  0.021s
accuracy:   0.516

================================================================================
L2 penalty
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.001,
     verbose=0)
train time: 0.228s
test time:  0.001s
accuracy:   0.747
dimensionality: 13434
density: 1.000000


________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,
       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False)
train time: 0.343s
test time:  0.002s
accuracy:   0.714
dimensionality: 13434
density: 0.225272


================================================================================
L1 penalty
________________________________________________________________________________
Training: 
LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
     verbose=0)
train time: 0.263s
test time:  0.001s
accuracy:   0.535
dimensionality: 13434
density: 0.000638


________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,
       n_jobs=1, penalty='l1', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False)
train time: 0.786s
test time:  0.003s
accuracy:   0.668
dimensionality: 13434
density: 0.007028


================================================================================
Elastic-Net penalty
________________________________________________________________________________
Training: 
SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',
       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='hinge', max_iter=50, n_iter=None,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, tol=None, verbose=0, warm_start=False)
train time: 1.191s
test time:  0.002s
accuracy:   0.760
dimensionality: 13434
density: 0.054919


================================================================================
NearestCentroid (aka Rocchio classifier)
________________________________________________________________________________
Training: 
NearestCentroid(metric='euclidean', shrink_threshold=None)
train time: 0.028s
test time:  0.003s
accuracy:   0.618

================================================================================
Naive Bayes
________________________________________________________________________________
Training: 
MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
train time: 0.023s
test time:  0.002s
accuracy:   0.571
dimensionality: 13434
density: 1.000000


________________________________________________________________________________
Training: 
BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)
train time: 0.023s
test time:  0.029s
accuracy:   0.493
dimensionality: 13434
density: 1.000000


================================================================================
LinearSVC with L1-based feature selection
________________________________________________________________________________
Training: 
Pipeline(memory=None,
     steps=[('feature_selection', SelectFromModel(estimator=LinearSVC(C=1.0, class_weight='balanced', dual=False, fit_intercept=True,
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,
     multi_class='ovr', penalty='l1', random_state=None, tol=0.001,
     verbose=0),
        norm_order=1, p...ax_iter=1000,
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,
     verbose=0))])
train time: 0.324s
test time:  0.002s
accuracy:   0.631

