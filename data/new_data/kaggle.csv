,Anonymised Labels ,Author,Column Name,Column Type,Description,File Type,Image Dimension,Keywords,License,No Columns,No Rows,Number of Downloads,Number of views,Size,Summary,Title,URL,Updated
0,,UCI Machine Learning,"[ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, default.payment.next.month]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Dataset Information This dataset contains information on default payments demographic factors credit data history of payment and bill statements of credit card clients in Taiwan from April 2005 to September 2005.  Content There are 25 variables  ID ID of each client LIMIT_BAL Amount of given credit in NT dollars (includes individual and family/supplementary credit SEX Gender (1=male 2=female) EDUCATION (1=graduate school 2=university 3=high school 4=others 5=unknown 6=unknown) MARRIAGE Marital status (1=married 2=single 3=others) AGE Age in years PAY_0 Repayment status in September 2005 (-1=pay duly 1=payment delay for one month 2=payment delay for two months ... 8=payment delay for eight months 9=payment delay for nine months and above) PAY_2 Repayment status in August 2005 (scale same as above) PAY_3 Repayment status in July 2005 (scale same as above) PAY_4 Repayment status in June 2005 (scale same as above) PAY_5 Repayment status in May 2005 (scale same as above) PAY_6 Repayment status in April 2005 (scale same as above) BILL_AMT1 Amount of bill statement in September 2005 (NT dollar) BILL_AMT2 Amount of bill statement in August 2005 (NT dollar) BILL_AMT3 Amount of bill statement in July 2005 (NT dollar) BILL_AMT4 Amount of bill statement in June 2005 (NT dollar) BILL_AMT5 Amount of bill statement in May 2005 (NT dollar) BILL_AMT6 Amount of bill statement in April 2005 (NT dollar) PAY_AMT1 Amount of previous payment in September 2005 (NT dollar) PAY_AMT2 Amount of previous payment in August 2005 (NT dollar) PAY_AMT3 Amount of previous payment in July 2005 (NT dollar) PAY_AMT4 Amount of previous payment in June 2005 (NT dollar) PAY_AMT5 Amount of previous payment in May 2005 (NT dollar) PAY_AMT6 Amount of previous payment in April 2005 (NT dollar) default.payment.next.month Default payment (1=yes 0=no)  Inspiration Some ideas for exploration  How does the probability of default payment vary by categories of different demographic variables? Which variables are the strongest predictors of default payment?  Acknowledgements Any publications based on this dataset should acknowledge the following  Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. The original dataset can be found here at the UCI Machine Learning Repository.,CSV,,[finance],CC0,,,7492,75028,3,Default Payments of Credit Card Clients in Taiwan from 2005,Default of Credit Card Clients Dataset,https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset,Thu Nov 03 2016
1,,UCI Machine Learning,"[ORIGIN, MOSTYPE, MAANTHUI, MGEMOMV, MGEMLEEF, MOSHOOFD, MGODRK, MGODPR, MGODOV, MGODGE, MRELGE, MRELSA, MRELOV, MFALLEEN, MFGEKIND, MFWEKIND, MOPLHOOG, MOPLMIDD, MOPLLAAG, MBERHOOG, MBERZELF, MBERBOER, MBERMIDD, MBERARBG, MBERARBO, MSKA, MSKB1, MSKB2, MSKC, MSKD, MHHUUR, MHKOOP, MAUT1, MAUT2, MAUT0, MZFONDS, MZPART, MINKM30, MINK3045, MINK4575, MINK7512, MINK123M, MINKGEM, MKOOPKLA, PWAPART, PWABEDR, PWALAND, PPERSAUT, PBESAUT, PMOTSCO, PVRAAUT, PAANHANG, PTRACTOR, PWERKT, PBROM, PLEVEN, PPERSONG, PGEZONG, PWAOREG, PBRAND, PZEILPL, PPLEZIER, PFIETS, PINBOED, PBYSTAND, AWAPART, AWABEDR, AWALAND, APERSAUT, ABESAUT, AMOTSCO, AVRAAUT, AAANHANG, ATRACTOR, AWERKT, ABROM, ALEVEN, APERSONG, AGEZONG, AWAOREG, ABRAND, AZEILPL, APLEZIER, AFIETS, AINBOED, ABYSTAND, CARAVAN]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This data set used in the CoIL 2000 Challenge contains information on customers of an insurance company. The data consists of 86 variables and includes product usage data and socio-demographic data derived from zip area codes. The data was collected to answer the following question Can you predict who would be interested in buying a caravan insurance policy and give an explanation why? Acknowledgements DISCLAIMER This dataset is owned and supplied by the Dutch datamining company Sentient Machine Research and is based on real world business data. You are allowed to use this dataset and accompanying information for non commercial research and education purposes only. It is explicitly not allowed to use this dataset for commercial education or demonstration purposes. For any other use please contact Peter van der Putten info@smr.nl. This dataset has been used in the CoIL Challenge 2000 datamining competition. For papers describing results on this dataset see the TIC 2000 homepage http//www.wi.leidenuniv.nl/~putten/library/cc2000/ Please cite/acknowledge P. van der Putten and M. van Someren (eds) . CoIL Challenge 2000 The Insurance Company Case. Published by Sentient Machine Research Amsterdam. Also a Leiden Institute of Advanced Computer Science Technical Report 2000-09. June 22 2000. The Data Originally this dataset was broken into two parts the training set and the evaluation set. As this was a competition the responses to the evaluation set were not given as part of the original release; they were however released after the end of the competition in a separate file. This dataset contains all three of these files combined into one. The field ORIGIN in the caravan-insurance-challenge.csv file has the values train and test corresponding to the training and evaluation sets respectively. To simulate the original challenge you can ignore the test rows and test your model's prediction on those observations once you've trained only on the training set. Each observation corresponds to a postal code. Variables beginning with M refer to demographic statistics of the postal code while variables beginning with P and A (as well as CARAVAN the target variable) refer to product ownership and insurance statistics in the postal code. The data file contains the following fields  ORIGIN train or test as described above MOSTYPE Customer Subtype; see L0 MAANTHUI Number of houses 1 - 10 MGEMOMV Avg size household 1 - 6 MGEMLEEF Avg age; see L1 MOSHOOFD Customer main type;  see L2  ** Percentages in each group per postal code (see L3)**  MGODRK Roman catholic MGODPR Protestant ... MGODOV Other religion MGODGE No religion MRELGE Married MRELSA Living together MRELOV Other relation MFALLEEN Singles MFGEKIND Household without children MFWEKIND Household with children MOPLHOOG High level education MOPLMIDD Medium level education MOPLLAAG Lower level education MBERHOOG High status MBERZELF Entrepreneur MBERBOER Farmer MBERMIDD Middle management MBERARBG Skilled labourers MBERARBO Unskilled labourers MSKA Social class A MSKB1 Social class B1 MSKB2 Social class B2 MSKC Social class C MSKD Social class D MHHUUR Rented house MHKOOP Home owners MAUT1 1 car MAUT2 2 cars MAUT0 No car MZFONDS National Health Service MZPART Private health insurance MINKM30 Income < 30.000 MINK3045 Income 30-45.000 MINK4575 Income 45-75.000 MINK7512 Income 75-122.000 MINK123M Income >123.000 MINKGEM Average income MKOOPKLA Purchasing power class  ** Total number of variable in postal code (see L4)**  PWAPART Contribution private third party insurance PWABEDR Contribution third party insurance (firms) ... PWALAND Contribution third party insurane (agriculture) PPERSAUT Contribution car policies PBESAUT Contribution delivery van policies PMOTSCO Contribution motorcycle/scooter policies PVRAAUT Contribution lorry policies PAANHANG Contribution trailer policies PTRACTOR Contribution tractor policies PWERKT Contribution agricultural machines policies PBROM Contribution moped policies PLEVEN Contribution life insurances PPERSONG Contribution private accident insurance policies PGEZONG Contribution family accidents insurance policies PWAOREG Contribution disability insurance policies PBRAND Contribution fire policies PZEILPL Contribution surfboard policies PPLEZIER Contribution boat policies PFIETS Contribution bicycle policies PINBOED Contribution property insurance policies PBYSTAND Contribution social security insurance policies AWAPART Number of private third party insurance 1 - 12 AWABEDR Number of third party insurance (firms) ... AWALAND Number of third party insurance (agriculture) APERSAUT Number of car policies ABESAUT Number of delivery van policies AMOTSCO Number of motorcycle/scooter policies AVRAAUT Number of lorry policies AAANHANG Number of trailer policies ATRACTOR Number of tractor policies AWERKT Number of agricultural machines policies ABROM Number of moped policies ALEVEN Number of life insurances APERSONG Number of private accident insurance policies AGEZONG Number of family accidents insurance policies AWAOREG Number of disability insurance policies ABRAND Number of fire policies AZEILPL Number of surfboard policies APLEZIER Number of boat policies AFIETS Number of bicycle policies AINBOED Number of property insurance policies ABYSTAND Number of social security insurance policies CARAVAN Number of mobile home policies 0 - 1  Keys (L1 - L4) L0 Customer subtype  1 High Income expensive child 2 Very Important Provincials 3 High status seniors 4 Affluent senior apartments 5 Mixed seniors 6 Career and childcare 7 Dinki's (double income no kids) 8 Middle class families 9 Modern complete families 10 Stable family 11 Family starters 12 Affluent young families 13 Young all american family 14 Junior cosmopolitan 15 Senior cosmopolitans 16 Students in apartments 17 Fresh masters in the city 18 Single youth 19 Suburban youth 20 Etnically diverse 21 Young urban have-nots 22 Mixed apartment dwellers 23 Young and rising 24 Young low educated  25 Young seniors in the city 26 Own home elderly 27 Seniors in apartments 28 Residential elderly 29 Porchless seniors no front yard 30 Religious elderly singles 31 Low income catholics 32 Mixed seniors 33 Lower class large families 34 Large family employed child 35 Village families 36 Couples with teens 'Married with children' 37 Mixed small town dwellers 38 Traditional families 39 Large religous families 40 Large family farms 41 Mixed rurals  L1 average age keys 1 20-30 years 2 30-40 years 3 40-50 years 4 50-60 years 5 60-70 years 6 70-80 years L2 customer main type keys  1 Successful hedonists 2 Driven Growers 3 Average Family 4 Career Loners 5 Living well 6 Cruising Seniors 7 Retired and Religeous 8 Family with grown ups 9 Conservative families 10 Farmers  L3 percentage keys  0 0% 1 1 - 10% 2 11 - 23% 3 24 - 36% 4 37 - 49% 5 50 - 62% 6 63 - 75% 7 76 - 88% 8 89 - 99% 9 100%  L4 total number keys  0 0 1 1 - 49 2 50 - 99 3 100 - 199 4 200 - 499 5 500 - 999 6 1000 - 4999 7 5000 - 9999 8 10000 - 19999 9 >= 20000 ,CSV,,"[finance, automobiles]",Other,,,1178,12354,2,Identify potential purchasers of caravan insurance policies,Caravan Insurance Challenge,https://www.kaggle.com/uciml/caravan-insurance-challenge,Mon Nov 28 2016
2,,Priya_ds,"[Food.Pinching.Efficiency, Individual, Chopstick.Length]","[numeric, numeric, numeric]",Question As per the 1992 ergonomics study What is the optimum length of chopsticks usable by adults & children? Acknowledgements An investigation for determining the optimum length of chopsticks. Hsu SH Wu SP. Appl Ergon. 1991 Dec;22(6)395-400. PMID 15676839 Inspiration Data Sets shared by a beginner for Beginners ) Thanks to David Venturi and Udacity that I was able to get started off on this. More details here  Link,CSV,,"[food and drink, psychometrics]",Other,,,100,1395,0.0029296875,Evaluate the effects of chopsticks length on food-serving performance,Beginner Projects - Ergonomic Study on Chopsticks,https://www.kaggle.com/priya2908/chopsticks-1992,Mon Jun 12 2017
3,,UCI Machine Learning,[],[],Context The problem is to predict user ratings for web pages (within a subject category). The HTML source of a web page is given. Users looked at each web page and indicated on a 3 point scale (hot medium cold) 50-100 pages per domain. Content This database contains HTML source of web pages plus the ratings of a single user on these web pages. Web pages are on four separate subjects (Bands- recording artists; Goats; Sheep; and BioMedical). Acknowledgement Data originally from the UCI ML Repository. Donated by Michael Pazzani Department of Information and Computer Science University of California Irvine Irvine CA 92697-3425 pazzani@ics.uci.edu Concept based Information Access with Google for Personalized Information Retrieval,Other,,"[web sites, internet]",Other,,,273,2876,2,Automatically Extracting Features for Concept Learning from the Web,Identifying Interesting Web Pages,https://www.kaggle.com/uciml/identifying-interesting-web-pages,Thu Sep 14 2017
4,,mrpantherson,"[, band_name, fans, formed, origin, split, style]","[numeric, string, numeric, numeric, string, numeric, string]",Context The story behind this data set and analysis is just a combination of my interest in data science and metal music.  I was looking for interesting data that I could analyze and this happen to be one of the first I started exploring. Content The world population information was a direct download so I did not have to do any work to get it.  This data set consists of population information for countries on earth from the years 1960 to 2015. The metal band information was scraped from the website http//metalstorm.net/ and consists of the following  - band name  - how many fans the band has on the website   - when the band formed  - when the band split  - the country of origin on the band  - the styles of the band Acknowledgements The metal information was compiled from information found on http//metalstorm.net/ the world population information is from http//www.worldbank.org/. Inspiration There has already been some great analysis of metal bands on Kaggle I wanted to contribute to the discussion by adding some new data and looking at it from a slightly different angle.  Also I thought it might be useful to share the process by which I came up with the visualizations and data management since the community has been a big help to me.,CSV,,"[music, international relations]",CC0,,,349,2863,0.37109375,The data sets within contain information on metal bands and world population,Metal Bands by Nation,https://www.kaggle.com/mrpantherson/metal-by-nation,Thu Feb 16 2017
5,,US Geological Survey,"[COMPOUND, YEAR, STATE_CODE, COUNTY_CODE, LOW_ESTIMATE, HIGH_ESTIMATE]","[string, numeric, numeric, numeric, numeric, numeric]",Content This dataset includes annual county-level pesticide use estimates for 423 pesticides (active ingredients) applied to agricultural crops grown in the contiguous United States. Two different methods were used to estimate a range of pesticide use for all states except California. Both low and high estimate methods incorporated proprietary surveyed rates for United States Department of Agriculture Crop Reporting Districts but the estimates differed in how they treated situations when a district was surveyed and pesticide use was not reported. Low estimates assumed zero use in the district for that pesticide; however high estimates treated the unreported use of pesticides as missing data and estimated the pesticide usage from neighboring locations within the same region. Acknowledgements Data for the state of California was provided by the 2014 Department of Pesticide Regulation Pesticide Use Report. The 2015 report is not yet available.,CSV,,[agriculture],CC0,,,490,4323,24,Which compounds are used most frequently in the United States?,Pesticide Use in Agriculture,https://www.kaggle.com/usgs/pesticide-use,Wed Jan 25 2017
6,,UCI Machine Learning,"[school, sex, age, address, famsize, Pstatus, Medu, Fedu, Mjob, Fjob, reason, guardian, traveltime, studytime, failures, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic, famrel, freetime, goout, Dalc, Walc, health, absences, G1, G2, G3]","[string, string, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The data were obtained in a survey of students math and portuguese language courses in secondary school. It contains a lot of interesting social gender and study information about students.  You can use it for some EDA or try to predict students final grade. Content Attributes for both student-mat.csv (Math course) and student-por.csv (Portuguese language course) datasets   school - student's school (binary 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira) sex - student's sex (binary 'F' - female or 'M' - male)  age - student's age (numeric from 15 to 22)  address - student's home address type (binary 'U' - urban or 'R' - rural)  famsize - family size (binary 'LE3' - less or equal to 3 or 'GT3' - greater than 3)  Pstatus - parent's cohabitation status (binary 'T' - living together or 'A' - apart) Medu - mother's education (numeric 0 - none  1 - primary education (4th grade) 2 – 5th to 9th grade 3 – secondary education or 4 – higher education) Fedu - father's education (numeric 0 - none  1 - primary education (4th grade) 2 – 5th to 9th grade 3 – secondary education or 4 – higher education) Mjob - mother's job (nominal 'teacher' 'health' care related civil 'services' (e.g. administrative or police) 'at_home' or 'other')  Fjob - father's job (nominal 'teacher' 'health' care related civil 'services' (e.g. administrative or police) 'at_home' or 'other')  reason - reason to choose this school (nominal close to 'home' school 'reputation' 'course' preference or 'other')  guardian - student's guardian (nominal 'mother' 'father' or 'other')  traveltime - home to school travel time (numeric 1 - <15 min. 2 - 15 to 30 min. 3 - 30 min. to 1 hour or 4 - >1 hour)  studytime - weekly study time (numeric 1 - <2 hours 2 - 2 to 5 hours 3 - 5 to 10 hours or 4 - >10 hours)  failures - number of past class failures (numeric n if 1<=n<3 else 4)  schoolsup - extra educational support (binary yes or no)  famsup - family educational support (binary yes or no)  paid - extra paid classes within the course subject (Math or Portuguese) (binary yes or no)  activities - extra-curricular activities (binary yes or no)  nursery - attended nursery school (binary yes or no)  higher - wants to take higher education (binary yes or no)  internet - Internet access at home (binary yes or no)  romantic - with a romantic relationship (binary yes or no)  famrel - quality of family relationships (numeric from 1 - very bad to 5 - excellent)  freetime - free time after school (numeric from 1 - very low to 5 - very high)  goout - going out with friends (numeric from 1 - very low to 5 - very high)  Dalc - workday alcohol consumption (numeric from 1 - very low to 5 - very high)  Walc - weekend alcohol consumption (numeric from 1 - very low to 5 - very high)  health - current health status (numeric from 1 - very bad to 5 - very good)  absences - number of school absences (numeric from 0 to 93)   These grades are related with the course subject Math or Portuguese   G1 - first period grade (numeric from 0 to 20)  G2 - second period grade (numeric from 0 to 20)  G3 - final grade (numeric from 0 to 20 output target)   Additional note there are several (382) students that belong to both datasets .  These students can be identified by searching for identical attributes that characterize each student as shown in the annexed R file. Source Information P. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds. Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12 Porto Portugal April 2008 EUROSIS ISBN 978-9077381-39-7. Fabio Pagnotta Hossain Mohammad Amran.  Emailfabio.pagnotta@studenti.unicam.it mohammadamra.hossain '@' studenti.unicam.it  University Of Camerino https//archive.ics.uci.edu/ml/datasets/STUDENT+ALCOHOL+CONSUMPTION,CSV,,"[food and drink, public health]",CC0,,,18468,108845,0.10546875,"Social, gender and study data from secondary school students",Student Alcohol Consumption,https://www.kaggle.com/uciml/student-alcohol-consumption,Wed Oct 19 2016
7,,Rachael Tatman,[],[],Context Character encodings are sets of mappings from raw bits (0’s and 1’s) to text characters. When a text encoded with a specific encoder is decoded with a different encoder it changes the output text. Sometimes this results in completely unreadable text. This dataset is intended to provide a list of example texts in different character encodings to help you diagnose which file encoding your source file actually in.  Content This dataset is made up of six text files that represent five different character encodings and six different languages. The character encodings represented in this dataset are ISO-8859-1 (also known as Latin 1)  ASCII Windows 1251 UTF-16 that has been successfully converted into the UTF-8 and BIG-5. More information on the files is available in the file_guide.csv file. Each  text file contains a header and footer. The body text is delimited by this text * START OF THE PROJECT GUTENBERG EBOOK [TITLE OF BOOK GOES HERE] * * END OF THE PROJECT GUTENBERG EBOOK [TITLE OF BOOK GOES HERE]* Acknowledgements The texts in this dataset were prepared by Project Gutenberg volunteers. These texts are in the public domain.  Inspiration  Can you build an tool to automatically detect when a file in the wrong encoding is read in? You can use this dataset to explore what happens when you read in text using different encoders. ,Other,,"[writing, languages, linguistics]",CC0,,,21,485,0.9248046875, Example text files for five popular text encodings,Character Encoding Examples,https://www.kaggle.com/rtatman/character-encoding-examples,Sat Dec 16 2017
8,,PyTorch,[],[],SqueezeNet 1.0  SqueezeNet AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy smaller DNN architectures offer at least three advantages (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).   Authors Forrest N. Iandola Song Han Matthew W. Moskewicz Khalid Ashraf William J. Dally Kurt Keutzer https//arxiv.org/abs/1602.07360  SqueezeNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,5,334,4,SqueezeNet 1.1 Pre-trained Model for PyTorch,SqueezeNet 1.1,https://www.kaggle.com/pytorch/squeezenet11,Fri Dec 15 2017
9,,FiveThirtyEight,"[cycle, branch, type, matchup, forecastdate, state, startdate, enddate, pollster, grade, samplesize, population, poll_wt, rawpoll_clinton, rawpoll_trump, rawpoll_johnson, rawpoll_mcmullin, adjpoll_clinton, adjpoll_trump, adjpoll_johnson, adjpoll_mcmullin, multiversions, url, poll_id, question_id, createddate, timestamp]","[numeric, string, string, string, dateTime, string, dateTime, dateTime, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, dateTime, string]",Dataset Information This dataset is a collection of state and national polls conducted from November 2015-November 2016 on the 2016 presidential election. Data on the raw and weighted poll results by state date pollster and pollster ratings are included. Content There are 27 variables  cycle branch type matchup forecastdate state startdate enddate pollster grade samplesize populaion poll_wt rawpoll_clinton rawpoll_trump rawpoll_johnson rawpoll_mcmullin adjpoll_clinton adjpoll_trump adjpoll_johnson adjpoll_mcmullin multiversions url poll_id question_id createddate timestamp  Inspiration Some questions for exploring this dataset are  What are the trends of the polls over time (by day/week/month)? How do the trends vary by state? What is the probability that Trump/Clinton will win the 2016 election?  Acknowledgements The original dataset is from the FiveThirtyEight 2016 Election Forecast and can be downloaded from here. Poll results were aggregated from HuffPost Pollster RealClearPolitics polling firms and news reports.,CSV,,"[news agencies, politics]",Other,,,2253,18181,3,Collection of Presidential Election Polls from 2015-2016,2016 Election Polls,https://www.kaggle.com/fivethirtyeight/2016-election-polls,Thu Nov 03 2016
10,,Tamber,"[151603712, The Elder Scrolls V Skyrim, purchase, 1.0, 0]","[numeric, string, string, numeric, numeric]",Context Steam is the world's most popular PC Gaming hub with over 6000 games and a community of millions of gamers. With a massive collection that includes everything from AAA blockbusters to small indie titles great discovery tools are a highly valuable asset for Steam. How can we make them better? Content This dataset is a list of user behaviors with columns user-id game-title behavior-name value. The behaviors included are 'purchase' and 'play'. The value indicates the degree to which the behavior was performed - in the case of 'purchase' the value is always 1 and in the case of 'play' the value represents the number of hours the user has played the game. Acknowledgements This dataset is generated entirely from public Steam data so we want to thank Steam for building such an awesome platform and community! Inspiration The dataset is formatted to be compatible with Tamber. Build a Tamber engine and take it for a spin!  Combine our collaborative filter's results with your favorite Machine Learning techniques with Ensemble Learning or make Tamber do battle with something else you've built. Have fun The Tamber Team,CSV,,[video games],ODbL,,,1838,16206,9,Recommend video games from 200k steam user interactions.,Steam Video Games,https://www.kaggle.com/tamber/steam-video-games,Thu Mar 09 2017
11,,Gun Violence Archive,"[Incident Date, State, City Or County, Address, # Killed, # Injured, Operations]","[dateTime, string, string, string, numeric, numeric, string]",Context The Gun Violence Archive is an online archive of gun violence incidents collected from over 2000 media law enforcement government and commercial sources daily in an effort to provide near-real time data about the results of gun violence. GVA in an independent data collection and research group with no affiliation with any advocacy organization. Content This dataset includes files that separate gun violence incidents by category including deaths and injuries of children and teens and a collection of mass shootings. Inspiration  What has been the trend of gun violence in the past few years?  What states have the highest incidents per capita per year? How has this metric changed over time? Are officer involved shootings on the rise? Where are they most concentrated? Do they correlate with the rates of accidental deaths and mass shootings?  Acknowledgements This dataset is owned by the Gun Violence Archive and can be accessed in its original form here.,CSV,,[crime],Other,,,1778,9152,0.41015625,"Archive of U.S. gun violence incidents collected from over 2,000 sources",Gun violence database,https://www.kaggle.com/gunviolencearchive/gun-violence-database,Sun Nov 27 2016
12,,cricketsavant,"[match_id, series_id, match_details, result, scores, date, venue, round, home, away, winner, win_by_runs, win_by_wickets, balls_remaining, innings1, innings1_runs, innings1_wickets, innings1_overs_batted, innings1_overs, innings2, innings2_runs, innings2_wickets, innings2_overs_batted, innings2_overs, D/L_method, target]","[numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, string]",Context Match details of over 6000 T20 matches including innings scores and results. Content The first 3 columns show the original data that was scraped. The remaining columns are individual data points extracted from these columns. • match_details summary of match including stage of tournament (if applicable) home/away teams venue and date of match. • result summary of final result. Includes ties (and any winners as a result of bowl-outs/super overs etc.) no results and abandoned matches. • scores summary of scores of both innings. Includes scores even if match ends in a no result. Blank indicates that match was abandoned without a ball bowled. • date date of match in standard date format dd/mm/yyyy. If match goes to reserve day this date is used. • venue city of match. Can be assumed to be main stadium within city. If more than one stadium in a city it is usually labelled. • round stage within tournament e.g. final semi-final group stage etc. Also includes 1st 2nd T20I etc. for bilateral series. • home home or designated home team. • away away or designated away team. • winner winner of match including any winners by any method to determine a winner after a tie. • win_by_runs number of runs team batting first wins by. • win_by_wickets number of wickets team batting second wins by. • balls_remaining number of balls remaining for team batting second after win. • innings1 team batting first • innings1_runs first innings score • innings1_wickets first innings wickets • innings1_overs_batted actual length of first innings • innings1_overs maximum length of first innings • innings2 team batting second • innings2_runs second innings score • innings2_wickets second innings wickets • innings2_overs_batted actual length of second innings • innings2_overs maximum length of second innings • D/L method 1 means that the D/L method (or VJB method) was used to determine winner. • target rain-adjusted target. If blank target is first innings score plus 1 as normal. NEW all T20 series added. Please let me know if you spot any mistakes!,CSV,,[cricket],Other,,,1310,8800,2,Details of over 6000 T20 matches since 2003,T20 cricket matches,https://www.kaggle.com/imrankhan17/t20matches,Wed Apr 12 2017
13,,Frank,"[, timestamp, brightness, doy, dow, year, tod]","[numeric, dateTime, numeric, numeric, numeric, numeric, dateTime]",Context These data are from a couple of sensors of my dad's house. Content The data are from motion sensors at the front door which also regularly logs brightness. The front door motion detection data also includes motions of three cats. Data structure is pretty self explanatory. Load the csv files. Data Files I already added day-of-year year weekday and time of day to the data for easier handling. Brightness These time-stamped values resemble the brightness readings. They range from dark (low numbers) to bright day light (high numbers). The brightness nightly mean values differ. The reasons are Christmas decoration during the winter and solar lights being set up some time in mid April this year. Contacts These data indicate door and window openings. They are time-stamped. The boolean value isClosed indicates wether the contact has been closed. Motion There is a motion sensor at the front door which indicates movement. Movement detections also are time-stamped data. Questions  At what time of the day the newspaper is delivered? How can I tell from the brightness value what kind of weather it was on this day?  Additional information  Newspaper is not delivered on Sundays Newspaper is in the mailbox by 630 AM Newspaper is usually taken out of the mailbox around 700 AM (not on Saturdays) There is regular front door activity - someon leaving the house - at 730 (not on Saturdays and Sundays) There are three cats living at the house ,CSV,,"[home, artificial intelligence]",Other,,,102,1374,0.9775390625,Can you tell when the newspaper is delivered?,Front Door Motion & Brightness,https://www.kaggle.com/fdraeger/frontdoormotionbrightness,Fri May 26 2017
14,,Zurda,"[, year, month, intent, police, sex, age, race, hispanic, place, education]","[numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric]",Context This dataset includes information about gun-death in the US in the years 2012-2014.  Content The data includes data regarding the victim's age sex race education intent time (month and year) and place of death and whether or not police was at the place of death.  Acknowledgements I came across this thanks to FiveThirtyEight's Gun Deaths in America project. The data originated from the CDC and can be found here.,CSV,,"[death, crime, violence, demographics]",Other,,,1831,10261,6,"Information about gun-deaths from the CDC: Ages, gender, intent and more",Gun Deaths in the US: 2012-2014,https://www.kaggle.com/hakabuk/gun-deaths-in-the-us,Wed Jan 25 2017
15,,Fortune,"[f500-2017-rank, name, data-avail, data-url, diversity-pg-url, data-year, PAYROLL_START, PAYROLL_END, HISPM1, HISPM1_2, HISPM2, HISPM3, HISPM4, HISPM5, HISPM6, HISPM7, HISPM8, HISPM9, HISPM10, HISPM11, HISPF1, HISPF1_2, HISPF2, HISPF3, HISPF4, HISPF5, HISPF6, HISPF7, HISPF8, HISPF9, HISPF10, HISPF11, WHM1, WHM1_2, WHM2, WHM3, WHM4, WHM5, WHM6, WHM7, WHM8, WHM9, WHM10, WHM11, BLKM1, BLKM1_2, BLKM2, BLKM3, BLKM4, BLKM5, BLKM6, BLKM7, BLKM8, BLKM9, BLKM10, BLKM11, NHOPIM1, NHOPIM1_2, NHOPIM2, NHOPIM3, NHOPIM4, NHOPIM5, NHOPIM6, NHOPIM7, NHOPIM8, NHOPIM9, NHOPIM10, NHOPIM11, ASIANM1, ASIANM1_2, ASIANM2, ASIANM3, ASIANM4, ASIANM5, ASIANM6, ASIANM7, ASIANM8, ASIANM9, ASIANM10, ASIANM11, AIANM1, AIANM1_2, AIANM2, AIANM3, AIANM4, AIANM5, AIANM6, AIANM7, AIANM8, AIANM9, AIANM10, AIANM11, TOMRM1, TOMRM1_2, TOMRM2, TOMRM3, TOMRM4, TOMRM5, TOMRM6, TOMRM7]","[numeric, string, string, string, string, numeric, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Workforce diversity is an increasingly salient issue but it can be difficult to easily check how a specific company is performing. This dataset was created by Fortune to show what was discoverable by someone considering employment with one of the Fortune 500 firms and curious about their commitment to diversity and inclusion could find. Content This dataset contains the name of each firm its rank in the 2017 Fortune 500 a link to its diversity and inclusion page or equal opportunity statement and whether the company releases full partial or no data about the gender race and ethnicity of its employees. Additional detail is included where it was available. As there are over 200 fields in this dataset; please consult the data dictionary for details about specific features. Acknowledgements This dataset was assembled by Fortune.com data reporter Grace Donnelly. The details of her data preparation process can be found here. Inspiration Are the companies that release the most information more or less diverse than their peers? Are there any particular industries that stand out?,CSV,,[demographics],Other,,,271,2313,0.44921875,Detailed diversity metrics for the Fortune 500 companies,Fortune 500 Diversity,https://www.kaggle.com/fortune-inc/f500-diversity,Mon Jun 26 2017
16,,Kyubyong Park,"[quizzes, solutions]","[numeric, numeric]","Context Sudoku is a popular number puzzle that requires you to fill blanks in a 9X9 grid with digits so that each column each row and each of the nine 3×3 subgrids contains all of the digits from 1 to 9. Sudoku-solving has gained much attention from various fields. As a deep learning researcher I was inclined to investigate the possibilities of neural networks solving Sudoku. This dataset was prepared for that. Content There are dozens of source codes to generate Sudoku games available. I picked one of them and ran the code. It took approximately 6 hours to generate 1 million games ( + solutions). A Sudoku puzzle is represented as a 9x9 Python numpy array. The blanks were replaced with 0's. You can easily load and explore the data by running this. import numpy as np quizzes = np.load('sudoku_quizzes.npy') # shape = (1000000 9 9) solutions = np.load('sudoku_solutions.npy') # shape = (1000000 9 9) for quiz solution in zip(quizzes[10] solutions[10])     print(quiz)     print(solution)  ** Updates for Version 3. ** I converted NumPy arrays to csv so they are easily accessible irrespective of language. In each line a Sudoku quiz and its corresponding solution are separated by a comma. You can restore the csv file content to Numpy arrays if needed as follows import numpy as np quizzes = np.zeros((1000000 81) np.int32) solutions = np.zeros((1000000 81) np.int32) for i line in enumerate(open('sudoku.csv' 'r').read().splitlines()[1])     quiz solution = line.split("""")     for j q_s in enumerate(zip(quiz solution))         q s = q_s         quizzes[i j] = q         solutions[i j] = s quizzes = quizzes.reshape((-1 9 9)) solutions = solutions.reshape((-1 9 9))  Acknowledgements I'm grateful to Arel Cordero who wrote and shared this great Sudoku generation code. https//www.ocf.berkeley.edu/~arel/sudoku/main.html. Inspiration  Check  https//github.com/Kyubyong/sudoku to see if CNNs can crack Sudoku puzzles. Also reinforcement learning can be a promising alternative to this task. Feel free to challenge Sudoku puzzles. ",CSV,,[games and toys],CC0,,,1588,28772,156,1 million numpy array pairs of Sudoku games and solutions,1 million Sudoku games,https://www.kaggle.com/bryanpark/sudoku,Thu Dec 29 2016
17,,NASA,"[Peak Brightness Date/Time (UT), Latitude (deg.), Longitude (deg.), Altitude (km), Velocity (km/s), vx, vy, vz, Total Radiated Energy (J), Calculated Total Impact Energy (kt)]","[dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Fireballs and bolides are astronomical terms for exceptionally bright meteors that are spectacular enough to to be seen over a very wide area. A world map shows a visual representation of the data table that provides a chronological data summary of fireball and bolide events provided by U.S. Government sensors. Ground-based observers sometimes also witness these events at night or much more rarely in daylight as impressive atmospheric light displays. This website is not meant to be a complete list of all fireball events. Only the brightest fireballs are noted. Content The accompanying table provides information on the date and time of each reported fireball event with its approximate total optical radiated energy and its calculated total impact energy. When reported the event’s geographic location altitude and velocity at peak brightness are also provided. Note that data are not provided in real-time and not all fireballs are reported. A blank (empty) field in the table indicates the associated value was not reported. For more information about fireballs and bolides please see https//cneos.jpl.nasa.gov/fireballs/intro.html. Field legend  Peak Brightness Date/Time (UT) The date and time in UT (Universal Time) of this event's peak brightness. Latitude (deg.) Geodetic latitude in degrees north (N) or south (S) of the equator for this event. Longitude (deg.) Geodetic longitude in degrees east (E) or west (W) of the prime meridian for this event. Altitude (km) Altitude in kilometers (km) above the reference geoid for this event. Velocity (km/s) The magnitude of the meteor's pre-impact velocity in kilometers per second (km/s). Velocity Components (km/s) The magnitude of the meteor's pre-impact velocity in a geocentric Earth-fixed reference frame defined as follows the z-axis is directed along the Earth's rotation axis towards the celestial north pole the x-axis lies in the Earth's equatorial plane directed towards the prime meridian and the y-axis completes the right-handed coordinate system. Total Radiated Energy (J) The approximate total radiated energy in the atmosphere in Joules [a unit of energy given in kilograms times velocity squared or kg × (m/s)2] Calculated Total Impact Energy (kt) The impact energy of the event in kilotons of TNT (kt) computed from an empirical expression relating radiated and impact energy  Acknowledgements This dataset was kindly made available by NASA. You can find the original dataset at https//cneos.jpl.nasa.gov/fireballs/ You might also be interested in their Planetary Defense FAQ.,CSV,,"[government agencies, space]",Other,,,114,1359,0.0498046875,Bolide impacts by the kiloton since 1988.,Fireballs,https://www.kaggle.com/nasa/fireballs,Wed Aug 23 2017
18,,Sam Harris,"[Offense Type, Offense Description, Report Date, Offense Start Date, Offense End Date, Block, District, Beat, 2000 Census Tract, Longitude, Latitude]","[string, string, dateTime, dateTime, numeric, string, string, string, numeric, numeric, numeric]",All recorded police reports as taken from https//data.seattle.gov/Public-Safety/Seattle-Police-Department-Police-Report-Incident/7ais-f98f,CSV,,"[cities, crime]",Other,,,234,3042,96,Seattle Police Reports,Seattle Police Reports,https://www.kaggle.com/samharris/seattle-crime,Tue Oct 25 2016
19,,churandy,"[Adult..15...literacy.rate......Total, X1975, X1976, X1977, X1978, X1979, X1980, X1981, X1982, X1983, X1984, X1985, X1986, X1987, X1988, X1989, X1990, X1991, X1992, X1993, X1994, X1995, X1996, X1997, X1998, X1999, X2000, X2001, X2002, X2003, X2004, X2005, X2006, X2007, X2008, X2009, X2010, X2011]","[string, string, string, string, string, numeric, numeric, string, string, string, numeric, string, string, numeric, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, numeric, string, numeric, string, string, numeric]",If you get richer your teeth could get worse (if you eat more sugar foods) or better (because of better health assistance or even more education and health-conciousness).  These variables can be analysed with these data downloaded from Gapminder Data  Bad teeth per child (12 yr WHO) GDP/capita (US$ inflation-adjusted World Bank) Government health spending per person (US$ WHO) Sugar comsumption per person (g per day FAO) Literacy rate adult total (% of people ages 15 and above UNESCO) ,CSV,,"[public health, health]",CC0,,,2038,12699,0.296875,Thanks to Gapminder Data,"Bad teeth, sugar and government health spending",https://www.kaggle.com/angelmm/healthteethsugar,Fri Aug 19 2016
20,,Dryad Digital Repository,"[VECTOR, OCCURRENCE_ID, SOURCE_TYPE, LOCATION_TYPE, POLYGON_ADMIN, Y, X, YEAR, COUNTRY, COUNTRY_ID, GAUL_AD0, STATUS]","[string, numeric, string, string, numeric, numeric, numeric, numeric, string, string, numeric, string]",Aedes aegypti and Ae. albopictus are the main vectors transmitting dengue and chikungunya viruses. Despite being pathogens of global public health importance knowledge of their vectors’ global distribution remains patchy and sparse.  A global geographic database of known occurrences of Ae. aegypti and Ae. albopictus between 1960 and 2014 was compiled. The database which comprises occurrence data linked to point or polygon locations was derived from peer-reviewed literature and unpublished studies including national entomological surveys and expert networks. The authors describe all data collection processes as well as geo-positioning methods database management and quality-control procedures in their 2015 paper cited below.  This is the first comprehensive global database of Ae. aegypti and Ae. albopictus occurrence consisting of 19930 and 22137 geo-positioned occurrence records respectively. The dataset can be used for a variety of mapping and spatial analyses of the vectors and by inference the diseases they transmit. Citations Kraemer MUG Sinka ME Duda KA Mylne A Shearer FM Brady OJ Messina JP Barker CM Moore CG Carvalho RG Coelho GE Van Bortel W Hendrickx G Schaffner F Wint GRW Elyazar IRF Teng H Hay SI (2015) The global compendium of Aedes aegypti and Ae. albopictus occurrence. Scientific Data 2(7) 150035. http//dx.doi.org/10.1038/sdata.2015.35 Kraemer MUG Sinka ME Duda KA Mylne A Shearer FM Brady OJ Messina JP Barker CM Moore CG Carvalho RG Coelho GE Van Bortel W Hendrickx G Schaffner F Wint GRW Elyazar IRF Teng H Hay SI (2015) Data from The global compendium of Aedes aegypti and Ae. albopictus occurrence. Dryad Digital Repository. http//dx.doi.org/10.5061/dryad.47v3c,CSV,,"[diseases, epidemiology, animals]",CC0,,,402,3484,3,Ocurrences of the mosquitos that transmit Zika,Ae. aegypti and Ae. albopictus occurrences,https://www.kaggle.com/dryad/ae-aegypti-and-ae-albopictus-occurrences,Sat Oct 15 2016
21,,NLTK Data,[],[],"Context The Universal Tagset (Petrov et al. 2012) was created to  facilitate future research in unsupervised induction of syntactic structure and to standardize best-practices. An up-to-date documentation can be found on http//universaldependencies.org/u/pos/  Content The files in this repository contain mappings from treebank specific tagsets to a set of 12 universal part-of-speech tags. The 12 universal tags are  VERB - verbs (all tenses and modes) NOUN - nouns (common and proper) PRON - pronouns  ADJ - adjectives ADV - adverbs ADP - adpositions (prepositions and postpositions) CONJ - conjunctions DET - determiners NUM - cardinal numbers PRT - particles or other function words X - other foreign words typos abbreviations . - punctuation  See ""A Universal Part-of-Speech Tagset"" by Slav Petrov Dipanjan Das and Ryan McDonald for more details http//arxiv.org/abs/1104.2086  The zipfile contains the <lang>-<tagset>.map files that maps the respective <tagset> POS tagsets in <lang> to the Universal Tagset e.g. the en-ptb.map contains the mapping from the English Penn Tree Bank tagset to the universal tagset.  The list of mappings includes  ar-padt.map bg-btb.map ca-cat3lb.map cs-pdt.map da-ddt.map de-negra.map de-tiger.map el-gdt.map en-brown.map en-ptb.map en-tweet.map es-cast3lb.map es-eagles.map es-iula.map es-treetagger.map eu-eus3lb.map fi-tdt.map fr-paris.map hu-szeged.map it-isst.map iw-mila.map ja-kyoto.map ja-verbmobil.map ko-sejong.map nl-alpino.map pl-ipipan.map pt-bosque.map ru-rnc.map sl-sdt.map sv-talbanken.map tu-metusbanci.map zh-ctb6.map zh-sinica.map  Additionally it contains  README A README file universal_tags.py A script use to convert tags to the universal tagset using the mappings by Nathan Schneider en-tweet.README A description of the tweeter tag mappings from (Noah et al. 2011)  Citations Slav Petrov  Dipanjan Das and Ryan McDonald.  A universal part-of-speech tagset.  In LREC 2012 ",Other,,[],Other,,,16,351,0.03515625,Mappings to the Universal Part-of-Speech Tagset,Universal Tagset,https://www.kaggle.com/nltkdata/universal-tagset,Sat Aug 19 2017
22,,SovBoc2018,"[Country Name, Country Code, 2010, 2011, 2012, 2013, 2014]","[string, string, numeric, numeric, numeric, numeric, numeric]","Context  A data driven look into answering the common question while travelling overseas  ""how easy is it to get a job in your country?"" Content This dataset contains youth unemployment rates (% of total labor force ages 15-24) (modeled ILO estimate) Latest data available from 2010 to 2014. Acknowledgements International Labour Organization. http//data.worldbank.org/indicator/SL.UEM.TOTL.ZS Released under Open license.",CSV,,"[employment, finance]",Other,,,2446,16634,0.0166015625,Youth Unemployment rates by country from 2010-2014,World Bank Youth Unemployment Rates,https://www.kaggle.com/sovannt/world-bank-youth-unemployment,Sat Nov 05 2016
23,,LiamLarsen,"[time, gender, sexuallity, age, income, race, bodyweight, virgin, prostitution_legal, pay_for_sex, friends, social_fear, depressed, what_help_from_others, attempt_suicide, employment, job_title, edu_level, improve_yourself_how]","[dateTime, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string]",Why? Last year a redditor created a survey to collect demographic data on the subreddit /r/ForeverAlone. Since then they have deleted their account but they left behind their data set. Columns are below Content  Timestamp  DateTime  What is your Gender?  String  What is your sexual orientation?  String  How old are you?  DateTime  What is your level of income?  DateTime  What is your race?  String  How would you describe your body/weight?  String  Are you a virgin?  String  Is prostitution legal where you live?  String  Would you pay for sex?  String  How many friends do you have IRL?  DateTime  Do you have social anxiety/phobia?  String  Are you depressed?  String  What kind of help do you want from others? (Choose all that apply)  String  Have you attempted suicide?  String  Employment Status Are you currently…?  String  What is your job title?  String  What is your level of education?  String  What have you done to try and improve yourself? (Check all that apply) ,CSV,,"[linguistics, sociology, internet]",Other,,,444,4022,0.10546875,A survey taken by redditers in /r/ForeverAlone. You would be surprised.,The Demographic /r/ForeverAlone Dataset,https://www.kaggle.com/kingburrito666/the-demographic-rforeveralone-dataset,Sat Apr 29 2017
24,,figshare,"[USPS, GEOID, POP10, HU10, ALAND, AWATER, ALAND_SQMI, AWATER_SQMI, INTPTLAT, INTPTLONG                                                                                                                  ]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context The emergence in the United States of large-scale “megaregions” centered on major metropolitan areas is a phenomenon often taken for granted in both scholarly studies and popular accounts of contemporary economic geography. This dataset comes from a paper (Nelson & Rae 2016. An Economic Geography of the United States From Commutes to Megaregions) that uses a data set of more than 4000000 commuter flows as the basis for an empirical approach to the identification of such megaregions.  Content This dataset consists of two files one contains the commuting data and one is a gazetteer describing the population and locations of the census tracts referred to by the commuting data. The fields Ofips and Dfips (FIPS codes for the originating and destination census tracts respectively) in commute_data.csv refer to the GEOID field in census_tracts_2010.csv. commute_data This file contains information on over 4 million commute flows. It has the following fields  Ofips the full FIPS code for the origin census tract of an individual flow line *Dfips * the full FIPS code for the destination census tract of an individual flow line Ostfips the FIPS code for the origin state of an individual flow line Octfips the FIPS code for the origin county of an individual flow line Otrfips the FIPS code for the destination census tract of an individual flow line Dstfips the FIPS code for the destination state of an individual flow line Dctfips the FIPS code for the destination county of an individual flow line Dtrfips the FIPS code for the destination census tract of an individual flow line Flow the total number of commuters associated with this individual point to point flow line (i.e. the total number of journeys to work) Moe margin of error of the Flow value above LenKM length of each flow line in Kilometers ESTDIVMOE the Flow value divided by the Margin of Error of the estimate  census_tracts_2010 This file contains the following fields which represent information about different U.S. Census Tracts  USPS United States Postal Service State Abbreviation GEOID Geographic Identifier - fully concatenated geographic code (State FIPS and County FIPS) ANSICODE American National Standards Institute code NAME Name POP10 2010 Census population count. HU10 2010 Census housing unit count. ALAND Land Area (square meters) - Created for statistical purposes only. AWATER Water Area (square meters) - Created for statistical purposes only. ALAND_SQMI Land Area (square miles) - Created for statistical purposes only. AWATER_SQMI Water Area (square miles) - Created for statistical purposes only. INTPTLAT Latitude (decimal degrees) First character is blank or ""-"" denoting North or South latitude respectively. INTPTLONG Longitude (decimal degrees) First character is blank or ""-"" denoting East or West longitude respectively.  Acknowledgements This dataset comes from the following article Nelson & Rae 2016. An Economic Geography of the United States From Commutes to Megaregions The full dataset (in GIS shapefile format) can be found on figshare here",CSV,,[transport],CC0,,,271,3752,278,Visualizing over 4 million home-to-work commutes in the United States,United States Commutes,https://www.kaggle.com/figshare/united-states-commutes,Sun Dec 04 2016
25,,UCI Machine Learning,[],[],"Context  The data were collected as the SCITOS G5 navigated through the room following the wall in a clockwise  direction for 4  rounds. To navigate the robot uses 24 ultrasound sensors arranged circularly around its ""waist"".  The numbering of the ultrasound sensors starts at the front of the robot and increases in clockwise direction. The provided files comprise three diferent data sets.  The first one contains the raw values of the measurements  of all 24 ultrasound sensors and the corresponding class label (Moving forward turning left etc). Sensor readings are sampled at a rate of 9 samples per second. The second one contains four sensor readings named 'simplified distances' and the corresponding class label l (Moving forward turning left etc). These simplified distances are referred to as the 'front distance' 'left distance' 'right distance' and 'back distance'.  They consist respectively of the minimum sensor readings among those within 60 degree arcs located at the front left  right and back parts of the robot. The third one contains only the front and left simplified distances and the corresponding class labell (Moving forward turning left etc).  It is worth mentioning that the 24 ultrasound readings and the simplified distances were collected at the same  time step so each file has the same number of rows (one for each sampling time step).                                                            The wall-following task and data gathering were designed to test the hypothesis that this apparently simple navigation task is indeed a non-linearly separable classification task. Thus linear classifiers such as the Perceptron network are not able to learn the task and command the robot around the room without collisions. Nonlinear neural classifiers such as the MLP network are able to learn the task and command the robot successfully without collisions.  If some kind of short-term memory mechanism is provided to the neural classifiers their performances are improved in general.  For example if past inputs are provided together with current sensor readings even the Perceptron becomes able to learn the task and command the robot successfully. If a recurrent neural network such as the Elman network is used to learn the task the resulting dynamical classifier is able to learn the task using less hidden neurons than the MLP network. Files with different number of sensor readings were built in order to evaluate the performance of the classifiers with respect to the number of inputs.  Content File sensor_readings_24.csv  US1 ultrasound sensor at the front of the robot (reference angle 180°) - (numeric real) US2 ultrasound reading (reference angle -165°) - (numeric real) US3 ultrasound reading (reference angle -150°) - (numeric real) US4 ultrasound reading (reference angle -135°) - (numeric real) US5 ultrasound reading (reference angle -120°) - (numeric real) US6 ultrasound reading (reference angle -105°) - (numeric real) US7 ultrasound reading (reference angle -90°) - (numeric real) US8 ultrasound reading (reference angle -75°) - (numeric real) US9 ultrasound reading (reference angle -60°) - (numeric real) US10 ultrasound reading (reference angle -45°) - (numeric real) US11 ultrasound reading (reference angle -30°) - (numeric real) US12 ultrasound reading (reference angle -15°) - (numeric real) US13 reading of ultrasound sensor situated at the back of the robot (reference angle 0°) - (numeric real) US14 ultrasound reading (reference angle 15°) - (numeric real) US15 ultrasound reading (reference angle 30°) - (numeric real) US16 ultrasound reading (reference angle 45°) - (numeric real) US17 ultrasound reading (reference angle 60°) - (numeric real) US18 ultrasound reading (reference angle 75°) - (numeric real) US19 ultrasound reading (reference angle 90°) - (numeric real) US20 ultrasound reading (reference angle 105°) - (numeric real) US21 ultrasound reading (reference angle 120°) - (numeric real) US22 ultrasound reading (reference angle 135°) - (numeric real) US23 ultrasound reading (reference angle 150°) - (numeric real) US24 ultrasound reading (reference angle 165°) - (numeric real) Classes  Move-Forward Slight-Right-Turn Sharp-Right-Turn Slight-Left-Turn  File sensor_readings_4.csv  SD_front minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric real) SD_left  minimum sensor reading within a 60 degree arc located at the left of the robot  - (numeric real) SD_right minimum sensor reading within a 60 degree arc located at the right of the robot - (numeric real) SD_back  minimum sensor reading within a 60 degree arc located at the back of the robot - (numeric real) Classes  Move-Forward Slight-Right-Turn Sharp-Right-Turn Slight-Left-Turn  File sensor_readings_2.csv  SD_front minimum sensor reading within a 60 degree arc located at the front of the robot - (numeric real) SD_left  minimum sensor reading within a 60 degree arc located at the left of the robot - (numeric real) Classes  Move-Forward Slight-Right-Turn Sharp-Right-Turn Slight-Left-Turn  Acknowledgements These datasets were downlaoded from the UCI Machine Learning Repository Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. Inspiration Use these ultrasound readings to predict the class i.e. given these readings is the robot moving straight? turning left?",CSV,,[robotics],Other,,,157,1866,1,Data collected from a robot while navigating around a room,Sensor readings from a wall-following robot,https://www.kaggle.com/uciml/wall-following-robot,Wed Sep 06 2017
26,,Ludovic Benistant,[],[],,CSV,,[employment],CC4,,,0,0,0.5400390625,Why are our best and most experienced employees leaving prematurely?,Human Resources Analytics,https://www.kaggle.com/ludobenistant/hr-analytics,Tue Nov 29 2016
27,,Ed King,"[dataset, fname, label, sublabel]","[string, string, string, string]","Try your hand at automatically separating normal heartbeats from abnormal heartbeats and heart murmur with this machine learning challenge by Peter Bentley et al The Data Here's a brief overview of the format of this dataset as uploaded to Kaggle. For a more detailed description look at the Description section below. The dataset is split into two sources A and B A was collected from the general public via an iPhone app and B was collected from a clinical trial in hospitals using a digital stethoscope. The goal of the task is to first (1) identify the locations of heart sounds from the audio and (2) to classify the heart sounds into one of several categories (normal v. various non-normal heartbeat sounds). The CSV files provided are set_a.csv  set_b.csv set_a_timing.csv The fields for set_a and set_b are as follows  dataset a or b fname the audio file label either ""normal"" blank (for unlabelled data) or one of various categories of abnormal heartbeats sublabel in set_b some recordings are categorized as noisy meaning they contain non-heart background noise; this field holds information on whether something is e.g. ""noisynormal"" or ""noisymurmur""  The file set_a_timing.csv contains gold-standard timing information for the ""normal"" recordings from Set A. This file contains the following fields  fname the audio file cycle anywhere from 1 to 19; the heartbeat cycle that the time observation refers to sound either S1 or S2; see below for what these mean location the time location of this sound in audio samples  Description The task as described by the original authors Task Overview Data has been gathered from two sources (A) from the general public via the iStethoscope Pro iPhone app provided in Dataset A and (B) from a clinic trial in hospitals using the digital stethoscope DigiScope provided in Dataset B. CHALLENGE 1 - Heart Sound Segmentation The first challenge is to produce a method that can locate S1(lub) and S2(dub) sounds within audio data segmenting the Normal audio files in both datasets. To enable your machine learning method to learn we provide the exact location of S1 and S2 sounds for some of the audio files. You need to use them to identify and locate the S1 and S2 sounds of all the heartbeats in the unlabelled group. The locations of sounds are measured in audio samples for better precision. Your method must use the same unit. CHALLENGE 2 - Heart Sound Classification The task is to produce a method that can classify real heart audio (also known as “beat classification”) into one of four categories for Dataset A  Normal Murmur Extra Heart Sound Artifact  and three classes for Dataset B  Normal Murmur Extrasystole  You may tackle either or both of these challenges. If you can solve the first challenge the second will be considerably easier! The winner of each challenge will be the method best able to segment and/or classify two sets of unlabelled data into the correct categories after training on both datasets provided below.  [Obviously no longer applicable -- ed.] The creator of the winning method will receive a WiFi 32Gb iPad as the prize awarded at a workshop at AISTATS 2012. The audio files are of varying lengths between 1 second and 30 seconds (some have been clipped to reduce excessive noise and provide the salient fragment of the sound). Most information in heart sounds is contained in the low frequency components with noise in the higher frequencies. It is common to apply a low-pass filter at 195 Hz. Fast Fourier transforms are also likely to provide useful information about volume and frequency over time. More domain-specific knowledge about the difference between the categories of sounds is provided below. Normal Category In the Normal category there are normal healthy heart sounds. These may contain noise in the final second of the recording as the device is removed from the body. They may contain a variety of background noises (from traffic to radios). They may also contain occasional random noise corresponding to breathing or brushing the microphone against clothing or skin. A normal heart sound has a clear “lub dub lub dub” pattern with the time from “lub” to “dub” shorter than the time from “dub” to the next “lub” (when the heart rate is less than 140 beats per minute). Note the temporal description of “lub” and “dub” locations over time in the following illustration …lub……….dub……………. lub……….dub……………. lub……….dub……………. lub……….dub… In medicine we call the lub sound ""S1"" and the dub sound ""S2"". Most normal heart rates at rest will be between about 60 and 100 beats (‘lub dub’s) per minute. However note that since the data may have been collected from children or adults in calm or excited states the heart rates in the data may vary from 40 to 140 beats or higher per minute. Dataset B also contains noisy_normal data - normal data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it however the test set will include some equally noisy examples. Murmur Category Heart murmurs sound as though there is a “whooshing roaring rumbling or turbulent fluid” noise in one of two temporal locations (1) between “lub” and “dub” or (2) between “dub” and “lub”. They can be a symptom of many heart disorders some serious. There will still be a “lub” and a “dub”. One of the things that confuses non-medically trained people is that murmurs happen between lub and dub or between dub and lub; not on lub and not on dub. Below you can find an asterisk* at the locations a murmur may be. …lub..*...dub……………. lub..*..dub ……………. lub..*..dub ……………. lub..*..dub … or …lub……….dub…*….lub………. dub…*….lub ………. dub…**….lub ……….dub… Dataset B also contains noisy_murmur data - murmur data which includes a substantial amount of background noise or distortion. You may choose to use this or ignore it however the test set will include some equally noisy examples Extra Heart Sound Category (Dataset A) Extra heart sounds can be identified because there is an additional sound e.g. a “lub-lub dub” or a “lub dub-dub”. An extra heart sound may not be a sign of disease.  However in some situations it is an important sign of disease which if detected early could help a person.  The extra heart sound is important to be able to detect as it cannot be detected by ultrasound very well. Below note the temporal description of the extra heart sounds …lub.lub……….dub………..………. lub. lub……….dub…………….lub.lub……..…….dub…….  or …lub………. dub.dub………………….lub.……….dub.dub………………….lub……..…….dub. dub…… Artifact Category (Dataset A) In the Artifact category there are a wide range of different sounds including feedback squeals and echoes speech music and noise. There are usually no discernable heart sounds and thus little or no temporal periodicity at frequencies below 195 Hz. This category is the most different from the others. It is important to be able to distinguish this category from the other three categories so that someone gathering the data can be instructed to try again. Extrasystole Category (Dataset B) Extrasystole sounds may appear occasionally and can be identified because there is a heart sound that is out of rhythm involving extra or skipped heartbeats e.g. a “lub-lub dub” or a “lub dub-dub”. (This is not the same as an extra heart sound as the event is not regularly occuring.) An extrasystole may not be a sign of disease. It can happen normally in an adult and can be very common in children. However in some situations extrasystoles can be caused by heart diseases. If these diseases are detected earlier then treatment is likely to be more effective. Below note the temporal description of the extra heart sounds …........lub……….dub………..………. lub. ………..……….dub…………….lub.lub……..…….dub…….  or …lub………. dub......………………….lub.…………………dub.dub………………….lub……..…….dub.…… Acknowledgments Please use the following citation if the data is used @misc{pascal-chsc-2011        author = ""Bentley P. and Nordehn G. and Coimbra M. and Mannor S.""        title = ""The {PASCAL} {C}lassifying {H}eart {S}ounds {C}hallenge 2011 {(CHSC2011)} {R}esults""        howpublished = ""http//www.peterjbentley.com/heartchallenge/index.html""}",CSV,,"[healthcare, human genetics, sound technology]",CC0,,,3380,36674,152,Classifying heartbeat anomalies from stethoscope audio,Heartbeat Sounds,https://www.kaggle.com/kinguistics/heartbeat-sounds,Sun Nov 27 2016
28,,Filippo,"[RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This is the dataset used in the section ""ANN (Artificial Neural Networks)"" of the Udemy course from Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist) called Deep Learning A-Z™ Hands-On Artificial Neural Networks. The dataset is very useful for beginners of Machine Learning and a simple playground where to compare several techniques/skills. It can be freely downloaded here https//www.superdatascience.com/deep-learning/  The story A bank is investigating a very high rate of customer leaving the bank. Here is a 10.000 records dataset to investigate and predict which of the customers are more likely to leave the bank soon. The story of the story I'd like to compare several techniques (better if not alone and with the experience of several Kaggle users) to improve my basic knowledge on Machine Learning. Content I will write more later but the columns names are very self-explaining. Acknowledgements Udemy instructors Kirill Eremenko (Data Scientist & Forex Systems Expert) and Hadelin de Ponteves (Data Scientist) and their efforts to provide this dataset to their students. Inspiration Which methods score best with this dataset? Which are fastest (or executable in a decent time)? Which are the basic steps with such a simple dataset very useful to beginners?",CSV,,[artificial intelligence],Other,,,558,7679,0.6533203125,"Kirill Eremenko ""Deep Learning A-Z™: Hands-On Artificial Neural Networks"" course",Deep Learning A-Z - ANN dataset,https://www.kaggle.com/filippoo/deep-learning-az-ann,Tue May 16 2017
29,,PromptCloud,"[Uniq Id, Url, Name, Street, Zip Code, City, State, Phone, Email, Website, Categories]","[string, string, string, string, numeric, string, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 85000 restaurants) that was created by extracting data from yellowpages.com.  Content This dataset has following fields  Url Name Street Zip Code City State Phone Email Website Categories - A comma-delimited () list of categories the listing in question falls under. Most listings are placed in multiple categories.  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of city and categories can be performed.,CSV,,"[hotels, internet]",CC4,,,102,1251,2,"6,000 Restaurants on Yellowpages.com",Restaurants on Yellowpages.com,https://www.kaggle.com/PromptCloudHQ/restaurants-on-yellowpagescom,Sat Sep 16 2017
30,,Jerad Rose,"[card_id, playerClass, type, name, set, text, cost, attack, health, rarity, collectible, flavor, race, how_to_earn, how_to_earn_golden, targeting_arrow_text, faction, durability]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",This dataset contains data for the entire collection of cards for Hearthstone the popular online card game by Blizzard. Launching to the public on March 11 2011 after being under development for almost 5 years Hearthstone has gained popularity as a freemium game launching into eSports across the globe and the source of many Twitch channels. The data in this dataset was extracted from hearthstonejson.com and the documentation for all the data can be found on the cards.json documentation page. The original data was extracted from the actual card data files used in the game so all of the data should be here enabling explorations like  Card strengths and weaknesses Card strengths relative to cost and rarity Comparisons across player classes bosses and sets Whether a set of optimal cards can be determined per class  The cards can be explored in one of four ways  cards.json The raw JSON pulled from hearthstonejson.com cards_flat.csv A flat CSV containing a row for each card and any nm data stored as arrays in single fields database.sqlite A SQLite database containing relational data of the cards cards.csv mechanics.csv dust_costs.csv play_requirements.csv and entourages.csv the normalized data in CSV format.  This dataset will be updated as new releases and expansions are made to Hearthstone. Currently any localized string values are in en-us but I may look into adding other languages if the demand seems to be there.,CSV,,"[games and toys, video games]",CC0,,,1245,14986,2,Explore the entire collection of Hearthstone cards,Hearthstone Cards,https://www.kaggle.com/jeradrose/hearthstone-cards,Wed Jan 04 2017
31,,Rachael Tatman,"[SDFB Group ID, Name, Description, Start Year Type, Start Year, End Year Type, End Year, Members List (Name with SDFB Person ID)]","[numeric, string, string, string, numeric, string, numeric, string]",Overview Six Degrees of Francis Bacon is a digital reconstruction of the early modern social network (EMSN). Historians and literary critics have long studied the way that early modern people associated with each other and participated in various kinds of formal and informal groups. By data-mining existing scholarship that describes relationships between early modern persons documents and institutions we have created a unified systematized representation of the way people in early modern England were connected. Contents This dataset contains information on 171419 relationships between 15824 early modern figures (including of course the titular Francis Bacon). The individuals have been separated into 109 distinct labelled groups and the relationships fall under one of 64 labelled categories. This dataset contains the following files  SDFB_groups.csv a list of the groups of individuals in the dataset SDFB_people.csv a list of all individuals in the dataset SDFB_relationships_100000000_100020000.csv This and the following “relationships” files contain information on relationships between specific individuals SDFB_relationships_100020001_100040000.csv SDFB_relationships_100040001_100060000.csv SDFB_relationships_100060001_100080000.csv SDFB_relationships_100080001_100100000.csv SDFB_relationships_100100001_100120000.csv SDFB_relationships_100120001_100140000.csv SDFB_relationships_100140001_100160000.csv SDFB_relationships_100160001_100180000.csv SDFB_relationships_greater_than_100180000.csv SDFB_RelationshipTypes.csv the types of relationships found in the database table-of-contents.csv a table of contents for the files  Acknowledgements Please cite Six Degrees of Francis Bacon as follows SDFB Team Six Degrees of Francis Bacon Reassembling the Early Modern Social Network. www.sixdegreesoffrancisbacon.com (August 29 2017). Inspiration This dataset is an excellent place to explore network analysis and visualization. Each individual is a node and each relationship an edge.   Can you visualize this social network?  Who is the most central figure in this social network?  Do different groups have different degrees of connectivity? Plexity? ,CSV,,"[europe, north america, history]",CC4,,,358,1467,12,An early modern social network,Six Degrees of Francis Bacon,https://www.kaggle.com/rtatman/six-degrees-of-francis-bacon,Wed Aug 30 2017
32,,Crowdflower,"[, X_unit_id, X_created_at, X_id, X_started_at, X_tainted, X_channel, X_trust, X_worker_id, X_country, X_region, X_city, X_ip, appeal_to_reader, conjunctions, connectivity, narrative_perspective, sensory_language, setting, ab, appeal_to_reader_gold, conjunctions_gold, connectivity_gold, narrative_perspective_gold, pmid, py, sensory_language_gold, setting_gold, so, tc, af, au, bp, di, ep, is, pd, pt, sn, ti, ut, vl, z9, cin_mas, firstauthor, numberauthors, pid_mas, title]","[numeric, numeric, dateTime, numeric, dateTime, boolean, string, numeric, numeric, string, numeric, string, string, string, numeric, numeric, string, numeric, string, string, string, string, string, string, numeric, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, string]","CrowdFlower was used in a recent study published in PLOS One on how narrative style affects the way scientific findings are cited and shared. Refer to the article’s supplementary materials for more information. The dataset contains abstracts from peer-reviewed studies on climate change that were labeled using the CrowdFlower platform. Abstracts were each assessed by multiple raters (n = 7) for their narrativity. Narrativity includes whether the abstract appeals to the reader has a narrative perspective using sensory language and other factors. The dataset also contains additional information about the studies including citation rate journal identity and number of authors. Past research From the study abstract  Peer-reviewed publications focusing on climate change are growing exponentially with the consequence that the uptake and influence of individual papers varies greatly. Here we derive metrics of narrativity from psychology and literary theory and use these metrics to test the hypothesis that more narrative climate change writing is more likely to be influential using citation frequency as a proxy for influence. From a sample of 732 scientific abstracts drawn from the climate change literature we find that articles with more narrative abstracts are cited more often. This effect is closely associated with journal identity higher-impact journals tend to feature more narrative articles and these articles tend to be cited more often. These results suggest that writing in a more narrative style increases the uptake and influence of articles in climate literature and perhaps in scientific literature more broadly.  Inspiration Can you replicate the authors' findings? Are abstracts with narrative qualities cited more often? What other variables are associated with narrativity in scientific abstracts about climate change? Examine relationships between citation rate abstract length abstract authors and more. Acknowledgements This dataset is made available via CrowdFlower's ""Data for Everyone"" collection which hosts open data jobs that have come through the crowdsourced labeling platform.",CSV,,"[research, science and culture]",CC4,,,82,1224,11,Is scientific research written in a narrative style more influential?,Narrativity in Scientific Publishing,https://www.kaggle.com/crowdflower/narrativity-in-scientific-publishing,Fri Apr 07 2017
33,,ArcGIS Open Data,[],[],Context The Vision Zero data contained in this layer pertain to parking violations issued by the District of Columbia's Metropolitan Police Department (MPD) and partner agencies with the authority. Parking violation locations are summarized ticket counts based on time of day week of year year and category of violation. Data was originally downloaded from the District Department of Motor Vehicle's eTIMS meter work order management system.  Content This dataset contains 132850 rows of  OBJECTID (unique ID)   ROWID_  DAY_OF_WEEK (text)     HOLIDAY (number)   WEEK_OF_YEAR (number)  MONTH_OF_YEAR (number)     ISSUE_TIME (number)    VIOLATION_CODE (text)  VIOLATION_DESCRIPTION (text)       LOCATION (text)        RP_PLATE_STATE (text)  BODY_STYLE (text)  ADDRESS_ID (number)    STREETSEGID (number)       XCOORD (number)    YCOORD (number) TICKET_ISSUE_DATE (date or time)   X Y  Acknowledgements The dataset is shared by DCGISopendata and the original dataset and metadata can be found here. Inspiration Can you use the dataset to determine  Which area has the highest concentration of parking violations? How about by type of violation? Do areas with high concentration of parking violations change throughout the month of December? Can you identify any trends? ,CSV,,"[automobiles, road transport]",Other,,,242,3029,25,Parking citation locations in the District of Columbia,"Parking Violations, December 2015",https://www.kaggle.com/arcgisopendata/dc-parking-violations,Wed Nov 30 2016
34,,Federal Election Commission,[],[],Context The Federal Election Commission (FEC) is an independent regulatory agency established in 1975 to administer and enforce the Federal Election Campaign Act (FECA) which requires public disclosure of campaign finance information. The FEC publishes campaign finance reports for presidential and legislative election campaign candidates on the Campaign Finance Disclosure Portal. Content The finance summary report contains one record for each financial report (Form 3P) filed by the presidential campaign committees during the 2016 primary and general election campaigns. Presidential committees file quarterly prior to the election year and monthly during the election year. The campaign expenditures file contains individual operating expenditures made by the campaign committee and reported on Form 3P Line 23 during the same period. Operating expenditures consist of the routine costs of campaigning for president which include staffing travel advertising voter outreach and other activities.,Other,,"[finance, politics]",CC0,,,462,3130,9,How did presidential candidates spend their campaign funds?,2016 Presidential Campaign Finance,https://www.kaggle.com/fec/presidential-campaign-finance,Wed Jan 18 2017
35,,NMIN,"[ID;nom;Year;Source;Types of armor;Nationality;Weight in Kg;Height of armor;Speed;Type of control;Cognitive assistance;Source of energy;Weapons;Means of production;Limitations / vulnerabilities;Capacities;Enhancement;Created to work in groupes;Protection f, nom, Year, Source, Types of armor, Nationality, Weight in Kg, Height of armor, Speed, Type of control, Cognitive assistance, Source of energy, Weapons, Means of production, Limitations / vulnerabilities, Capacities, Enhancement, Created to work in groupes, Protection from, Fields of operations, Type of informations produced by the armor, Life support]","[string, string, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context The file presents a listing of characters wearing powered armor / mini or giant meccha in movies comics animation etc.  The purpose was to analyse our imaginaries in a specific field (i.e armors in this case) in order to see what are the macro elements see how they evolve around time and if they are close to what is used in real life. Content Each armor is analyzed according to 13 characteristics (uses an AI or not what kind or power where is the weapon its capacities (does is fly gives enhanced strength etc.). Being a social science professor and not a data analysts I went on marvel wikia DC wikia etc. to compile it. Something like 80 heroes are fully presented and a list of almost 300 been found.  Inspiration Coming from social science I compiled that data during my free time but I understand that it is highly limiting and that there must be a way to aggregate much more data & faster. Also I am sure that it does not meet some of the standards for such work. Being a newbie here please tell me how to improve this & I will. The question after is to know if we can ""predict"" what future armors will look like  is there a trend showing that AI is used more and more ? That they all fly ? Once this done it would allow to ""delineate"" the ideal characteristics of a super hero and hence where we could innovate if we do not want to reproduce things that already done while imagining them ? The last questions correlate to social trends  do some characteristics appear during a certain period ? If yes is it correlated to some specific social context ? (new type of wars impacting how we imagine our heroes ?).",CSV,,[],Other,,,117,963,0.0419921875,"300 heroes listed, 80 fully detailed","Armors, Exoskeletons & Mecchas",https://www.kaggle.com/nicolasmin/armors-exoskeletons-mecchas,Fri Sep 08 2017
36,,PromptCloud,"[address, categories, description, image_count, name, payment_option, phone, profile_id, profile_url, uniq_id, video_count, website]","[string, string, string, string, string, string, string, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 2.4 million profiles) that was created by extracting data from Avvo.com. Content This dataset has following fields  address categories description image_count name payment_option phone profile_id profile_url video_count website  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analysis can be performed on the profile category.,CSV,,"[healthcare, law, internet]",CC4,,,51,756,6,"20,000 doctor and lawyer profiles",Doctor and lawyer profiles on Avvo.com,https://www.kaggle.com/PromptCloudHQ/doctor-and-lawyer-profiles-on-avvocom,Sat Sep 16 2017
37,,Christopher Clayford,[],[],"Context A dataset of information on deaths related to the Northern Ireland conflict (1969-2005). Content  Fatalities A list of deaths related to the Northern Ireland conflict (Fact table) Location A list of locations where the fatalities took place [including GPS coordinates] (Links to fact table via the ""Location"" field) Agency This refers to groups responsible for fatal incidents [including agency groups] (Links to fact table via the ""Agency"" field) Status This refers to the role of fatality victims [including status groups] (Links to fact table via the ""Status"" field)  Dimension Remarks  Name Spellings might vary from other sources Year Relates to year of death rather than fatal incident Religion Only that of Northern Ireland residents is listed Agency This refers to groups responsible for fatal incidents Status This refers to the role of fatality victims Location Westminster electoral areas are employed for Northern Ireland fatalities Rationale This refers to the inferred purpose underlying the fatal act Causality This probes the degree of inferred purposiveness of a fatal event Context This distinguishes between incidents such as gun fire explosions beatings New Incident This offers a count of discrete fatal incidents 1st Fatality This distinguishes multiple fatality incidents   Acknowledgements The information was compiled by Michael McKeown and was contributed by him to the CAIN Web site. Michael McKeown has taken the decision (June 2009) to make the dataset freely available via the CAIN site. While users are free to download the dataset for research purposes the database remains copyright © of Michael McKeown. Inspiration ""The following study represents both the revisiting and continuation of a task which had occupied me for over twenty years. The concluded work highlights complexities and ambiguities in the patterns of the violence in Northern Ireland over the past three decades which are often obscured by the polar interpretations offered by partizan commentaries. For that reason I believe it should be inserted into the public arena for further consideration and possibly as a methodological model for further enquiry."" Michael McKeown (2001).",Other,,"[crime, politics]",CC0,,,60,711,0.4560546875,Patterns of politically associated violence in Northern Ireland,Deaths related to the Northern Ireland conflict,https://www.kaggle.com/cclayford/deaths-related-to-the-northern-ireland-conflict,Wed Oct 04 2017
38,,devin,"[ability_id, ability_name]","[numeric, string]","Overview This dataset contains 50000 ranked ladder matches from the Dota 2 data dump created by Opendota. It was inspired by  the Dota 2 Matches data published here by Joe Ramir. This is an update and improved version of that dataset. I have kept the same image and a similar title. Dota 2 is a popular MOBA available as free to play and can take up thousands of hours of your life.  The number of games in this dataset are played about every hour. If you like the data there are an additional 2-3 million matches easily available for download.  The aim of this dataset is to enable the exploration of player behavior skill estimation or anything you find interesting. The intent is to create an accessible and easy to use resource which can be expanded and modified if needed.  As such I am open to a wide variety of suggestions as to what additions or changes to make.  Help getting started If there is some aspect of this data you would like to explore but seems difficult to get figure out how to work with please feel free to request some starter code in one of the following two Kernels discussion section. I usually check kaggle every day or so. If you post a request about the current data I will try to get something working.  Python https//www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/misc-howtos-dota-requests-welcome/ R https//www.kaggle.com/devinanzelmo/d/devinanzelmo/dota-2-matches/howtos-request-welcome/ Whats Currently Available See https//github.com/odota/core/wiki/JSON-Data-Dump for documentaion on data. I have found a few undocumented areas in the data including the objectives information. player_slot can be used to combine most of the data and it is available in most of the tables. Additionally all tables include match_id and some have account_id to make it easier to look at an individual players matches. match_id and account_id have been reencoded to save a little space. I can upload tables to allow conversion if needed.   matches contains top level information about each match.  see https//wiki.teamfortress.com/wiki/WebAPI/GetMatchDetails#Tower_Status%22tower_status_dire%22%202047) for interpreting tower and barracks status. Cluster can link matches to geographic region.  players  Individual players are identified by account_id but there is an option to play anonymously and roughly one third of the account_id are not available. Anonymous users have the value of 0 for account_id. Contains totals for kills deaths denies etc.  Player action counts are available and are indicated by variable names beginning with unit_order_.  Counts for reasons for acquiring or losing gold and gaining experience have prefixes gold_ and xp_.  player_time Contains last hits experience and gold sampled at one minute interval for all players in all matches. The column names indicate the player_slot.  For instance xp_t_1 indicates that this column has experience sums for the player in slot one. teamfights  Start and stop time of teamfights as well as last death time.  Teamfights appear to be all battles  with three or more deaths. As such this does not include all battles for the entire match. teamfights_players   Additional information provided for each player in each teamfight. player_slot can be  used to link this back to players.csv objectives  Gives information on all the objectives completed by which player and at what time. chat  All chat for the 50k matches. There is plenty of profanity and good natured trolling.  test_labels match_id and radiant_win(as integer 1 or 0)  test_player full player and match table with hero_id player_slot match_id and account_id   Nov 5th Update Added several additional tables. None of the previously uploaded data was altered. I plan to add several Kernels in the next week going over how to use the data and performing some EDA. Many improvements to the player rating method I used are possible for those interested in MMR.   player_ratings contains match counts  win counts and TrueSkill rating calculated on 900k matches which occurred prior to other uploaded data. trueskill ratings have two components mu which can be interpreted as the skill with higher value being better and sigma which is the uncertainty of the rating.  match_outcomes data for ~900k matches used to calculate player ratings. Use this to improve on the ratings I uploaded. purchase_log item purchase times ability_upgrade ability upgrade times and levels cluster_region allows the mapping cluster found in match.csv to geographic region. patch_dates release dates for various patches use start_time from match.csv to determine which patch a match was played in.  ability_ids use with ability_upgrades.csv to get the names of upgraded abilities item_ids use with purchase_log.csv to get the names of purchased items  Kernel showing how player skill was computed Contains several resources on trueskill rating system. Past Research There seem to be some efforts to establish indicators for skillfull play based on specific parts of gameplay. Opendota has many statistics and some analysis for specific benchmarks at different times in the game. Dotabuff has a lot of information I have not explored it deeply. This is an area to gather more information.   Some possible directions of investigation Insight from domain experts would also be useful to help clarify what problems are interesting to work on. Some initial task ideas  Predict match outcomes based on aggregates for individual players using only account_id as prior information Add hero id to this and see if there is a differences in performance Estimate player skill based on a sample of in game play(this might need an external mmr source or different definition skill) Create improved indicators of skillful play based game actions to help players target areas for improvement  All of these areas have been worked on but I am not aware of the most up to date research on dota2 gameplay. I plan on setting up several different predictive tasks in the upcoming weeks. A test set of an additional 50 to 100 thousand matches with just hero_id and account_id included along with outcome of the match.  The current dataset seems pretty small for modeling individual players. I would prefer to have a wide range of features instead of a larger dataset for the moment.   Dataset idea for anyone interested in creating their own Dota 2 dataset. It would be useful to have a few full matches available to work on. They would need to be extracted from the .dem replay file to something easily parsed by R and Python as available in kernels. Given the size of a full match data only a few matches would be needed.  There are files available from opendota' s website(check for replays).  Looking at fine grained match details would potentially allow for the creation of better high level parsed data. I think it would be a lot of work just to get a handle on working with full match data so a sample would be good to have.  Acknowledgements Orginal kaggle dataset on dota2 matches by Joe Ramir I also borrowed the image and some of the content for these acknowledgements from the above thanks!. image source Data download source created by yasp Description of original dataset creation https//github.com/yasp-dota/yasp/issues/924 yasp's license ""License CC BY-SA 4.0"" ""Terms We ask that you attribute yasp.co if you create or publish anything related to our data. Also please seed for as long as possible."" Yasp is now known as opendota here are links to their website and github page https//www.opendota.com/ the data is used to for this site and its a easy way to get familier with it https//github.com/odota/core  check here for info especially this wiki page which gives details on the schema.",CSV,,[video games],CC4,,,3688,35560,1024,Explore player behavior and predict match outcomes.,Dota 2 Matches,https://www.kaggle.com/devinanzelmo/dota-2-matches,Sun Nov 06 2016
39,,Jacob Boysen,[],[],Context This database of terrorism prosecutions and sentencing information was created using public records including three lists of prosecutions from the U.S. Department of Justice (from 2010 2014 and 2015) court files available through the federal judiciary’s case management system DOJ press releases and inmate data from the Bureau of Prisons.  Content Trevor Aaronson created the first iteration of this database as part of a project funded by the Investigative Reporting Program at the University of California Berkeley. Mother Jones magazine published that data in 2011 along with accompanying articles in a package that is still available online. Beginning in 2016 Aaronson and Margot Williams collaborated to update and expand the database with a new emphasis to include Bureau of Prisons data because so many post-9/11 terrorism defendants had been released. The cases include any prosecutions after September 11 2001 that the U.S. government labeled as related to international terrorism. The Intercept first published this database on April 20 2017. For each defendant in the database U.S. criminal code data related to charges has been categorized according to this legend Acknowledgements This database is licensed under Creative Commons for noncommercial uses with appropriate attribution. If you publish this database in part or whole you must credit Trevor Aaronson and Margot Williams. Inspiration  What are the most common charges? Are the sentence lengths similar? ,CSV,,[],CC4,,,65,932,0.4755859375,Database of US Terrorism Prosecutions and Sentencing Information,Trial and Terror,https://www.kaggle.com/jboysen/trial-and-terror,Wed Aug 16 2017
40,,cclark,"[id, description]","[numeric, string]",500 actual SKUs from an outdoor apparel brand's product catalog. It's somewhat rare to get real item level data in a real-world format. Very useful for testing things like recommendation engines. In fact...maybe I'll publish some code along with this ),CSV,,[business],Other,,,4583,38375,0.5400390625,500 SKUs and their descriptions. Great for content engine training!,eCommerce Item Data,https://www.kaggle.com/cclark/product-item-data,Thu Aug 18 2016
41,,Rachael Tatman,[],[],"Context Languages tend to be used differently in different places. One of the ways that language use varies is that different words are used in different areas. For example in Philadelphia you might hear someone refer to something they’ve forgotten the name for as a “jawn” and in Maine and the northeast hedgehogs are sometimes called “quill-pigs”. This dataset contains information on many regionally-specific words of American English and where they are used.. Content DAREDS is a dataset of words found in American English created from the Dictionary of American Regional English (DARE). DARE collects information on dialect regions which terms are used in them and the meaning of those terms. It is based on dialectal surveys from different rege  In order to construct a dataset based on DARE the web version of DARE was downloaded. It was then cleaned removing both multiword expressions and very common words. Dialect regions that didn’t correspond to a single state or set of cities (e.g. South) were mapped to the most populous cities within each region. For example within the Pacific Northwest dialect region the most populous cities (Seattle Tacoma Portland Salem Eugene) were added to this dataset as subregions. The resulting dataset (DAREDS) consists of around 4.3k dialect terms from 99 U.S. dialect regions.  Acknowledgements This dataset was compiled by Afshin Rahimi Trevor Cohn  and Timothy Baldwin. If you use this dataset please cite both their paper and the Dictionary of American Regional English.  Rahimi Afshin Cohn Trevor  and  Baldwin Timothy. ""A Neural Model for User Geolocation and Lexical Dialectology."" ACL 2017 (2017) 209. Cassidy F. G. (1985). Dictionary of American Regional English. Belknap Press of Harvard University Press.  Inspiration This dataset was originally composed to help validate the geographic origin of Twitter data (if someone uses a word that’s only used in a small geographic area then it’s more likely that they are from there). But there are lots of other interesting questions you can ask with this data!  How many of these words can you find in Twitter datasets like this one of celebrity tweets? How accurately can you guess the location of these celebrities given this dataset? Can you train word vectors based on these words? Do words tend to cluster by regional origin? Many of the words in this dataset have become less popular over time. Can you tell which ones are still popular perhaps by using Google’s n-gram viewer? ",{}JSON,,"[languages, united states, linguistics]",CC4,,,52,873,0.6279296875,American words and where to find them,Dictionary of American Regional English (DAREDS),https://www.kaggle.com/rtatman/dictionary-of-american-regional-english-dareds,Tue Aug 08 2017
42,,freeCodeCamp,"[Age, AttendedBootcamp, BootcampFinish, BootcampLoanYesNo, BootcampName, BootcampRecommend, ChildrenNumber, CityPopulation, CodeEventConferences, CodeEventDjangoGirls, CodeEventFCC, CodeEventGameJam, CodeEventGirlDev, CodeEventHackathons, CodeEventMeetup, CodeEventNodeSchool, CodeEventNone, CodeEventOther, CodeEventRailsBridge, CodeEventRailsGirls, CodeEventStartUpWknd, CodeEventWkdBootcamps, CodeEventWomenCode, CodeEventWorkshops, CommuteTime, CountryCitizen, CountryLive, EmploymentField, EmploymentFieldOther, EmploymentStatus, EmploymentStatusOther, ExpectedEarning, FinanciallySupporting, FirstDevJob, Gender, GenderOther, HasChildren, HasDebt, HasFinancialDependents, HasHighSpdInternet, HasHomeMortgage, HasServedInMilitary, HasStudentDebt, HomeMortgageOwe, HoursLearning, ID.x, ID.y, Income, IsEthnicMinority, IsReceiveDisabilitiesBenefits, IsSoftwareDev, IsUnderEmployed, JobApplyWhen, JobInterestBackEnd, JobInterestDataEngr, JobInterestDataSci, JobInterestDevOps, JobInterestFrontEnd, JobInterestFullStack, JobInterestGameDev, JobInterestInfoSec, JobInterestMobile, JobInterestOther, JobInterestProjMngr, JobInterestQAEngr, JobInterestUX, JobPref, JobRelocateYesNo, JobRoleInterest, JobWherePref, LanguageAtHome, MaritalStatus, MoneyForLearning, MonthsProgramming, NetworkID, Part1EndTime, Part1StartTime, Part2EndTime, Part2StartTime, PodcastChangeLog, PodcastCodeNewbie, PodcastCodePen, PodcastDevTea, PodcastDotNET, PodcastGiantRobots, PodcastJSAir, PodcastJSJabber, PodcastNone, PodcastOther, PodcastProgThrowdown, PodcastRubyRogues, PodcastSEDaily, PodcastSERadio, PodcastShopTalk, PodcastTalkPython, PodcastTheWebAhead, ResourceCodecademy, ResourceCodeWars, ResourceCoursera, ResourceCSS]","[numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, numeric, string, string, numeric, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, dateTime, dateTime, dateTime, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Free Code Camp is an open source community where you learn to code and build projects for nonprofits. We surveyed more than 20000 people who started coding within the past 5 years. We reached them through the twitter accounts and email lists of various organizations that help people learn to code. Our goal was to understand these people's motivations in learning to code how they're learning to code their demographics and their socioeconomic background. We've written in depth about this dataset here https//medium.freecodecamp.com/we-asked-20-000-people-who-they-are-and-how-theyre-learning-to-code-fff5d668969,CSV,,"[employment, computing and society, programming]",ODbL,,,304,3153,13,"An open data survey of 20,000+ people who are new to software development",The freeCodeCamp 2017 New Coder Survey,https://www.kaggle.com/free-code-camp/the-freecodecamp-2017-new-coder-survey,Fri May 26 2017
43,,Harvard University,"[academic.year, school, cost]","[numeric, string, numeric]","Harvard tuition data since 1985 for both the undergraduate College and the graduate and professional schools. The Data This dataset consists of two files tuition_graduate.csv and undergraduate_package.csv which contain the tuition and fees data for the graduate schools and undergraduate College respectively.  tuition_graduate.csv contains the following fields  academic.year the academic year between 1985 and 2017 school the name of the graduate or professional school; one of   GSAS Business (MBA) Design Divinity Education Government Law Medical/Dental Public Health (1-Year MPH) cost the cost of tuition at a given school in a given year  undergraduate_package.csv contains the following fields  academic.year the academic year between 1985 and 2017 component the component of undergraduate fees; one of Tuition*Health Services Fee*Student Services Fee*Room*Board*Total* cost the cost of the component; or if the component is Total the sum of the costs of the other components in that year  Acknowledgements All of the data in this dataset comes from The Harvard Open Data Dataverse. Specific citations are as follows for the graduate tuition data  Harvard Financial Aid Office 2015 ""Harvard graduate school tuition"" doi10.7910/DVN/LV0YSQ Harvard Dataverse V1 for the undergraduate tuition and fees data  Harvard Financial Aid 2015 ""Harvard College Tuition"" doi10.7910/DVN/MSS2BE Harvard Dataverse V1 [UNF6FyXNny+KBTgLX+DzewzEfg==]",CSV,,[education],CC0,,,293,2867,0.0107421875,Tuition data from Harvard's College and graduate/professional schools since 1985,Harvard Tuition,https://www.kaggle.com/harvard-university/harvard-tuition,Fri Nov 11 2016
44,,Consumer Financial Protection Bureau,"[date_received, product, sub_product, issue, sub_issue, consumer_complaint_narrative, company_public_response, company, state, zipcode, tags, consumer_consent_provided, submitted_via, date_sent_to_company, company_response_to_consumer, timely_response, consumer_disputed?, complaint_id]","[dateTime, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, string, dateTime, string, string, string, numeric]",Each week the CFPB sends thousands of consumers’ complaints about financial products and services to companies for response. Those complaints are published here after the company responds or after 15 days whichever comes first. By adding their voice consumers help improve the financial marketplace.,CSV,,[finance],Other,,,3453,28233,361,US consumer complaints on financial products and company responses,US Consumer Finance Complaints,https://www.kaggle.com/cfpb/us-consumer-finance-complaints,Wed Apr 27 2016
45,,Yan Ramos da Silva,"[Character, 2015, Age of X, Avengers Fairy Tales, Brilliant City, Earth-0, Earth-001, Earth-1000, Earth-10001, Earth-10001011, Earth-10003, Earth-10005, Earth-10011, Earth-1002, Earth-10021, Earth-10022, Earth-1003, Earth-1004, Earth-10041, Earth-1005, Earth-10051, Earth-1006, Earth-10063, Earth-1007, Earth-10071, Earth-1008, Earth-10081, Earth-10082, Earth-10083, Earth-1009, Earth-10091, Earth-1010, Earth-101001, Earth-10101, Earth-10102, Earth-1011, Earth-10112, Earth-1012, Earth-1013, Earth-1014, Earth-1015, Earth-1016, Earth-1017, Earth-1018, Earth-10182, Earth-1019, Earth-10190, Earth-10197, Earth-1020, Earth-10201, Earth-10208, Earth-1021, Earth-10219, Earth-1022, Earth-10221, Earth-10223, Earth-1023, Earth-10235, Earth-1024, Earth-10245, Earth-10246, Earth-1025, Earth-1026, Earth-10267, Earth-1027, Earth-1028, Earth-10280, Earth-1029, Earth-10298, Earth-1030, Earth-1031, Earth-10310, Earth-103173, Earth-1032, Earth-1033, Earth-10330, Earth-1034, Earth-1035, Earth-1036, Earth-10363, Earth-1037, Earth-1038, Earth-10382, Earth-1039, Earth-1040, Earth-1041, Earth-1042, Earth-1043, Earth-1044, Earth-1045, Earth-10508, Earth-10511, Earth-10515, Earth-105709, Earth-1059, Earth-1064, Earth-10710, Earth-10711, Earth-10724, Earth-107342]","[numeric, numeric, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean]",Context I have been a comic book fan for many years and when I started writing web scrapers for practice it was only natural that I did one inspired by my passion for Marvel. Content This dataset has 27.290 rows each one representing a distinct Marvel character such as Peter Parker Tony Stark or Jean Grey.  As for columns there are 1822 of them one for each Marvel universe. All the cells contain a boolean value true if there is a version of that character from that universe or false otherwise. Acknowledgements I would like to thank the Marvel Wikia for its amazing amount of information as well as very practical API and Marvel for having such a huge and diverse multiverse that inspires many possibilities of analysis. Inspiration This was my first attempt at data science. It was challenging but very fun and rewarding. I would really appreciate any feedback or suggestions for next works.,CSV,,[],CC0,,,352,3491,285,All distinct Marvel characters and every universe where they appear,Marvel Characters and Universes,https://www.kaggle.com/yrdasilva/marvel-characters-and-universes,Tue Jan 31 2017
46,,Vered Shwartz,"[menheniot, village, TRUE]","[string, string, boolean]","Context Hypernymy is an important lexical-semantic relation for NLP tasks. For instance knowing that Tom Cruise is an actor can help a question answering system answer the question ""which actors are involved in Scientology?"". While semantic taxonomies like WordNet define hypernymy relations between word types they are limited in scope and domain. Therefore automated methods have been developed to determine for a given term-pair (x y) whether y is an hypernym of x based on their occurrences in a large corpus. To facilitate training neural methods for hypernymy detection which typically require a large amount of training data we followed the common methodology of creating a dataset using distant supervision from knowledge resources and extracted hypernymy relations from WordNet DBPedia Wikidata and Yago. Content All instances in our dataset both positive and negative are pairs of terms that are directly related in at least one of the resources. These resources contain thousands of relations some of which indicate hypernymy with varying degrees of certainty. To avoid including questionable relation types we consider as denoting positive examples only indisputable hypernymy relations (Table 1) which we manually selected from the set of hypernymy indicating relations in Shwartz et al. (2015). Term-pairs related by other relations (including hyponymy) are considered as negative instances and there is a ratio of 14 positive to negative pairs in the dataset.    resourcerelations  WordNetinstance hypernym hypernym  DBPediatype  Wikidatasubclass of instance of  Yagosubclass of    Table 1 hypernymy relations in each resource. We provide two dataset splits random - 70/25/5 ratio for train/test/validation and lexical - where each set has a distinct vocabulary preventing models from overfitting to the most common class of x/y (this split maintains a similar ratio). Files  train_rnd.csv test_rnd.csv val_rnd.csv - the training test and validation set of the random split. train_lex.csv test_lex.csv val_lex.csv - the training test and validation set of the lexical split.  Each file is a comma-separated file with the following fields  x - the first term y - the second term label - TRUE if y is a hypernym of x else FALSE  Acknowledgements If you use this dataset for research purposes please cite the following publication Improving Hypernymy Detection with an Integrated Path-based and Distributional Method.  Vered Shwartz Yoav Goldberg and Ido Dagan. ACL 2016.",CSV,,[],GPL,,,58,500,2,Pairs of terms annotated to whether one is a hypernym of the other,Hypernymy,https://www.kaggle.com/vered1986/hypernymy,Sun Aug 13 2017
47,,melvincheung,"[Overall Position, Gender Position, Category Position, Category, Race No, Country , Official Time, Net Time, 10km Time, Half Way Time, 30km Time]","[numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric]","From the data you will have   - Results from 12k+ participants with the fastest one of 2hr12mins from world class athlete  - Midway time at 10km halfway and 30km  - Overall and gender ranking  The original source The data are captured from its official site http//www.hkmarathon.com/Results/Search_2016_Results.htm Only marathon results are included (but not 10km nor half marathon) because only this results has midway time which can serve better analysis purposes.  The fields Race No runner ID Category gender and age group. (e.g. MMS and MFS denote male and female while the age group are the same.) Official Time the ""gun time"" Net Time the time between one passes the starting line and final line. It is usually a few minutes less than Official Time. 10km Time Half Way Time 30km Time they are the midway times as described  The files Marathon challenge and Marathon Run 1 uses the same running path for racing but with a different starting time. Athletes in challenge group are generally run faster.  Improving the dataset   - Comparing the results of different marathons all over the world to find which one is the toughest or having the best participants etc.  - Please let me know if there is any centralized database collecting the results from different races.",CSV,,"[running, walking]",Other,,,1138,9398,0.9443359375,A race participated by 12k+ athletes from 50+ countries,Hong Kong Marathon 2016 results,https://www.kaggle.com/melvincheung/hong-kong-marathon-2016,Tue Oct 04 2016
48,,NLTK Data,[],[],"Context This test set was created as a regression test to illustrate the differences in the Porter stemmer variants implemented in NLTK. This was added in issue 1261 during a major rewrite of the Porter stemmer by Mark Amery. The details of the unit test can be found in nltk.test.unit.test_stem.py Content  porter_vocabulary.txt Input words used in the regression test.  porter_martin_output.txt Output of the updated Porter stemmer implemented by Martin Porter porter_original_output.txt Output of the original ""Martin-blessed"" Porter stemmer written in C porter_nltk_output.txt Output of the Porter stemmer implemented in NLTK  Acknowledgements Credits go to Mark Amery for creating this regression test for NLTK version of the Porter stemmer as well as the major rewrite of the stemmer itself!!",Other,,[],Other,,,17,448,0.6484375,Regression test to compare NLTK vs Tatarus Porter stemmer,Porter Test,https://www.kaggle.com/nltkdata/porter-test,Sat Aug 19 2017
49,,Dan Ofer,"[Person_ID, AssessmentID, Case_ID, Agency_Text, LastName, FirstName, MiddleName, Sex_Code_Text, Ethnic_Code_Text, DateOfBirth, ScaleSet_ID, ScaleSet, AssessmentReason, Language, LegalStatus, CustodyStatus, MaritalStatus, Screening_Date, RecSupervisionLevel, RecSupervisionLevelText, Scale_ID, DisplayText, RawScore, DecileScore, ScoreText, AssessmentType, IsCompleted, IsDeleted]","[numeric, numeric, numeric, string, string, string, string, string, string, dateTime, numeric, string, string, string, string, string, string, dateTime, numeric, string, numeric, string, numeric, numeric, string, string, numeric, numeric]","Context COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendant’s likelihood of reoffending (recidivism). It has been shown that the algorithm is biased in favor of white defendants and against black inmates based on a 2 year follow up study (i.e who actually committed  crimes or violent crimes after 2 years). The pattern of mistakes as measured by precision/sensitivity is notable.  Quoting from ProPublica ""  Black defendants were often predicted to be at a higher risk of recidivism than they actually were. Our analysis found that black defendants who did not recidivate over a two-year period were nearly twice as likely to be misclassified as higher risk compared to their white counterparts (45 percent vs. 23 percent).   White defendants were often predicted to be less risky than they were. Our analysis found that white defendants who re-offended within the next two years were mistakenly labeled low risk almost twice as often as black re-offenders (48 percent vs. 28 percent).   The analysis also showed that even when controlling for prior crimes future recidivism age and gender black defendants were 45 percent more likely to be assigned higher risk scores than white defendants.   Black defendants were also twice as likely as white defendants to be misclassified as being a higher risk of violent recidivism. And white violent recidivists were 63 percent more likely to have been misclassified as a low risk of violent recidivism compared with black violent recidivists. The violent recidivism analysis also showed that even when controlling for prior crimes future recidivism age and gender black defendants were 77 percent more likely to be assigned higher risk scores than white defendants. ""  Content Data contains  variables used by the COMPAS algorithm in scoring defendants along with their outcomes within 2 years of the decision for over 10000 criminal defendants in Broward County Florida.  3 subsets of the data are provided including a subset of only violent recividism (as opposed to e.g. being reincarcerated for non violent offenses such as vagrancy or Marijuana). Indepth analysis by ProPublica can be found in their data methodology article. Acknowledgements Data & original analysis gathered by ProPublica.  Original Data methodology article https//www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm Original Article https//www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing Original data from ProPublica https//github.com/propublica/compas-analysis Additional ""simple"" subset provided by FairML based on the proPublica data http//blog.fastforwardlabs.com/2017/03/09/fairml-auditing-black-box-predictive-models.html Inspiration Ideas  Feature importance when predicting the COMPASS score itself or recividism/crime risks. Reweighting data to compensate for bias e.g. subsetting for the violent offenders or adjusting better for base risk.  Feature selection based on ""legal usage""/fairness (E.g. exclude race and see how well your model works. It worked for me). ",CSV,,"[crime, law, demographics]",ODbL,,,116,1715,23,Racial Bias in inmate COMPAS reoffense risk scores for Florida (ProPublica),COMPAS Recidivism Racial Bias,https://www.kaggle.com/danofer/compass,Thu Jun 29 2017
50,,Abhinav Maurya,"[author, child_id, comment, confidence, decision, paper_id, parent_id, question, rating, review]","[string, string, string, string, string, string, string, string, string, string]","Context ICLR (International Conference on Learning Representations) is a premier machine learning conference. Unlike the other two flagship machine learning conferences ICML and NIPS ICLR chooses a single-blind public review process in which the reviews and their rebuttals are both carried out transparently and in the open. This dataset was created by crawling the public ICLR 2017 paper review site. It seems ICLR is going double-blind from 2018 so my guess is that authors will remain anonymous during the review process. So this dataset is unique because it captures a public academic review process with academic affiliations and all paper decisions including rejections. Content The dataset consists of two CSV files  iclr2017_papers.csv This file has a row per submission. It includes the paper title authors author conflicts abstracts tl;dr (a simplified abstract) and final decision (Accept/Oral Accept/Poster Accept/InviteToWorkshop Reject). Each row has a unique identifier key called the ""paper_id."" iclr2017_conversations.csv This file has a row per textual review rebuttal or comment. It is related to the previous papers dataset using the secondary key ""paper_id."" All rows talking about a single paper share the same ""paper_id."" The conversations associated with each paper can be thought of as a forest. Each tree in the forest begins with a review followed by rebuttals and further comments/conversation. Each such textual entry composed by an individual is listed in its own row. The nodes of the tree are connected using the fields ""child_id"" and ""parent_id"" which can be used to construct the entire conversation hierarchy.  Acknowledgements All rights for abstracts rest with the paper authors. Reproduction of abstracts here is solely for purposes of research. Thanks to the authors of Beautiful Soup 4 Python package which considerably simplified the process of curating this dataset. Inspiration This dataset was created to understand gender disparities in paper submissions and acceptances. Annotating each author with a binary gender is a pending task. The dataset can also be used to model communication processes employed in negotiation persuasion and decision-making. Another use of this dataset could be in modeling and understanding textual time-series data.",CSV,,"[research, linguistics, artificial intelligence, computer science]",GPL,,,41,935,10,"ICLR 2017 paper titles, authors, abstracts, reviews, and 4-way decisions.",ICLR 2017 Reviews,https://www.kaggle.com/ahmaurya/iclr2017reviews,Tue Aug 22 2017
51,,Rachael Tatman,"[name, comment]","[string, string]",Context The Eurovision Song Contest which originated in 1956 is present on YouTube through uploads of songs performed in the Contest. Any user can freely comment on these songs. This dataset is made of up a collection of comments made on four YouTube videos of Eurovision entries by Belgium. The comments are in a number of languages. Content The YouTube online forums associated with the Eurovision Song Contest have a large number of users from varied linguistic backgrounds who because of their interests in song performance are particularly attentive to language-related issues such as the accent of the performers and the choice of language of the songs. Commentaries are made by forum participants from disparate locations on a variety of topics one of the most prominent being language including language features and perceptions of language use. Acknowledgements This dataset was collected by Dejan Ivković for the purpose of linguistic research. If you made use of this data please cite the following article Ivković D. (2013). The Eurovision Song Contest on YouTube A corpus-based analysis of language attitudes. Language@Internet 10 article 1. (urnnbnde0009-7-35977) Inspiration  This dataset contains multiple languages. Can you identify and the language of each comment? Can you automatically find positive and negative comments about different country’s songs? Are some commenters more positive or more negative than others? ,CSV,,"[languages, music, linguistics, internet]",CC4,,,98,1563,0.3564453125,YouTube comments on entries from the 2003-2008 Eurovision Song Contests,Eurovision YouTube Comments,https://www.kaggle.com/rtatman/eurovision-youtube-comments,Thu Jul 20 2017
52,,Department of Homeland Security,"[Continent/Country of Nationality, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context A refugee is a person outside his or her country of nationality who is unable or unwilling to return to his or her country of nationality because of persecution or a well-founded fear of persecution on account of race religion nationality membership in a particular social group or political opinion. An asylee is a person who meets the definition of refugee and is already present in the United States or is seeking admission at a port of entry. Refugees are required to apply for lawful permanent resident (“green card”) status one year after being admitted and asylees may apply for green card status one year after being granted asylum. Content The Office of Immigration Statistics (OIS) Annual Flow Reports on refugees and asylees contain information obtained from the Worldwide Refugee Admissions Processing System (WRAPS) of the Bureau of Population Refugees and Migration of the US Department of State on the numbers and demographic profiles of persons admitted to the United States as refugees and those granted asylum status during a given fiscal year.,CSV,,"[demographics, international relations]",CC0,,,1015,6145,0.0146484375,Where do most people granted refugee or asylum status come from?,"Refugees in the United States, 2006-2015",https://www.kaggle.com/dhs/refugee-report,Fri Jan 20 2017
53,,Sohier Dane,"[ID #, Page, Description, Underlying Assets, Location, EIF, attached schedule+2016, 2016 Value Min, 2016 Value Max, Income Type, 2016 Income Exact, 2016 Income Min, 2016 Income Max]","[numeric, numeric, string, string, string, string, numeric, string, string, string, string, string, string]",Context This is a general disclosure of Donald Trump's assets debts and sources of income. Because the forms are only meant to reveal on potential conflicts of interests a filer might have they provide far less specificity than tax returns would. This is an unpacked version of the pdf of Trump's form that was made available by the Federal Election Commission June 16th 2017. It contains some information about his financial interests but not enough to paint a complete picture of his net worth. It may be possible to use some of these forms to identify his foreign business partners. Acknowledgements This dataset was kindly extracted from the original PDF and made publicly available by the Center for Responsive Politics. Inspiration Previous work on similar disclosures by Trump has often focused on identifying his foreign business ties. Are you able to find any new ones? You might also like  Trump's 2016 Financial Disclosure Trump's World Trump's Tweets Trump Campaign Expenditures ,CSV,,"[finance, politics]",Other,,,130,2544,0.1220703125,Donald Trump's 2017 Form 278e,Trump Financial Disclosure 2017,https://www.kaggle.com/sohier/trump-financial-disclosure-2017,Tue Jul 11 2017
54,,Mike Johnson Jr,"[Location, Diversity-Index, Black or African American alone, percent, 2013, American Indian and Alaska Native alone, percent, 2013, Asian alone, percent, 2013, Native Hawaiian and Other Pacific Islander alone, percent,, Two or More Races, percent, 2013, Hispanic or Latino, percent, 2013, White alone, not Hispanic or Latino, percent, 2013]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Diversity of United States Counties Content Diversity Index of Every US County using the Simpson Diversity Index D = 1 - ∑(n/N)^2 (where n = number of people of a given race and N is the total number of people of all races to get the probability of randomly selecting two people and getting two people of different races (ecological entropy)),CSV,,[demographics],CC0,,,755,5629,0.18359375,Simpson Diversity Index to quantify racial diversity of US counties,Diversity Index of US counties,https://www.kaggle.com/mikejohnsonjr/us-counties-diversity-index,Mon Aug 22 2016
55,,Anton Prokopyev,"[Oregonian ID, Site Name, Address, City, State, Lat, Long, Inspection conducted?, Inspection report available?, Inspection year, Lead present?, Earlier lead discovery reported, Highest lead level detected (ug), Where highest lead level detected?, Lead present outside range?, Notes, Had firing range?, Firing range use, Additional details]","[numeric, string, string, string, string, numeric, numeric, string, string, numeric, string, string, numeric, string, string, string, string, string, string]",Context According to The Oregonian hundreds of National Guard armories across the U.S. may have been contaminated with lead from indoor firing ranges. It was reported that areas populated by children under 7 years of age should have less than 40 micrograms of lead per square foot.  Content The Oregonian collected over 23000 pages of public records following a Freedom Of Information Act request.  The dataset covers armory inspections conducted since 2012 and may facilitate investigation of lead contamination in the U.S. Acknowledgements The data assembly process is described by Melissa Lewis here. Inspiration This dataset can be used to conduct research in the realm of public health. It will be especially useful if  1) you know about health effects of exposure to lead in relatively short terms periods; 2) you are able to find relevant health data to conduct a study on lead poisoning.,CSV,,"[environment, military]",CC0,,,127,1828,0.201171875,Data from The Oregonian's investigation,Toxic Armories,https://www.kaggle.com/prokopyev/armories,Tue Feb 21 2017
56,,Amandeep Rathee,"[, X, text, favorited, favoriteCount, replyToSN, created, truncated, replyToSID, id, replyToUID, statusSource, screenName, retweetCount, isRetweet, retweeted]","[numeric, numeric, string, boolean, numeric, string, dateTime, boolean, string, numeric, numeric, string, string, numeric, boolean, boolean]","Context The demonetization of ₹500 and ₹1000 banknotes was a step taken by the Government of India on 8 November 2016 ceasing the usage of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as a form of legal tender in India from 9 November 2016. The announcement was made by the Prime Minister of India Narendra Modi in an unscheduled live televised address to the nation at 2015 Indian Standard Time (IST) the same day. In the announcement Modi declared circulation of all ₹500 and ₹1000 banknotes of the Mahatma Gandhi Series as invalid and announced the issuance of new ₹500 and ₹2000 banknotes of the Mahatma Gandhi New Series in exchange for the old banknotes. Content The data contains 6000 most recent tweets on #demonetization. There are 6000 rows(one for each tweet) and 14 columns. Metadata  Text (Tweets) favorited favoriteCount replyToSN created truncated replyToSID id replyToUID statusSource screenName retweetCount isRetweet retweeted  Acknowledgement The data was collected using the ""twitteR"" package in R using the twitter API. Past Research I have performed my own analysis on the data. I only did a sentiment analysis and formed a word cloud. Click here to see the analysis on GitHub Inspiration  What percentage of tweets are negative positive or neutral ? What are the most famous/re-tweeted tweets ? ",CSV,,"[finance, twitter, human-computer interaction, internet]",Other,,,3260,30507,5,Data extracted from Twitter regarding the recent currency demonetization,Demonetization in India Twitter Data,https://www.kaggle.com/arathee2/demonetization-in-india-twitter-data,Fri Apr 21 2017
57,,University of Copenhagen,"[order, family, name, year, date1, date2, individuals]","[string, string, string, numeric, dateTime, dateTime, numeric]",Context The University of Copenhagen’s Zoological Museum placed a light trap on their roof and for 18 years they documented the types of insects being caught.  The data was collected as part of a study to determine insect responses to recent climate change. Content This file contains the raw data from the light trapping study ordered by Order (Lepidoptera/Coleoptera) family name (species) year date1 (start) date2 (end) and number of individuals Acknowledgements Original publication Thomsen PF Jørgensen PS Bruun HH Pedersen J Riis-Nielsen T Jonko K Słowińska I Rahbek C Karsholt O (2016) Resource specialists lead local insect community turnover associated with temperature – analysis of an 18-year full-seasonal record of moths and beetles. Journal of Animal Ecology 85(1) 251–261. http//dx.doi.org/10.1111/1365-2656.12452 The original dataset can be found at http//datadryad.org/resource/doi10.5061/dryad.s4945/1 Inspiration Climate change is on everyone's mind for one reason or another and insects are susceptible to climate change just like humans. Using these data can you determine which species have become more or less prevalent over the 18 years of collection?,CSV,,"[biology, environment]",CC0,,,217,1461,3,The University of Copenhagen’s Zoological Museum zapped insects for 18 years,Insect Light Trap,https://www.kaggle.com/University-of-Copenhagen/insect-light-trap,Wed Jul 12 2017
58,,Jean Pierre Rukundo,"[HHID, PROVINCE, DISTRICT, URB2002, QUINTILE, POVERTY, HH_WT, CLUSTER, ITEM, S8A1Q2, S8A1Q3, S8A1Q4, S8A1Q5, S8A1Q6, S8A1Q7, S8A1Q8, S8A1Q9, S8A1Q10, S8A1Q11, S8A1Q12, S8A1Q13, S8A1Q14, S8A1Q15, S8A1Q16]","[numeric, string, numeric, string, string, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, string, numeric, string, string, numeric, numeric]",Overview Identification COUNTRY Rwanda TITLE Integrated Household Living Conditions Survey 2010-2011 TRANSLATED TITLE Enquête Intégrale sur les conditions de vie des ménages 2010-2011 STUDY TYPE Income/Expenditure/Household Survey SERIES INFORMATION This is the third in a series of periodic standardized income and expenditure surveys. The Rwanda EICV is conducted with a periodicity of 5 years. The surveys in the series are as follows EICV1 2000-2001 EICV2 2005-2006 EICV3 2010-2011 ID NUMBER RWA-NISR-EICV3-02 Version VERSION DESCRIPTION Version 2.0 Final public-use dataset PRODUCTION DATE 2012-10-19 NOTES Version 2.0 The date of this version corresponds to the date of NISR approval of the final public-use datasets. Overview ABSTRACT The 2010/11 Integrated Household Living Conditions Survey or EICV3 (Enquête Intégrale sur les Conditions de Vie des Ménages) is the third in the series of surveys which started in 2000/01 and is designed to monitor poverty and living conditions in Rwanda. The survey methodology has changed little over its 10 years making it ideal for monitoring changes in the country. In 2010/11 for the first time the achieved sample size of 14308 households in the EICV3 was sufficient to provide estimates which are reliable at the level of the district. KIND OF DATA Sample survey data [ssd] UNITS OF ANALYSIS For the purposes of this study the following units of analysis are considered -communities -households -persons Scope NOTES The scope of survey is defined by the need to evaluate poverty determinants and effects of poverty in various domains. This includes gathering data in specific sectors and examning summary statistics and computed indicators by consumption indicator gender etc. The survey primarily seeks to compute household consumption aggregates and correlate consumption to the following areas are within the scope and integrated into the survey  Education (education expenditures) general education curriculum vocational training and higher learning school-leaving literacy and apprenticeship. Health (health expenditures) disability and health problems general health and preventative vaccination over the past 12 months. Migration (travel expenditures) rural-urban migration internal and external migration. Housing (expenditures on utilities rent etc.) status of the housing occupancy services and installations physical characteristics of the dwelling access and satisfaction towards basic services. Economic activity (revenue) unemployment underemployment and job search occupation wage or salaried employment characteristics VUP Activities all other activities domestic work. Non-agricultural activities (revenue) activity status formal and informal sector activity. Agriculture (income and expenditure)  livestock land and agricultural equipment details of holding parcels/blocs and agricultural policy changes crop harvests and use on a large and small scale crop production harvests and use transformation (processing) of agricultural products.  In addition to the specific sector information consumption and/or wealth holding information was collected  Consumption Expenditure on non food items food expenditure subsistence farming (own consumption) with different recall periods. Other cash flows  transfers out by household transfers received by the household income support programs & other revenues (excluding all incomes accrued from saving) VUP UBUDEHE & RSSP schemes other expenditure (excluding expenditures related to any form of saving). Stock items credit durable assets and savings (household assets and liabilities)  TOPICS Topic   Vocabulary  URI consumption/consumer behaviour [1.1]    CESSDA  http//www.nesstar.org/rdf/common economic conditions and indicators [1.2]    CESSDA  http//www.nesstar.org/rdf/common EDUCATION [6]   CESSDA  http//www.nesstar.org/rdf/common general health [8.4]    CESSDA  http//www.nesstar.org/rdf/common employment [3.1]    CESSDA  http//www.nesstar.org/rdf/common unemployment [3.5]  CESSDA  http//www.nesstar.org/rdf/common housing [10.1]  CESSDA  http//www.nesstar.org/rdf/common time use [13.9] CESSDA  http//www.nesstar.org/rdf/common migration [14.3]    CESSDA  http//www.nesstar.org/rdf/common information technology [16.2]   CESSDA  http//www.nesstar.org/rdf/common Coverage GEOGRAPHIC COVERAGE This is a national survey with representivity at the (5) provicial and (30) district level and includes urban and rural households. GEOGRAPHIC UNIT The cluster UNIVERSE All household members. Producers and Sponsors PRIMARY INVESTIGATOR(S) Name    Affiliation National Institute of Statistics of Rwanda (NISR)   Ministry of finance and economics planning (MINECOFIN) OTHER PRODUCER(S) Name    Affiliation Role Oxford Policy Management    DFID    Permanante assistance Geoffrey Greenwell  UNDP    Designer of data system David Megill    UNDP    Statistician Metadata Production METADATA PRODUCED BY Name    Abbreviation    Affiliation Role Juste NITIEMA       Oxford Policy Management (OPM)  Developed the document Geoffrey Greenwell      UNDP    Reviewed and edited document Ruben Muhayiteto        NISR    Revision of metadata DATE OF METADATA PRODUCTION 2011-06-02 DDI DOCUMENT VERSION Version 1.0 (Oct. 192012)  This version of the document represents the first draft of the public-use dataset of the EICV 3 study. Version 1.1 (June 28th 2016) Changed the title from French into English DDI DOCUMENT ID RWA-NISR-DDI-EICV3-02,CSV,,[economics],CC0,,,24,687,10,Ubudehe Livestock 1 from Rwanda NISR,Ubudehe Livestock 1,https://www.kaggle.com/jprukundo/ubudehelivestock1,Fri Aug 04 2017
59,,GeneBurin,"[state, year, congress house, congress sen class 1, congress sen class 2, congress sen class 3, electoral, governor, legislature house, legislature sen, congress sen]","[string, numeric, string, string, string, string, string, string, string, string, string]","Data on party strength in each US state The repository contains data on party strength for each state as shown on each state's corresponding party strength Wikipedia page (for example here is Virginia ) Each state has a table of a detailed summary of the state of its governing and representing bodies on Wikipedia but there is no data set that collates these entries. I scraped each state's Wikipedia table and collated the entries into a single dataset. The data are stored in the state_party_strength.csv and state_party_strength_cleaned.csv. The code that generated the file can be found in corresponding Python notebooks.  Data contents The data contain information from 1980 on each state's   1. governor and party   2. state house and senate composition   3. state representative composition in congress   4. electoral votes Clean Version Data in the clean version has been cleaned and processed substantially. Namely - all columns now contain homogenous data within the column - names and Wiki-citations have been removed - only the party counts and party identification have been left The notebook that created this file is here Uncleaned Data Version The data contained herein have not been altered from their Wikipedia tables except in two instances - Forced column names to be in accord across states - Any needed data modifications (ie concatenated string columns) to retain information when combining columns  To use the data  Please note that the right encoding for the dataset is ""ISO-8859-1"" not 'utf-8' though in future versions I will try to fix that to make it more accessible.  This means that you will likely have to perform further data wrangling prior to doing any substantive analysis. The notebook that has been used to create this data file is located here Raw scraped data The raw scraped data can be found in the pickle. This file contains a Python dictionary where each key is a US state name and each element is the raw scraped table in Pandas DataFrame format. Hope it proves as useful to you in analyzing/using political patterns at the state level in the US for political and policy research.",CSV,,[politics],CC0,,,142,2077,0.1201171875,From 1980 to present; a collection of political office compositions per state,Party strength in each US state,https://www.kaggle.com/kiwiphrases/partystrengthbystate,Fri Jan 13 2017
60,,Jacob Boysen,"[id, state_id, city, klan_number, nickname, notes, latitude, longitude, year]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric]","Context Mapping the Klan is a rough timeline of the rise of the second Ku Klux Klan between 1915 and 1940. Each red dot shows a local unit or ""Klavern."" The official numbers for each Klavern indicate a basic chronology for the chartering of the Klaverns and they also reveal patterns of Klan organizing. Content The data for Mapping the Klan is based on a variety of sources mostly newspapers sponsored by or sympathetic to the Ku Klux Klan. These publications reported on the activities of local units known officially as Klaverns. Data includes approximate date of charter location(lat/lon) nickname source for data and related notes. Dates The dates for each Klavern come from the publication listed for that entry. So it is likely that the Klaverns identified were established even earlier than the date indicated. The Klan’s recruitment methods make it harder to accurately date the beginning of a Klavern. Each local group had to recruit a set number of members before it could get its charter and number. Numbers The Klaverns in each state were numbered in chronological order of their chartering. So we can assume that if a Klan number 40 is dated October 1923 Klans 1 to 39 were established before 1923. As historians agree the busiest years of Klan expansion were 1922-1924 with big declines thereafter. The large number of klaverns established after 1925 when the Ku Klux Klan largely disappeared from the national news media is intriguing. The continued organizing of Klaverns after 1925 is more difficult to study for lack of sources. That history remains to be explored. Learn more. Acknowledgements Source data here available through the VCU Library site. Data was compiled by  John Kneebone lead author and professor of History VCU Shariq Torres lead web developer and data co-author VCU Libraries Erin White project manager VCU Libraries Lauren Work digital collections VCU Libraries Alison Tinker web designer VCU Libraries John Glover digital humanities consultant VCU Libraries  Inspiration  Where was the densest concentrations of KKK? What years saw the biggest rises? ",CSV,,"[united states, sociology]",CC4,,,35,583,0.296875,Location and Charter Date of over 2000 “Klaverns”,Mapping the KKK 1921-1940,https://www.kaggle.com/jboysen/mapping-the-kkk,Sat Sep 16 2017
61,,DhruvMangtani,[],[],Context This dataset contains the metadata of over 50000 tweets from Christmas Eve and Christmas. We are hoping the data science and research community can use this to develop new and informative conclusions about this holiday season. Content We acquired this data through a web crawler written in Java. The first field is the id of the tweet and the second is the HTML metadata. We recommend using BeautifulSoup or another library to parse this data and extract information from each tweet. Inspiration We would especially like to see research on the use of emojis in tweets the type of sentiment there is on Christmas (Maybe determine how grateful each country is) or some kind of demographic on the age or nationality of active Twitter users during Christmas.,CSV,,"[faith and traditions, internet]",ODbL,,,346,4782,70,"50,000 scraped tweet metadata from this 2k16 Christmas",Christmas Tweets,https://www.kaggle.com/dhruvm/christmastwitterdata,Mon Dec 26 2016
62,,Team AI,"[Article Date, Article Title, Organization, City, State, URL, Keywords, Summary, ]","[dateTime, dateTime, string, string, string, string, string, string, string]",Context We should definitely stop hate crimes. Let's use data science to stop them. This is mainly classification but any other approach is welcome. Example; prediction for next possible hate crime. Content 3700 rows of CSV from Google Trend. Headline date location URL. Acknowledgements Special thanks to ; http//googletrends.github.io/data/ Inspiration Media data is mainly NLP CSV. We have to come up with other ways to add the value from it.,CSV,,[],CC0,,,285,3348,3,Let's stop hate crimes with the power of data science!,Hate Crime Classification,https://www.kaggle.com/team-ai/classification-of-hate-crime-in-the-us,Sun Aug 20 2017
63,,James Littiebrant,"[qb, att, cmp, yds, ypa, td, int, lg, sack, loss, rate, game_points, home_away, year]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric]",Why? The NFL ESPN and many others have their own Quarterback rating system. Can you create your own? How many points does a QB contribute to a given game? And with MVP trophy season coming up who really stands out as an MVP and who is carried by their team? QB Stats This is scraped from footballdb.com using Pandas' read_html function. This dataset contains every regular season NFL game and every NFL passer (including non-quarterbacks) from 1996 to 2016. Individual years are available for the past 10 years and all 21 years are in QBStats_all. In addition to the traditional stats the total points from the game have been appended to the stats. Win/Loss is up and coming but is not a priority at the moment since a QB cannot control how well the defense stops the opposing offense. Content Inside you'll find  Quarterback Name (qb) Attempts (att) Completions (cmp) Yards (yds) Yards per Attempt (ypa) Touchdowns (td) Interceptions (int) Longest Throw (lg) Sacks (sack) Loss of Yards (loss) The NFL's Quarterback Rating for the game (rate) Total points scored in the game (game_points) Home or Away Game (home_away) Year (year)  Important Note Because of the way that these were scraped the the game week is not supplied. However the games are all in oldest to most recent which would allow for some time-series analysis. Additionally Feel free to make any requests for additional information. But due to the time that it takes to scrape 21 years of NFL stats it will likely take a while before I finish updating the dataset. Acknowledgements I would very much like to thank footballdb.com for not blacklisting me after numerous scrapes and potential future scrapes for information on other positions.,CSV,,"[american football, sports]",ODbL,,,697,4249,1,Over 5000 Regular Season Games,Quarterback Stats from 1996 - 2016,https://www.kaggle.com/speckledpingu/nfl-qb-stats,Thu Jan 12 2017
64,,LouweAL,[AC],[string],This dataset was created to as part of an ongoing research on what information about the human mind could be possibly hidden in word-word associations.  Inspiration Here are a few questions you might try to answer with this dataset  How well can we classify word pairs as originating from a specific online community? What are the properties of the graph/network of word associations? What are frequently used words? Are there differences among communities?  Content The data was scraped from 10 public internet forums. The data is anonymous (all usernames are converted to unique user IDs) and cleaned[script].  Most topics were still active at the time of scraping (june 2017) thus rescraping will result in a (slightly) bigger dataset. The dataset contains 4 columns {author word1 word2 source} where author is the person who wrote the second word as reaction to the first word. A word can also be a phrase or (in some cases) a sentence.,CSV,,"[linguistics, internet]",Other,,,130,1909,8,0.3M word-word associations scraped from 10 internet forums,Wordgame,https://www.kaggle.com/anneloes/wordgame,Fri Jul 21 2017
65,,Federal Bureau of Investigation,"[State/Tribal/Other , Agency, Unit/Office, Total law
enforcement
employees, Total
officers, Total
civilians]","[string, string, string, numeric, numeric, numeric]",Context The Uniform Crime Reporting (UCR) Program has been the starting place for law enforcement executives students of criminal justice researchers members of the media and the public at large seeking information on crime in the nation. The program was conceived in 1929 by the International Association of Chiefs of Police to meet the need for reliable uniform crime statistics for the nation. In 1930 the FBI was tasked with collecting publishing and archiving those statistics. Today four annual publications Crime in the United States National Incident-Based Reporting System Law Enforcement Officers Killed and Assaulted and Hate Crime Statistics are produced from data received from over 18000 city university/college county state tribal and federal law enforcement agencies voluntarily participating in the program. The crime data are submitted either through a state UCR Program or directly to the FBI’s UCR Program. This dataset focuses on the crime rates and law enforcement employment data in the state of California. Content Crime and law enforcement employment rates are separated into individual files focusing on offenses by enforcement agency college/university campus county and city. Categories of crimes reported include violent crime murder and nonnegligent manslaughter rape robbery aggravated assault property crime burglary larceny-theft motor vehicle damage and arson. In the case of rape data is collected for both revised and legacy definitions. In some cases a small number of enforcement agencies switched definition collection sometime within the same year.  Acknowledgements This dataset originates from the FBI UCR project and the complete dataset for all 2015 crime reports can be found here.  Inspiration  What are the most common types of crimes in California? Are there certain crimes that are more common in a particular place category such as a college/university campus compared to the rest of the state? How does the number of law enforcement officers compare to the crime rates of a particular area? Is the ratio similar throughout the state or do certain campuses counties or cities have a differing rate?  How does the legacy vs. refined definition of rape differ and how do the rape counts compare? If you pulled the same data from FBI datasets for previous years can you see a difference in rape rates over time? ,CSV,,"[crime, law]",Other,,,602,4232,0.095703125,Crime and law enforcement employment data from 2015,California Crime and Law Enforcement,https://www.kaggle.com/fbi-us/california-crime,Thu Dec 08 2016
66,,Vered Shwartz,"[bicycle, riding, Bolotta recounted finding 22 sharpened <x>bicycle</x> spokes jabbed into the lawn while she was out with the lawn mower., A lesser known work of Hopper's, 'Bridal Path' shows a horseback <y>riding</y> path in Central Park., other-related, 0.6]","[string, string, string, string, string, numeric]","Context Recognizing lexical inference is an essential component in natural language understanding. In question answering for instance identifying that broadcast and air are synonymous enables answering the question ""When was 'Friends' first aired?"" given the text ""'Friends' was first broadcast in 1994"". Semantic relations such as synonymy (tall high) and hypernymy (cat pet) are used to infer the meaning of one term from another in order to overcome lexical variability. This inference should typically be performed within a given context considering both the term meanings in context and the specific semantic relation that holds between the terms.  Content This dataset provides annotations for fine-grained lexical inferences in-context.  The dataset consists of 3750 term pairs each given within a context sentence built upon a subset of terms from PPDB.  Each term pair is annotated to the semantic relation that holds between the terms in the given contexts. Files  full_dataset.csv - the full dataset is provided as well as the train-test-validation split.  train.csv test.csv validation.csv - A split of the dataset to 70% train 25% test and 5% validation sets. Each of the sets contains different term-pairs to avoid overfitting for the most common relation of a term-pair in the training set.  File Structure comma-separated file Fields  x the first term y the second term context_x the sentence in which x appears (highlighted by x) context_y the sentence in which y appears (highlighted by y) semantic_relation the (directional) semantic relation that holds between x and y equivalence forward_entailment reverse_entailment alternation other-related and independence. confidence the relation annotation confidence (percentage of annotators that selected this relation) in a scale of 0-1  Acknowledgements If you use this dataset please cite the following paper Adding Context to Semantic Data-Driven Paraphrasing. Vered Shwartz and Ido Dagan. *SEM 2016. Inspiration I hope that this dataset will motivate the development of context-sensitive lexical inference methods which have been relatively overlooked although they are crucial for applications.",CSV,,[],GPL,,,31,574,3,Annotations for semantic relations between words within context sentences,Fine-grained Context-sensitive Lexical Inference,https://www.kaggle.com/vered1986/context-lexinf,Sat Aug 12 2017
67,,HugoDarwood,"[age, birth_year, cause_of_death, death_month, death_year, famous_for, name, nationality, fame_score]","[numeric, numeric, string, string, numeric, string, string, string, numeric]",Context I created this dataset to investigate the claim that 2016 had an unnaturally large number of celebrity deaths. Content Points listed by Name Age Cause of death and Reason for fame Acknowledgements Lifted from https//en.wikipedia.org/wiki/Deaths_in_2016 for all years,CSV,,"[celebrity, death]",CC4,,,2488,24689,2,All wikipedia-listed celebrity deaths from 2006,Celebrity Deaths,https://www.kaggle.com/hugodarwood/celebrity-deaths,Sat Jan 14 2017
68,,DaveRosenman,"[feature_name, address, city, state, side, category, group_class, year_dedicated, year_rededicated, latitude, longitude]","[string, string, string, string, string, string, string, string, string, numeric, numeric]","Context The Southern Poverty Law Center maintains a list of publicly supported symbols of the confederacy. It is available here. There data table can be downloaded in multiple formats here Content I cleaned up the data set a bit.  Removed the following columns cartodb_id the_geomfield_1uidsecondary_class_for_internal_use Changed 'Unknown' dates to NA Arranged records by states and by feature_name Removed Holidays Changed year_dedicated for the following feature names     - 'City of Confederate Corners' from '1860's (late)' to '1865'         (sourcehttp//www.amap1.org/images/2008%20Folder/AMAP%20Newsletter%2012-08.pdf ...says '1865-ish' so just an estimate) - 'Confederate Monument La Plaza Del Constitucion' from '~1880' to 1879. Source http//www.drbronsontours.com/bronsonconfederatememorial.html  - Changed 'Confederate Women Fountain (Women of the Sixties)' from 1911-1920 to 1916. Source http//www.arkansaspreservation.com/National-Register-Listings/PDF/PU4770S.nr.pdf  - Changed 'Confederate Monument' 'Carline County Courthouse' from 'Unknown (perhaps 1906 see last link in sources)' to 1906.  source  'A History of Caroline County Virginia' By Marshall Wingfield page 243 (https//books.google.com/books?id=xxVhymOH3usC&pg=PA276&lpg=PA276&dq=%22caroline+county%22+confederate+memorial+1906&source=bl&ots=qyRGTV13Js&sig=ii-kA__3BhZg9WzTaD5VCKHb3b8&hl=en&sa=X&ved=0ahUKEwjq-eqj-uHVAhUKJCYKHU-aAs4Q6AEIUzAM#v=snippet&q=monument&f=false)  - Changed ""To Our Soldiers of the Confederacy"" ""King William Courthouse"" from '1901-1903' to 1903. Source http//docsouth.unc.edu/commland/monument/15/   Added rededicated column. Removed rededicated values from year_dedicated column. Rededicated column also includes     -remodeled replacereopened and relocated monuments.  -readopted flags   Acknowledgements Would like to thank the SPLC for making the dataset that this dataset is based on available and for their interesting and important report on the history of confederate public symbols.",CSV,,[],Other,,,31,536,0.1669921875,"Dataset containing over 1,500 publicly supported symbols of the confederacy.",Publicly Supported Symbols of the Confederacy,https://www.kaggle.com/daverosenman/publicly-supported-symbols-of-the-confederacy,Sat Aug 26 2017
69,,United Nations Development Program,"[GDI Rank, Country, Gender Development Index (GDI), Human Development Index (Female), Human Development Index (Male), Life Expectancy at Birth (Female), Life Expectancy at Birth (Male), Expected Years of Education (Female), Expected Years of Education (Male), Mean Years of Education (Female), Mean Years of Education (Male), Estimated Gross National Income per Capita (Female), Estimated Gross National Income per Capita (Male)]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The Human Development Index (HDI) is a summary measure of achievements in key dimensions of human development a long and healthy life access to knowledge and a decent standard of living. The HDI is the geometric mean of normalized indices for each of the three dimensions. The health dimension is assessed by life expectancy at birth the education dimension is measured by mean of years of education for adults aged 25 years and more and expected years of education for children and the standard of living dimension is measured by gross national income per capita. The Inequality-Adjusted Human Development Index (IHDI) adjusts the HDI for inequality in the distribution of each dimension across the population. The Gender Development Index (GDI) measures gender inequalities in achievement in three basic dimensions of human development health measured by female and male life expectancy at birth; education measured by female and male expected years of education for children and female and male mean years of education for adults ages 25 and older; and command over economic resources measured by female and male estimated earned income. The Gender Inequality Index (GII) reflects gender-based disadvantage in three dimensions—reproductive health empowerment and the labour market—for as many countries as data of reasonable quality allow. It shows the loss in potential human development due to inequality between female and male achievements in these dimensions. The Multidimensional Poverty Index (MPI) identifies multiple deprivations at the household level in education health and standard of living as indicators of poverty. It uses micro data from household surveys and — unlike the IHDI — all the indicators needed to construct the measure must come from the same survey.,CSV,,"[demographics, international relations]",CC4,,,1594,7276,0.263671875,"Countries ranked by human development, gender inequality, and poverty",Human Development Report 2015,https://www.kaggle.com/undp/human-development,Wed Jan 25 2017
70,,ShradhaJoshi,[],[],Context Hey everyone out there! Wikipedia is a publicly available encyclopedia which can be modified by anyone. Some of these modifications are useful whereas some are not. This data set captures all the edits done to English Wikipedia by anyone across the globe. As there are two edits per second the data which I have collected is for just 20 minutes. Content I have revised the original data set removed the duplicates and included only the relevant and useful columns. This data set has below mentioned columns a) action  only edits action is captured. Other actions maybe Talk etc. b) change_size  the number of characters added or deleted. Positive size means the change was added and negative means the change was deleted. c) geo_ip  This is null if the user is registered in Wikipedia otherwise it is a JSON object containing city latitude country_name region_name and longitude d) is_anonymous  This is a flag/boolean value(true/false) that notifies whether the user is registered or unregistered(anonymous) e) is_bot  This flag/boolean value(true/false) determines if the user is a bot(robot) or a human. f) is_minor Thus flag/boolean value(true/false) identifies whether the change made to Wikipedia article was minor or major one. g) page_title  This is the title of the Wikipedia article edited by the user. h) url  This field has the URL or link which compares the Wikipedia article before and after the change. i) user  If the user is unregistered this field will have IP Address either in IPv4 or IPv6 format and if the user is register it will contain the username used when registering on Wikipedia. Acknowledgements I would like to thank hatnote.com from which I could get this data. If you need the original data you may visit www.hatnote.com or directly connect this WebSocket  - ws//wikimon.hatnote.com/en/,CSV,,[],ODbL,,,61,702,0.115234375,Dataset containing list of wikipedia edits over a period of 20 minutes,Wikipedia Edits,https://www.kaggle.com/shradhapj/wikipedia-edits,Mon Aug 21 2017
71,,UCI Machine Learning,"[class, cap-shape, cap-surface, cap-color, bruises, odor, gill-attachment, gill-spacing, gill-size, gill-color, stalk-shape, stalk-root, stalk-surface-above-ring, stalk-surface-below-ring, stalk-color-above-ring, stalk-color-below-ring, veil-type, veil-color, ring-number, ring-type, spore-print-color, population, habitat]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context Although this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago mushroom hunting (otherwise known as ""shrooming"") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be? Content This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible definitely poisonous or of unknown edibility and not recommended. This latter class was combined with the poisonous one. The Guide clearly states that there is no simple rule for determining the edibility of a mushroom; no rule like ""leaflets three let it be'' for Poisonous Oak and Ivy.  Time period Donated to UCI ML 27 April 1987  Inspiration  What types of machine learning models perform best on this dataset? Which features are most indicative of a poisonous mushroom?  Acknowledgements This dataset was originally donated to the UCI Machine Learning repository. You can learn more about past research using the data here.  Start a new kernel",CSV,,[food and drink],CC0,,,14803,103797,0.3564453125,Safe to eat or deadly poison?,Mushroom Classification,https://www.kaggle.com/uciml/mushroom-classification,Fri Dec 02 2016
72,,Miroslav Sabo,"[original, short]","[string, string]",Introduction In 2013 students of the Statistics class at FSEV UK were asked to invite their friends to participate in this survey.  The data file (responses.csv) consists of 1010 rows and 150 columns (139 integer and 11 categorical). For convenience the original variable names were shortened in the data file. See the columns.csv file if you want to match the data with the original names. The data contain missing values. The survey was presented to participants in both electronic and written form. The original questionnaire was in Slovak language and was later translated into English. All participants were of Slovakian nationality aged between 15-30.  The variables can be split into the following groups  Music preferences (19 items) Movie preferences (12 items) Hobbies & interests (32 items) Phobias (10 items) Health habits (3 items) Personality traits views on life & opinions (57 items) Spending habits (7 items) Demographics (10 items)  Research questions Many different techniques can be used to answer many questions e.g.  Clustering Given the music preferences do people make up any clusters of similar behavior? Hypothesis testing Do women fear certain phenomena significantly more than men? Do the left handed people have different interests than right handed? Predictive modeling Can we predict spending habits of a person from his/her interests and movie or music preferences? Dimension reduction Can we describe a large number of human interests by a smaller number of latent concepts? Correlation analysis Are there any connections between music and movie preferences? Visualization How to effectively visualize a lot of variables in order to gain some meaningful insights from the data? (Multivariate) Outlier detection Small number of participants often cheats and randomly answers the questions. Can you identify them? Hint Local outlier factor may help. Missing values analysis Are there any patterns in missing responses? What is the optimal way of imputing the values in surveys? Recommendations If some of user's interests are known can we predict the other? Or if we know what a person listen can we predict which kind of movies he/she might like?  Past research  (in slovak) Sleziak P. - Sabo M. Gender differences in the prevalence of specific phobias. Forum Statisticum Slovacum. 2014 Vol. 10 No. 6. [Differences (gender + whether people lived in village/town) in the prevalence of phobias.]  Sabo Miroslav. Multivariate Statistical Methods with Applications. Diss. Slovak University of Technology in Bratislava 2014. [Clustering of variables (music preferences movie preferences phobias) + Clustering of people w.r.t. their interests.]  Questionnaire MUSIC PREFERENCES  I enjoy listening to music. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I prefer. Slow paced music 1-2-3-4-5 Fast paced music (integer) Dance Disco Funk Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Folk music Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Country Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Classical Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Musicals Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Pop Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Rock Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Metal Hard rock Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Punk Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Hip hop Rap Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Reggae Ska Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Swing Jazz Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Rock n Roll Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Alternative music Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Latin Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Techno Trance Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Opera Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)  MOVIE PREFERENCES  I really enjoy watching movies. Strongly disagree 1-2-3-4-5 Strongly agree (integer) Horror movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Thriller movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Comedies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Romantic movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Sci-fi movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) War movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Tales Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Cartoons Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Documentaries Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Western movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer) Action movies Don't enjoy at all 1-2-3-4-5 Enjoy very much (integer)  HOBBIES & INTERESTS  History Not interested 1-2-3-4-5 Very interested (integer) Psychology Not interested 1-2-3-4-5 Very interested (integer) Politics Not interested 1-2-3-4-5 Very interested (integer) Mathematics Not interested 1-2-3-4-5 Very interested (integer) Physics Not interested 1-2-3-4-5 Very interested (integer) Internet Not interested 1-2-3-4-5 Very interested (integer) PC Software Hardware Not interested 1-2-3-4-5 Very interested (integer) Economy Management Not interested 1-2-3-4-5 Very interested (integer) Biology Not interested 1-2-3-4-5 Very interested (integer) Chemistry Not interested 1-2-3-4-5 Very interested (integer) Poetry reading Not interested 1-2-3-4-5 Very interested (integer) Geography Not interested 1-2-3-4-5 Very interested (integer) Foreign languages Not interested 1-2-3-4-5 Very interested (integer) Medicine Not interested 1-2-3-4-5 Very interested (integer) Law Not interested 1-2-3-4-5 Very interested (integer) Cars Not interested 1-2-3-4-5 Very interested (integer) Art Not interested 1-2-3-4-5 Very interested (integer) Religion Not interested 1-2-3-4-5 Very interested (integer) Outdoor activities Not interested 1-2-3-4-5 Very interested (integer) Dancing Not interested 1-2-3-4-5 Very interested (integer) Playing musical instruments Not interested 1-2-3-4-5 Very interested (integer) Poetry writing Not interested 1-2-3-4-5 Very interested (integer) Sport and leisure activities Not interested 1-2-3-4-5 Very interested (integer) Sport at competitive level Not interested 1-2-3-4-5 Very interested (integer) Gardening Not interested 1-2-3-4-5 Very interested (integer) Celebrity lifestyle Not interested 1-2-3-4-5 Very interested (integer) Shopping Not interested 1-2-3-4-5 Very interested (integer) Science and technology Not interested 1-2-3-4-5 Very interested (integer) Theatre Not interested 1-2-3-4-5 Very interested (integer) Socializing Not interested 1-2-3-4-5 Very interested (integer) Adrenaline sports Not interested 1-2-3-4-5 Very interested (integer) Pets Not interested 1-2-3-4-5 Very interested (integer)  PHOBIAS  Flying Not afraid at all 1-2-3-4-5 Very afraid of (integer) Thunder lightning Not afraid at all 1-2-3-4-5 Very afraid of (integer) Darkness Not afraid at all 1-2-3-4-5 Very afraid of (integer) Heights Not afraid at all 1-2-3-4-5 Very afraid of (integer) Spiders Not afraid at all 1-2-3-4-5 Very afraid of (integer) Snakes Not afraid at all 1-2-3-4-5 Very afraid of (integer) Rats mice Not afraid at all 1-2-3-4-5 Very afraid of (integer) Ageing Not afraid at all 1-2-3-4-5 Very afraid of (integer) Dangerous dogs Not afraid at all 1-2-3-4-5 Very afraid of (integer) Public speaking Not afraid at all 1-2-3-4-5 Very afraid of (integer)  HEALTH HABITS  Smoking habits Never smoked - Tried smoking - Former smoker - Current smoker (categorical) Drinking Never - Social drinker - Drink a lot (categorical) I live a very healthy lifestyle. Strongly disagree 1-2-3-4-5 Strongly agree (integer)  PERSONALITY TRAITS VIEWS ON LIFE & OPINIONS  I take notice of what goes on around me. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I try to do tasks as soon as possible and not leave them until last minute. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always make a list so I don't forget anything. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I often study or work even in my spare time. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I look at things from all different angles before I go ahead. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I believe that bad people will suffer one day and good people will be rewarded. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am reliable at work and always complete all tasks given to me. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always keep my promises. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I can fall for someone very quickly and then completely lose interest. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I would rather have lots of friends than lots of money. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always try to be the funniest one. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I can be two faced sometimes. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I damaged things in the past when angry. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I take my time to make decisions. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always try to vote in elections. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I often think about and regret the decisions I make. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I can tell if people listen to me or not when I talk to them. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am a hypochondriac. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am emphatetic person. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I eat because I have to. I don't enjoy food and eat as fast as I can. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I try to give as much as I can to other people at Christmas. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I don't like seeing animals suffering. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I look after things I have borrowed from others. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I feel lonely in life. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I used to cheat at school. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I worry about my health. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I wish I could change the past because of the things I have done. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I believe in God. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always have good dreams. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always give to charity. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I have lots of friends. Strongly disagree 1-2-3-4-5 Strongly agree (integer) Timekeeping. I am often early. - I am always on time. - I am often running late. (categorical) Do you lie to others? Never. - Only to avoid hurting someone. - Sometimes. - Everytime it suits me. (categorical) I am very patient. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I can quickly adapt to a new environment. Strongly disagree 1-2-3-4-5 Strongly agree (integer) My moods change quickly. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am well mannered and I look after my appearance. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I enjoy meeting new people. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always let other people know about my achievements. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I think carefully before answering any important letters. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I enjoy childrens' company. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am not afraid to give my opinion if I feel strongly about something. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I can get angry very easily. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always make sure I connect with the right people. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I have to be well prepared before public speaking. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I will find a fault in myself if people don't like me. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I cry when I feel down or things don't go the right way. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am 100% happy with my life. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I am always full of life and energy. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I prefer big dangerous dogs to smaller calmer dogs. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I believe all my personality traits are positive. Strongly disagree 1-2-3-4-5 Strongly agree (integer) If I find something the doesn't belong to me I will hand it in. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I find it very difficult to get up in the morning. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I have many different hobbies and interests. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I always listen to my parents' advice. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I enjoy taking part in surveys. Strongly disagree 1-2-3-4-5 Strongly agree (integer) How much time do you spend online? No time at all - Less than an hour a day - Few hours a day - Most of the day (categorical)  SPENDING HABITS  I save all the money I can. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I enjoy going to large shopping centres. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I prefer branded clothing to non branded. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I spend a lot of money on  partying and socializing. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I spend a lot of money on my appearance. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I spend a lot of money on gadgets. Strongly disagree 1-2-3-4-5 Strongly agree (integer) I will hapilly pay more money for good quality or healthy food. Strongly disagree 1-2-3-4-5 Strongly agree (integer)  DEMOGRAPHICS  Age  (integer) Height  (integer) Weight  (integer) How many siblings do you have?  (integer) Gender Female - Male (categorical) I am Left handed - Right handed (categorical) Highest education achieved Currently a Primary school pupil - Primary school - Secondary school - College/Bachelor degree (categorical) I am the only child No - Yes (categorical) I spent most of my childhood in a City - village (categorical) I lived most of my childhood in a house/bungalow - block of flats (categorical) ,CSV,,"[social groups, psychometrics, demographics]",CC0,,,17894,67658,0.4375,"Explore the preferences, interests, habits, opinions, and fears of young people",Young People Survey,https://www.kaggle.com/miroslavsabo/young-people-survey,Tue Dec 06 2016
73,,Global Footprint Network,[],[],Context The ecological footprint measures the ecological assets that a given population requires to produce the natural resources it consumes (including plant-based food and fiber products livestock and fish products timber and other forest products space for urban infrastructure) and to absorb its waste especially carbon emissions. The footprint tracks the use of six categories of productive surface areas cropland grazing land fishing grounds built-up (or urban) land forest area and carbon demand on land. A nation’s biocapacity represents the productivity of its ecological assets including cropland grazing land forest land fishing grounds and built-up land. These areas especially if left unharvested can also absorb much of the waste we generate especially our carbon emissions. Both the ecological footprint and biocapacity are expressed in global hectares — globally comparable standardized hectares with world average productivity. If a population’s ecological footprint exceeds the region’s biocapacity that region runs an ecological deficit. Its demand for the goods and services that its land and seas can provide — fruits and vegetables meat fish wood cotton for clothing and carbon dioxide absorption — exceeds what the region’s ecosystems can renew. A region in ecological deficit meets demand by importing liquidating its own ecological assets (such as overfishing) and/or emitting carbon dioxide into the atmosphere. If a region’s biocapacity exceeds its ecological footprint it has an ecological reserve. Acknowledgements The ecological footprint measure was conceived by Mathis Wackernagel and William Rees at the University of British Columbia. Ecological footprint data was provided by the Global Footprint Network. Inspiration Is your country running an ecological deficit consuming more resources than it can produce per year? Which countries have the greatest ecological deficits or reserves? Do they consume less or produce more than the average country? When will Earth Overshoot Day the day on the calendar when humanity has used one year of natural resources occur in 2017?,CSV,,[ecology],CC4,,,707,5899,0.021484375,Does your country consume more resources than it produces in a year?,2016 Global Ecological Footprint,https://www.kaggle.com/footprintnetwork/ecological-footprint,Thu Mar 02 2017
74,,Allen Institute for Artificial Intelligence,"[questionID, originalQuestionID, totalPossiblePoint, AnswerKey, isMultipleChoiceQuestion, includesDiagram, examName, schoolGrade, year, question, subject, category]","[string, numeric, numeric, string, numeric, numeric, string, numeric, numeric, string, string, string]",Context Project Aristo at the Allen Institute for Artificial Intelligence (AI2) is focused on building a system that acquires and stores a vast amount of knowledge in computable form then applies this knowledge to answer a variety of science questions from standardized exams for students in multiple grade levels. We are inviting the wider AI research community to work on this grand challenge with us by providing this dataset of student science assessment questions. Content These are English language questions that span several grade levels as indicated in the files. Each question is a 4-way multiple choice structure. Some of these questions include a diagram either as part of the question text as an answer option or both. The diagrams are represented in the text with filenames that correspond to the diagram file itself in the companion folder. These questions come pre-split into Train Development and Test sets. The data set includes the following fields  questionID a unique identifier for the question  originalQuestionID the question number on the test totalPossiblePoints how many points the question is worth AnswerKey the correct answer option isMultipleChoiceQuestion 1 = multiple choice 0 = other includesDiagram 1 = includes diagram 0 = other examName the source of the exam schoolGrade grade level year year the source exam was published question the question itself subject Science category Test Train or Dev (data comes pre-split into these categories)  Evaluation AI2 has made available Aristo mini a light-weight question answering system that can quickly evaluate science questions with an evaluation web server and provided baseline solvers. You can extend the provided solvers with your own implementation to try out new approaches and compare results. Acknowledgements The Aristo project team at AI2 compiled this dataset and we use it actively in our research. For a description of the motivations and intention for this data please see Clark Peter. “Elementary School Science and Math Tests as a Driver for AI Take the Aristo Challenge!” AAAI (2015).,CSV,,"[linguistics, artificial intelligence]",CC4,,,323,4755,58,"2,707 multiple choice science questions from student assessments",AI2 Science Questions,https://www.kaggle.com/allenai/ai2-science-questions,Wed Dec 14 2016
75,,LiamLarsen,"[Leading Causes, Year, Age Adjusted Death Rate]","[string, numeric, numeric]","Content Age-adjusted Death Rates for Selected Major Causes of Death United States 1900-2013 Age adjusting rates is a way to make fairer comparisons between groups with different age distributions. For example a county having a higher percentage of elderly people may have a higher rate of death or hospitalization than a county with a younger population merely because the elderly are more likely to die or be hospitalized. (The same distortion can happen when comparing races genders or time periods.) Age adjustment can make the different groups more comparable. A ""standard"" population distribution is used to adjust death and hospitalization rates. The age-adjusted rates are rates that would have existed if the population under study had the same age distribution as the ""standard"" population. Therefore they are summary measures adjusted for differences in age distributions. Acknowledgements Scrap data from data.gov",CSV,,"[death, demographics]",Other,,,620,4590,1,Age-adjusted death rates for the top 10 leading causes of death in the US,Leading Causes of Death in the USA,https://www.kaggle.com/kingburrito666/leading-causes-of-death-usa,Thu Mar 30 2017
76,,Zeeshan-ul-hassan Usmani,"[S#, Teacher Name, University Currently Teaching, Department, Province University Located, Designation, Terminal Degree, Graduated from, Country, Year, Area of Specialization/Research Interests, Other Information]","[numeric, string, string, string, string, string, string, string, string, string, string, string]",Context Pakistan has a large number of public and private universities offering degrees in multiple disciplines. There are 162 universities out of which 64 are in private sector and 98 are public sector/government universities recognized by the Higher Education Commission of Pakistan (HEC).  According to HEC Pakistani universities are producing over half a million graduates per year which include over more than 10000 Computer Science/IT graduates.   From year 2001 to 2015 there is a mass increase in number of enrollment in universities. The recent statistics shows that in 2015 1298600 students enrolled in different levels of degree 869378 in Bachelors (16 years) 63412 in Bachelors (17 years) 219280 in Masters (16 years) 124107 in M.Phil/MS 14373 in Ph.D and 8319 in P.G.D. However in 2014 the number of doctoral degree awarded were 1351 only.  Moreover according to HEC report in 2014-2015 there are over 10125 fulltime Ph.D. faculty teaching in Pakistan in all disciplines. Computer Science and related disciplines are widely taught in Pakistan with over 90 universities offering this discipline with qualified faculty. According to our dataset there are 504 PhD faculty members in Computer Science in Pakistan for 10000 students.  So we have a PhD faculty member for every 20 students on average in computer science program.  Current Student to PhD Professor Ratio in Pakistan is 1301 (while India is going towards 101 in Post-Graduate and 251 in Undergrad education). Here is world's Top 100 universities with Student to Staff Ratio. Content Dataset The dataset contains list of computer science/IT professors from 89 different universities of Pakistan.   Variables The dataset contains Serial No Teacher’s Name University Currently Teaching Department Province University Located Designation Terminal Degree Graduated from (university for professor) Country of graduation Year Area of Specialization/Research Interests and some Other Information Acknowledgements Data has been collected from respective university websites. Some of the universities did not mention about their faculty profiles or were unavailable (hence the limitation of this dataset). The statistics mentioned above are gathered by Higher Education Commission of Pakistan (HEC) website and other web resources. Inspiration Here is what I like you to do  Which area of interest/expertise is in abundance in Pakistan and where we need more people? How many professors we have in Data Sciences Artificial Intelligence or Machine Learning? Which country and university hosted majority of our teachers? Which research areas were most common in Pakistan? How does Pakistan Student to PhD Professor Ratio compare against rest of the world especially with USA India and China? Any visualization and patterns you can generate from this data  Let me know how I can improve this dataset and best of luck with your work,CSV,,"[visual arts, computer science, machine learning]",CC0,,,47,3011,0.3173828125,Teaching Computer Science ,Pakistan Intellectual Capital ,https://www.kaggle.com/zusmani/pakistanintellectualcapitalcs,Mon Dec 25 2017
77,,JorgeZazueta,"[Code, Indicator Name, Long definition, Source]","[string, string, string, string]","Context PISA stands for ""Program for International Student Assessment"" and it is applied to 15 year-old students across the world to assess their performance in Math Reading and Science. These are the 2015 scores. Content The dataset contains mean (Pisa) attainment scores in math reading and science by country and gender. Acknowledgements Queried from the World Bank Learning outcomes database http//datatopics.worldbank.org/education/wDataQuery/QLearning.aspx Inspiration How attainment compares by country? Why some perform better than others? Can Pisa scores predict social environments such as freedom of press?",CSV,,[education],Other,,,209,1819,0.109375,Program for International Student Assessment mean scores (Pisa) from 2015,Pisa Scores,https://www.kaggle.com/zazueta/pisa-scores-2015,Wed May 03 2017
78,,Parole Hearing Data Project,"[parole board interview date, din, scrape date, nysid, sex, birth date, race / ethnicity, housing or interview facility, parole board interview type, interview decision, year of entry, aggregated minimum sentence, aggregated maximum sentence, release date, release type, housing/release facility, parole eligibility date, conditional release date, maximum expiration date, parole me date, post release supervision me date, parole board discharge date, crime 1 - crime of conviction, crime 1 - class, crime 1 - county of commitment, crime 2 - crime of conviction, crime 2 - class, crime 2 - county of commitment, crime 3 - crime of conviction, crime 3 - class, crime 3 - county of commitment, crime 4 - crime of conviction, crime 4 - class, crime 4 - county of commitment, crime 5 - crime of conviction, crime 5 - class, crime 5 - county of commitment, crime 6 - crime of conviction, crime 6 - class, crime 6 - county of commitment, crime 7 - crime of conviction, crime 7 - class, crime 7 - county of commitment, crime 8 - crime of conviction, crime 8 - class, crime 8 - county of commitment]","[string, string, dateTime, string, string, dateTime, string, string, string, string, numeric, dateTime, string, dateTime, string, string, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context In New York over 10000 parole eligible prisoners are denied release every year and while the consequences of these decisions are costly (at $60000 annually to incarcerate one individual and more to incarcerate older individuals with illnesses) the process of how these determinations are made is unclear.  Advocates for parole reform argue that parole commissioners too often base their decisions on ""the nature of the crime"" for which the individual was convicted rather than on that individual's accomplishments and growth while serving  a sentence in prison. The Parole Hearing Data Project is part of a broader body of work that can be found on the Museum of the American Prison's website.   Content Dataset includes sex race / ethnicity housing or interview facility parole board interview type and interview decision among other factors. Scraping is up-to-date as of July 2016.  Descriptions provided by the Department of Corrections and Community Service (DOCCS) can be found here. Acknowledgements Data was collected and managed by Nikki Zeichner Rebecca Ackerman and John Krauss. Original dataset including a scraper to gather the latest updates can be found here. Inspiration  Does the housing or interview facility play a role in the parole decision? Are any of these facilities particularly likely to influence a positive decision? Do interview decisions vary based on race / ethnicity? Sex? What is the typical time between entry and release? ",CSV,,"[crime, law]",Other,,,156,1848,9,Scraped list of parole hearings between 2014-2016,Parole hearings in New York State,https://www.kaggle.com/parole-hearing-data/parole-hearings-in-new-york-state,Thu Dec 08 2016
79,,People HR Analytics Repository,"[EmployeeID, recorddate_key, birthdate_key, orighiredate_key, terminationdate_key, age, length_of_service, city_name, department_name, job_title, store_name, gender_short, gender_full, termreason_desc, termtype_desc, STATUS_YEAR, STATUS, BUSINESS_UNIT]","[numeric, dateTime, dateTime, dateTime, dateTime, numeric, numeric, string, string, string, numeric, string, string, string, string, numeric, string, string]",Context This data was originally posted on my personal oneDrive account. It represent fictitious/fake data on terminations. For each of 10 years it show employees that are active and those that terminated. The intent is to see if  individual terminations can be predicted from the data provided. The thing to be predicted is status of active or terminated Content The data contains employee id employee record date ( year of data) birth date hire date termination date age length of service city department job title  store number gender termination reason termination type status year status business unit These might be typical types of data in hris Acknowledgements None- its fake data Inspiration A lot of turnover analyses occur at an aggregate level-such as turnover rates. But few analyses concentrate on trying to identify exactly which individuals might leave based on  patterns that might be present in existing data. Machine learning algorithms often showcase customer churn examples for telcos or product marketing. Those algorithms equally apply to employee churn.,CSV,,"[employment, business]",CC0,,,2444,22132,7,Can you forecast employee attrition?,Employee Attrition,https://www.kaggle.com/HRAnalyticRepository/employee-attrition-data,Thu Apr 27 2017
80,,Megan Risdal,"[text, favorited, favoriteCount, replyToSN, created, truncated, replyToSID, id, replyToUID, statusSource, screenName, retweetCount, isRetweet, retweeted, longitude, latitude]","[string, boolean, numeric, string, dateTime, boolean, string, numeric, string, string, string, numeric, boolean, boolean, string, string]",Context I was looking for something Ben Hamner Kaggle's CTO tweeted a while back and it turned out just using R's TwitteR package was easier than scrolling through his timeline. Since I collected all of his tweets I figured I would share them here as well. Content What you get All of Ben Hamner's tweets current through today (12 December 2017). What's inside The text from his tweets plus metadata like favorites retweets timestamps etc. You can even see whether or not I've personally favorited or retweeted his tweets. Acknowledgements Thanks to Ben for his insightful tweets! Check out his tweets at @benhamner on Twitter.,CSV,,"[linguistics, twitter, internet]",Other,,,10,322,0.7724609375,A complete Twitter timeline from Kaggle's CTO,Ben Hamner's Tweets,https://www.kaggle.com/mrisdal/ben-hamners-tweets,Tue Dec 12 2017
81,,The Guardian,"[uid, name, age, gender, raceethnicity, armed, month, day, year, streetaddress, city, state, latitude, longitude, classification, lawenforcementagency]","[numeric, string, numeric, boolean, string, string, dateTime, numeric, numeric, string, string, string, string, string, string, string]",The Counted is a project by the Guardian – and you – working to count the number of people killed by police and other law enforcement agencies in the United States throughout 2015 and 2016 to monitor their demographics and to tell the stories of how they died. The database will combine Guardian reporting with verified crowdsourced information to build a more comprehensive record of such fatalities. The Counted is the most thorough public accounting for deadly use of force in the US but it will operate as an imperfect work in progress – and will be updated by Guardian reporters and interactive journalists frequently. Any deaths arising directly from encounters with law enforcement will be included in the database. This will inevitably include but will likely not be limited to people who were shot tasered and struck by police vehicles as well those who died in police custody. Self-inflicted deaths during encounters with law enforcement or in police custody or detention facilities will not be included. The US government has no comprehensive record of the number of people killed by law enforcement. This lack of basic data has been glaring amid the protests riots and worldwide debate set in motion by the fatal police shooting of Michael Brown in August 2014. The Guardian agrees with those analysts campaign groups activists and authorities who argue that such accounting is a prerequisite for an informed public discussion about the use of force by police. Contributions of any information that may improve the quality of our data will be greatly welcomed as we work toward better accountability. Please contact us at thecounted@theguardian.com. CREDITS Research and Reporting Jon Swaine Oliver Laughland Jamiles Lartey Design and Production Kenan Davis Rich Harris Nadja Popovich Kenton Powell,CSV,,[crime],CC4,,,766,5453,0.3251953125,Use of deadly force by police officers in United States,"The Counted: Killed by Police, 2015-2016",https://www.kaggle.com/the-guardian/the-counted,Sat Jan 07 2017
82,,Crowdflower,"[_unit_id, _golden, _unit_state, _trusted_judgments, _last_judgment_at, audience, audience:confidence, bias, bias:confidence, message, message:confidence, orig__golden, audience_gold, bias_gold, bioid, embed, id, label, message_gold, source, text]","[numeric, string, string, numeric, dateTime, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, string, string, numeric, string, numeric, string, string]","This dataset from Crowdflower's Data For Everyone Library provides text of 5000 messages from politicians' social media accounts along with human judgments about the purpose partisanship and audience of the messages. How was it collected? Contributors looked at thousands of social media messages from US Senators and other American politicians to classify their content. Messages were broken down into audience (national or the tweeter’s constituency) bias (neutral/bipartisan or biased/partisan) and finally tagged as the actual substance of the message itself (options ranged from informational announcement of a media appearance an attack on another candidate etc.) Acknowledgments Data was provided by the Data For Everyone Library on Crowdflower. Our Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They're available free of charge for the community forever. Inspiration Here are a couple of questions you can explore with this dataset  what words predict partisan v. neutral messages? what words predict support messages v. attack messages? do politicians use Twitter and Facebook for different purposes? (e.g. Twitter for attack messages Facebook for policy messages)?  The Data The dataset contains one file with the following fields  _unit_id a unique id for the message _golden always FALSE; (presumably whether the message was in Crowdflower's gold standard) _unit_state always ""finalized"" _trusted_judgments the number of trusted human judgments that were entered for this message; an integer between 1 and 3 _last_judgment_at when the final judgment was collected audience one of national or constituency audienceconfidence a measure of confidence in the audience judgment; a float between 0.5 and 1 bias one of neutral or partisan biasconfidence a measure of confidence in the bias judgment; a float between 0.5 and 1 message the aim of the message. one of -- attack the message attacks another politician  -- constituency the message discusses the politician's constituency  -- information an informational message about news in government or the wider U.S.  -- media a message about interaction with the media  -- mobilization a message intended to mobilize supporters  -- other a catch-all category for messages that don't fit into the other  -- personal a personal message usually expressing sympathy support or condolences or other personal opinions  -- policy a message about political policy  -- support a message of political support   messageconfidence a measure of confidence in the message judgment; a float between 0.5 and 1 orig__golden always empty; presumably whether some portion of the message was in the gold standard audience_gold always empty; presumably whether the audience response was in the gold standard bias_gold always empty; presumably whether the bias response was in the gold standard bioid a unique id for the politician embed HTML code to embed this message id unique id for the message WITHIN whichever social media site it was pulled from label a string of the form ""From firstname lastname (position from state)"" message_gold always blank; presumably whether the message response was in the gold standard source where the message was posted; one of ""facebook"" or ""twitter"" text the text of the message ",CSV,,"[politics, internet]",CC0,,,505,4948,4,"Classify partisan bias, audience, and goal based on politicians' social media",Political Social Media Posts,https://www.kaggle.com/crowdflower/political-social-media-posts,Mon Nov 21 2016
83,,alopez247,[],[],"Context With the rise of the popularity of machine learning this is a good opportunity to share a wide database of the even more popular video-game Pokémon by Nintendo Game freak and Creatures originally released in 1996. Pokémon started as a Role Playing Game (RPG) but due to its increasing popularity its owners ended up producing many TV series manga comics and so on as well as other types of video-games (like the famous Pokémon Go!). This dataset is focused on the stats and features of the Pokémon in the RPGs. Until now (08/01/2017) seven generations of Pokémon have been published. All in all this dataset does not include the data corresponding to the last generation since 1) I created the databased when the seventh generation was not released yet and 2) this database is a modification+extension of the database ""721 Pokemon with stats"" by Alberto Barradas (https//www.kaggle.com/abcsds/pokemon) which does not include (of course) the latest generation either. Content This database includes 21 variables per each of the 721 Pokémon of the first six generations plus the Pokémon ID and its name. These variables are briefly described next  Number. Pokémon ID in the Pokédex. Name. Name of the Pokémon. Type_1. Primary type. Type_2. Second type in case the Pokémon has it. Total. Sum of all the base stats (Health Points Attack Defense Special Attack Special Defense and Speed).  HP. Base Health Points. Attack. Base Attack.   Defense. Base Defense. Sp_Atk. Base Special Attack. Sp_Def. Base Special Defense. Speed. Base Speed. Generation. Number of the generation when the Pokémon was introduced. isLegendary. Boolean that indicates whether the Pokémon is Legendary or not. Color. Color of the Pokémon according to the Pokédex. hasGender. Boolean that indicates if the Pokémon can be classified as female or male. Pr_male. In case the Pokémon has Gender the probability of its being male. The probability of being female is of course 1 minus this value. Egg_Group_1. Egg Group of the Pokémon. Egg_Group_2. Second Egg Group of the Pokémon in case it has two. hasMegaEvolution. Boolean that indicates whether the Pokémon is able to Mega-evolve or not. Height_m. Height of the Pokémon in meters. Weight_kg. Weight of the Pokémon in kilograms. Catch_Rate. Catch Rate. Body_Style. Body Style of the Pokémon according to the Pokédex.  Notes Please note that many Pokémon are multi-form and also some of them can Mega-evolve. I wanted to keep the structure of the dataset as simple and general as possible as well as the Number variable (the ID of the Pokémon) unique. Hence  in the cases of the multi-form Pokémon or the ones capable of Mega-evolve I just chose one of the forms the one I (and my brother) considered the standard and/or the most common. The specific choice for each of this Pokémon are shown below  Mega-Evolutions are not considered as Pokémon. Kyogre Groudon. Primal forms not considered. Deoxis. Only normal form considered. Wormadam. Only plant form considered. Rotom. Only normal form considered the one with types Electric and Ghost. Giratina. Origin form considered. Shaymin. Land form considered. Darmanitan. Standard mode considered. Tornadus Thundurus Landorus. Incarnate form considered. Kyurem. Normal form considered not white or black forms. Meloetta. Aria form considered. Mewstic. Both female and male forms are equal in the considered variables. Aegislash. Shield form considered. Pumpkaboo Gourgeist. Average size considered. Zygarde. 50% form considered. Hoopa. Confined form considered.  Acknowledgements As said at the beginning this database was based on the Kaggle database  ""721 Pokemon with stats"" by Alberto Barradas (https//www.kaggle.com/abcsds/pokemon). The other resources I mainly used are listed below  WikiDex (http//es.pokemon.wikia.com/wiki/WikiDex). Bulbapedia the community driven Pokémon encyclopedia (http//bulbapedia.bulbagarden.net/wiki/Main_Page). Smogon University (http//www.smogon.com/).  Possible future work This dataset can be used with different objectives such as Pokémon clustering trying to find relations or dependencies between the variables and also for supervised classification purposes where the class could be the Primary Type but also many of the other variables. Author Asier López Zorrilla",CSV,,[video games],CC4,,,2260,69645,0.78125,(Almost) all Pokémon stats until generation 6: 21 variables per Pokémon,Pokémon for Data Mining and Machine Learning,https://www.kaggle.com/alopez247/pokemon,Sun Mar 05 2017
84,,Stack Overflow,"[Id, OwnerUserId, CreationDate, ParentId, Score, Body]","[numeric, numeric, dateTime, numeric, numeric, string]",Full text of questions and answers from Cross Validated the statistics and machine learning Q&A site from the Stack Exchange network. This is organized as three tables  Questions contains the title body creation date score and owner ID for each question. Answers contains the body creation date score and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table. Tags contains the tags on each question  For space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 19 October 2016 (UTC). License All Stack Exchange user contributions are licensed under CC-BY-SA 3.0 with attribution required.,CSV,,"[statistics, internet]",Other,,,276,4258,453,"Full text of Q&A from Cross Validated, the Stack Exchange statistics site",Questions from Cross Validated Stack Exchange,https://www.kaggle.com/stackoverflow/statsquestions,Fri Oct 21 2016
85,,UCI Machine Learning,[],[],The Iris dataset was used in R.A. Fisher's classic 1936 paper The Use of Multiple Measurements in Taxonomic Problems and can also be found on the UCI Machine Learning Repository. It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two but the other two are not linearly separable from each other. The columns in this dataset are  Id SepalLengthCm SepalWidthCm PetalLengthCm PetalWidthCm Species  ,SQLite,,[botany],CC0,,,24458,162947,0.0146484375,Classify iris plants into three species in this classic dataset,Iris Species,https://www.kaggle.com/uciml/iris,Tue Sep 27 2016
86,,Megan Risdal,"[Line, Speaker, Text, Date]","[numeric, string, string, dateTime]","Context In November the United States will elect a new president. Before then three presidential debates will take place between Hillary Clinton and Donald Trump as well as one vice presidential debate between Tim Kaine and Mike Pence. While you can watch the debates live why not also read deeply into the candidates' responses with text analytics? You can now answer any questions you have about the platforms of our presidential hopefuls or their speaking skills.  Which candidate is the most given to loquaciousness?  How many times does Clinton get interrupted? Who gets the most audience applause? When is positive sentiment at its highest during the candidates' word play?  Content & Acknowledgements For consistency full transcripts of the debates were all taken from The Washington Post who made annotated transcripts available following each debate  First debate taking place September 26th 2016 was obtained from The Washington Post.  The vice presidential debate from October 4th 2016 was similarly obtained here. The ""town hall"" presidential debate on October 9th is found here. The final presidential debate taking place on October 19th is found here.   Please make any dataset suggestions or requests on the forum. Word cloud from Debate Visualization by Jane Yu.",CSV,,"[politics, political science]",CC0,,,3047,29359,0.357421875,Full transcripts of the face-off between Clinton & Trump,2016 US Presidential Debates,https://www.kaggle.com/mrisdal/2016-us-presidential-debates,Mon Oct 24 2016
87,,leigh,"[time, x, y, cf, realx, realy]","[numeric, numeric, numeric, numeric, numeric, numeric]",Context Global Positioning Systems (GPS) available in many consumer products such as mobile phones has mostly solved the problem of navigation but remains a challenge for indoor locations. A possible solution exists with 802.11 wireless networks and Location Based Services (LBS) that are able to compute location of a Wireless Station (WS) using triangulation of telemetry such as Receiver Signal Strength Indicators (RSSI) from nearby Wireless Access Points (WAP). The WS coordinates have inaccuracies due to it being “a function of distance geometry and materials” (Mengual Marbán & Eibe 2010) making distance travelled calculation inaccurate. In an experiment plotting a moving workstation the estimated distanced travelled was 483 metres compared to the actual distance of 149 metres (322% difference). Content The LBS system receives data from the wireless network computes location information for each workstation and stores the data for later retrieval. The data can be sourced from the LBS using an REST API that returns JSON formatted data. To enable comparison to the estimated calculation a controlled experiment with a wireless station moving to 20 known locations and turning on the wireless interface for 90 sec periods at a time was conducted. The continual stream of coordinates from the LBS can change not only due to the WS physically moving but also due to the errors in the location calculation itself. These errors can be significant and render any distance calculation meaningless. The experiment captured the calculated position from the wireless network and the actual measured x and y coordinates of a workstation in 20 locations in an office building.  The challenge is to figure out using the wireless location ways to improve the accuracy of the prediction.  Field Name  Description time - Conversion of Singapore time to to seconds from 000000  x - x axis coordinates of floor map in feet y - y axis coordinates of floor map origin is top left of floor map cf - 95% confidence in feet of radius away from x and y client likely to be. realx - x axis measured coordinates of the real location of the test subject realy - x axis measured coordinates of the real location of the test subject Inspiration The distance travelled by the workstation was 149 mtrs. How close can you get to this calculation used the predicted locations ?,CSV,,[],Other,,,51,877,0.0029296875,Figure out a way to navigate accurately with your wireless network data,Internal Navigation Dataset,https://www.kaggle.com/ljewell/internal-navigation-dataset,Fri Aug 11 2017
88,,Stanford University,[],[],Context The Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset consisting of questions posed by crowdworkers on a set of Wikipedia articles. The answer to every question is a segment of text or span from the corresponding reading passage. There are 100000+ question-answer pairs on 500+ articles. Content There are two files to help you get started with the dataset and evaluate your models   *train-v1.1.json*fasdf dev-v1.1.json  Acknowledgements The original datasets can be found here. Inspiration  Can you build a prediction model that can accurately predict answers to different types of questions? You can also explore SQuAD here ,{}JSON,,"[languages, linguistics]",CC4,,,891,12819,34,"New Reading Comprehension Dataset on 100,000+ Question-Answer Pairs",Stanford Question Answering Dataset,https://www.kaggle.com/stanfordu/stanford-question-answering-dataset,Tue Nov 15 2016
89,,The Washington Post,"[table, field name, description]","[string, string, string]","Context In early 2016 The Washington Post wrote that the Justice Department is ""resuming a controversial practice that allows local police departments to funnel a large portion of assets seized from citizens into their own coffers under federal law. The ""Equitable Sharing Program"" gives police the option of prosecuting some asset forfeiture cases under federal instead of state law particularly in instances where local law enforcement officers have a relationship with federal authorities as part of a joint task force. Federal forfeiture policies are more permissive than many state policies allowing police to keep up to 80 percent of assets they seize."" (link to the full article can be found here). This is the raw data from the Department of Justice’s Equitable Sharing Agreement and Certification forms that was released by the U.S. Department of Justice Asset Forfeiture and Money Laundering Section. Content  spending_master.csv is the main spending dataset that contains 58 variables.  notes.csv lists the descriptions for all variables.  Acknowledgements The original dataset can be found here. The data was originally obtained from a Freedom of Information Act request fulfilled in December 2014. Inspiration  Which agency/sector/item received the most amount of funds from the Justice Department? How many agencies received non-cash assets from the federal government through Equitable Sharing?  Are there any trends in the total equitable sharing fund across agencies? ",CSV,,[finance],Other,,,98,1883,10,Raw Data from the Controversial Equitable Sharing Program,Equitable Sharing Spending Dataset,https://www.kaggle.com/washingtonpost/equitable-sharing-spending-dataset,Sat Dec 03 2016
90,,NASA,"[name, id, nametype, recclass, mass, fall, year, reclat, reclong, GeoLocation]","[string, numeric, string, string, numeric, string, numeric, numeric, numeric, string]","The Meteoritical Society collects data on meteorites that have fallen to Earth from outer space. This dataset includes the location mass composition and fall year for over 45000 meteorites that have struck our planet. Notes on missing or incorrect data points   a few entries here contain date information that was incorrectly parsed into the NASA database. As a spot check any date that is before 860 CE or after 2016 are incorrect; these should actually be BCE years. There may be other errors and we are looking for a way to identify them. a few entries have latitude and longitude of 0N/0E (off the western coast of Africa where it would be quite difficult to recover meteorites). Many of these were actually discovered in Antarctica but exact coordinates were not given. 0N/0E locations should probably be treated as NA.  The starter kernel for this dataset has a quick way to filter out these observations using dplyr in R provided here for convenience meteorites.geo <- meteorites.all %>%    filter(year>=860 & year<=2016) %>%  # filter out weird years    filter(reclong<=180 & reclong>=-180 & (reclat!=0 | reclong!=0))  # filter out weird locations   The Data Note that a few column names start with ""rec"" (e.g. recclass reclat reclon). These are the recommended values of these variables according to The Meteoritical Society. In some cases there were historical reclassification of a meteorite or small changes in the data on where it was recovered; this dataset gives the currently recommended values. The dataset contains the following variables    name the name of the meteorite (typically a location often modified with a number year composition etc) id a unique identifier for the meteorite  nametype one of  -- valid a typical meteorite  -- relict a meteorite that has been highly degraded by weather on Earth   recclass the class of the meteorite; one of a large number of classes based on physical chemical and other characteristics (see the Wikipedia article on meteorite classification for a primer) mass the mass of the meteorite in grams fall whether the meteorite was seen falling or was discovered after its impact; one of  -- Fell the meteorite's fall was observed  -- Found the meteorite's fall was not observed   year the year the meteorite fell or the year it was found (depending on the value of fell) reclat the latitude of the meteorite's landing reclong the longitude of the meteorite's landing GeoLocation a parentheses-enclose comma-separated tuple that combines reclat and reclong  What can we do with this data? Here are a couple of thoughts on questions to ask and ways to look at this data  how does the geographical distribution of observed falls differ from that of found meteorites? -- this would be great overlaid on a cartogram or alongside a high-resolution population density map are there any geographical differences or differences over time in the class of meteorites that have fallen to Earth?  Acknowledgements This dataset was downloaded from NASA's Data Portal and is based on The Meteoritical Society's Meteoritical Bulletin Database (this latter database provides additional information such as meteorite images links to primary sources etc.).",CSV,,"[astronomy, space]",CC0,,,2227,18222,4,Data on over 45k meteorites that have struck Earth,Meteorite Landings,https://www.kaggle.com/nasa/meteorite-landings,Sat Nov 05 2016
91,,Stack Overflow,"[Id, OwnerUserId, CreationDate, ParentId, Score, Body]","[numeric, numeric, dateTime, numeric, numeric, string]",Dataset with the text of 10% of questions and answers from the Stack Overflow programming Q&A website. This is organized as three tables  Questions contains the title body creation date closed date (if applicable) score and owner ID for all non-deleted Stack Overflow questions whose Id is a multiple of 10. Answers contains the body creation date score and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table. Tags contains the tags on each of these questions  Datasets of all R questions and all Python questions are also available on Kaggle but this dataset is especially useful for analyses that span many languages. Example projects include  Identifying tags from question text Predicting whether questions will be upvoted downvoted or closed based on their text Predicting how long questions will take to answer  License All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.,CSV,,"[internet, programming languages]",Other,,,1152,10310,3072,Text from 10% of Stack Overflow questions and answers on programming topics,StackSample: 10% of Stack Overflow Q&A,https://www.kaggle.com/stackoverflow/stacksample,Fri Oct 21 2016
92,,LiamLarsen,"[id, created_at, text]","[numeric, dateTime, string]",Content  tweet id contains tweet-stamp date + time date and time of day (24hr) tweet text text of tweet remove 'b'  usage What's someone going to do with a bunch of tweets?  Maybe someone would want to generate text using this dataset or do sentiment analysis Or find out the most likely time of day Elon would tweet. pie his tweets per month ITS DATA!!  Either way its up to you! Inspiration ,CSV,,"[celebrity, technology forecasting, internet]",Other,,,207,2737,0.3837890625,All Elon Musk Tweets from 2010 to 2017,"Elon Musk Tweets, 2010 to 2017",https://www.kaggle.com/kingburrito666/elon-musk-tweets,Sun Apr 23 2017
93,,dgoke1,"[Y, filename, sex, age, age_years, corpus, group, child_TNW, child_TNS, examiner_TNW, freq_ttr, r_2_i_verbs, mor_words, num_pos_tags, n_dos, repetition, retracing, fillers, s_1g_ppl, s_2g_ppl, s_3g_ppl, d_1g_ppl, d_2g_ppl, d_3g_ppl, z_mlu_sli, z_mlu_td, z_word_errors_sli, z_word_errors_td, z_r_2_i_verbs_sli, z_r_2_i_verbs_td, z_utts_sli, z_utts_td, total_syl, average_syl, mlu_words, mlu_morphemes, mlu100_utts, verb_utt, dss, ipsyn_total, present_progressive, propositions_in, propositions_on, plural_s, irregular_past_tense, possessive_s, uncontractible_copula, articles, regular_past_ed, regular_3rd_person_s, irregular_3rd_person, uncontractible_aux, contractible_copula, contractible_aux, word_errors, f_k, n_v, n_aux, n_3s_v, det_n_pl, det_pl_n, pro_aux, pro_3s_v, total_error]","[numeric, string, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Specific Language Impairment  is a condition that effects roughly 7% of 5-year old children. It is characterised by a lack of language ability in comparison to your peers but with no obvious mental or physical disability. Diagnosis can tend to be laborious thus automating this process using NLP and ML techniques might be of interest to paediatricians and speech pathologists.  Content This study evaluated three datasets obtained via the CHILDES project. All the datasets consist of narratives from a child attempting to complete a wordless picture task. The choice to use only narrative corpora was based on previous research which indicated it has the best ability to distinguish a language impairment in children. The first dataset consists of samples from British adolescents the second from Canadian children aged 4 to 9 and the third from U.S. children aged 4 to 12.  Unfortunately finding transcript data of this kind is rare I have tried to find more data to no avail so 1163 samples will have to do. Conti-Ramsden 4 The Conti-Ramsden 4 dataset was collected for a study to assess the effectiveness of narrative tests on adolescents. It consists of 99 TD and 19 SLI samples of children between the ages of 13.10 and 15.90. Ideally all the corpora would only be from children as SLI is most prominent in children aged five years old and is best detected early. However it was included to enable a direct comparison between classifiers created by Gabani and this study. The corpus contains transcripts of a story telling task based on Mayer’s wordless picture book “Frog Where Are You”. The children first viewed the picture book in their own time before being prompted to retell the story in the past tense. If the children started telling the story in the present tense the interviewer would prompt them with the phrase “What happened next?” in order to attempt to revert them back to the past tense. If they failed to start to retell the story in the past tense after two prompts no further prompts were made. ENNI The ENNI dataset was collected during the course of a study aimed at identifying SLI children using an index of storytelling ability based on the story grammar model. The corpus consists of 300 TD and 77 SLI samples of children aged between 4 and 9 years old. Each child was presented with two wordless picture stories with one more complicated than the other. Unlike Conti-Ramsden 4 the examiner held the book and turned the page after the child appeared to be finished telling the story for a particular picture. The children were also given the opportunity to practice on a training story where the examiner gave more explicit prompts to the child about what to do. Gillam The Gillam dataset is based on another tool for narrative assessment known as “The Test of Narrative Language (TNL). It consists of 250 language impaired children and 520 controls aged between 5 and 12. A detailed description of each of the participants does not exist. The TNL consists of four storytelling tasks the first is a recall of a script based story the rest being wordless picture books. The first picture set depicts a story with a main protagonist having repeated attempts at the goal and the rest are single picture stories. The single picture stories require more input from the child and thus is better suited to older children. The TNL appears to be intermediary in difficulty compared to ENNI. Features Attribute | Name | Description   Y                         | The label  | 0 for typically developing children 1 for language impaired  child_TNW         | Total Number of Words | The total number of words in the transcript  child_TNS          | Total Number of Sentences | Children with SLI are more likely to speak in short sentences  group                 | Same as Y | BEWARE Is the same as Y but easier to graph in Python and R  examiner_TNW | Total Number of Words spoken by the examiner | Children with SLI are more likely to need support  freq_ttr                     | Frequency of Word Types to Word Token Ratio | Divides word types by word tokens and provides a rough measure of lexical diversity.  r_2_i_verbs| Ratio of raw to inflected verbs | Children with SLI often have difficulty with the morphemes -ed -s be and do. This results in the use of raw verbs instead of their inflected forms.  mor_words | Number of words in the %mor tier |  num_pos_tags | Number of different Part-of-Speech tags |  n_dos | Number of Do's | The number of time the word 'do' is used  repetition | Number of Repetitions  | Counts the number of repetitions as tagged in the CHAT format inside square brackets e.g. milk milk milk milk = milk [x 4]  retracing | Number of Retracings | A retracing is defined as when a speaker abandons an utterance but then continues again.  fillers | Number of Fillers | Counts the number of fillers used in total. A list of fillers was created by searching through the entire corpus (all 1038 samples) for all common variants of fillers such as um umm uh uhh etc.  s_1g_ppl | Perplexity of 1-gram SLI | The perplexity of this sample in comparison to a language model trained on all the SLI group for this corpora except the sample  s_2g_ppl | Perplexity of 2-gram SLI | Same as above but with a 2-gram LM  s_3g_ppl | Perplexity of 3-gram SLI | Same as above but with a 3-gram LM  d_1g_ppl | Perplexity of 1-gram TD | The perplexity of this sample in comparison to a language model trained on all the TD group for this corpora except the sample  d_2g_ppl | Perplexity of 2-gram TD | Same as above but with a 2-gram LM  d_3g_ppl | Perplexity of 3-gram TD | Same as above but with a 3-gram LM  z_mlu_sli | Sample Z-score using SLI group's Mean Length of Utterance |  z_mlu_td | Sample Z-score using TD group's Mean Length of Utterance |  z_ndw_sli | Sample Z-score using SLI group's RawInflected Verbs Ratio  |  z_ndw_td | Sample Z-score using TD group's RawInflected Verbs Ratio |  z_ipsyn_sli | Sample Z-score using SLI group's Developmental Sentence Score |  z_ipsyn_td | Sample Z-score using TD group's Developmental Sentence Score |  z_utts_sli |Sample Z-score using SLI group's Number of Verb Utterances |  z_utts_td |Sample Z-score using TD group's Number of Verb Utterances |  total_syl |  Total number of Syllables | Using a technique from  average_syl | Average number of Syllables per word |  mlu_words | Mean Length of Utterance of Words  |  mlu_morphemes |  Mean Length of Utterance of Morphemes |  mlu100_utts | Mean Length of Utterance of 1st 100 words  |  verb_utt | Number of verb utterances |  dss | Developmental Sentence Score |  ipsyn_total | Index of Productive Syntax Score |   The following fields are counts of instances of Brown's Stages of Morphological Development (see https//www.speech-language-therapy.com/index.php?option=com_content&view=article&id=33brown&catid=2uncategorised&Itemid=117)    present_progressive  propositions_in  propositions_on  plural_s  irregular_past_tense  possessive_s  uncontractible_copula  articles  regular_past_ed  regular_3rd_person_s  irregular_3rd_person  uncontractible_aux  contractible_copula  contractible_aux  Back to normal      word_errors |  Number of Word Errors | As marked in the transcripts  f_k | Flesch-Kincaid Score | See https//en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests  n_v | Number of Nouns followed immediately by a verb  n_aux | Number of Nouns followed immediately by an Auxillary verb  n_3s_v | Number of Third Singular Nouns followed immediately by a verb  det_n_pl | * Number of Determinant Nouns followed by a Personal Pronoun*  det_pl_n | * Number of Determinant Pronouns followed by a Noun  pro_aux | * Pronouns followed by Auxillary Verb*  pro_3s_v | * 3rd. singular nominative pronoun followed by Verb*  total_error | Total number of morphosyntactic errors | Sum of the columns from nouns verbs down       This table will take some time to finish will get to it within a few days Past Research I have spent the last few months playing around with this data I have uploaded it here mainly to speed up the computation of the analysis I have already done. But I'm excited to see what other people can do with this. There are some nice graphs to be made; especially using the MLU attributes. Thus far using the combined corpora the best I have managed to get in terms of creating a predictive classifier is using Neural Networks with Feature Extraction and SMOTE to get a mean ROC of 0.8709 under 10-repeated-10-k-folds CV. You'll find that Random Forest and SVM with an RBF kernel do comparably well with SMOTE. Acknowledgements All the data here was derived by me using the open source transcripts provided on the CHILDES Talkbank (http//childes.talkbank.org/). The methods I used are very close to those in  K. Gabani T. Solorio Y. Liu K.-n. Hassanali and C. A. Dollaghan “Exploring a corpus-based approach for detecting language impairment in monolingual english-speaking children” Artificial Intelligence in Medicine vol. 53 no. 3 pp. 161–170 2011. Conti-4 D. Wetherell N. Botting and G. Conti-Ramsden “Narrative skills in adolescents with a history of SLI in relation to non-verbal IQ scores” Child Language Teaching and Therapy vol. 23 no. 1 pp. 95–113 2007. ENNI P. Schneider D. Hayward and R. V. Dub “Storytelling from pictures using the Edmonton Narrative Norms Instrument” 2006. Gillam R. Gillam and N. Pearson Test of Narrative Language. Austin TX Pro-Ed Inc. 2004. Inspiration I'm hoping somebody can beat my score. I'm keen to learn more and see if this can become anything that might be of use to some child/family someday.,CSV,,"[healthcare, children, linguistics]",CC0,,,343,3959,0.603515625,Explore and create models using data derived from transcripts in CHILDES,Diagnose Specific Language Impairment in Children,https://www.kaggle.com/dgokeeffe/specific-language-impairment,Thu Nov 23 2017
94,,AdhokshajaPradeep,"[, text, favorited, favoriteCount, replyToSN, created, truncated, replyToSID, id, replyToUID, statusSource, screenName, retweetCount, isRetweet, retweeted, longitude, latitude]","[numeric, string, boolean, numeric, string, dateTime, boolean, numeric, numeric, numeric, string, string, numeric, boolean, boolean, string, string]","The Presidency of Donald Trump On Jan 20th 2017 Donald J. Trump was elected as the 45th President of the United States. This marked the end of a brutal and contentious campaign. He goes in as one of the most unpopular presidents in modern history(based on the popular vote).  The Inauguration and the Women's March Trump's election to the presidency led to the organization of the Women's March  where millions of men and women took to the streets to protest the new government's stance on women's rights and healthcare. Social media blew up with searchable terms like ""#WomensMarch"" prompting major news organizations to cover the mass protests.  Data Acquisition The data was acquired using the twitteR package's searchTwitter() function. This function makes a call to the Twitter API. A total of 30000 tweets containing #Inauguration and #WomensMarch were obtained (15000 for each). Data Set Attributes 1 ""X""  Serial Number     2 ""text""   Tweet Text         3 ""favorited""  TRUE/FALSE    4 ""favoriteCount""  Number of Likes 5  ""replyToSN""  Screen Handle name of the receiver 6  ""created""  YYYY-MM-DD HMS 7 ""truncated""  If the Tweet is Truncated (TRUE/FALSE) 8 ""replyToSID"" ID of the receiver  9  ""id""  ID          10 ""replyToUID"" User ID of the receiver     11 ""statusSource"" Device Information (Web ClientIPhoneAndroid etc) 12 ""screenName""  Screen name of the Tweeter    13 ""retweetCount"" Number of Retweets  14 ""isRetweet""  TRUE/FALSE     15 ""retweeted""  Has this tweet been retweeted(TRUE/FALSE) 16 ""longitude""  longitude   17 ""latitude""  latitude  Some Questions How do the polarity/number of tweets change by time? Which locations had negative sentiments about the Inauguration? What about the Women's March? How to the retweet and mention networks look like for each case? Number of Tweets per Day? Which day has the most activity? What are the other hashtags used?",CSV,,"[gender, politics, internet]",CC0,,,195,2177,8,A look into how social media reacted to Trump's Inauguration,#Inauguration and #WomensMarch Tweets,https://www.kaggle.com/adhok93/inauguration-and-womensmarch-tweets,Wed Feb 08 2017
95,,NOAA,"[year, month, lat, lon_175_180W, lon_170_175W, lon_165_170W, lon_160_165W, lon_155_160W, lon_150_155W, lon_145_150W, lon_140_145W, lon_135_140W, lon_130_135W, lon_125_130W, lon_120_125W, lon_115_120W, lon_110_115W, lon_105_110W, lon_100_105W, lon_95_100W, lon_90_95W, lon_85_90W, lon_80_85W, lon_75_80W, lon_70_75W, lon_65_70W, lon_60_65W, lon_55_60W, lon_50_55W, lon_45_50W, lon_40_45W, lon_35_40W, lon_30_35W, lon_25_30W, lon_20_25W, lon_15_20W, lon_10_15W, lon_5_10W, lon_0_5W, lon_0_5E, lon_5_10E, lon_10_15E, lon_15_20E, lon_20_25E, lon_25_30E, lon_30_35E, lon_35_40E, lon_40_45E, lon_45_50E, lon_50_55E, lon_55_60E, lon_60_65E, lon_65_70E, lon_70_75E, lon_75_80E, lon_80_85E, lon_85_90E, lon_90_95E, lon_95_100E, lon_100_105E, lon_105_110E, lon_110_115E, lon_115_120E, lon_120_125E, lon_125_130E, lon_130_135E, lon_135_140E, lon_140_145E, lon_145_150E, lon_150_155E, lon_155_160E, lon_160_165E, lon_165_170E, lon_170_175E, lon_175_180E]","[numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Global Historical Climatology Network-Monthly (GHCN-M) Context The Global Historical Climatology Network (GHCN) is an integrated database of climate summaries from land surface stations across the globe. This data set contains gridded mean temperature anomalies or departures from a reference value or long-term average from the Global Historical Climatology Network-Monthly (GHCN-M) version 3.2.1 temperature data set. The gridded anomalies were produced from GHCN-M bias corrected data. Each month of data consists of 2592 gridded data points produced on a 5° by 5° basis for the entire globe (72 longitude by 36 latitude grid boxes).  Frequency Monthly Period 1880 to 2016  Content Gridded data for every month from January 1880 to the most recent month is available. The data are temperature anomalies in degrees Celsius. Each gridded value was multiplied by 100 and written to file as an integer. Missing values are represented by the value -9999. The data are formatted by year month latitude and longitude. There are 72 longitude grid values per line -- each grid is labeled as a concatenation of ""lon"" ""w"" or ""e"" then the degree. The latitude is captured in the ""lat"" field where the value indicates the lower bound of a grid cell (e.g. 85 indicates 85-90N whereas -90 indicates 85-90S). Longitude values are written from 180°W to 180°E and latitude values from 90°N to 90°S. This dataset permits the quantification of changes in the mean monthly temperature and precipitation for the earth's surface. Changes in the observing system itself have been carefully removed to allow for the true climate variability at the earth's surface to be represented in the data. Many surface weather stations undergo minor relocations through their history of observation. Stations may also be subject to changes in instrumentation as measurement technology evolves. Further the land use/land cover in the vicinity of an observing site may also change with time. Such modifications to an observing site have the potential to alter a thermometer's microclimate exposure characteristics and/or change the bias of measurements the impact of which can be a systematic shift in the mean level of temperature readings that is unrelated to true climate variations. The process of removing such ""non-climatic"" artifacts in a climate time series is called homogenization. In version 3 of the GHCN-Monthly temperature data the apparent impacts of documented and undocumented inhomogeneities are detected and removed through automated pairwise comparisons of mean monthly temperature series as detailed in Menne and Williams [2009]. Inspiration This granular dataset permits extensive historical analysis of the earth’s climate to answer questions about climate change including how different regions of the planet have been affected by changes in temperature over time. Get started by forking the kernel Mapping Historical Temperature Anomalies with R.  Acknowledgements This data is a product of NOAA's National Centers for Environmental Information (NCEI). It was compiled through the aggregation and analysis of many thousands of weather station records. The compete description of the processes and methods used may be found at https//www.ncdc.noaa.gov/ghcnm/v3.php. The Global Historical Climatology Network-Monthly (GHCN-M) temperature dataset was first developed in the early 1990s (Vose et al. 1992). A second version was released in 1997 following extensive efforts to increase the number of stations and length of the data record (Peterson and Vose 1997). Methods for removing inhomogeneities from the data record associated with non-climatic influences such as changes in instrumentation station environment and observing practices that occur over time were also included in the version 2 release (Peterson and Easterling 1994; Easterling and Peterson 1995). Since that time efforts have focused on continued improvements in dataset development methods including new quality control processes and advanced techniques for removing data inhomogeneities (Menne and Williams 2009). License Public Domain License",CSV,,"[climate, history]",Other,,,871,9707,20,Land and ocean temperature anomalies,Global Historical Climatology Network,https://www.kaggle.com/noaa/global-historical-climatology-network,Mon Oct 24 2016
96,,NASA,"[Object, Epoch, TP, e, i, w, Node, q, Q, P, MOID, A1, A2, A3, DT, ref, Object_name]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",NASA tracks about 15000 near-Earth objects -- small Solar System bodies whose orbits bring them less than 1.3 AU from the Sun (i.e. within 130% of the the average distance between the Earth and the Sun). Of these 15000 160 are comets. This dataset provides orbital data for these comets. The Data Notes on Time and Space Timing information for each of these comets is given in Barycentric Dynamical Time or TDB. This is very roughly the number of days since January 1st 4713 BC (see the Wikipedia article on Julian Day for more info). Check out those Wikipedia articles for details. For information on inclination argument and longitude of the ascending node look at this article. The non-gravitational forces are effects that accelerate or decelerate the comet such as jets of gas. This dataset contains the following fields  Object the name of the comet Epoch the epoch for the comet in TDB TP time of perihelion passage in TDB; this is the time when the comet was closest to the Sun e the orbital eccentricity of the comet i Inclination of the orbit with respect to the ecliptic plane and the equinox of J2000 (J2000-Ecliptic) in degrees w Argument of perihelion (J2000-Ecliptic) in degrees Node Longitude of the ascending node (J2000-Ecliptic) in degrees q comet's distance at perihelion in AU Q comet's distance at aphelion in AU P orbital period in Julian years A1 Non-gravitational force parameter A1 A2 Non-gravitational force parameter A2 A3 Non-gravitational force parameter A3 MOID (AU) Minimum orbit intersection distance (the minimum distance between the osculating orbits of the NEO and the Earth) ref Orbital solution reference  What Should We Try? What can we do with this dataset? - plot the comets' orbits - combine with Earth's orbital data to predict close approaches Acknowledgements This dataset was downloaded from the NASA data portal.,CSV,,[space],CC0,,,327,3943,0.0244140625,Heliocentric orbital data for comets that make approaches near Earth's orbit,Near-Earth Comets,https://www.kaggle.com/nasa/near-earth-comets,Sat Nov 05 2016
97,,Stack Overflow,"[Id, OwnerUserId, CreationDate, ParentId, Score, IsAcceptedAnswer, Body]","[numeric, numeric, dateTime, numeric, numeric, boolean, string]",Full text of questions and answers from Stack Overflow that are tagged with the r tag useful for natural language processing and community analysis. This is organized as three tables  Questions contains the title body creation date score and owner ID for each R question. Answers contains the body creation date score and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table. Tags contains the tags on each question besides the R tag.  For space reasons only non-deleted and non-closed content are included in the dataset. The dataset contains questions up to 24 September 2017 (UTC). License All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.,CSV,,"[linguistics, internet, programming languages]",Other,,,959,15658,516,Full text of Stack Overflow Q&A about the R statistical programming language,R Questions from Stack Overflow,https://www.kaggle.com/stackoverflow/rquestions,Tue Sep 26 2017
98,,0rangutan,"[SERVICE_TYPE, SERVICE_CODE, ENROLLMENT, BRANCH, RANK, PAY_GRADE, POSITION, BIRTH_YEAR, SEX, HOME_CITY, HOME_COUNTY, NATIONALITY, STATE_CODE, HOME_STATE, MARITAL_STATUS, ETHNICITY, ETHNICITY_1, ETHNICITY_2, DIVISION, INCIDENT_DATE, FATALITY_YEAR, FATALITY_DATE, HOSTILITY_CONDITIONS, FATALITY, BURIAL_STATUS]","[string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string]",Context Information reproduced from the National Archives The Korean Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of U.S. military fatal casualties of the Korean War. These records were transferred into the custody of the National Archives and Records Administration in 2008. The Defense Casualty Analysis System Extract Files were created by the Defense Manpower Data Center (DMDC) of the Office of the Secretary of Defense. The records correspond to the Korean War Conflict statistics on the DMDC web site which is accessible online at https//www.dmdc.osd.mil/dcas/pages/main.xhtml .  A full series description for the Defense Casualty Analysis System (DCAS) Extract Files is accessible online via the National Archives Catalog under the National Archives Identifier 2240988. The Korean War Conflict Extract Data File is also accessible for direct download via the National Archives Catalog file-level description National Archives Identifier 2240988.  Content The raw data files have been cleaned and labelled as best as I can with reference to the accompanying Supplemental Code Lists. Names and ID numbers have been removed out of respect and to provide anonymity. Data fields * SERVICE_TYPE * SERVICE_CODE * ENROLLMENT * BRANCH * RANK * PAY_GRADE * POSITION * BIRTH_YEAR * SEX * HOME_CITY * HOME_COUNTY * HOME_STATE * STATE_CODE * NATIONALITY * MARITAL_STATUS  * ETHNICITY * ETHNICITY_1 * ETHNICITY_2 * DIVISION * FATALITY_YEAR * FATALITY_DATE * HOSTILITY_CONDITIONS * FATALITY * BURIAL_STATUS Acknowledgements Data provided by The U.S. National Archives and Records Administration. Raw data can be accessed via the following link https//catalog.archives.gov/id/2240988 Inspiration By cleaning the data I hope to give wider access to this resource.,CSV,,"[history, war]",Other,,,128,1488,7,Data from The U.S. National Archives and Records Administration,US Casualties of the Korean War,https://www.kaggle.com/orangutan/koreanconflict,Fri Feb 17 2017
99,,Team AI,"[url, Full Name Show, Username Dir, Tweet Nav, Tweet Nav_link, Tweet Text Size Block, English Translation, Tweet Text Size Link, Tweet Text Size Link_link, Profile Tweet 1, Profile Tweet 2, Profile Tweet 3, Reply, Re Tweet, Like]","[string, string, string, dateTime, string, string, string, string, string, string, string, string, numeric, string, string]","Context This dataset contains Japanese Prime Minister Tweet.   Japanese culture diplomatic problem ( North Korea and Tramp etc) time of disaster economics... For example14.April 2014 ""Removing radiation contaminated water in all weather 365/24 at Fukushima. I am deeply thankful for dedication and commitment of our peers."" Maybe if you analyze his tweets about  Japanese economy this data will be useful for stock price forecasting etc. Content This dataset contains following the data  url Full name show user name dir tweet nav tweet nav_link tweet text size block tweet text size link tweet text size Link_link Profile tweet 1 Profile tweet 2 Profile tweet 3 replay re tweet like  Inspiration Inspired by Trump vs Clinton NLP",CSV,,"[politicians, politics, twitter]",CC3,,,60,1598,0.0576171875,Let's analyze Japanese politician,Shinzo Abe (Japanese Prime Minister) Twitter NLP,https://www.kaggle.com/team-ai/shinzo-abe-japanese-prime-minister-twitter-nlp,Fri Oct 27 2017
100,,lukebyrne,"[id, name]","[numeric, string]",Context Daily horse racing (thoroughbred) information that has(is) being actively collected and aggregated from a variety of sources. Years covered are just 2016 country is irrelevant to the dataset. Acknowledgements This data has(is) being actively collected and aggregated from a variety of sources all in the public domain. Past Research None of merit data is used currently to influence some betting decisions but no solid machine learning model(s) have been developed. Have thrown various versions of the data into  Google Prediction Amazon Machine Learning Azure Machine Learning Watson Analytics as a way to learn how these systems work. Inspiration Probably one of the hardest things to do is pick stocks and horses. I have been involved in the stocks and horses industry for many years and through publishing previous libraries and software I have met many interesting people and also one of my long term clients/friends. I am currently trying enhance my software development skills by learning data science / machine learning. I have a done a few tutorials and I am hoping that by publishing this data I can learn and collaborate with members of the Kaggle Community. Content markets.csv id start_time what time did the race start datetime in UTC venue_id race_number distance(m) condition_id track condition see conditions.csv weather_id weather on day see weathers.csv total_pool_win_one rough $ amount wagered across all runners for win market total_pool_place_one rough $ amount wagered across all runners for place market total_pool_win_two total_pool_place_two total_pool_win_three total_pool_place_three runners.csv id collected what time was this row created/data collected datetime in UTC market_id position THIS IS THE FIELD WE WANT TO PREDICT!!!! Will either be 123456 etc or 0/null if the horse was scratched or failed to finish If all positions for a market_id are null it means we were unable to match up the positional data for this market place_paid Will either be 1/0 or null If you see a race that only has 2 booleans of 1 it means that the race only paid out places on the first two positions margin If the runner didnt win how many lengths behind the 1st place was it horse_id see horses.csv trainer_id rider_id see riders.csv handicap_weight number barrier blinkers emergency did it come into the race at the last minute form_rating_one form_rating_two form_rating_three last_five_starts favourite_odds_win from one of the odds sources will it win  - true/false favourite_odds_place from one of the odds sources  will it win  - true/false favourite_pool_win favourite_pool_place tip_one_win from a tipster will it win    - true/false tip_one_place from a tipster will it place  - true/false tip_two_win tip_two_place tip_three_win tip_three_place tip_four_win tip_four_place tip_five_win tip_five_place tip_six_win tip_six_place tip_seven_win tip_seven_place tip_eight_win tip_eight_place tip_nine_win tip_nine_place odds.csv (collected for every runner 10 minutes out from race start until race starts) runner_id collected what time was this row created/data collected datetime in UTC odds_one_win from odds source win odds odds_one_win_wagered from odds source rough $ amount wagered on win odds_one_place from odds source place odds odds_one_place_wagered from odds source rough $ amount wagered on place odds_two_win odds_two_win_wagered odds_two_place odds_two_place_wagered odds_three_win odds_three_win_wagered odds_three_place odds_three_place_wagered odds_four_win odds_four_win_wagered odds_four_place odds_four_place_wagered forms.csv collected what time was this row created/data collected datetime in UTC market_id horse_id runner_number last_twenty_starts e.g. f9x726x753x92222x35 f = failed to finish 7 = finished 7th 6 = finished 6th  7 = finished 7th x = runner was scratched class_level_id 1 = eq (in same class as other horses) 2 = up (up in class) 3 = dn (down in class) field_strength days_since_last_run runs_since_spell overall_starts overall_wins overall_places track_starts track_wins track_places firm_starts firm_wins firm_places good_starts good_wins good_places dead_starts dead_wins dead_places slow_starts slow_wins slow_places soft_starts soft_wins soft_places heavy_starts heavy_wins heavy_places distance_starts distance_wins distance_places class_same_starts class_same_wins class_same_places class_stronger_starts class_stronger_wins class_stronger_places first_up_starts first_up_wins first_up_places second_up_starts second_up_wins second_up_places track_distance_starts track_distance_wins track_distance_places conditions.csv id name weathers.csv id name riders.csv (jockeys) id sex horses.csv id age sex_id see horse_sexes.csv sire_id not related to horses.id there is another table called horse_sires that is not present here dam_id not related to horses.id there is another table called horse_dams that is not present here prize_money total aggregate prize money horse_sexes.csv id name ,CSV,,[horse racing],CC4,,,3273,36304,26,"Daily horse racing (thoroughbred) data, machine learning for fun and profit",Horses For Courses,https://www.kaggle.com/lukebyrne/horses-for-courses,Wed Jan 11 2017
101,,Jeanpat,[],[],As a cytogeneticist studying some features of the chromosomes (their telomeres structural anomalies ...) you need to take pictures from chromosomal preparations fixed on glass slides . Unfortunately like the sticks in the mikado game sometime a chromosome or more can fall on an other one yielding overlapping chromosomes in the image the plague of cytogeneticists. Before computers and images processing with photography chromosomes were cut from a paper picture and then classified (at least two paper pictures were required) with computers and quantitative analysis automatic methods were developped to overcome this problem (Lunsteen & Piper Minaee et al.) This dataset modelizes overlapping chromosomes. Couples of non overlapping chromosomes were combined by relative translations and rotations to generate two kind of images   a grey scaled image of the two overlapping chromosomes combining a DAPI stained chromosome and the labelling of the telomeres (showing the chromosomes extremities) a label image (the ground truth) were the value equal to 3 labels the pixels (displayed in red)  belonging to the overlapping domain.   The images were saved in a numpy array as an hdf5 file. The following minimalist python 2 code can load the data (assuming that the unzipped data are in the same folder than the code)  import h5py import numpy as np from matplotlib import pyplot as plt  #h5f = h5py.File('overlapping_chromosomes_examples.h5''r') h5f = h5py.File('LowRes_13434_overlapping_pairs.h5''r') pairs = h5f['dataset_1'][] h5f.close()  grey = pairs[2200] mask = pairs[2201] #%matplotlib inline plt.subplot(121) plt.imshow(grey) plt.title('max='+str(grey.max())) plt.subplot(122) plt.imshow(mask)  I hope this dataset to be suitable to apply supervised learning methods possibly similar to segnet or its implementation with keras.,Other,,[human genetics],CC4,,,492,8034,23,Learn to resolve them,Overlapping chromosomes,https://www.kaggle.com/jeanpat/overlapping-chromosomes,Wed Oct 12 2016
102,,Zillow,"[RegionName, City, State, Metro, CountyName, SizeRank, 2010-01, 2010-02, 2010-03, 2010-04, 2010-05, 2010-06, 2010-07, 2010-08, 2010-09, 2010-10, 2010-11, 2010-12, 2011-01, 2011-02, 2011-03, 2011-04, 2011-05, 2011-06, 2011-07, 2011-08, 2011-09, 2011-10, 2011-11, 2011-12, 2012-01, 2012-02, 2012-03, 2012-04, 2012-05, 2012-06, 2012-07, 2012-08, 2012-09, 2012-10, 2012-11, 2012-12, 2013-01, 2013-02, 2013-03, 2013-04, 2013-05, 2013-06, 2013-07, 2013-08, 2013-09, 2013-10, 2013-11, 2013-12, 2014-01, 2014-02, 2014-03, 2014-04, 2014-05, 2014-06, 2014-07, 2014-08, 2014-09, 2014-10, 2014-11, 2014-12, 2015-01, 2015-02, 2015-03, 2015-04, 2015-05, 2015-06, 2015-07, 2015-08, 2015-09, 2015-10, 2015-11, 2015-12, 2016-01, 2016-02, 2016-03, 2016-04, 2016-05, 2016-06, 2016-07, 2016-08, 2016-09]","[string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset includes the median list price divided by the square footage of a 1-bedroom home for a select number of neighborhoods around the United States.  Content When available data includes median price per square foot on a monthly basis between January 2010 and September 2016. Selected neighborhoods include  Upper East Side New York NY Spring Valley Las Vegas NV Hollywood Los Angeles CA  Williamsburg New York NY Harlem New York NY Enterprise Las VegasNV Downtown San Jose CA Sheepshead Bay New York NY Forest Hills New York NY Jackson Heights New York NY Gramercy New York NY Flagami Miami FL Downtown Memphis TN Chelsea New York NY Oak Lawn Dallas TX Greater Uptown Houston TX South Loop Chicago IL Makiki-Lower Punchbowl-Tantalus Honolulu HI Downtown Los Angeles CA Capitol Hill Seattle WA Clinton New York NY Alexandria West Alexandria VA Financial District New York NY Flatiron District New York NY Landmark-Van Dom Alexandria VA Flamingo Lummus Miami Beach FL Winchester Las Vegas NV Brickell Miami FL Waikiki Honolulu HI Back Bay Boston MA Sutton Place New York NY and several others  Inspiration  What neighborhoods have the most expensive real estate per square foot? Least expensive? Which neighborhoods and/or cities have the fastest growth rates in price? Are there any neighborhoods that remain relatively steady in price? Given that this metric is listing price per square foot is there a similar dataset that could help you compare median square footage in a 1-bedroom home across neighborhoods?  Acknowledgement This dataset is part of Zillow Data and the original source can be found here under the Neighborhoods link.,CSV,,"[cities, home]",Other,,,600,5350,0.0498046875,"Aggregated by neighborhood, price per square foot",Median Listing Price (1 Bedroom),https://www.kaggle.com/zillow/median-listing-price-1-bedroom,Mon Nov 07 2016
103,,Nathan,"[state, msn, year, value, units_code]","[string, string, numeric, numeric, string]","State Energy Data Systems (SEDS) data for all US states including DC from 1960 to 2014F Context This dataset is derived from my general interest in energy systems. It was originally composed for this exercise as part of this Coursera/John Hopkins Data Science Specilisation. The code that produced this dataset is in https//www.kaggle.com/nathanto/d/nathanto/seds-1960-2014F/data-wrangling-code-for-seds-1960-2014f Content The data is a composition of the State Energy Data Systems (SEDS) data for all US states including DC from 2016 to 2014F for data released June 29 2016. It has been tidied from a wide format to a long format and includes unit codes for the values associated with the observations for each MSN code for each state for each year.  The ""F"" in the final year number indicates that these are the final observations. There is a lag of some 18 months after year end and final readings. The columns are  state - State postal code composed from the function states.abb and including ""DC"". msn - A mnemonic series name identifying the value being observed. year - Year of the observation. value - Of the observation. units_code representing the units of the value e.g. BBtu is Billion British Thermal Units.  Note that the units_codes are mostly my own invention based on the EIA Writng Style Guide. Acknowledgements Thank you to the US Energy Information Administration for making the data available. Special thanks to Yvonne Taylor for guidance on style for the codes. Inspiration The first goal for this data was to support some plotting and forecast testing exercises which is a work in progress. To what extent do past observations predict future observations? Since the data is readily available and consistent within limits over a long period this format is a good basis for experimenting with techniques in that space. ",CSV,,[energy],CC0,,,238,2976,26,State Energy Data Systems (SEDS) data for all US states including DC,"State Energy System Data, 1960-2014",https://www.kaggle.com/nathanto/seds-1960-2014F,Tue Mar 14 2017
104,,NLTK Data,[],[],Context The wmt15_eval dataset contains the files to machine translation evaluation output from Workshop on Machine Translation (WMT15).  NLTK uses this dataset to validate the machine translation BLEU score implementations. Content The wmt15_eval directory contains the files to evaluate MT evaluation metrics it's not production standards data neither will it be helpful in shared task participation but it provides a good testbed for new metrics implementation and comparison against metrics already available in nltk.translate.*_score.py to validate the numbers. It includes the first 100 sentences from the newstest 2015 development set for the English-Russian language part made available at Workshop for Machine Translation 2016 (WMT16) and the Google Translate of the English source sentences. [Plaintext]  newstest-2015-100sents.en-ru.src.en newstest-2015-100sents.en-ru.ref.ru newstest-2015-100sents.en-ru.google.ru  [SGM]  newstest2015-100sents-enru-ref.ru.sgm  newstest2015-100sents-enru-src.en.sgm newstest2015-100sents-enru-google.ru.sgm  And the original sgm files from WMT16  newstest2015-enru-ref.ru.sgm  newstest2015-enru-src.en.sgm   The plaintext are converted from the .sgm files from the development sets in WMT with  the following command sed -e 's/<[^>]*>//g; /^\s*$/d' newstest-2015.enru.src.en.sgm | head -n100 > newstest-2015-100sents.en-ru.src.en sed -e 's/<[^>]*>//g; /^\s*$/d' newstest-2015.enru.ref.ru.sgm | head -n100 > newstest-2015-100sents.en-ru.ref.en  The tokenized versions of the natural text files above are processed using Moses tokenizer.perl ~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.ref.ru > ref.ru ~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l ru < newstest-2015-100sents.en-ru.google.ru > google.ru ~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en < newstest-2015-100sents.en-ru.src.en > src.en  The Google translate outputs are created on 25 Oct 2016 10am. using the English source sentences. The newstest2015-100sents-enru-google.ru.sgm is created using the wrap-xml.perl tool in Moses ~/mosesdecoder/scripts/ems/support/wrap-xml.perl ru newstest2015-100sents-enru-src.en.sgm Google < google.ru > newstest2015-100sents-enru-google.ru.sgm  The BLEU scores output from multi-bleu.perl is as such ~/mosesdecoder/scripts/generic/multi-bleu.perl ref.ru < google.ru  BLEU = 23.17 53.8/29.6/17.6/10.3 (BP=1.000 ratio=1.074 hyp_len=1989 ref_len=1852)  The mteval-13a.output file is produced using the mteval-v13a.pl ~/mosesdecoder/scripts/generic/mteval-v13a.pl -r newstest2015-100sents-enru-ref.ru.sgm -s newstest2015-100sents-enru-src.en.sgm -t newstest2015-100sents-enru-google.ru.sgm  > mteval-13a.output  Acknowledgements Credits go to the organizers of WMT15 and WMT16.,Other,,[],Other,,,16,560,1,Machine Translation Evaluation for WMT15,WMT15 Evaluation,https://www.kaggle.com/nltkdata/wmt15-eval,Sun Aug 20 2017
105,,Caroline Cypranowska,"[, CampsiteID, CampsiteName, CampsiteType, FacilityID, FacilityLatitude, FacilityLongitude, FacilityName, AddressStateCode, OrgAbbrevName]","[numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string]",New See this dataset visualized in D3 Context This data acquired from the Recreation Information Database Catalog contains campsite info for all campground facilities run by the United States National Park Service the United States Forest Service the Bureau of Land Management and other Federal Government agencies. Read the API documentation. Content Fields include facility ID campsite ID campsite type facility name facility location (latitude longitude state) and managing agency. Acknowledgements A humble thank you to the federal agencies that collect these data and maintain public access including USFS NPS BLM USACE FWS and BOR. Thank you to American tax payers for keeping these facilities afloat. Thank you to Levi Gadye for granting permission for the use of the cover photo.  Inspiration I was looking for a campsite for a group of friends in between Salt Lake City and Grand Teton NP and was struggling to find a tent-only campsite along the highway corridors leading to Grand Teton NP. In the past I had always felt that finding tent only campsites in California was quite easy compared to other places I’ve camped in the American West. I tidied this data set from RIDB to determine whether or not my ease in booking tent-only campsites in CA was due to a larger number of those sites or if the state of CA had a relative enrichment for tent-only campsites. ,CSV,,[sports],CC0,,,159,1424,8,Data on campsites run by US Federal Government,US campsites,https://www.kaggle.com/cypranowska/us-campsites,Sun Apr 30 2017
106,,Charlie H.,"[author, posted_on, rating, text]","[string, dateTime, numeric, string]",Comcast is notorious for terrible customer service and despite repeated promises to improve they continue to fall short. Only last month (October 2016) the FCC fined them a cool $2.3 million after receiving over 1000 consumer complaints. After dealing with their customer service for hours yesterday I wanted to find out more about others' experiences.  This will serve as a repository of public customer complaints filed against Comcast as I scrape them from the web. The data should not only provide a fun source for analysis but it will help to pin down just what is wrong with Comcast's customer service.,CSV,,"[business, internet, networks]",Other,,,645,5630,11,Public complaints made about Comcast internet and television service.,Comcast Consumer Complaints,https://www.kaggle.com/archaeocharlie/comcastcomplaints,Tue Nov 29 2016
107,,Jason McNeill,"[MSN, YYYYMM, Value, Column_Order, Description, Unit]","[string, numeric, numeric, numeric, string, string]",Monthly/Annual carbon dioxide emissions from electricity generation from the Energy Information Administration. Data is broken down by fuel type. http//www.eia.gov/electricity/data.cfm#elecenv,CSV,,"[environment, energy]",Other,,,991,7799,0.59765625,Carbon emissions from electicity production,Carbon Emissions,https://www.kaggle.com/txtrouble/carbon-emissions,Sun Nov 06 2016
108,,Myles O'Neill,[],[],On the internet of the 1980's everything was stored in ASCII text files. During these early days many literary works were manually typed up and shared widely. TEXTFILES.COM is a website by Jason Scott dedicated to collecting and preserving text files from this internet of the past. This dataset is a small subset of his total collection - focussing exclusively on english literary works.  Each book is stored in its own ASCII text file and all are in English. How similar are the writing styles of so-called classic authors? Can you train a model to determine if a work is fictional or not? What words or phrases are the most popular in these books?,Other,,"[literature, internet]",CC0,,,64,750,124,Explore some of the most influential english language works,Classic Literature in ASCII,https://www.kaggle.com/mylesoneill/classic-literature-in-ascii,Thu Nov 16 2017
109,,Nosbielcs,"[codigo_ocorrencia, ocorrencia_classificacao, ocorrencia_tipo, ocorrencia_dia, ocorrencia_horario, ocorrencia_cidade, ocorrencia_uf, ocorrencia_pais, ocorrencia_aerodromo, aeronave_matricula, aeronave_equipamento, aeronave_fabricante, aeronave_modelo, aeronave_tipo_motor, aeronave_quantidade_motores, aeronave_peso_maximo_decolagem, aeronave_quantidade_assentos, aeronave_ano_fabricacao, aeronave_pais_registro, aeronave_categoria_registro, aeronave_segmento_aviacao, aeronave_origem_voo, aeronave_destino_voo, aeronave_fase_voo, aeronave_tipo_operacao, aeronave_nivel_dano, quantidade_fatalidades, quantidade_fatores_contribuintes, fator_1, fator_2, fator_3, fator_4, fator_5, fator_6, fator_7, fator_8, fator_9, fator_10, fator_11, fator_12, fator_13, fator_14, fator_15, fator_16, fator_17, fator_18, fator_19, fator_20, fator_21, fator_22, fator_23, fator_24, fator_25, fator_26, fator_27, fator_28, fator_29, fator_30, fator_31, fator_32, fator_33, fator_34, fator_35, fator_36, fator_37, fator_38, fator_39, fator_40, fator_41, fator_42, fator_43, fator_44, fator_45, fator_46, fator_47, fator_48, fator_49, fator_50, fator_51, fator_52, fator_53, fator_54, fator_55, fator_56, fator_57, fator_58, fator_59, fator_60, fator_61, fator_62, fator_63, fator_64, fator_65, fator_66, fator_67, fator_68, fator_69, fator_70, fator_71, fator_72]","[numeric, string, string, dateTime, dateTime, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Opendata AIG Brazil Sobre o Projeto  Download dos Dados  OCORRÊNCIAS AERONÁUTICAS AERONAVES ENVOLVIDAS FATORES CONTRIBUINTES RECOMENDAÇÕES DE SEGURANÇA  Notas Técnicas  Os textos dentro das colunas estão denotados por aspas duplas (""""). As colunas das tabelas estão separadas por til (~). As tabelas contém cabeçalhos que identificam suas colunas. Em cada tabela existe uma coluna contendo a informação sobre a data de extração dos dados.  Outras Informações ""For Dummies""  Os relatórios finais podem ser consultados no site do CENIPA - Relatórios. As recomendações de segurança podem ser consultadas no site do CENIPA - Recomendações. Artigos científicos sobre o tema podem ser encontrados / publicados na Revista Conexão SIPAER.  Outros Recursos Outras bases de dados para consultas  NTSB BEA RISCO DA FAUNA RAIO LASER RISCO BALOEIRO AERÓDROMOS BRASILEIROS AEROVIAS BRASILEIRAS  Dicas para melhor aproveitamento dos recursos  Antes de fazer o download dos dados leia com calma todo o texto desta página. Este recurso irá guiá-lo(a) para um adequado entendimento sobre os relacionamentos entre os conjuntos de dados disponíveis (ocorrencia aeronave envolvida fator_contribuinte e recomendações de segurança). Para aprofundar-se no tema visite o site do CENIPA e confira as LEGISLAÇÕES que norteiam a investigação e prevenção de acidentes aeronáuticos no Brasil. Conheça o Manual de Investigação do SIPAER. Nos anexos deste documento você encontrará uma tabela de domínios (taxonomia) para algumas das variáveis disponíveis nos conjuntos de dados. Devido ao dinamismo dos trabalhos de investigação e preocupação do CENIPA com a agilidade na disponibilização dos dados os conjuntos de dados estarão sujeitos a modificações sempre que forem atualizados. Portanto sempre que possível utilize a ""data de extração"" dos conjuntos de dados para justificar/referenciar os seus estudos e análises. Saiba como trabalhar com dados no formato CSV. Clique aqui para aprender  Dúvidas Se persistirem dúvidas por gentileza me enviem uma Issue (relatar problema). Clique aqui para relatar um Problema",CSV,,"[brazil, aviation]",ODbL,,,122,1883,5,Aircraft Accidents in Brazil,Opendata AIG Brazil,https://www.kaggle.com/nosbielcs/opendataaigbrazil,Mon Oct 02 2017
110,,US Department of Health and Human Services,"[State, Uninsured Rate (2010), Uninsured Rate (2015), Uninsured Rate Change (2010-2015), Health Insurance Coverage Change (2010-2015), Employer Health Insurance Coverage (2015), Marketplace Health Insurance Coverage (2016), Marketplace Tax Credits (2016), Average Monthly Tax Credit (2016), State Medicaid Expansion (2016), Medicaid Enrollment (2013), Medicaid Enrollment (2016), Medicaid Enrollment Change (2013-2016), Medicare Enrollment (2016)]","[string, string, string, string, numeric, numeric, numeric, numeric, string, boolean, numeric, numeric, numeric, numeric]",Context The Affordable Care Act (ACA) is the name for the comprehensive health care reform law and its amendments which addresses health insurance coverage health care costs and preventive care. The law was enacted in two parts The Patient Protection and Affordable Care Act was signed into law on March 23 2010 by President Barack Obama and was amended by the Health Care and Education Reconciliation Act on March 30 2010. Content This dataset provides health insurance coverage data for each state and the nation as a whole including variables such as the uninsured rates before and after Obamacare estimates of individuals covered by employer and marketplace healthcare plans and enrollment in Medicare and Medicaid programs. Acknowledgements The health insurance coverage data was compiled from the US Department of Health and Human Services and US Census Bureau. Inspiration How has the Affordable Care Act changed the rate of citizens with health insurance coverage? Which states observed the greatest decline in their uninsured rate? Did those states expand Medicaid program coverage and/or implement a health insurance marketplace? What do you predict will happen to the nationwide uninsured rate in the next five years?,CSV,,[health],CC0,,,1281,10218,0.0048828125,Coverage rates before and after the Affordable Care Act,Health Insurance Coverage,https://www.kaggle.com/hhs/health-insurance,Fri Mar 03 2017
111,,Javier Villanueva-Valle,"[Video Time, Neutral, Happy, Sad, Angry, Surprised, Scared, Disgusted, Contempt, Valence, Arousal, Gender, Age, Beard, Moustache, Glasses, Ethnicity, Y - Head Orientation, X - Head Orientation, Z - Head Orientation, Quality, Mouth, Left Eye, Right Eye, Left Eyebrow, Right Eyebrow, Gaze Direction, Identity, Action Unit 01 - Inner Brow Raiser, Action Unit 02 - Outer Brow Raiser, Action Unit 04 - Brow Lowerer, Action Unit 05 - Upper Lid Raiser, Action Unit 06 - Cheek Raiser, Action Unit 07 - Lid Tightener, Action Unit 09 - Nose Wrinkler, Action Unit 10 - Upper Lip Raiser, Action Unit 12 - Lip Corner Puller, Action Unit 14 - Dimpler, Action Unit 15 - Lip Corner Depressor, Action Unit 17 - Chin Raiser, Action Unit 18 - Lip Puckerer, Action Unit 20 - Lip Stretcher, Action Unit 23 - Lip Tightener, Action Unit 24 - Lip Pressor, Action Unit 25 - Lips Part, Action Unit 26 - Jaw Drop, Action Unit 27 - Mouth Stretch, Action Unit 43 - Eyes Closed, Heart Rate, Stimulus, Event Marker]","[dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context An interview was videotaped to analyze the facial expressions of emotion. This interview lasts for 2130 minutes. Content An interview with a duration of 2130 minutes was videotaped. Emotional facial expressions were analyzed every 0.12 seconds with FaceReader software. Emotions were neutral joy fear anger surprise fear and contemp positive-negative valences arousal degrees of head direction facial action units among others. Acknowledgements Acknowledgements to The Clinic of Borderline Personality Disorder for contributing the material also to the Laboratory of Chronoecology and Human Ethology. Inspiration How does the direction of the gaze relate to the expressions of emotions? From what score of any emotion is considered open closed and / or neutral in the right eye left eye right eyebrow and left eyebrow? In addition to descriptive statistical analyzes what other analyzes can be performed?,CSV,,"[film, sociology]",ODbL,,,843,8135,5,Frame-by-frame analysis of facial expressions in a video interview,Facial Expression of Emotion,https://www.kaggle.com/sivlemx/facial-expression-of-emotion,Thu Mar 09 2017
112,,athontz,"[speechiness, key, time_signature, liveness, loudness, duration_ms, danceability, duration, valence, acousticness, spotify_id, volume_number, energy, tempo, instrumentalness, mode, number, artist, title]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context I am working on building a classifier that will examine today's 'top 40' and determine whether or not they are worthy of appearing on the next 'Now That's What I Call Music' album. Content The dataset includes all 61 US released Now That's what I call Music tracklistings. Columns are volume_number - the album number corresponding with the volume. (ex a value of 60 would represent the album 'Now That's What I Call Music Vol. 60) artist - the name of the artist singing the track title - the song name number - the song's track number on it's album duration - the song's length in seconds Acknowledgements Thanks to Wikipedia contributors for maintaining this data! Improvements I am currently working on adding another csv file that contains this same data joined with each song's audio features from the Spotify Web API. In the future I would also be interested in scraping Now releases in other countries as well as the 'special' releases (ex Now That's What I Call Christmas music etc).,CSV,,"[popular culture, music]",CC0,,,286,3562,0.1708984375,A collection of all 61 original Now That is What I Call Music tracklistings,Now That's What I Call Music (U.S. releases),https://www.kaggle.com/athontz/nowthatswhaticallmusic,Thu Feb 02 2017
113,,Joel Wilson,"[fips, area_name, state_abbreviation, PST045214, PST040210, PST120214, POP010210, AGE135214, AGE295214, AGE775214, SEX255214, RHI125214, RHI225214, RHI325214, RHI425214, RHI525214, RHI625214, RHI725214, RHI825214, POP715213, POP645213, POP815213, EDU635213, EDU685213, VET605213, LFE305213, HSG010214, HSG445213, HSG096213, HSG495213, HSD410213, HSD310213, INC910213, INC110213, PVY020213, BZA010213, BZA110213, BZA115213, NES010213, SBO001207, SBO315207, SBO115207, SBO215207, SBO515207, SBO415207, SBO015207, MAN450207, WTN220207, RTN130207, RTN131207, AFN120207, BPS030214, LND110210, POP060210]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",These data files contain election results for both the 2012 and 2016 US Presidential Elections include proportions of votes cast for Romney Obama (2012) and Trump Clinton (2016). The election results were obtained from this Git repository https//github.com/tonmcg/County_Level_Election_Results_12-16 The county facts data was obtained from another Kaggle election data set https//www.kaggle.com/benhamner/2016-us-election,CSV,,[politics],Other,,,2712,15658,3,"Election results with county information on race, income and education",2012 and 2016 Presidential Elections,https://www.kaggle.com/joelwilson/2012-2016-presidential-elections,Mon Nov 28 2016
114,,Amandeep Rathee,"[Arrest, Age, Sex, Race, ArrestDate, ArrestTime, ArrestLocation, IncidentOffense, IncidentLocation, Charge, ChargeDescription, District, Post, Neighborhood, Location 1]","[numeric, numeric, string, string, dateTime, string, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric]",Context This data represents the top arrest charge of those processed at Baltimore's Central Booking & Intake Facility. This data does not contain those who have been processed through Juvenile Booking. Content The data set was created on October 18 2011. The data set was last updated on November 18 2016. It is updated on a monthly basis. Metadata  Arrest-ID Age Sex Race ArrestDate ArrestTime ArrestLocation IncidentOffense IncidentLocation Charge ChargeDescription District Post Neighborhood Location1(Location Coordinates)  Past Research I have done my own analysis on the data which can be found on the following GitHub repository. Feel free to give any suggestions regarding the data. Github Link Inspiration  How arrests vary across different gender race age ? Which area in Baltimore has most number of arrests made ? What are the top offences and/or charges made while making arrests ?  Acknowledgements The data is hosted on Data set Source Baltimore Police Depratment's website Baltimore Police Department,CSV,,[crime],CC0,,,606,5561,19,Data of 131k arrests made by the Baltimore Police Department,Arrests by Baltimore Police Department,https://www.kaggle.com/arathee2/arrests-by-baltimore-police-department,Wed Nov 23 2016
115,,PreetSinghKhalsa,"[, State, District, Persons, Males, Females, Growth..1991...2001., Rural, Urban, Scheduled.Caste.population, Percentage...SC.to.total, Number.of.households, Household.size..per.household., Sex.ratio..females.per.1000.males., Sex.ratio..0.6.years., Scheduled.Tribe.population, Percentage.to.total.population..ST., Persons..literate, Males..Literate, Females..Literate, Persons..literacy.rate, Males..Literatacy.Rate, Females..Literacy.Rate, Total.Educated, Data.without.level, Below.Primary, Primary, Middle, Matric.Higher.Secondary.Diploma, Graduate.and.Above, X0...4.years, X5...14.years, X15...59.years, X60.years.and.above..Incl..A.N.S.., Total.workers, Main.workers, Marginal.workers, Non.workers, SC.1.Name, SC.1.Population, SC.2.Name, SC.2.Population, SC.3.Name, SC.3.Population, Religeon.1.Name, Religeon.1.Population, Religeon.2.Name, Religeon.2.Population, Religeon.3.Name, Religeon.3.Population, ST.1.Name, ST.1.Population, ST.2.Name, ST.2.Population, ST.3.Name, ST.3.Population, Imp.Town.1.Name, Imp.Town.1.Population, Imp.Town.2.Name, Imp.Town.2.Population, Imp.Town.3.Name, Imp.Town.3.Population, Total.Inhabited.Villages, Drinking.water.facilities, Safe.Drinking.water, Electricity..Power.Supply., Electricity..domestic., Electricity..Agriculture., Primary.school, Middle.schools, Secondary.Sr.Secondary.schools, College, Medical.facility, Primary.Health.Centre, Primary.Health.Sub.Centre, Post..telegraph.and.telephone.facility, Bus.services, Paved.approach.road, Mud.approach.road, Permanent.House, Semi.permanent.House, Temporary.House]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Census of India is a rich database which can tell stories of over a billion Indians. It is important not only for research point of view but commercially as well for the organizations that want to understand India's complex yet strongly knitted heterogeneity.  However nowhere on the web there exists a single database that combines the district- wise information of all the variables (most include no more than 4-5 out of over 50 variables!). Extracting and using data from Census of India 2001 is quite a laborious task since all data is made available in scattered PDFs district wise. Individual PDFs can be extracted from http//www.censusindia.gov.in/(S(ogvuk1y2e5sueoyc5eyc0g55))/Tables_Published/Basic_Data_Sheet.aspx.  Content This database has been extracted from Census of 2001 and includes data of 590 districts having around 80 variables each.  In case of confusion regarding the context of the variable refer to the following PDF and you will be able to make sense out of it http//censusindia.gov.in/Dist_File/datasheet-2923.pdf  All the extraction work can be found @ https//github.com/preetskhalsa97/census2001auto  The final CSV can be found at finalCSV/all.csv The subtle hack that was used to automate extraction to a great extent was the the URLs of all the PDFs were same except the four digits (that were respective state and district codes).  A few abbreviations used for states AN- Andaman and Nicobar CG- Chhattisgarh D_D- Daman and Diu D_N_H- Dadra and Nagar Haveli JK- Jammu and Kashmir MP- Madhya Pradesh TN- Tamil Nadu UP- Uttar Pradesh WB- West Bengal  A few variables for clarification  Growth..1991...2001- population growth from 1991 to 2001 X0..4 years- People in age group 0 to 4 years SC1- Scheduled Class with highest population Acknowledgements Inspiration This is a massive dataset which can be used to explain the interplay between education caste development gender and much more.  It really can explain a lot about India and propel data driven research.  Happy Number Crunching!,CSV,,"[demographics, sociology]",ODbL,,,2106,14214,0.3466796875,"One billion hearts, a single CSV","Govt. of India Census, 2001 District-Wise",https://www.kaggle.com/bazuka/census2001,Wed Jan 18 2017
116,,Alberto Barradas,[],[],,CSV,,[],Other,,,0,0,0.0126953125,Wikipedia's list of world's highest mountains,World's Highest Mountains,https://www.kaggle.com/abcsds/highest-mountains,Wed Aug 31 2016
117,,XavierMartinezBartra,"[Year, GDP, Domestic demand, Consumer expenditure household, Consumer public adm, Gross capital, Equip. Goods others, Const., Ext. Balance , Foreign balance , Total exports goods and services, Exports goods and services, Foreign consump. Territory, Total imports goods and services, Imports goods and services, National residents consump. Abroad]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context In this DataSet we have a compilation of demand components of the GDP - Gross Domestic Product -of CATALONIA -one of the 17 autonomous comunities of Spain- and the spanish region with the highest GDP output.  Content Columns  GDP  Domestic demand  Consumer expenditure household  Consumer public adm     Gross capital  Equip. Goods others  Const.  Ext. Balance    Foreign balance  Total exports goods and services  Exports goods and services  Foreign consump.  Territory   Total imports goods and services  Imports goods and services  National residents consump. Abroad Acknowledgements All units of the DataFrame are presented in Millions of euros (Base 2010). The data has been extracted from the Idescat economic annual Accounts of Catalonia.,CSV,,"[business, social sciences, economics]",CC0,,,56,922,0.001953125,Macroeconomic GDP indicators,Catalonia GDP by demand components (2000-2016),https://www.kaggle.com/xavier14/catalonia-gdp-by-demand-components-20002016,Wed Aug 09 2017
118,,Andrea Girardi,"[id, Marathon, Name, Category, km4week, sp4week, CrossTraining, Wall21, MarathonTime, CATEGORY]","[numeric, string, string, string, numeric, numeric, string, numeric, numeric, string]","Context Every Marathoner has a time goal in mind and this is the result of all the training done in months of exercises. Long runs Strides Kilometers and phisical exercise all add improvement to the result. Marathon time prediction is an art generally guided by expert physiologists that prescribe the weekly exercises and the milestones to the marathon.  Unfortunately Runners have a lot of distractions while preparing the marathon work family illnes and therefore each one of us arrives to the marathon with his own story.  The ""simple"" approach is to look at data after the competition the Leaderboard.   But what if we could link the Marathon result to the training history of the Athlete? Could we find that ""non orthodox"" training plans give good results? The Athlete Training History As a start I'll take just two data from the Athlete History easy to extract. Two meaningful data the average km run during the 4 weeks before the marathon and the average speed that the athlete has run these km.   Meaningful because in the last month of the training I have the recap of all the previous months that brought me to the marathon.   Easy to extract because I can go to Strava and I have a ""side-by-side"" comparison myself and the reference athlete. I said easy well that's not so easy since I have to search every athlete and write down those numbers the exact day the marathon happened otherwise I will put in the average the rest days after the marathon. I've set my future work in extracting more data and build better algorithms. Thank you for helping me to understand or suggest. Content id  simple counter Marathon  the Marathon name where the data were extracted. I use the data coming out from Strava ""Side by side comparison"" and the data coming from the final marathon result Name  The athlete's name still some problems with UTF-8 I'll fix that soon Category  the sex and age group of a runner - MAM Male Athletes under 40 years - WAM Women under 40 Years - M40 Male Athletes between 40 and 45 years km4week  This is the total number of kilometers run in the last 4 weeks before the marathon marathon included. If for example the km4week is 100 the athlete has run 400 km in the four weeks before the marathon  sp4week  This is the average speed of the athlete in the last 4 training weeks. The average counts all the kilometers done included the slow kilometers done before and after the training. A typic running session can be of 2km of slow running then 12-14km of fast running and finally other 2km of slow running. The average of the speed is this number and with time this is one of the numbers that has to be refined cross training  If the runner is also a cyclist or a triathlete does it counts? Use this parameter to see if the athlete is also a cross trainer in other disciplines Wall21 In decimal. The tricky field. To acknowledge a good performance as a marathoner I have to run the first half marathon with the same split of the second half. If for example I run the first half marathon in 1h30m I must finish the marathon in 3h (for doing a good job). If I finish in 3h20m I started too fast and I hit ""the wall"". My training history is therefore less valid since I was not estimating my result Marathon time  In decimal. This is the final result. Based on my training history I must predict my expected Marathon time Category  This is an ancillary field. It gives some direction so feel free to use or discard it. It groups in  - A results under 3h  - B results between 3h and 3h20m  - C results between 3h20m and 3h40m  - D results between 3h40 and 4h Acknowledgements Thank you to the main Athletes data sources GARMIN and STRAVA The Goal of this Competition Based on my training history I must predict my expected Marathon time. Which other relevant data could help me to be more precise? Heart rate cadence speed training what else? And how could I get those data?",CSV,,[running],CC0,,,568,5216,0.005859375,Predict Marathon Results from Athletes Open Data Sources,Marathon time Predictions,https://www.kaggle.com/girardi69/marathon-time-predictions,Sat May 13 2017
119,,Rachael Tatman,[],[],Context Paranormal Romance is a subgenre of romance that combines fantasy and romance elements. Notable examples are the Twilight series and The Southern Vampire Mysteries which the T.V. show True Blood was based on.  Content This is a list of 4000 paranormal romance novel titles scraped from the web by Mark Riedl. Some longer titles have been truncated and end with ellipses (...).  Inspiration  Can you generate new titles using a Markov chain text generator? Some novel titles are truncated. Can you generate the complete version? Can you cluster novels into sub genres based on their titles? ,Other,,[literature],CC4,,,46,1007,0.0888671875,4000 paranormal romance novel titles,Paranormal Romance Novel Titles,https://www.kaggle.com/rtatman/paranormal-romance-novel-titles,Mon Jul 17 2017
120,,Myles O'Neill,[],[],Magic The Gathering (MTG or just Magic) is a trading card game first published in 1993 by Wizards of the Coast. This game has seen immense popularity and new cards are still released every few months. The strength of different cards in the game can vary wildly and as a result some cards now sell on secondary markets for as high as thousands of dollars. MTG JSON has an excellent collection of every single Magic Card - stored in JSON data. Version 3.6 (collected September 21 2016) of their database is provided here. Full documentation for the data is provided here http//mtgjson.com/documentation.html Also if you want to include images of the cards in your writeups you can grab them from the official Wizards of the Coast website using the following URL http//gatherer.wizards.com/Handlers/Image.ashx?multiverseid=180607&type=card Just replace the multiverse ID with the one provided in the mtgjson file.,{}JSON,,[games and toys],Other,,,619,12864,53,Analyze cards from this classic trading card game,Magic The Gathering Cards,https://www.kaggle.com/mylesoneill/magic-the-gathering-cards,Tue Sep 27 2016
121,,University of North Texas,"[state_code, state_name]","[numeric, string]",Content This dataset contains territorial claims across the entire interstate system between 1816-2001 and includes information on participants dates the significance of the claimed territories and militarization of these claims. A territorial claim is defined as explicit contention between two or more nation-states claiming sovereignty over a specific piece of territory. Official government representatives (i.e. individuals who are authorized to make or state foreign policy positions for their governments) must make explicit statements claiming sovereignty over the same territory. Our goal is to identify cases where nation-states have disagreed over specific issues in the modern era as well as measuring what made those issues valuable to them and studying how they chose to manage or settle those issues. The Issue Correlates of War (ICOW) project does not endorse official positions on any territorial claim. Inclusion/exclusion of specific cases and coding of details related to those cases follows strict guidelines presented in the project's coding manuals. Acknowledgements This data was collected by Professor Paul Hensel of the University of North Texas and his research assistants.,CSV,,"[oceans, international relations]",Other,,,103,1112,1,What territory claimed by two or more countries has caused the most violence?,"Disputed Territories and Wars, 1816-2001",https://www.kaggle.com/unt/disputed-territories,Thu Feb 02 2017
122,,"Open Sourcing Mental Illness, LTD","[Timestamp, Age, Gender, Country, state, self_employed, family_history, treatment, work_interfere, no_employees, remote_work, tech_company, benefits, care_options, wellness_program, seek_help, anonymity, leave, mental_health_consequence, phys_health_consequence, coworkers, supervisor, mental_health_interview, phys_health_interview, mental_vs_physical, obs_consequence, comments]","[numeric, numeric, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Dataset Information This dataset is from a 2014 survey that measures attitudes towards mental health and frequency of mental health disorders in the tech workplace. You are also encouraged to analyze data from the ongoing 2016 survey found here. Content This dataset contains the following data  Timestamp Age Gender Country state If you live in the United States which state or territory do you live in? self_employed Are you self-employed? family_history Do you have a family history of mental illness? treatment Have you sought treatment for a mental health condition? work_interfere If you have a mental health condition do you feel that it interferes with your work? no_employees How many employees does your company or organization have? remote_work Do you work remotely (outside of an office) at least 50% of the time? tech_company Is your employer primarily a tech company/organization? benefits Does your employer provide mental health benefits? care_options Do you know the options for mental health care your employer provides? wellness_program Has your employer ever discussed mental health as part of an employee wellness program? seek_help Does your employer provide resources to learn more about mental health issues and how to seek help? anonymity Is your anonymity protected if you choose to take advantage of mental health or substance abuse treatment resources? leave How easy is it for you to take medical leave for a mental health condition? mental_health_consequence Do you think that discussing a mental health issue with your employer would have negative consequences? phys_health_consequence Do you think that discussing a physical health issue with your employer would have negative consequences? coworkers Would you be willing to discuss a mental health issue with your coworkers? supervisor Would you be willing to discuss a mental health issue with your direct supervisor(s)? mental_health_interview Would you bring up a mental health issue with a potential employer in an interview? phys_health_interview Would you bring up a physical health issue with a potential employer in an interview? mental_vs_physical Do you feel that your employer takes mental health as seriously as physical health? obs_consequence Have you heard of or observed negative consequences for coworkers with mental health conditions in your workplace? comments Any additional notes or comments  Inspiration Some questions worth exploring  How does the frequency of mental health illness and attitudes towards mental health vary by geographic location? What are the strongest predictors of mental health illness or certain attitudes towards mental health in the workplace?  Acknowledgements The original dataset is from Open Sourcing Mental Illness and can be downloaded here.,CSV,,"[mental health, healthcare, human genetics]",CC4,,,8738,62213,0.2900390625,Survey on Mental Health in the Tech Workplace in 2014,Mental Health in Tech Survey,https://www.kaggle.com/osmi/mental-health-in-tech-survey,Thu Nov 03 2016
123,,Department of Justice,"[Agency, Department, Year, Requests Pending Start Year, Requests Received, Requests Processed, Requests Pending End Year, Released in Full Ratio, Released in Part Ratio, Denied in Full Ratio]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Since 1967 the Freedom of Information Act (FOIA) has provided the public the right to request access to records from any federal agency. It is often described as the law that keeps citizens in the know about their government. Federal agencies are required to disclose any information requested under the FOIA unless it falls under one of nine exemptions which protect interests such as personal privacy national security and law enforcement. As Congress the President and the Supreme Court have all recognized the Act is a vital part of our democracy. A FOIA request can be made for any agency record. You can also specify the format in which you wish to receive the records (for example printed or electronic form). The Act does not require agencies to create new records or to conduct research analyze data or answer questions when responding to requests. Each federal agency handles its own records in response to requests. There are currently one hundred agencies subject to the FOIA with several hundred offices that process FOIA requests. Content Annual Freedom of Information Act Reports are submitted to Congress and published by FOIA.gov to promote agency accountability for the administration of the Act.,CSV,,"[information, politics]",CC0,,,81,1453,0.0986328125,Which agency or department receives the most FOIA requests?,Freedom of Information Act Requests,https://www.kaggle.com/doj/foia-requests,Sat Jan 21 2017
124,,UCI Machine Learning,"[date, TL BASED ISE, USD BASED ISE, SP, DAX, FTSE, NIKKEI, BOVESPA, EU, EM]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset was used in the paper ""A novel Hybrid RBF Neural Networks model as a forecaster Statistics and Computing"" (Akbilgic et al) to show off a new forecasting algorithm. The paper showed good results when using a HRBF-NN model for predicting daily stock movements.  Content The data was collected (by the owners) from imkb.gov.tr and finance.yahoo.com and is organized by working days in the Istanbul Stock Exchange. Columns  Istanbul stock exchange national 100 index Standard & poor's 500 return index Stock market return index of Germany Stock market return index of UK Stock market return index of Japan Stock market return index of Brazil MSCI European index MSCI emerging markets index  Acknowledgements Akbilgic O. Bozdogan H. Balaban M.E. (2013) A novel Hybrid RBF Neural Networks model as a forecaster Statistics and Computing. DOI 10.1007/s11222-013-9375-7  PhD Thesis Oguz Akbilgic (2011) Hibrit Radyal TabanlÄ± Fonksiyon AÄŸlarÄ± ile DeÄŸiÅŸken SeÃ§imi ve Tahminleme Menkul KÄ±ymet YatÄ±rÄ±m KararlarÄ±na Ä°liÅŸkin Bir Uygulama Istanbul University This dataset was downloaded from the UCI ML Repository https//archive.ics.uci.edu/ml/datasets/ISTANBUL+STOCK+EXCHANGE Inspiration Use this dataset to create predictive algorithms. Then get rich!",CSV,,[finance],CC0,,,108,1728,0.060546875,Exchange data from 2009 to 2011,Istanbul Stock Exchange,https://www.kaggle.com/uciml/istanbul-stock-exchange,Wed Sep 20 2017
125,,Committee to Protect Journalists,"[Type, Date, Name, Sex, Country_killed, Organization, Nationality, Medium, Job, Coverage, Freelance, Local_Foreign, Source_fire, Type_death, Impunity_for_murder, Taken_captive, Threatened, Tortured]","[string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context CPJ began compiling detailed records on journalist deaths in 1992. CPJ applies strict journalistic standards when investigating a death. One important aspect of their research is determining whether a death was work-related. As a result they classify deaths as ""motive confirmed"" or ""motive unconfirmed."" Content The dataset contains 18 variables  Type CPJ classified deaths as motive confirmed or motive confirmed as well as Media Workers Date Name Sex Country_killed Organization Nationality Medium Job Coverage Freelance Local_Foreign Source_fire Type_death Impunity_for_murder Taken_captive Threatened Tortured  Acknowledgements The original dataset can be found here. Inspiration Some ideas for exploring the dataset  What is the trend in journalist deaths over time and how does this differ by type of death job coverage and country? Are there differences by sex and/or nationality? ",CSV,,"[news agencies, journalism, death, crime]",Other,,,913,6179,0.3056640625,Journalist Deaths from 1992-2016,Journalists Killed Worldwide Since 1992,https://www.kaggle.com/cpjournalists/journalists-killed-worldwide-since-1992,Sun Nov 13 2016
126,,US Department of Health and Human Services,[],[],"The Health Insurance Marketplace Public Use Files contain data on health and dental plans offered to individuals and small businesses through the US Health Insurance Marketplace.  Exploration Ideas To help get you started here are some data exploration ideas  How do plan rates and benefits vary across states? How do plan benefits relate to plan rates? How do plan rates vary by age? How do plans vary across insurance network providers?  See this forum thread for more ideas and post there if you want to add your own ideas or answer some of the open questions! Data Description This data was originally prepared and released by the Centers for Medicare & Medicaid Services (CMS). Please read the CMS Disclaimer-User Agreement before using this data. Here we've processed the data to facilitate analytics. This processed version has three components 1. Original versions of the data The original versions of the 2014 2015 2016 data are available in the ""raw"" directory of the download and ""../input/raw"" on Kaggle Scripts. Search for ""dictionaries"" on this page to find the data dictionaries describing the individual raw files. 2. Combined CSV files that contain In the top level directory of the download (""../input"" on Kaggle Scripts) there are six CSV files that contain the combined at across all years  BenefitsCostSharing.csv BusinessRules.csv Network.csv PlanAttributes.csv Rate.csv ServiceArea.csv  Additionally there are two CSV files that facilitate joining data across years  Crosswalk2015.csv - joining 2014 and 2015 data Crosswalk2016.csv - joining 2015 and 2016 data  3. SQLite database The ""database.sqlite"" file contains tables corresponding to each of the processed CSV files. The code to create the processed version of this data is available on GitHub.",CSV,,"[healthcare, economics]",CC0,,,10578,79954,11264,Explore health and dental plans data in the US Health Insurance Marketplace,Health Insurance Marketplace,https://www.kaggle.com/hhs/health-insurance-marketplace,Tue May 02 2017
127,,Lislejoem,"[StateCodes, State, Region, Division, Coast, Great Lakes, TotalC2010, TotalC2011, TotalC2012, TotalC2013, TotalC2014, TotalP2010, TotalP2011, TotalP2012, TotalP2013, TotalP2014, TotalE2010, TotalE2011, TotalE2012, TotalE2013, TotalE2014, TotalPrice2010, TotalPrice2011, TotalPrice2012, TotalPrice2013, TotalPrice2014, TotalC10-11, TotalC11-12, TotalC12-13, TotalC13-14, TotalP10-11, TotalP11-12, TotalP12-13, TotalP13-14, TotalE10-11, TotalE11-12, TotalE12-13, TotalE13-14, TotalPrice10-11, TotalPrice11-12, TotalPrice12-13, TotalPrice13-14, BiomassC2010, BiomassC2011, BiomassC2012, BiomassC2013, BiomassC2014, CoalC2010, CoalC2011, CoalC2012, CoalC2013, CoalC2014, CoalP2010, CoalP2011, CoalP2012, CoalP2013, CoalP2014, CoalE2010, CoalE2011, CoalE2012, CoalE2013, CoalE2014, CoalPrice2010, CoalPrice2011, CoalPrice2012, CoalPrice2013, CoalPrice2014, ElecC2010, ElecC2011, ElecC2012, ElecC2013, ElecC2014, ElecE2010, ElecE2011, ElecE2012, ElecE2013, ElecE2014, ElecPrice2010, ElecPrice2011, ElecPrice2012, ElecPrice2013, ElecPrice2014, FossFuelC2010, FossFuelC2011, FossFuelC2012, FossFuelC2013, FossFuelC2014, GeoC2010, GeoC2011, GeoC2012, GeoC2013, GeoC2014, GeoP2010, GeoP2011, GeoP2012, GeoP2013, GeoP2014, HydroC2010, HydroC2011, HydroC2012]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","The purpose of this data set is to allow exploration between various types of data that is commonly collected by the US government across the states and the USA as a whole. The data set consists of three different types of data  Census and Geographic Data; Energy Data; and  Economic Data.  When creating the data set I combined data from many different types of sources all of which are cited below. I have also provided the fields included in the data set and what they represent below. I have not performed any research on the data yet but am going to dive in soon. I am particularly interested in the relationships between various types of data (i.e. GDP or birth rate) in prediction algorithms. Given that I have compiled 5 years’ worth of data this data set was primarily constructed with predictive algorithms in mind. An additional note before you delve into the fields  * There could have been many more variables added across many different fields of metrics. I have stopped here but it could potentially be beneficial to observe the interaction of these variables with others (i.e. the GDP of certain industries the average age in a state the male/female gender ratio etc.) to attempt to find additional trends. Census and Geographic Data  StateCodes The state 2-letter abbreviations. Note that I added ""US"" for the United States. Region The number corresponding to the region the state lies within according to the 2010 census. (1 = Northeast 2 = Midwest 3 = South 4 = West) Division The number corresponding to the division the state lies within according to the 2010 census. (1 = New England 2 = Middle Atlantic 3 = East North Central 4 = West North Central 5 = South Atlantic 6 = East South Central 7 = West South Central 8 = Mountain 9 = Pacific) Coast Whether the state shares a border with an ocean. (1 = Yes 0 = No) Great Lakes Whether the state shares a border with a great lake. (1 = Yes 0 = No CENSUS2010POP 4/1/2010 resident total Census 2010 population POPESTIMATE{year} 7/1/{year} resident total population estimate RBIRTH{year} Birth rate in period 7/1/{year - 1} to 6/30/{year} RDEATH{year} Death rate in period 7/1/{year - 1} to 6/30/{year} RNATURALINC{year} Natural increase rate in period 7/1/{year - 1} to 6/30/{year} RINTERNATIONALMIG{year} Net international migration rate in period 7/1/{year - 1} to 6/30/{year} RDOMESTICMIG{year} Net domestic migration rate in period 7/1/{year - 1} to 6/30/{year} RNETMIG{year} Net migration rate in period 7/1/{year - 1} to 6/30/{year}  As noted from the census  Net international migration for the United States includes the international migration of both native and foreign-born populations. Specifically it includes (a) the net international migration of the foreign born (b) the net migration between the United States and Puerto Rico (c) the net migration of natives to and from the United States and (d) the net movement of the Armed Forces population between the United States and overseas. Net international migration for Puerto Rico includes the migration of native and foreign-born populations between the United States and Puerto Rico.  Codes for most of the data information about the geographic terms and coditions and more information about the methodology behind the population estimates can be found on the US Census website. Energy Data  TotalC{year} Total energy consumption in billion BTU in given year. TotalP{year} Total energy production in billion BTU in given year. TotalE{year} Total Energy expenditures in million USD in given year. TotalPrice{year} Total energy average price in USD/million BTU in given year. TotalC{first year}–{second year} The first year’s total energy consumption divided by the second year’s total energy consumption times 100. (The percent change between years in total energy consumption.) TotalP{first year}–{second year} The first year’s total energy production divided by the second year’s total energy production times 100. (The percent change between years in total energy production.) TotalE{first year}–{second year} The first year’s total energy expenditure divided by the second year’s total energy expenditure times 100. (The percent change between years in total energy expenditure.) TotalPrice{first year}–{second year} The first year’s total energy average price divided by the second year’s total energy average price times 100. (The percent change between years in total energy average price.) BiomassC{year} Biomass total consumption in billion BTU in given year. CoalC{year} Coal total consumption in billion BTU in given year. CoalP{year} Coal total production in billion BTU in given year. CoalE{year} Coal total expenditures in million USD in given year. CoalPrice{year} Coal average price in USD per million BTU in given year. ElecC{year} Electricity total consumption in billion BTU in given year. ElecE{year} Electricity total expenditures in million USD in given year. ElecPrice{year} Electricity average price in USD per million BTU in given year. FossFuelC{year} Fossil fuels total consumption in billion BTU in given year. GeoC{year} Geothermal energy total consumption in billion BTU in given year. GeoP{year} Geothermal energy net generation in the electric power sector in million kilowatt hours in given year. HydroC{year} Hydropower total consumption in billion BTU in given year. HydroP{year} Hydropower total net generation in million kilowatt hours in given year. NatGasC{year} Natural gas total consumption (including supplemental gaseous fuels) in billion BTU in given year. NatGasE{year} Natural gas total expenditures in million USD in given year. NatGasPrice{year} Natural gas average price in USD per million BTU in given year.  LPGC{year} LPG total consumption in billion BTU in given year. LPGE{year} LPG total expenditures in million USD in given year. LPGPrice{year} LPG average price in USD per million BTU in given year.   Notes  BTU stands for British Thermal Unit and is a unit of measurement for energy. One BTU is equal to the amount of energy used to raise the temperature of one pound of water on degree Fahrenheit. Many other types of energy and their associated consumption production expenditure and price totals can be found from the EIA; this is where I received the data I used in compiling this dataset.  Economic Data  GDP{year}{quarter} The GDP in the provided quarter of the given year (in million USD). GDP{year} The average GDP throughout the given year (in million USD).  Notes  The GDP is reported by the Bureau of Economic Analysis from the U.S. Department of Commerce and measures the value of the goods and services produced by the economy in a given period. The quarterly GDP data can be downloaded from the BEA. The yearly GDP data can be downloaded from the BEA.  Image credit http//www.freelargeimages.com/wp-content/uploads/2014/11/Map_of_united_states-3.jpg",CSV,,[energy],CC0,,,1590,12218,0.072265625,Examining the relationships between various data collected by the US government,"United States Energy, Census, and GDP 2010-2014",https://www.kaggle.com/lislejoem/us_energy_census_gdp_10-14,Sat Mar 25 2017
128,,Ben Hamner,[],[],The Bay Area Bike Share enables quick easy and affordable bike trips around the San Francisco Bay Area. They make regular open data releases (this dataset is a transformed version of the data from this link) plus maintain a real-time API.  Exploration Ideas  How does weather impact bike trips? How do bike trip patterns vary by time of day and the day of the week? ,CSV,,"[cycling, road transport]",Other,,,5179,31061,4096,Anonymized bike trip data from August 2013 to August 2015,SF Bay Area Bike Share,https://www.kaggle.com/benhamner/sf-bay-area-bike-share,Tue Jun 14 2016
129,,Nolan Conaway,[],[],"I used the What.CD API to obtain information on all torrents tagged ""hip.hop"" (75719 releases as of October 22 2016). The result is a sqlite database with two tables torrents This table contains information about all 75719 releases in the database. It has the following fields  groupName (text) Release title totalSnatched  (integer) Number of times the release has been downloaded. artist (text) Artist / group name. groupYear (integer) Release year. releaseType (text) Release type (e.g. album single mixtape)  groupId (integer) Unique release identifier from What.CD. Used to ensure no releases are duplicates. id (integer) unique identifier (essentially an index).  tags This table contains the tags associated with each of the releases in the torrents table. Because being tagged 'hip.hop' is requisite for inclusion in the database this tag is not listed.  index (integer) Index. id (integer) Release identifier (can be matched with id field in the torrents table). tag (text) Tag.  I love hip hop so this database has become my go-to data for satisfying curiosities I might have.",SQLite,,[music],Other,,,176,2771,10,"75k releases tagged ""hip.hop"" on What.CD",What.CD Hip Hop,https://www.kaggle.com/nolanbconaway/whatcd-hiphop,Mon Oct 31 2016
130,,University of Connecticut,"[Year, CIRI Country ID, Country, UN Country ID, UN Region, UN Subregion, Physical Integrity Rights Index, Disappearance, Extrajudicial Killing, Political Imprisonment, Torture, Empowerment Rights Index (Old), Empowerment Rights Index (New), Freedom of Assembly and Association, Freedom of Foreign Movement, Freedom of Domestic Movement, Freedom of Movement (Old), Freedom of Speech, Electoral Self-Determination, Freedom of Religion (Old), Freedom of Religion (New), Employment Rights, Women’s Economic Rights, Women’s Political Rights, Women’s Social Rights, Independence of Judiciary]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Content The Cingranelli-Richards human rights database contains quantitative information on government recognition of 15 internationally recognized human rights in more than 200 countries from 1981-2011. It includes measures of the practices of governments that allow or impede citizens who wish to exercise their physical integrity rights like the rights not to be tortured summarily executed disappeared or imprisoned for political beliefs; civil liberties such as free speech freedom of association and assembly freedom of movement freedom of religion and the right to participate in the selection of government leaders; employment rights; and rights of women to equal treatment politically economically and socially. The database is designed for use by scholars and students who seek to test theories about the causes and consequences of human rights violations as well as policy makers and analysts who seek to estimate the human rights effects of a wide variety of institutional changes and public policies including democratization economic aid military aid structural adjustment and humanitarian intervention. The primary source of information about human rights practices is obtained from the annual United States Department of State’s Country Reports on Human Rights Practices. Coders are instructed to use this source for all variables. For a group of four rights known as ""Physical Integrity Rights"" (the rights to freedom from extrajudicial killing disappearance torture and political imprisonment) coders use Amnesty International’s Annual Report in addition the Department of State reports. If discrepancies exist between the two sources coders are instructed to treat the Amnesty International report as authoritative; some scholars believe that this step is necessary to remove a potential bias in favor of American allies. Acknowledgements The human rights database was developed updated and published by Professor David Cingranelli of Binghamton University SUNY Professor David Richards of the University of Connecticut's Human Rights Institute and Professor K. Chad Clay of the University of Georgia.",CSV,,"[crime, war, international relations]",Other,,,241,2152,0.513671875,Human rights data from US State Department and Amnesty International,Human Rights Project: Country Profiles by Year,https://www.kaggle.com/uconn/human-rights,Tue Jan 31 2017
131,,AdhokshajaPradeep,"[, Name, Inaugural Address, Date, text]","[numeric, string, string, dateTime, string]",Context Every Presidency starts off with the Inaugural Address. This defines the course for the next 4 years. How do the length word usage lexical diversity change from President to President? Content The data was scraped from http//www.bartleby.com/124/ using R's rvest web scraping library. The procedure can be found here The data set is in the .csv format. The columns are  Name Inaugural Address  Date and text. Acknowledgements I would like to thank http//www.bartleby.com for making their data available for free. Inspiration I saw a documentary by Vox on Presidential Inaugural Speeches. They conducted a study on the common characteristics amongst speeches by influential presidents. Through a data driven approach we can find several interesting insights which help correlate a President's influence and his inaugural speech. What word patterns do influential Presidents use often? How does speech length vary?,CSV,,"[politics, linguistics]",CC0,,,241,2126,0.7685546875,"US president name, date, and speech text",Presidential Inaugural Addresses,https://www.kaggle.com/adhok93/presidentialaddress,Fri Mar 31 2017
132,,Rachael Tatman,"[language (ISO key), reference (as Bibtex entry), movement, location, handshape, palm orientation, number of hands, Non-manuals, others, notes]","[string, string, string, string, string, string, string, string, string, string]",Context Signed languages have many unique ways to encode meaning. Some of these ways include using different handshapes motions which direction the palm and wrist are facing whether one hand or two is used and facial expressions. This dataset compares which different sign languages use which of these grammatical building blocks. Content This database contains information on the parameters used by 87 signed languages taken from various academic sources and compiled by hand. Acknowledgements This dataset was collected by Rachael Tatman during the process of linguistic research and is released to the public domain. The database can be cited by reference to this paper   Tatman R. (2015). The Sign Language Analyses (SLAY) Database⋆ (Vol. 33). University of Washington Working Papers in Linguistics.  https//depts.washington.edu/uwwpl/vol33/2-Tatman-SLAY.pdf  Inspiration One analysis of this data is presented can be found in this paper  but there are plenty of additional questions that could be asked. Some examples - Does a language’s geographic location factor into what parameters it uses? - Does the year that a grammatical analysis was published have an effect on how many parameters it proposes for a language? You may also be interested in  Atlas of Pidgin and Creole Language Structures World Language Family Map World Atlas of Language Structures Information on the linguistic structures in 2679 languages ,CSV,,"[languages, linguistics]",CC0,,,103,1487,0.029296875,A database of information on the grammers of signed languages,The Sign Language Analyses (SLAY) Database,https://www.kaggle.com/rtatman/sign-language-analyses,Fri Jul 14 2017
133,,Aniruddha Achar,[],[],"Context This data was compiled as part of our undergrad project that used machine learning to classify songs based on themes or activities songs are associated with. For the project we four activities were choose.   Dinner Songs that sound good when played in a dinner setting or at a restaurant. Sleep Songs that promote sleep when they are played. Party Songs that sound good when played at a party. Workout Songs that sound good when one is exercising/ working out.  The collection of data started with collecting playlist details form Spotify. Spotify web API was used for the collection of the playlist of each category. Track title album name and artist names were used to extract low level and high level Audio features like MFCC Spectral centroid Spectral Roll-off Spectral Bandwidth Tempo Spectral Contrast and Root Mean Square Energy of the songs. For ease of computation the mean of the values were calculated and added to the tables. Data was also curated using Spotify's audio analysis API. A larger set of songs is part of this data set.  Content The data set has eight tables.   Four tables with names playlist_audio_features have the signal processing features like MFCC spectral centroid etc. Four more tables with names playlist_spotify_features have the data extracted from Spotify's audio feature API. These tables have larger number of features. The data set size is quite large.  Description of the ""playlist""_audio_features columns  The first column has the simple integer id if the track. (This id is local to that file). The second column has the name of the track. The third column name mfcc has the mean of the calculated MFCC for that track. 20 MFC coefficients were extracted from one frame of the track. The forth column is named scem This is the mean of Spectral centroid. Spectral centroid was calculated for each frame. The fifth column is named scom This is the mean of Spectral contrast. Spectral contrast was calculated for each frame. The sixth column is named srom This is the mean of Spectral Roll-off. Spectral roll-off was calculated for each frame. The seventh column is named sbwm This is the mean of Spectral Bandwidth. Spectral Bandwidth was calculated for each frame. The eight column is name tempo This is the estimated tempo of the track. The ninth column is name rmse This is the mean of the RSME was calculated for each frame.  Description of the _spotify_features columns  id This is the Spotify id of the track. name This is the name of the track. url This is a Spotify uri of the track. artist This is a one or more artists who worked on the track. 5-13 Description of each of the column can be found at https//developer.spotify.com/web-api/get-audio-features/  Acknowledgements We would like to thank Librosa an opensource audio feature extraction library in python for developing a great tool. We would also thank the large research done on music genre classification using audio feature which helped us in developing this data set as well as  the classification. A special thanks to Spotify",CSV,,"[music, acoustics, sound technology]",CC4,,,416,5094,0.65234375,"Playlist is based on themes like dinner, sleep, party and workout",Audio Features for Playlist Creation,https://www.kaggle.com/aniruddhaachar/audio-features,Fri Mar 03 2017
134,,NLTK Data,[],[],"Context VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media and works well on texts from other domains. The VADER lexicon is an empirically validated by multiple independent human judges VADER incorporates a ""gold-standard"" sentiment lexicon that is especially attuned to microblog-like contexts.  The documentation of the lexicon and the algorithm can be found from the original implementation https//github.com/cjhutto/vaderSentiment  The NLTK port has slight modification to integrate with the NLTK interfaces and it comes with better Python3 support https//github.com/nltk/nltk/blob/develop/nltk/sentiment/vader.py Citation Hutto C.J. & Gilbert E.E. (2014). VADER A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor MI June 2014.  Inspiration We look forward to more sentiment analysis datasets perhaps a JEDI (Joint Entropy Decay Iteration) algorithm someday?",Other,,[linguistics],Other,,,59,1399,0.4140625,Lexicon use for the Vader Sentiment Algorithm,Vader Lexicon,https://www.kaggle.com/nltkdata/vader-lexicon,Sat Aug 19 2017
135,,NLTK Data,[],[],Context The averaged_perceptron_tagger.zip contains the pre-trained English [Part-of-Speech (POS]](https//en.wikipedia.org/wiki/Part_of_speech) tagger in NLTK.   The nltk.tag.AveragedPerceptronTagger is the default tagger as of NLTK version 3.1. The model was trained on on Sections 00-18 of the Wall Street Journal sections of OntoNotes 5 The original implementation comes from Matthew Honnibal it outperforms the predecessor maximum entropy POS model in NLTK.  The version from Textblob was ported over to NLTK in pull-request #122. Acknowledgements  Credit goes to Matthew Honnibal.  The reimplementation in Textblob by Steven Loria Re-reimplementation in NLTK by Long Duong ,Other,,[],Other,,,28,956,6,The model that nltk.pos_tag loads,Averaged Perceptron Tagger,https://www.kaggle.com/nltkdata/averaged-perceptron-tagger,Fri Aug 18 2017
136,,Megan Risdal,"[Nr, Rmag, e.Rmag, ApDRmag, mumax, Mcz, e.Mcz, MCzml, chi2red, UjMAG, e.UjMAG, BjMAG, e.BjMAG, VjMAG, e.VjMAG, usMAG, e.usMAG, gsMAG, e.gsMAG, rsMAG, e.rsMAG, UbMAG, e.UbMAG, BbMAG, e.BbMAG, VnMAG, e.VbMAG, S280MAG, e.S280MA, W420FE, e.W420FE, W462FE, e.W462FE, W485FD, e.W485FD, W518FE, e.W518FE, W571FS, e.W571FS, W604FE, e.W604FE, W646FD, e.W646FD, W696FE, e.W696FE, W753FE, e.W753FE, W815FS, e.W815FS, W856FD, e.W856FD, W914FD, e.W914FD, W914FE, e.W914FE, UFS, e.UFS, BFS, e.BFS, VFD, e.VFD, RFS, e.RFS, IFD, e.IFD]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset was obtained here and their description is reproduced below. Astronomical background Galaxies are fundamental structures in the Universe.  Our Sun lives in the Milky Way Galaxy we can see as a patchy band of light across the sky.  The components of a typical galaxy are a vast number of stars (total mass ~106-1011 Mo where Mo is the unit of a solar mass) a complex interstellar medium of gas and dust from which stars form (typically 1-100% of the stellar component mass) a single supermassive black hole at the center (typically <1% of the stellar component mass) and a poorly understood component called Dark Matter with mass ~5-10-times all the other components combined. Over the ~14 billion years since the Big Bang  the rate at which galaxies convert interstellar matter into stars has not been constant and thus the brightness and color of galaxies change with cosmic time.  This phenomenon has several names in the astronomical community the history of star formation in the Universe chemical evolution of galaxies or simply galaxy evolution.  A major effort over several decades has been made to quantify and understand galaxy evolution using telescopes at all wavelengths. The traditional tool for such studies has been optical spectroscopy which easily reveals signatures of star formation in nearby galaxies.  However to study star formation in the galaxies recently emerged after the Big Bang we must examine extremely faint galaxies which are too faint for spectroscopy even using the biggest available telescopes.  A feasible alternative is to obtain images of faint galaxies at random locations in the sky in narrow spectral bands and thereby construct crude spectra.  First statistical analysis of such multiband photometric datasets are used to classify galaxies stars and quasars.  Second for the galaxies multivariate regression is made to develop photometric estimates of redshift which is a measure both of distance from us and age since the Big Bang.  Third one can examine galaxy colors as a function of redshift (after various corrections are made) to study the evolution of star formation. The present dataset is taken after these first two steps are complete. Contents Wolf et al. (2004) provide the first public catalog of a large dataset (63501 objects) with brightness measurements in 17 bands in the visible band.  (Note that the Sloan Digital Sky Survey provides a much larger dataset of 108 objects with measurements in 5 bands.)  We provide here a subset of their catalog with 65 columns of information on 3462 galaxies.  These are objects in the Chandra Deep Field South field which Wolf and colleagues have classified as `Galaxies'.  The column headings are formally described in their Table 3 and the columns we provide are summarized here with brief commentary Col 1  Nr object number Col 2-3 Total R (red band) magnitude and its error.  This was the band at which the basic catalog was constructed.  Magnitudes are inverted logarithmic measures of brightness.  A  galaxy with R=21 is 100-times brighter than one with R=26.  The error is the standard deviation derived from detailed knowledge of the measurement process.  This dataset is an excellent example of astronomical datasets where each variable is accompanied by heteroscedastic measurement errors of known variances.  Col 4-5  ApDRmag is the difference between the total and aperture magnitude in the R band.  This is a rough measure of the size of the galaxy in the image where ApDRmag=0 corresponds to a point source.  Negative values are not physically meaningful.  mu_max is the central surface brightness of the object in the R band.  The difference between Rmag and mu_max should also be an indicator of galaxy size. Col 6-9 Mcz and MCzml are two redshift estimates.  Mcz is the preferred value.  e.Mcz is its estimated error and chi2red is the reduced chi-squared value of the least-squares fit of the 17-band magnitudes to the best-fit template galaxy spectrum.  Galaxies with large e.Mcz or chi2red might be omitted as unreliable. Col 10-29 These give the absolute magnitudes (i.e. intrinsic luminosities) of the galaxy in 10 bands with their measurement errors.  They are based on the measured magnitudes and the redshifts and represent the intrinsic luminosities of the galaxies; a galaxy with M=-15 is 100-times less luminous than one with M=-20.    These magnitudes are not all independent of each others but the are important for representing intrinsic properties of the galaxies.  Below is one of several redshift-stratified plots of the B-band absolute magnitude (abscissa) against the difference of magnitude (i.e. ratio of luminosities) between the 2800A ultraviolet and blue band which is a sensitive indicator of star formation.  A redshift-dependent bimodal distribution is seen.  Col 30-55  Observed brightnesses in 13 bands in sequence from 420 nm in the ultraviolet to 915 nm in the far red.  These are given in linear variables with units of photon flux densities photons/m2/s/nm.  Again each measurement is accompanied by a measurement error which can be used to distinguish measurement from intrinsic dispersions in the distributions. Col 56-65 Observed brightnesses in 5 traditional broad spectral bands UBVRI.  These are largely redundant with the 13 bands in the previous columns.  Statistical exercises  Examine basic characteristics of the survey which are not of scientific interest.  These include the absence of high-redshift (i.e. distant) high-absolute-magnitude (i.e. faint) galaxies; the dropoff in flux with redshift; the dropoff in image size with redshift. Study these two populations as a function of redshift to investigate the evolution of star formation.  ,CSV,,"[astronomy, space]",Other,,,131,1841,2,Galaxies with brightness measurements in 17 visible bands,COMBO-17 Galaxy Dataset,https://www.kaggle.com/mrisdal/combo17-galaxy-dataset,Mon Apr 17 2017
137,,Athni,"[, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, roll_belt, pitch_belt, yaw_belt, total_accel_belt, kurtosis_roll_belt, kurtosis_picth_belt, kurtosis_yaw_belt, skewness_roll_belt, skewness_roll_belt.1, skewness_yaw_belt, max_roll_belt, max_picth_belt, max_yaw_belt, min_roll_belt, min_pitch_belt, min_yaw_belt, amplitude_roll_belt, amplitude_pitch_belt, amplitude_yaw_belt, var_total_accel_belt, avg_roll_belt, stddev_roll_belt, var_roll_belt, avg_pitch_belt, stddev_pitch_belt, var_pitch_belt, avg_yaw_belt, stddev_yaw_belt, var_yaw_belt, gyros_belt_x, gyros_belt_y, gyros_belt_z, accel_belt_x, accel_belt_y, accel_belt_z, magnet_belt_x, magnet_belt_y, magnet_belt_z, roll_arm, pitch_arm, yaw_arm, total_accel_arm, var_accel_arm, avg_roll_arm, stddev_roll_arm, var_roll_arm, avg_pitch_arm, stddev_pitch_arm, var_pitch_arm, avg_yaw_arm, stddev_yaw_arm, var_yaw_arm, gyros_arm_x, gyros_arm_y, gyros_arm_z, accel_arm_x, accel_arm_y, accel_arm_z, magnet_arm_x, magnet_arm_y, magnet_arm_z, kurtosis_roll_arm, kurtosis_picth_arm, kurtosis_yaw_arm, skewness_roll_arm, skewness_pitch_arm, skewness_yaw_arm, max_roll_arm, max_picth_arm, max_yaw_arm, min_roll_arm, min_pitch_arm, min_yaw_arm, amplitude_roll_arm, amplitude_pitch_arm, amplitude_yaw_arm, roll_dumbbell, pitch_dumbbell, yaw_dumbbell, kurtosis_roll_dumbbell, kurtosis_picth_dumbbell, kurtosis_yaw_dumbbell, skewness_roll_dumbbell, skewness_pitch_dumbbell, skewness_yaw_dumbbell, max_roll_dumbbell, max_picth_dumbbell, max_yaw_dumbbell, min_roll_dumbbell, min_pitch_dumbbell, min_yaw_dumbbell, amplitude_roll_dumbbell, amplitude_pitch_dumbbell]","[numeric, string, numeric, numeric, dateTime, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context Using devices such as Jawbone Up Nike FuelBand and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement – a group of enthusiasts who take measurements about themselves regularly to improve their health to find patterns in their behavior or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do but they rarely quantify how well they do it. Our goal here will be to use data from accelerometers on the belt forearm arm and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here http//groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). Content The dataset contains about 160 predictors (most of which are not required) and classifiers column is 'classe' and the exercise pattern is classified into 5 types- A B C D E Acknowledgements Velloso E.; Bulling A.; Gellersen H.; Ugulino W.; Fuks H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart Germany ACM SIGCHI 2013. Read more http//groupware.les.inf.puc-rio.br/har#wle_paper_section#ixzz4dPxKFugX Inspiration What better ways of cleaning up the data? Which model will fit it best and how to go about handling it in R,CSV,,[],Other,,,376,4996,12,What does your exercise pattern fall into?,Exercise Pattern Prediction,https://www.kaggle.com/athniv/exercisepatternpredict,Thu Apr 06 2017
138,,Myles O'Neill,"[ability, description]","[string, string]",Pokemon Sun and Moon (released November 18th 2016) are the latest games in the widely popular Pokemon video game franchise. Pokemon games are usually released in pairs (red and blue gold and silver x and y etc.) and collectively each pair that introduces new pokemon to the game is known as a Generation. Pokemon Sun and Moon are the 7th Generation adding new pokemon to the franchise as well as adjusting the stats and movesets of some of the older pokemon. (Please note that the recently popular PokemonGo games are unrelated to the Pokemon Video Games). This dataset contains a full set of in-game statistics for all 802 pokemon in the Sun and Moon. It also includes full information on which pokemon can learn which moves (movesets.csv) what moves can do (moves.csv) and how damage is modified by pokemon type (type-chart.csv). Pokemon Battle Simulation With the level of detail in the data provided here it is possible to almost fully simulate pokemon battles using the information provided (status effects and some other nuances are still missing). If you are interested in simulating how Pokemon battles would pan out between different opponents make sure to read up on the math behind how Pokemon battles work.  ,CSV,,"[games and toys, video games]",Other,,,1312,13550,2,Explore all 802 Pokemon from the newly released 7th Generation,Pokemon Sun and Moon (Gen 7) Stats,https://www.kaggle.com/mylesoneill/pokemon-sun-and-moon-gen-7-stats,Sat Nov 19 2016
139,,Jean-Phillipe,"[Occupation, All_workers, All_weekly, M_workers, M_weekly, F_workers, F_weekly]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",Many people say the gender gap in income levels is overstated in the United States where some say that inequality in the labor force is a thing of the past. Is there a gender gap at all? Is it stronger in some industries than in others? This dataset retrieved from the Bureau of Labor Statistics shows the median weekly incomes for 535 different occupations. The data encompasses information for all working American citizens as of January 2015. The incomes are broken into male and female statistics preceded by the total median income when including both genders. The data has been re-formatted from the original PDF-friendly arrangement to make it easier to clean and analyze. Analysis thus far has found that there is indeed a sizable gender gap between male and female incomes. Use of this dataset should cite the Bureau of Labor Statistics as per their copyright information  The Bureau of Labor Statistics (BLS) is a Federal government agency and everything that we publish both in hard copy and electronically is in the public domain except for previously copyrighted photographs and illustrations. You are free to use our public domain material without specific permission although we do ask that you cite the Bureau of Labor Statistics as the source. ,CSV,,"[gender, employment, income]",CC0,,,2601,14977,0.0302734375,Analyze gender gap and differences in industry's incomes,U.S. Incomes by Occupation and Gender,https://www.kaggle.com/jonavery/incomes-by-career-and-gender,Fri Sep 23 2016
140,,Hendrik Hilleckes,"[aid, type, url, gender]","[string, string, string, string]","Context I was thinking about a dataset that I could provide and when I was reading through the LiveFromNewYork subreddit I got the idea what about a Saturday Night Live dataset? Wouldn't it be fun to analyze the data about a TV show that airs since the 70s? Content I aim to improve the dataset over time and update the files with more data. But I think that I have enough already so that people can work with it. There are files for the following objects  season episode title actor actor_title (mapping of actors and titles) rating (episode rating from IMDb.com)  Acknowledgements A lot of the data comes from http//www.snlarchives.net where Joel Navaroli (@snlmedia) created a great snl archive. You can find everything about SNL there. Want to know about the 5th sketch in the 3rd episode in season 13? Go there to find out! I got the ratings from IMDb.com.  I used Scrapy to get the data from the websites. Inspiration Since SNL is such a long running TV show I thought it would be interesting to see how it developed over time. There are also some prejudices around like ""there was a big slump from season X to Y"". Do the user ratings reflect that? I provided an example analysis so that everyone can get started easily with the data. Source You can find everything about the dataset in the GitHub repository http//www.github.com/hhllcks/snldb",CSV,,"[popular culture, film]",Other,,,653,6846,1,Over 40 seasons of hilarious data,Saturday Night Live,https://www.kaggle.com/hhllcks/snldb,Fri Feb 09 2018
141,,Centers for Medicare & Medicaid Services,"[Provider ID, Hospital Name, Address, City, State, ZIP Code, County Name, Phone Number, Hospital Type, Hospital Ownership, Emergency Services, Meets criteria for meaningful use of EHRs, Hospital overall rating, Hospital overall rating footnote, Mortality national comparison, Mortality national comparison footnote, Safety of care national comparison, Safety of care national comparison footnote, Readmission national comparison, Readmission national comparison footnote, Patient experience national comparison, Patient experience national comparison footnote, Effectiveness of care national comparison, Effectiveness of care national comparison footnote, Timeliness of care national comparison, Timeliness of care national comparison footnote, Efficient use of medical imaging national comparison, Efficient use of medical imaging national comparison footnote, Location]","[numeric, string, string, string, string, numeric, string, numeric, string, string, string, string, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string]","Context There are all sorts of reasons why you'd want to know a hospital's quality rating.  Your mom is having her second hip replacement. Her first one went terribly and you're nervous about how she'll do. Which hospital would you suggest she have her surgery? You're selecting a health plan on your state's Exchange but your top two choices partner with different hospitals. How will you decide which plan to pick? Your brother has Cystic Fibrosis and has to go to the ER frequently. He hates waiting. Which hospitals/states provide care in the timeliest manner? Your in-laws moved to Florida recently to retire and have been trying to convince you to move there too. You're looking for any way possible to show them that your state is better. Does your state have better hospitals?  Every hospital in the United States of America that accepts publicly insured patients (Medicaid or MediCare) is required to submit quality data quarterly to the Centers for Medicare & Medicaid Services (CMS). There are very few hospitals that do not accept publicly insured patients so this is quite a comprehensive list. Content This file contains general information about all hospitals that have been registered with Medicare including their addresses type of hospital and ownership structure. It also contains information about the quality of each hospital in the form of an overall rating (1-5 where 5 is the best possible rating & 1 is the worst) and whether the hospital scored above same as or below the national average for a variety of measures.  This data was updated by CMS on July 25 2017. CMS' overall rating includes 60 of the 100 measures for which data is collected & reported on Hospital Compare website (https//www.medicare.gov/hospitalcompare/search.html). Each of the measures have different collection/reporting dates so it is impossible to specify exactly which time period this dataset covers.  For more information about the timeframes for each measure see  https//www.medicare.gov/hospitalcompare/Data/Data-Updated.html# For more information about the data itself APIs and a variety of formats see https//data.medicare.gov/Hospital-Compare Acknowledgements Attention Works of the U.S. Government are in the public domain and permission is not required to reuse them. An attribution to the agency as the source is appreciated. Your materials however should not give the false impression of government endorsement of your commercial products or services. See 42 U.S.C. 1320b-10.  Inspiration Which hospital types & hospital ownerships are most common? Which hospital types & ownerships are associated with better than average ratings/mortality/readmission/etc? What is the average hospital rating by state? Which hospital types & hospital ownerships are more likely to have not submitted proper data (""Not Available"" & ""Results are not available for this reporting period"")? Which parts of the country have the highest & lowest density of religious hospitals?",CSV,,"[healthcare, hospitals, public health]",Other,,,1376,11250,3,General information & quality ratings for almost all US hospitals,Hospital General Information,https://www.kaggle.com/cms/hospital-general-information,Thu Aug 10 2017
142,,Ben Dilday,"[retroid, namefirst, namelast, birthyear, birthmonth, birthday, birthstate, birthcity, time_birth_1900, birth_lat, birth_lon]","[string, string, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric]","This data set comprises events for major league baseball provided by http//retrosheet.org.  The information used here was obtained free of  charge from and is copyrighted by Retrosheet.  Interested  parties may contact Retrosheet at ""www.retrosheet.org"".  Roughly speaking an event is an outcome in a baseball game. This includes the end result of a plate appearance (strikeout out in the field hit base on balls) events that occur within a plate appearance (stolen bases caught stealing) and rare other occurrences. The retrosheet event data prior to 1955 are not complete. The data subsequent to 1988 include pitch counts while the data prior do not. The data here cover the years 1970-2015 in three divisions (1970-1992 1993-2004 2005-2015) that correspond roughly to distinct eras with different run-scoring environments. These data have specifically been obtained with a mix of the data dumps available at baseball heatmaps and with the py-retrosheet Python package available on github. I have augmented the data provided by retrosheet with some additional fields. Most substantively the rows include the wOBA value of the event in the field woba_pts and an estimated time stamp in units of seconds since Jan. 1 1900 (time_since_1900). The conversion from retrosheet files to sql and csv is done by the chadwick software. A detailed description of all of the fields is available on the documentation for chadwick http//chadwick.sourceforge.net/doc/cwevent.html. In order to keep the file sizes down I have limited the fields in this data set to a subset of the fields described in the chadwick documentation. The master.csv file is a subset of the Baseball Databank data and is released under a Creative Commons Attribution-ShareAlike 3.0 Unported License.https//creativecommons.org/licenses/by-sa/3.0/. More details are available on the Baseball Databank github https//github.com/chadwickbureau/baseballdatabank",CSV,,"[baseball, history]",CC0,,,351,4356,960,A granular history of baseball,Retrosheet events 1970 - 2015,https://www.kaggle.com/bdilday/retrosheet-events-1970-2015,Sun Aug 21 2016
143,,edX,"[Institution, Course Number, Launch Date, Course Title, Instructors, Course Subject, Year, Honor Code Certificates, Participants (Course Content Accessed), Audited (> 50% Course Content Accessed), Certified, % Audited, % Certified, % Certified of > 50% Course Content Accessed, % Played Video, % Posted in Forum, % Grade Higher Than Zero, Total Course Hours (Thousands), Median Hours for Certification, Median Age, % Male, % Female, % Bachelor's Degree or Higher]","[string, string, dateTime, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context In 2012 the Massachusetts Institute of Technology (MIT) and Harvard University launched open online courses on edX a non-profit learning platform co-founded by the two institutions. Four years later what have we learned about these online “classrooms” and the global community of learners who take them? Content This report provides data on 290 Harvard and MIT online courses 250 thousand certifications 4.5 million participants and 28 million participant hours on the edX platform since 2012. Acknowledgements Isaac Chuang a professor at MIT and Andrew Ho a professor at Harvard University published this data as an appendix to their paper ""HarvardX and MITx Four Years of Open Online Courses"".",CSV,,[education],Other,,,2053,15180,0.0634765625,What subjects or courses are the most popular on edX?,Online Courses from Harvard and MIT,https://www.kaggle.com/edx/course-study,Fri Jan 27 2017
144,,JeffTennis,"[om, yr, mo, dy, date, time, tz, st, stf, stn, mag, inj, fat, loss, closs, slat, slon, elat, elon, len, wid, fc]","[numeric, numeric, numeric, numeric, dateTime, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]", Database of tornado activity from 1950 to 2015 Created by National Weather service and available at http//www.spc.noaa.gov/gis/svrgis/ Enhance understanding of where tornados happen indicators of damage and weather conditions associated with tornados (temp/El Nino La Nina)  Metadata available at http//www.spc.noaa.gov/wcm/data/SPC_severe_database_description.pdf,CSV,,[climate],Other,,,1217,10243,5,"Historic tornado data, 1950 to 2015",Storm Prediction Center,https://www.kaggle.com/jtennis/spctornado,Tue Nov 22 2016
145,,Rachael Tatman,"[순위, 빈도, 항목, 풀이, , ]","[numeric, numeric, string, string, string, string]",Context How frequently a word occurs in a language is an important piece of information for natural language processing and linguists. In natural language processing very frequent words tend to be less informative than less frequent one and are often removed during preprocessing.  This dataset contains frequency information on Korean which is spoken by 80 million people. For each item both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided. Content This dataset contains six sub-files with frequency information. The files have been renamed in English but no changes have been made to the file contents. The files and their headers are listed below.  The text in this dataset is UTF-8.  Frequency by Jamo (letter) 순위 Rank 빈도 Frequency 위치 Location 자모 Jamo (Hangul letter) Frequency 순위 Rank 빈도 Frequency  항목 Location 범주 Category Frequency by Syllable 순위 Rank 빈도 Frequency 음절 Syllable Borrowings 순위 Rank 빈도 Frequency 항목 Item 풀이 Root Non Standard Words 순위 Rank 빈도 Frequency 어휘 Vocabulary 풀이 Notes  품사 Part of Speech Frequency (Longer version) 순위 Rank 빈도 Frequency  항목 Location 범주 Category  Acknowledgements This dataset was collected and made available by the National Institute of Korean Language. The dataset and additional documentation (in Korean) can be found here.  This dataset is distributed under a Korean Open Government Liscence type 4. It may be redistributed with attribution without derivatives and not for commercial purposes. Inspiration  What are the most frequent jamo (Hangul characters) in Korean? Least frequent? What qualities do borrowed words have? Is there a relationship between word length and frequency?  You may also like  English word frequency Japanese lemma frequency List of simplified Chinese characters ordered by frequency rank Stopword lists for African languages ,CSV,,"[languages, asia, linguistics]",Other,,,62,914,2,Korean frequency lists for NLP,National Institute of the Korean Language Corpus,https://www.kaggle.com/rtatman/national-institute-of-the-korean-language-corpus,Sat Oct 07 2017
146,,Rodrigo Domingos,"[Coenti, Noenti, Cogrupo, Nogrupo]","[numeric, string, string, string]",Context The Brazilian government compiles motor insurance data from ago/2000 to ago/2016 and makes it available for public consumption. Content There's information about the performance of Brazilian insurance motor like Premium claim and commission. Acknowledgements SUSEP is a governmental office responsible for collect house and share this information http//www2.susep.gov.br/menuestatistica/SES/principal.aspx Inspiration This information can answer questions about performance and trends regarding to Brazilian motor line of business and make user able to gain insight about this market. I know this data well but my inspiration to share this data on Kaggle is to discuss and see different points of view.,CSV,,[automobiles],Other,,,381,5131,2,An introduction to the Brazilian motor insurance market trends,Brazilian Motor Insurance Market,https://www.kaggle.com/rodrigodomingos/brazilian-insurance-motor-market,Wed Jan 11 2017
147,,Aaron Miles,"[Pick, Player, Yrs, Draft]","[numeric, string, numeric, numeric]",These datasets contain information from NBA draft classes and subsequent advanced stats years. The idea is to evaluate which draft classes are better than others and in which ways they are better (did they produce more stars in the top 10 or more solid role players at the middle or end. I posted an analysis of this data here but there's a lot more to be done. For example my initial analysis just looks at drafts as a whole not breaking it down by top 10 top 30 or top 60 picks. I also just analyzed drafts since 2000 but the dataset I uploaded has info all the way back to 1978. This data was scraped from baskeball-reference.com There are 2 datasets season78 which has the fields  Season The NBA season data is drawn from. The later year is the Season value (e.g. 2015-2016 season is 2016) Player Name of the player WS Win Shares produced that season.  draft78 has the fields  Pick The draft pick the player was. Player Name of the player Yrs Number of years the player played in the NBA Draft Year of the draft. ,CSV,,[basketball],Other,,,1002,7739,0.478515625,Data from NBA Drafts and Seasons to evaluate draft effectiveness,NBA Draft Value,https://www.kaggle.com/amiles/nbadraftvalue,Mon Aug 22 2016
148,,Izzie Toren,"[pdf_link, pdf_year, row_id, Receiver, From, Justification, Gift, Disposition, Gift_Rec_Date, Value_USD]","[string, numeric, numeric, string, string, string, string, string, dateTime, numeric]","Context This data-set contains information from the ""Protocol Gift Util"" in the US department of state which documents all of the official gifts accepted by the president and white house staff. Quoting from the U.S. Department of State website  The Protocol Gift Unit within the Office of the Chief of Protocol serves as the central processing point for all tangible gifts received from foreign sources by employees of the Executive Branch of the Federal government. The Unit is responsible for the creation and maintenance of the official record of all gifts presented by the Department of State to officials of foreign governments. Working closely with the Chief of Protocol and the staffs of the President the Vice President and the Secretary of State the Gift Unit selects the gifts presented to foreign dignitaries. Gifts received by the President Vice President and the Secretary of State and their spouses from foreign governments are also handled by the Gift Unit in the Office of Protocol.   Content The file contains data scraped from the the Protocol Gift Unit website (the R script and more information about exclusions and possible issues can be found here.  Number of recorded gifts 1913 (after some exclusions) Years 2002 to 2015 Encoding UTF8 (with many special characters)  Inspiration Looking forward to see how people can use creative text mining techniques to extract more information about the different columns (for example classify givers / receivers tag geographies extract the gift object from the description text etc.). You can find my future humble attempts here.",CSV,,"[politics, international relations]",CC4,,,86,1805,0.828125,"Data from the ""Protocol Gift Unit"" in the US Department of State",Protocol Gifts,https://www.kaggle.com/ytoren/protocol-gifts,Wed Jan 11 2017
149,,Nick Rose,"[number_people, date, timestamp, day_of_week, is_weekend, is_holiday, temperature, is_start_of_semester, is_during_semester, month, hour]","[numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Background When is my university campus gym least crowded so I know when to work out? We measured how many people were in this gym once every 10 minutes over the last year. We want to be able to predict how crowded the gym will be in the future. Goals  Given a time of day (and maybe some other features including weather) predict how crowded the gym will be.  Figure out which features are actually important which are redundant and what features could be added to make the predictions more accurate.  Data The dataset consists of 26000 people counts (about every 10 minutes) over the last year. In addition I gathered extra info including weather and semester-specific information that might affect how crowded it is. The label is the number of people which I'd like to predict given some subset of the features.  Label  Number of people   Features  date (string; datetime of data) timestamp (int; number of seconds since beginning of day) day_of_week (int; 0 [monday] - 6 [sunday]) is_weekend (int; 0 or 1) [boolean if 1 it's either saturday or sunday otherwise 0] is_holiday (int; 0 or 1) [boolean if 1 it's a federal holiday 0 otherwise] temperature (float; degrees fahrenheit) is_start_of_semester (int; 0 or 1) [boolean if 1 it's the beginning of a school semester 0 otherwise] month (int; 1 [jan] - 12 [dec]) hour (int; 0 - 23)  Acknowledgements This data was collected with the consent of the university and the gym in question.,CSV,,"[health, demographics, sociology]",ODbL,,,1613,16907,3,Number of attendees every 10 minutes from the last year in the gym,Crowdedness at the Campus Gym,https://www.kaggle.com/nsrose7224/crowdedness-at-the-campus-gym,Sun Mar 19 2017
150,,Elena Cuoco,[],[],"On February 11th 2016 LIGO-Virgo collaboration gave the announce of the discovery of Gravitational Waves just 100 years after the Einstein’s paper on their prediction. The LIGO Scientific Collaboration (LSC) and the Virgo Collaboration prepared a web page to inform the broader community about a confirmed astrophysical event observed by the gravitational-wave detectors and to make the data around that time available for others to analyze https//losc.ligo.org/events/GW150914/ You can find much more information on the LOSC web site and a good starting tutorial at the following link https//losc.ligo.org/tutorial00/ These data sets contain 32 secs of data sampled at 4096Hz an 16384Hz around the GW event detected on 14/09/2015. Longer sets of data can be downloaded here https//losc.ligo.org/s/events/GW150914/H-H1_LOSC_4_V1-1126257414-4096.hdf5 https//losc.ligo.org/s/events/GW150914/L-L1_LOSC_4_V1-1126257414-4096.hdf5 https//losc.ligo.org/s/events/GW150914/H-H1_LOSC_16_V1-1126257414-4096.hdf5 https//losc.ligo.org/s/events/GW150914/L-L1_LOSC_16_V1-1126257414-4096.hdf5  How to acknowledge use of this data  If your research used data from one of the data releases please cite as  LIGO Scientific Collaboration ""LIGO Open Science Center release of    S5"" 2014 DOI 10.7935/K5WD3XHR     LIGO Scientific Collaboration ""LIGO Open Science Center release of S6"" 2015 DOI 10.7935/K5RN35SD     LIGO Scientific Collaboration ""LIGO Open Science Center release of GW150914"" 2016 DOI10.7935/K5MW2F23  and please include the statement ""This research has made use of data software and/or web tools obtained from the LIGO Open Science Center (https//losc.ligo.org) a service of LIGO Laboratory and the LIGO Scientific Collaboration. LIGO is funded by the U.S. National Science Foundation."" If you would also like to cite a published paper M Vallisneri et al. ""The LIGO Open Science Center"" proceedings of the 10th LISA Symposium  University of Florida Gainesville May 18-23 2014; also arxiv1410.4839 Publications We request that you let the LOSC team know if you publish (or intend to publish) a paper using data released from this site. If you would like we may be able to review your work prior to publication as we do for our colleagues in the LIGO Scientific Collaboration. Credits LOSC Development The LOSC Team and The LIGO Scientific Collaboration The data products made available through the LOSC web service are created and maintained by LIGO Lab and the LIGO Scientific Collaboration. The development of this web page was a team effort with all members of the LOSC team making contributions in most areas. In addition to the team members listed below a large number of individuals in the LIGO Scientific Collaboration have contributed content and advice. The LOSC team includes Alan Weinstein LOSC Director Roy Williams LOSC Developer web services and data base architecture Jonah Kanner LOSC Developer tutorials documentation data set curation Michele Vallisneri LOSC Developer data quality curation Branson Stephens LOSC Developer event database and web site architecture  Please send any comments questions or concerns to losc@ligo.org",Other,,"[physics, space]",Other,,,1410,17408,10,The GW150914 Gravitational Waves event data,The Gravitational Waves Discovery Data,https://www.kaggle.com/elenacuoco/the-gravitational-waves-discovery-data,Thu Jun 09 2016
151,,UCI Machine Learning,"[RI, Na, Mg, Al, Si, K, Ca, Ba, Fe, Type]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This is a Glass Identification Data Set from UCI. It contains 10 attributes including id. The response is glass type(discrete 7 values) Content Attribute Information  Id number 1 to 214 (removed from CSV file) RI refractive index  Na Sodium (unit measurement weight percent in corresponding oxide as are attributes 4-10)  Mg Magnesium  Al Aluminum  Si Silicon  K Potassium  Ca Calcium  Ba Barium  Fe Iron  Type of glass (class attribute)  -- 1 building_windows_float_processed  -- 2 building_windows_non_float_processed  -- 3 vehicle_windows_float_processed  -- 4 vehicle_windows_non_float_processed (none in this database)  -- 5 containers  -- 6 tableware  -- 7 headlamps  Acknowledgements https//archive.ics.uci.edu/ml/datasets/Glass+Identification Source Creator  B. German  Central Research Establishment  Home Office Forensic Science Service  Aldermaston Reading Berkshire RG7 4PN  Donor  Vina Spiehler Ph.D. DABFT  Diagnostic Products Corporation  (213) 776-0180 (ext 3014) Inspiration Data exploration of this dataset reveals two important characteristics  1) The variables are highly corelated with each other including the response variables So which kind of ML algorithm is most suitable for this dataset Random Forest  KNN or other? Also since dataset is too small is there any chance of applying PCA or it should be completely avoided? 2) Highly Skewed Data Is scaling sufficient or are there any other techniques which should be applied to normalize data? Like BOX-COX Power transformation?,CSV,,"[chemistry, artificial intelligence]",ODbL,,,3255,28887,0.009765625,Can you correctly identify glass type?,Glass Classification,https://www.kaggle.com/uciml/glass,Fri Jan 27 2017
152,,Chris Crawford,"[date, plate]","[dateTime, string]","Context This is a small dataset New York personalized license plate applications received from the New York DMV in response to a July 2014 Freedom of Information Law (FOIL) request. Covers applications from 10/1/2010 to 9/26/2014. Content  accepted-plates.csv CSV of plate applications that were accepted and issued with order date and plate configuration. rejected-plates.csv CSV of plate applications that were rejected by the department with order date and plate configuration. red-guide.csv A copy of the Red Guide the list of ""inappropriate"" plate configurations that are automatically disallowed by the New York DMV as of July 2015. procedure.pdf A document listing the DMV's plate review and cancellation procedures as of June 2014.  Acknowledgements This dataset is from a FOIL request by Noah Veltman at Data News. Here are some notes about the process but check the original source for more information. Thanks to Noah for letting us share this dataset with the Kaggle community! https//github.com/datanews/license-plates  This data may contain explicit or offensive language. Plate configurations in accepted-plates.csv may have since been revoked by the DMV. Plate configurations in rejected-plates.csv were rejected by the department. It does not include plates that were reserved banned by the Red Guide or cancelled for administrative reasons. Some plate configurations may exist in multiple applications. A small number of rows may contain erroneous data because of Excel cell formatting in the original prepared files. Although the DMV collects an explanation of the requested combination from each online applicant that information is not preserved.  Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?https//github.com/datanews/license-plateshttps//github.com/datanews/license-plates",CSV,,"[linguistics, automobiles]",Other,,,69,1116,2,Rejected vanity license plates in NYC from 2010 - 2014,NYC Rejected Vanity Plates,https://www.kaggle.com/crawford/nyc-rejected-vanity-plates,Sat Jul 15 2017
153,,Megan Risdal,"[account_id, post_id, Category, Page, Post URL, Date Published, Post Type, Rating, Debate, share_count, reaction_count, comment_count]","[numeric, numeric, string, string, string, dateTime, string, string, string, numeric, numeric, numeric]",Context During the 2016 US presidential election the phrase “fake news” found its way to the forefront in news articles tweets and fiery online debates the world over after misleading and untrue stories proliferated rapidly. BuzzFeed News analyzed over 1000 stories from hyperpartisan political Facebook pages selected from the right left and mainstream media to determine the nature and popularity of false or misleading information they shared. Content This dataset supports the original story “Hyperpartisan Facebook Pages Are Publishing False And Misleading Information At An Alarming Rate” published October 20th 2016. Here are more details on the methodology used for collecting and labeling the dataset (reproduced from the story) More on Our Methodology and Data Limitations “Each of our raters was given a rotating selection of pages from each category on different days. In some cases we found that pages would repost the same link or video within 24 hours which caused Facebook to assign it the same URL. When this occurred we did not log or rate the repeat post and instead kept the original date and rating. Each rater was given the same guide for how to review posts  “Mostly True The post and any related link or image are based on factual information and portray it accurately. This lets them interpret the event/info in their own way so long as they do not misrepresent events numbers quotes reactions etc. or make information up. This rating does not allow for unsupported speculation or claims. “Mixture of True and False Some elements of the information are factually accurate but some elements or claims are not. This rating should be used when speculation or unfounded claims are mixed with real events numbers quotes etc. or when the headline of the link being shared makes a false claim but the text of the story is largely accurate. It should also only be used when the unsupported or false information is roughly equal to the accurate information in the post or link. Finally use this rating for news articles that are based on unconfirmed information.     “Mostly False Most or all of the information in the post or in the link being shared is inaccurate. This should also be used when the central claim being made is false.     “No Factual Content This rating is used for posts that are pure opinion comics satire or any other posts that do not make a factual claim. This is also the category to use for posts that are of the “Like this if you think...” variety.  “In gathering the Facebook engagement data the API did not return results for some posts. It did not return reaction count data for two posts and two posts also did not return comment count data. There were 70 posts for which the API did not return share count data. We also used CrowdTangle's API to check that we had entered all posts from all nine pages on the assigned days. In some cases the API returned URLs that were no longer active. We were unable to rate these posts and are unsure if they were subsequently removed by the pages or if the URLs were returned in error.” Acknowledgements This dataset was originally published on GitHub by BuzzFeed News here https//github.com/BuzzFeedNews/2016-10-facebook-fact-check  Inspiration Here are some ideas for exploring the hyperpartisan echo chambers on Facebook  How do left mainstream and right categories of Facebook pages differ in the stories they share? Which types of stories receive the most engagement from their Facebook followers? Are videos or links more effective for engagement? Can you replicate BuzzFeed’s findings that “the least accurate pages generated some of the highest numbers of shares reactions and comments on Facebook”?  Start a new kernel,CSV,,"[news agencies, politics, political science, internet]",Other,,,217,2374,0.34765625,Hyperpartisan Facebook pages and misleading information during the 2016 election,Fact-Checking Facebook Politics Pages,https://www.kaggle.com/mrisdal/fact-checking-facebook-politics-pages,Tue Jun 06 2017
154,,US Bureau of Labor Statistics,"[tucaseid, tuactivity_n, tuactdur24, tucc5, tucc5b, trtcctot_ln, trtcc_ln, trtcoc_ln, tustarttim, tustoptime, trcodep, trtier1p, trtier2p, tucc8, tucumdur, tucumdur24, tuactdur, tr_03cc57, trto_ln, trtonhh_ln, trtohh_ln, trthh_ln, trtnohh_ln, tewhere, tucc7, trwbelig, trtec_ln, tuec24, tudurstop]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The American Time Use Survey (ATUS) is the Nation’s first federally administered continuous survey on time use in the United States. The goal of the survey is to measure how people divide their time among life’s activities. In ATUS individuals are randomly selected from a subset of households that have completed their eighth and final month of interviews for the Current Population Survey (CPS). ATUS respondents are interviewed only one time about how they spent their time on the previous day where they were and whom they were with. The survey is sponsored by the Bureau of Labor Statistics and is conducted by the U.S. Census Bureau. The major purpose of ATUS is to develop nationally representative estimates of how people spend their time. Many ATUS users are interested in the amount of time Americans spend doing unpaid nonmarket work which could include unpaid childcare eldercare housework and volunteering. The survey also provides information on the amount of time people spend in many other activities such as religious activities socializing exercising and relaxing. In addition to collecting data about what people did on the day before the interview ATUS collects information about where and with whom each activity occurred and whether the activities were done for one’s job or business. Demographic information—including sex race age educational attainment occupation income marital status and the presence of children in the household—also is available for each respondent. Although some of these variables are updated during the ATUS interview most of this information comes from earlier CPS interviews as the ATUS sample is drawn from a subset of households that have completed month 8 of the CPS.  The user guide can be found here. Content There are 8 datasets containing microdata from 2003-2015  Respondent file The Respondent file contains information about ATUS respondents including their labor force status and earnings. Roster file The Roster file contains information about household members and nonhousehold children (under 18) of ATUS respondents. It includes information such as age and sex. Activity file The Activity file contains information about how ATUS respondents spent their diary day. It includes information such as activity codes activity start and stop times and locations. Because Activity codes have changed somewhat between 2003 and 2015 this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF). Activity summary file The Activity summary file contains information about the total time each ATUS respondent spent doing each activity on the diary day. Because Activity codes have changed somewhat between 2003 and 2015 this file uses activity codes that appear in the 2003-2015 ATUS Coding Lexicon (PDF). Who file The Who file includes codes that indicate who was present during each activity. CPS 2003-2015 file The ATUS-CPS file contains information about each household member of all individuals selected to participate in ATUS. The information on the ATUS-CPS file was collected 2 to 5 months before the ATUS interview. Eldercare Roster file The ATUS Eldercare Roster file contains information about people for whom the respondent provided care. Eldercare data have been collected since 2011. Replicate weights file The Replicate weights file contains miscellaneous ATUS weights.  The ATUS interview data dictionary can be found here. The ATUS Current Population Survey (CPS) data dictionary can be found here. The ATUS occupation and industry codes can be found here. The ATUS activity lexicon can be found here. Acknowledgements The original datasets can be found here. Inspiration How do daily activities differ by  labor force status income household composition geographical region disability status ,CSV,,"[time series, demographics]",Other,,,1160,22934,2048,Multi-Year Survey Microdata Files from 2003-2015,American Time Use Survey,https://www.kaggle.com/bls/american-time-use-survey,Thu Jun 15 2017
155,,SpaceX,"[Flight Number, Launch Date, Launch Time, Launch Site, Vehicle Type, Payload Name, Payload Type, Payload Mass (kg), Payload Orbit, Customer Name, Customer Type, Customer Country, Mission Outcome, Failure Reason, Landing Type, Landing Outcome]","[string, dateTime, dateTime, string, string, string, string, numeric, string, string, string, string, string, string, string, string]",Context SpaceX has gained worldwide attention for a series of historic milestones. It is the only private company ever to return a spacecraft from low-Earth orbit which it first accomplished in December 2010. The company made history again in May 2012 when its Dragon spacecraft attached to the International Space Station exchanged cargo payloads and returned safely to Earth — a technically challenging feat previously accomplished only by governments. Since then Dragon has delivered cargo to and from the space station multiple times providing regular cargo resupply missions for NASA. Under a $1.6 billion contract with NASA SpaceX is flying numerous cargo resupply missions to the International Space Station for a total of at least 20 flights under the Commercial Resupply Services contract. In 2016 NASA awarded SpaceX a second version of that contract that will cover a minimum of 6 additional flights from 2019 onward. In the near future SpaceX will carry crew as part of NASA’s Commercial Crew Program as well. Dragon was designed from the outset to carry astronauts and SpaceX is in the process of upgrading Dragon to make it crew-ready. SpaceX is the world’s fastest-growing provider of launch services and has over 70 future missions on its manifest representing over $10 billion in contracts. These include commercial satellite launches as well as NASA and other US Government missions. Content This dataset includes a record for each payload carried during a SpaceX mission into outer space. Acknowledgements The data was scraped from the SpaceX and NASA website. Inspiration Has the rate of SpaceX rocket launches increased in the past decade? How many missions do you predict will be launched in 2018?,CSV,,"[space, business]",Other,,,754,6619,0.0068359375,"Location, date/time, and outcome for every rocket launch","SpaceX Missions, 2006-Present",https://www.kaggle.com/spacex/spacex-missions,Wed Mar 01 2017
156,,Rachael Tatman,"[variant_spelling, standard_spelling]","[string, string]",Context Urban Dictionary is an online dictionary of informal language that anyone can add to. As a result a lot of the user-provided dictionary entries contain interesting variant spellings. While some are (presumably) typos others are new linguistic innovations. Content This dataset contains 716 variant spellings found in text scraped from Urban Dictionary as well as the standard spellings of those words (in UK English). Acknowledgements If you use this dataset in your work please cite the following paper Saphra N. & Lopez A. (2016). Evaluating Informal-Domain Word Representations With UrbanDictionary. ACL 2016 94. URL http//www.anthology.aclweb.org/W/W16/W16-2517.pdf Inspiration  What’s the average edit distances between a variant spelling and the standard spelling? Does this differ by part of speech? (You can find part of speech using the Natural Language Toolkit pos_tag function which is available in Python kernels.) Can you automatically classify whether a variant spelling is a typo or an intentional innovation (like gr8t)?  Can you come up with an edit distance metric that takes into account how close letters are to each other on a standard keyboard? Does that help you identify typos? Can you build an Standard English to Urban Dictionary “translator”? ,CSV,,"[languages, popular culture, literature, linguistics]",CC4,,,33,1162,0.0087890625,A gr8t dataset of wrods used on Urban Dictionary,Spelling Variation on Urban Dictionary,https://www.kaggle.com/rtatman/spelling-variation-on-urban-dictionary,Thu Jul 27 2017
157,,Centers for Disease Control and Prevention,"[Year, State, Smoke everyday, Smoke some days, Former smoker, Never smoked, Location 1]","[numeric, string, string, string, string, string, string]",Context This dataset contains the prevalence and trends of tobacco use for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements as well as risk factors and calculated variables see the Technical Documents and Survey Data for a specific year - http//www.cdc.gov/brfss/annual_data/annual_data.htm.  Content This dataset has 7 variables  Year State Smoke everyday Smoke some days Former smoker Never smoked Location 1  Acknowledgements The original dataset can be found here. Recommended citation Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta Georgia U.S. Department of Health and Human Services Centers for Disease Control and Prevention [appropriate year]. Inspiration  How does tobacco use change over time? Does tobacco use differ by state? ,CSV,,[health],Other,,,1458,9905,0.076171875,Prevalence and Trends: Four Level Smoking Data,Tobacco Use 1995-2010,https://www.kaggle.com/cdc/tobacco-use,Thu Nov 17 2016
158,,Rachael Tatman,"[Abbreviation, Meaning (English), Meaning (Welsh)]","[numeric, string, string]","Context Welsh is a member of the Brittonic branch of the Celtic languages. It is spoken natively in Wales by some in England and in Y Wladfa (the Welsh colony in Chubut Province Argentina). Historically it has also been known in English as ‘Cambrian’ ‘Cambric’ and ‘Cymric’. The current number of Welsh speakers in Wales is over 562000. Content Eurfa is the largest Welsh dictionary under a free license and it was the first dictionary of a Celtic language to list verbal inflections and mutated forms. It also includes in-context citations for most words from a number of corpora Bilingual (Welsh-English Welsh-Spanish)  The 18m-word Kynulliad3 corpus (K3). This contains formal written Welsh (the majority of it translated from English). The 450k-word Siarad corpus (S). These transcribed conversations contain ""Welsh as she is spoke"" including English codeswitches. For readability the version here (download) removes much of the transcription marking. The 200k-word Patagonia corpus (P). These transcribed conversations contain spoken Welsh from Patagonia. This has fewer codeswitches and many of them are in Spanish rather than English. For readability the version here (download) removes much of the transcription marking. The 200k-word Korrect/Kywiro corpus (Ko). This contains Welsh translations of English text in free/open software programs.  Monolingual (Welsh only)  A 220k-word subset of the 300k-word CIG1 child (18-30 months) language acquisition corpus (Kig1) containing non-child utterances only. The version here removes much of the transcription marking. A 100k-word subset of the 560k-word CIG2 child (3-7 years) language acquisition corpus (Kig2) containing non-child utterances only. The version here removes much of the transcription marking.  Acknowledgements This dictionary was created by  Kevin Donnelly and is redistributed here under the GNU General Public License. For more information see the attached LICENSE file. You may also like  4 million word corpus of contemporary Welsh ",CSV,,"[languages, europe, linguistics]",GPL,,,27,385,15,"212,403 word dictionary of Welsh",Eurfa Welsh Dictionary,https://www.kaggle.com/rtatman/eurfa-welsh-dictionary,Tue Aug 29 2017
159,,FiveThirtyEight,"[name, chamber, party, state, district, trump_score, trump_margin, predicted_score, trump_plus_minus]","[string, string, string, string, string, numeric, dateTime, numeric, numeric]","Context This is FiveThirtyEight's Congress Trump Score. As the website itself puts it it's ""an updating tally of how often every member of the House and the Senate votes with or against the president"". Content There are two tables cts and votes. The first one has summary information for every congressperson their name their state their Trump Score Trump's share of the votes in the election etc. The second one has information about every vote each congressperson has cast their vote Trump's position on the issue etc. The data was extracted using R. The code is available as a package on github. Acknowledgements The data is 100% collected and maintained by FiveThirtyEight. They are awesome.",CSV,,[politics],Other,,,312,4874,2,How often do congresspeople vote with or against Trump?,Congress Trump Score,https://www.kaggle.com/fivethirtyeight/trump-score,Mon Jun 26 2017
160,,"AndrewMalinow, PhD",[],[],The data was scraped from www.reddit.com on 1/20/17 using the query string https//www.reddit.com/search?q=inauguration Attributes in order (left to right)  Title (string) Post (string) Post Date  (datetime) Metadata (#points #comments author)- needs additional parsing (string) Comments (string) Post Location (string) latlon (post location) latlon (comments location)  If you are interested in having help with the analytics email me amalinow1973@gmail.com,CSV,,"[politics, internet]",CC4,,,89,2603,8,What was the world saying while Donald Trump was being sworn in?,Reddit Comments on the Presidential Inauguration,https://www.kaggle.com/amalinow/reddit-comments-on-presidential-inauguration,Mon Jan 30 2017
161,,Zeeshan-ul-hassan Usmani,"[START_DATE*, END_DATE*, CATEGORY*, START*, STOP*, MILES*, PURPOSE*]","[dateTime, dateTime, string, string, string, numeric, string]",Context My Uber Drives (2016) Here are the details of my Uber Drives of 2016. I am sharing this dataset for data science community to learn from the behavior of an ordinary Uber customer. Content Geography  USA Sri Lanka and Pakistan Time period January - December 2016  Unit of analysis Drives Total Drives 1155 Total Miles 12204 Dataset The dataset contains Start Date End Date Start Location End Location Miles Driven and Purpose of drive (Business Personal Meals Errands Meetings Customer Support etc.) Acknowledgements & References Users are allowed to use download copy distribute and cite the dataset for their pet projects and training. Please cite it  as follows “Zeeshan-ul-hassan Usmani My Uber Drives Dataset Kaggle Dataset Repository March 23 2017.” Past Research Uber TLC FOIL Response - The dataset contains over 4.5 million Uber pickups in New York City from April to September 2014 and 14.3 million more Uber pickups from January to June 2015 https//github.com/fivethirtyeight/uber-tlc-foil-response 1.1 Billion Taxi Pickups from New York -  http//toddwschneider.com/posts/analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/ What you can do with this data - a good example by Yao-Jen Kuo - https//yaojenkuo.github.io/uber.html Inspiration Some ideas worth exploring •   What is the average length of the trip? •   Average number of rides per week or per month? •   Total tax savings based on traveled business miles? •   Percentage of business miles vs personal vs. Meals •   How much money can be saved by a typical customer using Uber Careem or Lyft versus regular cab service?,CSV,,[road transport],ODbL,,,1878,18413,0.08203125,Complete Details of My Uber Drives in 2016,My Uber Drives,https://www.kaggle.com/zusmani/uberdrives,Thu Mar 23 2017
162,,NLTK Data,[],[],Context The perluniprops dataset in NLTK is a subset of the index of Unicode Version 7.0.0 character properties in Perl The Pythonic equivalence of the Perl Uniprops is created primarily to ease the porting of regex related Perl code to NLTK inspired by this Stackoverflow question. Content The NLTK port of the Perl Uniprops contains the following character sets  Close_Punctuation.txt Currency_Symbol.txt IsAlnum.txt IsAlpha.txt IsLower.txt IsN.txt IsSc.txt IsSo.txt IsUpper.txt Line_Separator.txt Number.txt Open_Punctuation.txt Punctuation.txt Separator.txt Symbol.txt ,Other,,[],Other,,,9,310,0.1298828125,Perl Unicode Properties,Perluniprops,https://www.kaggle.com/nltkdata/perluniprops,Sun Aug 20 2017
163,,Kevin Mader,"[HALTER_ID, ALTER, GESCHLECHT, STADTKREIS, STADTQUARTIER, RASSE1, RASSE1_MISCHLING, RASSE2, RASSE2_MISCHLING, RASSENTYP, GEBURTSJAHR_HUND, GESCHLECHT_HUND, HUNDEFARBE]","[numeric, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string]",All the data was taken from Open Data Zurich (https//data.stadt-zuerich.ch/dataset/pd-stapo-hundebestand) with the idea of making a useful few Kernel demos from it and let people look at information about the dogs that live here.  German Since German is the official language of Zurich most of the columns are in German but the translations to English aren't too tricky  ALTER -> Age GESCHLECHT ->  Gender STADTKREIS -> City Quarter or District RASSE1 -> Dog's Primary Breed RASSE2 -> Dog's Secondary Breed GEBURTSJAHR_HUND -> Dog's Year of Birth GESCHLECHT_HUND -> Dog's Gender HUNDEFARBE -> Dog's Color  Utility  It might also help people trying to find apartments in areas with the right kind of dogs Could be used to look at how dog trends have changed in time (by looking at the numbers by birth year) Helpful for picking the right kind of dog to get your 90 year old grandmother (what kind of dogs do other 90 year old women have) ,CSV,,"[animals, sociology]",CC0,,,303,3386,1,"Data about Dog Owners in Zurich, Switzerland",Dogs of Zurich,https://www.kaggle.com/kmader/dogs-of-zurich,Wed Mar 08 2017
164,,Centers for Disease Control and Prevention,"[report_date, location, location_type, data_field, data_field_code, time_period, time_period_type, value, unit]","[dateTime, string, string, string, string, numeric, string, numeric, string]",An outbreak of the Zika virus an infection transmitted mostly by the Aedes species mosquito (Ae. aegypti and Ae. albopictus) has been sweeping across the Americas and the Pacific since mid-2015. Although first isolated in 1947 in Uganda a lack of previous research has challenged the scientific community to quickly understand its devastating effects as the epidemic continues to spread. All Countries & Territories with Active Zika Virus Transmission  The data This dataset shares publicly available data related to the ongoing Zika epidemic. It is being provided as a resource to the scientific community engaged in the public health response. The data provided here is not official and should be considered provisional and non-exhaustive. The data in reports may change over time reflecting delays in reporting or changes in classifications. And while accurate representation of the reported data is the objective in the machine readable files shared here that accuracy is not guaranteed. Before using any of these data it is advisable to review the original reports and sources which are provided whenever possible along with further information on the CDC Zika epidemic GitHub repo. The dataset includes the following fields  report_date - The report date is the date that the report was published. The date should be specified in standard ISO format (YYYY-MM-DD). location - A location is specified for each observation following the specific names specified in the country place name database. This may be any place with a 'location_type' as listed below e.g. city state country etc. It should be specified at up to three hierarchical levels in the following format [country]-[state/province]-[county/municipality/city] always beginning with the country name. If the data is for a particular city e.g. Salvador it should be specified Brazil-Bahia-Salvador. location_type - A location code is included indicating city district municipality county state province or country. If there is need for an additional 'location_type' open an Issue to create a new 'location_type'. data_field - The data field is a short description of what data is represented in the row and is related to a specific definition defined by the report from which it comes. data_field_code - This code is defined in the country data guide. It includes a two letter country code (ISO-3166 alpha-2 list) followed by a 4-digit number corresponding to a specific report type and data type. time_period - Optional. If the data pertains to a specific period of time for example an epidemiological week that number should be indicated here and the type of time period in the 'time_period_type' otherwise it should be NA. time_period_type - Required only if 'time_period' is specified. Types will also be specified in the country data guide. Otherwise should be NA. value - The observation indicated for the specific 'report_date' 'location' 'data_field' and when appropriate 'time_period'. unit - The unit of measurement for the 'data_field'. This should conform to the 'data_field' unit options as described in the country-specific data guide.  If you find the data useful please support data sharing by referencing this dataset and the original data source. If you're interested in contributing to the Zika project from GitHub you can read more here. The source for the Zika virus structure is available here.,CSV,,"[brazil, healthcare, epidemiology]",CC0,,,4492,36553,11,Analyze the ongoing spread of this infectious disease,Zika Virus Epidemic,https://www.kaggle.com/cdc/zika-virus-epidemic,Sat Jul 16 2016
165,,Harvard University,"[COURSE, DEPARTMENT, COURSEID, CLASSNBR, TOTALENROLLMENT, GSAS, HCOL, NONDGR, VUS, XREG]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","This dataset contains enrollment numbers for every course offered at Harvard during Fall Term 2015. The Data The course enrollment data contains the following fields  COURSE the course name (consists of the department/program abbreviation and a course number/letter; the abbreviation and the number/letter are separated by a space) DEPARTMENT the abbreviation for this course's department COURSEID a unique identifier for the course CLASSNBR another unique identifier for the course? TOTALENROLLMENT total number of students enrolled from every school GSAS number of students enrolled from the Graduate School of Arts and Sciences HCOL number of students enrolled from Harvard College (undergraduate) NONDGR number of non-degree-seeking students enrolled VUS number of students enrolled from the Visiting Undergraduate Students Program XREG number of students from other universities who are cross-registered in the course  Note that there is also a row whose COURSE value is TOTALS and whose DEPARTMENT COURSEID and CLASSNBR values are empty. This row lists the total number of students from each school (GSAS HCOL etc) in all of the courses. For more info on what each of the courses is check out the Harvard Course Catalog. Acknowledgments All of the data in this dataset comes from The Harvard Open Data Dataverse. The specific citation is Mehta Neel 2016 ""Course enrollment stats"" doi10.7910/DVN/9MWTYO Harvard Dataverse V1 [UNF6PA8A+2yr3nGT9I9XGhWeIg==]",CSV,,[education],CC0,,,288,3144,0.134765625,Enrollment numbers for every Harvard course offered in Fall Term 2015,"Harvard Course Enrollments, Fall 2015",https://www.kaggle.com/harvard-university/course-enrollment-stats,Sun Nov 13 2016
166,,danielwatabe,"[Name, Role, Team, Results, Total Points, Avg Points Per Game, Game Played, Kill(+2), D(-0.5), A(+1.5), CS(+0.01), 10+K/A(+2), 3K(+2),4K(+5),5k(+10)]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]","As the Summer Split Regular Season comes to a close. We can look back on which players and teams performed well in terms of fantasy points.  Next to the Column Names. In the parenthesis is how many points are awarded.  For example in team stats Dragon Kills(+1) gives 1 point per dragon killed. Baron Kills gives 2 points.  In player stats ""10+K/A(+2)"" means +2 points when Kill+Assists is greater than 10 in a match. In player stats ""3K(+2)4K(+5)5k(+10)"" means +2 points per triple kill +5 points per quadra and +10 points per penta These are the point breakdowns according to the Fantasy LCS website ""LCS Players are scored accordingly  2 points per kill -0.5 points per death 1.5 points per assist 0.01 points per creep kill 2 points for a triple kill 5 points for a quadra kill (doesn't also count as a triple kill) 10 points for a penta kill (doesn't also count as a quadra kill) 2 points if a player attains 10 or more assists or kills in a game (this bonus only applies once)  LCS Teams are scored accordingly  2 points per win  2 points per Baron Nashor killed 1 point per Dragon killed 2 points per First Blood earned 1 point per Tower destroyed 2 points if the team wins in less than 30 minutes""  Source http//fantasy.na.lolesports.com/en-US/stats",CSV,,[video games],CC0,,,48,749,0.115234375,LCS Summer 2017 Fantasy Stats,LCS 2017 Summer Split Fantasy Player & Team Stats,https://www.kaggle.com/danielwatabe/lcs-2017-summer-split-fantasy-player-team-stats,Wed Oct 11 2017
167,,The Smithsonian Institution,"[Number, Name, Country, Region, Type, Activity Evidence, Last Known Eruption, Latitude, Longitude, Elevation (Meters), Dominant Rock Type, Tectonic Setting]","[numeric, string, string, string, string, string, string, numeric, numeric, numeric, string, string]",Content The Smithsonian Institution's Global Volcanism Program (GVP) documents Earth's volcanoes and their eruptive history over the past 10000 years. The GVP reports on current eruptions from around the world and maintains a database repository on active volcanoes and their eruptions. The GVP is housed in the Department of Mineral Sciences part of the National Museum of Natural History on the National Mall in Washington D.C. The GVP database includes the names locations types and features of more than 1500 volcanoes with eruptions during the Holocene period (approximately the last 10000 years) or exhibiting current unrest.,CSV,,[geology],CC0,,,542,4307,0.248046875,"Name, location, and type of volcanoes active in the past 10,000 years",Volcanic Eruptions in the Holocene Period,https://www.kaggle.com/smithsonian/volcanic-eruptions,Tue Jan 24 2017
168,,"Alan ""AJ"" Pryor","[Drug Name, Generic Name]","[string, string]",U.S. Opiate Prescriptions Accidental death by fatal drug overdose is a rising trend in the United States. What can you do to help? This dataset contains summaries of  prescription records for 250 common opioid and non-opioid drugs written by 25000 unique licensed medical professionals in 2014 in the United States for citizens covered under Class D Medicare as well as some metadata about the doctors themselves. This is a small subset of data that was sourced from cms.gov. The full dataset contains almost 24 million prescription instances in long format. I have cleaned and compiled this data here in a format with 1 row per prescriber and limited the approximately 1 million total unique prescribers down to 25000 to keep it manageable. If you are interested in more data you can get the script I used to assemble the dataset here and run it yourself. The main data is in prescriber-info.csv. There is also opioids.csv that contains the names of all opioid drugs included in the data and overdoses.csv that contains information on opioid related drug overdose fatalities. The increase in overdose fatalities is a well-known problem and the search for possible solutions is an ongoing effort. My primary interest in this dataset is detecting sources of significant quantities of opiate prescriptions. However there is plenty of other studies to perform and I am interested to see what other Kagglers will come up with or if they can improve the model I have already built. The data consists of the following characteristics for each prescriber    NPI – unique National Provider Identifier number   Gender - (M/F)   State - U.S. State by abbreviation Credentials - set of initials indicative of medical degree Specialty - description of type of medicinal practice A long list of drugs with numeric values indicating the total number of prescriptions written for the year by that individual Opioid.Prescriber - a boolean label indicating whether or not that individual prescribed opiate drugs more than 10 times in the year ,CSV,,[healthcare],CC0,,,3394,22229,14,Can you save lives through predictive modeling?,U.S. Opiate Prescriptions/Overdoses,https://www.kaggle.com/apryor6/us-opiate-prescriptions,Sun Oct 23 2016
169,,Ali Ghafour,"[Participant ID, Sex, Age, Weight (kg), Experience (years), Belt]","[string, string, numeric, numeric, numeric, string]",Context This dataset contains information obtained from an impact sensor within a Taekwondo chest protector. Participants were asked to perform various Taekwondo techniques on this chest protector for analysis of sensor readings. Content Data was obtained from 6 participants performing 4 different Taekwondo techniques – Roundhouse/Round Kick Back Kick Cut Kick & Punch. Participant details are summarized in Table 1. The table is organized in ascending order according to participant weight/experience level. In the file 'Taekwondo_Technique_Classification_Stats.csv’ the data is organized as follows. The rows display  Technique – Roundhouse/Round Kick (R) Back Kick (B) Cut Kick (C) Punch (P)  Participant ID – P1 P2 P3 P4 P5 P6  Trial # – For each Technique each participant performed a total of 5 trials  Sensor Readings – Data shows the ADC readings obtained from a 12-bit ADC connected to the sensor (not listed as Voltage but can be converted to it)  The columns identify type of technique participant trial # and showcase the sensor readings. There are a total of 115 columns of sensor readings. Each participant performed 5 trials for each type of technique with hard intensity. The only exception is that Participant 6 (P6) did not perform Back Kicks.  Acknowledgements The dataset was collected at a local Taekwondo school. We would like to thank the instructor and students for taking their time to participate in our data collection! Past Research Previous work to classify Taekwondo techniques included using a butter-worth low pass filter on Matlab and performing integration around the maximum signal peak. The goal was to observe a pattern in the resulting integration values to determine impact intensity for each type of technique. Inspiration Main analysis questions 1) Determine impact intensity that is proportional to the participant’s weight/experience level  Ideally impact intensity should increase with the increasing participant weight/experience level or ID (Participant ID in Table 1 corresponds to ascending weight/experience level)  2) Classify or distinguish between types of impact (Round Kick Back Kick Cut Kick or Punch)  Each Taekwondo technique usually has a unique waveform to be identified ,CSV,,[sports],CC4,,,216,3181,2,Can you determine technique type & intensity of a Taekwondo impact?,Taekwondo Techniques Classification,https://www.kaggle.com/ali2020armor/taekwondo-techniques-classification,Wed Jan 18 2017
170,,NHTSA,"[Record ID, NHTSA Campaign, Manufacturer Campaign, Vehicle Make, Vehicle Model, Model Year, Vehicle Manufacturer, Recall Type, Recall Component, Manufacture Start Date, Manufacture End Date, Estimated Units, Recall Initiative, Recall Manufacturer, Recall Notification Date]","[numeric, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, string, string, numeric]",Context The National Traffic and Motor Vehicle Safety Act (1966) gives the Department of Transportation’s National Highway Traffic Safety Administration (NHTSA) the authority to issue vehicle safety standards and to require manufacturers to recall vehicles that have safety-related defects or do not meet federal safety standards. More than 390 million cars trucks buses recreational vehicles motorcycles and mopeds 46 million tires 66 million pieces of motor vehicle equipment and 42 million child safety seats have been recalled to correct safety defects since 1967. Manufacturers voluntarily initiate many of these recalls while others are influenced by NHTSA investigations or ordered by NHTSA via the courts. If a safety defect is discovered the manufacturer must notify NHTSA vehicle or equipment owners dealers and distributors. The manufacturer is then required to remedy the problem at no charge to the owner. NHTSA is responsible for monitoring the manufacturer’s corrective action to ensure successful completion of the recall campaign. Acknowledgements This dataset was compiled and published by the NHTSA's Office of Defects Investigation (ODI).,CSV,,[automobiles],CC0,,,252,2437,19,What manufacturer has recalled the most vehicles in the past fifty years?,"Vehicle and Tire Recalls, 1967-Present",https://www.kaggle.com/nhtsa/safety-recalls,Tue Feb 07 2017
171,,Rachael Tatman,"[#misspelt, corrected, error-category]","[string, string, numeric]",Context FASpell dataset was developed for the evaluation of spell checking algorithms. It contains a set of pairs of misspelled Persian (Farsi) words and their corresponding corrected forms similar to the ASpell dataset used for English. Content The dataset consists of two parts  faspell_main list of 5050 pairs collected from errors made by elementary school pupils and professional typists. faspell_ocr list of 800 pairs collected from the output of a Farsi OCR system.   Acknowledgements Based on a work at http//pars.ie/lr/FAspell_Dataset. Please acknowledge the use of this dataset by referencing one of the following papers  Barari L. & QasemiZadeh B. (2005). CloniZER spell checker adaptive language independent spell checker. In AIML 2005 Conference CICC Cairo Egypt (pp. 65-71). QasemiZadeh B. Ilkhani A. & Ganjeii A. (2006 June). Adaptive language independent spell checking using intelligent traverse on a tree. In Cybernetics and Intelligent Systems 2006 IEEE Conference on (pp. 1-6). IEEE.  License FASpell by Behrang QasemiZadeh is licensed under a Creative Commons Attribution 4.0 International License. Based on a work at http//pars.ie/lr/FAspell_Dataset. Inspiration  Which kinds of misspellings occurs more often?  Are certain characters more likely to be misspelled? Certain words?  Can you construct a finite state automaton spell checker for Persian based on this data? ,CSV,,[linguistics],Other,,,18,542,0.142578125,Naturally-occurring Persian (Farsi) spelling mistakes,FAspell,https://www.kaggle.com/rtatman/faspell,Sat Jul 15 2017
172,,Centers for Disease Control and Prevention,[],[],"Every year the CDC releases the country’s most detailed report on death in the United States under the National Vital Statistics Systems. This mortality dataset is a record of every death in the country for 2005 through 2015 including detailed information about causes of death and the demographic background of the deceased. It's been said that ""statistics are human beings with the tears wiped off."" This is especially true with this dataset. Each death record represents somebody's loved one often connected with a lifetime of memories and sometimes tragically too short. Putting the sensitive nature of the topic aside analyzing mortality data is essential to understanding the complex circumstances of death across the country. The US Government uses this data to determine life expectancy and understand how death in the U.S. differs from the rest of the world. Whether you’re looking for macro trends or analyzing unique circumstances we challenge you to use this dataset to find your own answers to one of life’s great mysteries. Overview This dataset is a collection of CSV files each containing one year's worth of data and paired JSON files containing the code mappings plus an ICD 10 code set. The CSVs were reformatted from their original fixed-width file formats using information extracted from the CDC's PDF manuals using this script. Please note that this process may have introduced errors as the text extracted from the pdf is not a perfect match. If you have any questions or find errors in the preparation process please leave a note in the forums. We hope to publish additional years of data using this method soon. A more detailed overview of the data can be found here. You'll find that the fields are consistent within this time window but some of data codes change every few years. For example the 113_cause_recode entry 069 only covers ICD codes (I10I12) in 2005 but by 2015 it covers (I10I12I15). When I post data from years prior to 2005 expect some of the fields themselves to change as well. All data comes from the CDC’s National Vital Statistics Systems with the exception of the Icd10Code which are sourced from the World Health Organization. Project ideas  The CDC's mortality data was the basis of a widely publicized paper by Anne Case and Nobel prize winner Angus Deaton arguing that middle-aged whites are dying at elevated rates. One of the criticisms against the paper is that it failed to properly account for the exact ages within the broad bins available through the CDC's WONDER tool. What do these results look like with exact/not-binned age data? Similarly how sensitive are the mortality trends being discussed in the news to the choice of bin-widths? As noted above the data preparation process could have introduced errors. Can you find any discrepancies compared to the aggregate metrics on WONDER? If so please let me know in the forums! WONDER is cited in numerous economics sociology and public health research papers. Can you find any papers whose conclusions would be altered if they used the exact data available here rather than binned data from Wonder?  Differences from the first version of the dataset  This version of the dataset was prepared in a completely different many. This has allowed us to provide a much larger volume of data and ensure that codes are available for every field. We've replaced the batch of sql files with a single JSON per year. Kaggle's platform currently offer's better support for JSON files and this keeps the number of files manageable. A tutorial kernel providing a quick introduction to the new format is available here. Lastly I apologize if the transition has interrupted anyone's work! If need be you can still download v1. ",{}JSON,,"[death, demographics]",CC0,,,10113,61097,4096,Learn more about the leading causes of death from 2005-2015,Death in the United States,https://www.kaggle.com/cdc/mortality,Fri Aug 04 2017
173,,National Health Service,"[Year, ICD10 Code, ICD10 Diagnosis, Diagnosis Type, Metric, Sex, Value]","[string, string, string, string, string, string, numeric]",Context Conditions that could be caused by smoking resulted in 1.7 million admissions to hospitals in England for adults aged 35 and over in 2014-2015 -- an average of 4700 admissions per day! These figures refer to admissions with a primary diagnosis of a disease that can be caused by smoking but for which smoking may or may not have actually been the cause. Content The Statistics on Smoking in England report aims to present a broad picture of health issues relating to smoking in England and covers topics such as smoking prevalence habits behaviours and attitudes smoking-related health issues and mortality and associated costs.  Acknowledgements This report contains data and information previously published by the Health and Social Care Information Centre (HSCIC) Department of Health the Office for National Statistics and Her Majesty’s Revenue and Customs.,CSV,,[healthcare],CC0,,,1230,7386,0.4130859375,"Hospital admissions, prescriptions, and fatalities in England","Tobacco Use and Mortality, 2004-2015",https://www.kaggle.com/nhs/tobacco-use,Wed Jan 18 2017
174,,liwste,"[Distance A, Distance B, Distance C, Position X, Position Y, Date, Time]","[numeric, numeric, numeric, numeric, numeric, dateTime, string]",Context I am interested in indoor positioning technology and had been playing with blue-tooth beacons. Typically blue-tooth beacons have accuracy of a range between 1 to 4 meters. I am thinking of maybe using machine learning can produce results of greater accuracy compared to using traditional filtering methods e.g. kalman filters particle filters. Content 3 Kontakt blue-tooth beacons are mounted in a 2.74 meters wide x 4.38 meters long (width x length) room. The 3 beacons are transmitting at a transmit power of -12dbm. A Sony Xperia E3 smartphone with blue-tooth turned on is used as a receiver to the record the data. Recordings are done on several positions in the room of an interval of 30-60 seconds in the same position. Beacons location #  Beacon                       X(meters)         Y(meters) BeaconA                        0.10                   1.80 BeaconB                        2.74                   2.66 BeaconC                        1.22                   4.54  Fields  The distance in meters between beacon A and the device calculated by using the rssi of this blue-tooth beacon. The distance in meters between beacon B and the device calculated by using the rssi of this blue-tooth beacon. The distance in meters between beacon C and the device calculated by using the rssi of this blue-tooth beacon. X coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy. Y coordinate in centimeters rounded to the nearest centimeter measured using a measuring tape with +/-1cm accuracy. Date and time of the recording.  Acknowledgements The data is collected solely by me. Inspiration To try and improve on the accuracy of indoor position using blue-tooth beacons which typically have accuracy of range between 1 meters to 4 meters.,CSV,,[networks],CC4,,,303,5111,0.021484375,Dataset of bluetooth beacons readings indoor,Indoor Positioning,https://www.kaggle.com/liwste/indoor-positioning,Mon Mar 13 2017
175,,Stack Overflow,"[Id, Tag]","[numeric, string]",A dataset of Stack Overflow programming questions.  For each question it includes  Question ID Creation date Closed date if applicable Score Owner user ID Number of answers Tags  This dataset is ideal for answering questions such as  The increase or decrease in questions in each tag over time Correlations among tags on questions Which tags tend to get higher or lower scores Which tags tend to be asked on weekends vs weekdays  This dataset was extracted from the Stack Overflow database at 2016-10-13 180948 UTC and contains questions up to 2016-10-12. This includes 12583347 non-deleted questions and 3654954 deleted ones. This is all public data within the Stack Exchange Data Dump which is much more comprehensive (including question and answer text) but also requires much more computational overhead to download and process. This dataset is designed to be easy to read in and start analyzing. Similarly this data can be examined within the Stack Exchange Data Explorer but this offers analysts the chance to work with it locally using their tool of choice. Note that for space reasons only non-deleted questions are included in the sqllite dataset but the csv.gz files include deleted questions as well (with an additional DeletionDate file). See the GitHub repo for more.,CSV,,"[linguistics, internet, programming languages]",ODbL,,,633,13599,2048,"Stack Overflow questions and tags, without text included",StackLite: Stack Overflow questions and tags,https://www.kaggle.com/stackoverflow/stacklite,Tue Feb 07 2017
176,,Jean-NicholasHould,"[, abv, ibu, id, name, style, brewery_id, ounces]","[numeric, numeric, string, numeric, string, string, numeric, numeric]","This dataset contains a list of 2410 US craft beers and 510 US breweries. The beers and breweries are linked together with an ""id"". This data was collected in January 2017 on CraftCans.com. The dataset is an a tidy format and values have been cleaned up for your enjoyment. If you are interested in learning more about how this dataset was acquired I wrote an extensive blogpost about it (http//www.jeannicholashould.com/python-web-scraping-tutorial-for-craft-beers.html). Enjoy!",CSV,,[food and drink],ODbL,,,4254,34358,0.173828125,2K+ craft canned beers from the US and 500+ breweries in the United States.,Craft Beers Dataset,https://www.kaggle.com/nickhould/craft-cans,Wed Jan 18 2017
177,,Modeling Online Auctions,"[auctionid, bid, bidtime, bidder, bidderrate, openbid, price, item, auction_type]","[numeric, numeric, numeric, string, numeric, numeric, numeric, string, string]",Context The datasets are from a companion website for the book Modeling Online Auctions by Wolfgang Jank and Galit Shmueli (Wiley and Sons ISBN 978-0-470-47565-2 July 2010). Content The datasets contain eBay auction information on Cartier wristwatches Palm Pilot M515 PDAs Xbox game consoles and Swarowski beads. auction.csv includes 9 variables  auctionid unique identifier of an auction bid the proxy bid placed by a bidder bidtime the time in days that the bid was placed from the start of the auction bidder eBay username of the bidder bidderrate eBay feedback rating of the bidder openbid the opening bid set by the seller price the closing price that the item sold for (equivalent to the second highest bid + an increment) item auction item auction_type  swarovski.csv includes 5 variables  Seller Bidder Weight Bidder.Volume Seller.Volume  Acknowledgements The original dataset can be found here. Inspiration Some ideas worth exploring  For each item what is the relationship between bids bid time and the closing price? Does this differ by length of the auction opening bid or by bidder rating? ,CSV,,"[business, internet]",Other,,,914,9184,1,Modeling Online Auctions Dataset from eBay,Online Auctions Dataset,https://www.kaggle.com/onlineauctions/online-auctions-dataset,Sun Nov 13 2016
178,,SkyLord,"[Organization A, Organization B, Connection, Source(s)]","[string, string, string, string]",Context The dataset has been downloaded from a BuzzFeed news article that was posted on Jan 15 2017. The link to the original source can be checked in the Acknowledgements section.  The authors have created a database of more than 1500 people/organization who have a connection with the Trump family his top advisors or his cabinet picks. The dataset can help us to capture how policy decisions may be impacted by these varied connections. Content You have three datasets to play with.  Person_Person.csv Each row represents a connection between a person and another person (eg. Charles Kushner and Alan Hammer) Person_Org.csv Each row represents a connection between a person and an organization (eg. 401 NORTH WABASH VENTURE LLC. and Donald J. Trump) Org_Org.csv Each row represents a connection between an organization and another organization (eg. TRUMP COMMERCIAL CHICAGO LLC and 401 NORTH WABASH VENTURE LLC. )  All the three files are in the following format  Column1 Person or Organization (A) Column2 Person or Organization (B) Column3 Connection between (A) and (B) Column4 Source url from which the connection is derived  Acknowledgements Source  https//www.buzzfeed.com/johntemplon/help-us-map-trumpworld Inspiration https//github.com/BuzzFeedNews/trumpworld This is an incomplete database and you are free to add more connections.,CSV,,"[geography, politics]",CC0,,,395,6425,0.3876953125,Help us map Trump's World,Trump's World,https://www.kaggle.com/skylord/trumpworld,Wed Feb 22 2017
179,,Ben Hamner,"[name, vertical, year, batch, url, description]","[string, string, numeric, string, string, string]", Data scraped from www.ycombinator.com/companies on September 8 2016.,CSV,,"[business, finance]",Other,,,836,9474,0.119140625,Publicly launched YC companies funded from summer 2005 to summer 2016,Y Combinator Companies,https://www.kaggle.com/benhamner/y-combinator-companies,Thu Sep 08 2016
180,,Rachael Tatman,"[rank, frequency, lemma]","[numeric, numeric, string]",Context A lemma is the uninflected form of a word. So while “tree” and “trees” are two words they are the same lemma “tree”. Similarly “go” “went” and “going” are all forms of the underlying lemma “to go”. This dataset contains the most common lemmas in Japanese. Content This dataset contains the most common Japanese lemmas from the Internet Corpus as tagged by the ChaSen morphological tagger for Japanese (http//chasen.naist.jp/hiki/ChaSen/). For each lemma both the frequency (number of times it occurs in the corpus) and its relative rank to other lemmas is provided. The total corpus size is 253071774 tokens with a lexicon of 451963 types. Acknowledgements This dataset was developed at the University of Leeds by Centre for Translation Studies(more information http//corpus.leeds.ac.uk/list.html) and is distributed under a CC-BY license. Inspiration This dataset is an especially helpful resource for work on Japanese texts.  What is the distribution of hiragana katakana and kanji characters among common lemmas? Can you use machine translation to find the equivalent lemmas and their frequency in other languages? Is there a lot of cross-linguistic difference between what concepts are the most frequent? Which parts of speech are the most common in Japanese? Are these different across languages? ,CSV,,"[languages, linguistics]",Other,,,79,1211,0.2744140625,"A list of the 15,000 most common word forms  in Japanese",Japanese lemma frequency,https://www.kaggle.com/rtatman/japanese-lemma-frequency,Tue Jul 25 2017
181,,Datafiniti,[],[],About This Data This is a list of over 2000 vacation rental properties in Palm Springs CA provided by Datafiniti's Property Database.  The dataset includes property name # beds # bathrooms price and more. Note that each property will have an entry for each price found for it and so a single property may have multiple entries.   What You Can Do With This Data A similar dataset was used to determine the most and least expensive cities for short-term vacation rentals in the US. E.g.  What are the least and most expensive one-bed rentals? What are the least and most expensive two-bed rentals? What are the least and most expensive three-bed rentals? What is the median price for short-term rental properties? What is the variation in rental prices?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, home]",CC4,,,172,2108,21,"A list of over 2,000 vacation rental properties in Palm Springs, CA","Vacation Rental Properties in Palm Springs, CA",https://www.kaggle.com/datafiniti/palm-springs-vacation-rentals,Wed Jan 18 2017
182,,kveykva,"[s2_id, s2_token, num, name, lat, lng, encounter_ms, disppear_ms]","[numeric, string, numeric, string, numeric, numeric, numeric, numeric]",Pokemon Go type latitude longitude and despawn times - July 26 to July 28. This was an early mined dataset from Pokemon Go.  Representing a fairly large spatial area within a thin time frame. Mostly useful in identifying potential spatial patterns in pokemon spawns.,CSV,,[video games],Other,,,741,10125,32,"Find patterns in spawn location, type and time",SF Bay Area Pokemon Go Spawns,https://www.kaggle.com/kveykva/sf-bay-area-pokemon-go-spawns,Thu Sep 15 2016
183,,Neel Shah,"[Authors, Title, Year, Source title, Volume, Issue, Art. No., Page start, Page end, Page count, Cited by, DOI, Link, Affiliations, Authors with affiliations, Abstract, Author Keywords, Index Keywords, Correspondence Address, Editors, Publisher, ISSN, ISBN, CODEN, PubMed ID, Language of Original Document, Abbreviated Source Title, Document Type, Source, EID]","[string, string, numeric, string, numeric, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string]",Context The main aim of this data analysis is to identify the ongoing research in Indian Universities and Indian Industry. It gives a basic answer about research source and trend with top authors and publication. It also shows the participation of Industry and Universities in research. Content  It is a collection of 1387 paper dataset from SCOPUS journal between 2001 to 2016 published by Indian Universities or India based research center of any industry. If a paper has multiple authors from Industry and Indian University we count that paper as university paper. If a paper published by industry and non-Indian university we count that paper as Industry paper. During cleaning of data we consider the different name of Institute as single Institute. For example IIT-Madras Indian Institute of Technology and IIT-M count as the same institute. We also consider the different name of same industry as single industry For example TCS and tata consultancy service count as the same industry.  Acknowledgements This dataset is available as open source on Scopus journal. We took only Indian researcher's detail from it. Detail of analysis and Blog  scopus journal blog,CSV,,"[research, education]",CC0,,,215,3508,13,Collection of 1387 SCOPUS journal papers from Indian authors and institutions,Academic Research from Indian Universities,https://www.kaggle.com/neelshah18/scopusjournal,Wed Mar 15 2017
184,,University of Michigan,"[court_term, justice_code, justice_name, posterior_mean, standard_deviation, posterior_median, twosigma_left, twosigma_right]","[numeric, numeric, string, numeric, numeric, numeric, numeric, numeric]","Context Measuring the relative location of U.S. Supreme Court justices on an ideological continuum allows us to better understand the politics of the high court. Such measures are an important building blocking of statistical models of the Supreme Court the separation of powers system and the judicial hierarchy. Content The ""Martin-Quinn"" judicial ideology scores are estimated for every justice serving from the October 1937 term to the present. The measures are estimated using a dynamic item response theory model allowing judicial ideology to trend smoothly through time. Since the scores are estimated from a probability model they can be used to form other quantities of interest such as locating the pivotal ""median"" justice as all well the location of each case in the policy space.  Acknowledgements The Martin-Quinn scores were developed by Andrew D. Martin of the University of Michigan and Kevin M. Quinn of the UC Berkeley School of Law and supported by a National Science Foundation grant. The scores are based on the 2016 release of the Supreme Court Database.",CSV,,"[law, politics]",Other,,,84,2200,0.0390625,Measure of individual justices' ideology on political spectrum per term,Ideology Scores of Supreme Court Justices,https://www.kaggle.com/umichigan/court-justices,Fri Feb 10 2017
185,,NASA,"[id, date, time, continent_code, country_name, country_code, state/province, population, city/town, distance, location_description, latitude, longitude, geolocation, hazard_type, landslide_type, landslide_size, trigger, storm_name, injuries, fatalities, source_name, source_link]","[numeric, dateTime, string, string, string, string, string, numeric, string, numeric, string, numeric, numeric, string, string, string, string, string, string, string, numeric, string, string]",Context Landslides are one of the most pervasive hazards in the world causing more than 11500 fatalities in 70 countries since 2007. Saturating the soil on vulnerable slopes intense and prolonged rainfall is the most frequent landslide trigger. Content The Global Landslide Catalog (GLC) was developed with the goal of identifying rainfall-triggered landslide events around the world regardless of size impacts or location. The GLC considers all types of mass movements triggered by rainfall which have been reported in the media disaster databases scientific reports or other sources. Acknowledgements The GLC has been compiled since 2007 at NASA Goddard Space Flight Center.,CSV,,"[geology, climate]",CC0,,,366,3538,0.4208984375,Location and cause of landslide events around the world,"Landslides After Rainfall, 2007-2016",https://www.kaggle.com/nasa/landslide-events,Thu Jan 19 2017
186,,Lantana Camara,"[finishing_position, horse_number, horse_name, horse_id, jockey, trainer, actual_weight, declared_horse_weight, draw, length_behind_winner, running_position_1, running_position_2, running_position_3, running_position_4, finish_time, win_odds, running_position_5, running_position_6, race_id]","[numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, string, string, string]",Can You Predict The Result? Horse racing is one of the sport which involved many gambling activities. Million of people in the world tried to find their 'winning formula' in order to gain profit from betting. Since there are many factors which could affect the race result data analysis on horse racing became much interesting. Hong Kong horse racing is especially interesting due to the follow reasons  - The handicap system made the race more competitive  - Horse pool is small compared to other countries so that horses will meet their rivalries very often in the races  - Limited number of jockey/trainer  - Data are well managed by the official )   The Dataset The dataset contains the race result of 1561 local races throughout Hong Kong racing seasons 2014-16 and more information will be added into the dataset. The dataset is divided into two tables (which can be joined by race_id). Most of the column description can be found below with one extra piece of information finishing_position -  For special incident please refer to here So can you find any pattern for the winner under some condition? Did you spot out a winning strategy? (FYI betting on all horse equally will bring a loss of ~17.5% on average)  Which jockey/trainer is worth to follow? Don't wait and start the data analysis! You may find some of the kernels I created useful. Enjoy! And please remember to share your finding with the community! Acknowledgement The data are extracted from the website of The Hong Kong Jockey Club How to get started? In case you are not familiar with Hong Kong horse racing please see this notebook as a get started tutorial.,CSV,,[horse racing],Other,,,1036,10237,8,"Race details include track, time,  and horse",Hong Kong Horse Racing Results 2014-17 Seasons,https://www.kaggle.com/lantanacamara/hong-kong-horse-racing,Fri Aug 18 2017
187,,The Huffington Post,"[survey_organization, party_affiliation, start_date, end_date, survey_method, survey_question, survey_population, survey_sample, approve_percent, disapprove_percent, undecided_percent, margin_of_error]","[string, string, dateTime, dateTime, string, string, string, numeric, numeric, numeric, numeric, numeric]",Context HuffPost Pollster (originally Pollster.com) aims to report the results of every public poll that claims to provide a representative sample of the population or electorate. We have included polls of varying methodology including automated or recorded voice telephone polls and online surveys using non-probability internet samples. As of 2010 we require all polls from a new organization (or one new to us) to meet all of the minimal disclosure requirements of the National Council on Public Polling. We have always excluded polls that fail to disclose survey dates sample size and sponsorship; however we may now choose in our editorial discretion to exclude polls or pollsters that do not provide sufficient methodological information for us or our readers to determine their quality. Content This dataset lists results from public opinion surveys regarding the president's job approval since 2008. Acknowledgements The presidential approval ratings were provided by the organization listed and aggregated by HuffPost Pollster. Inspiration How does public approval of the president change over time? How do current events and presidential responses impact approval ratings? Can you predict how Trump's approval rating will change over the course of his presidency?,CSV,,[politics],Other,,,201,2652,0.9560546875,Public opinion survey methodology and results since 2008,Presidential Approval Ratings,https://www.kaggle.com/huffingtonpost/presidential-approval,Tue Feb 28 2017
188,,Kyubyong Park,"[ID, HEADWORD, POS, ENGLISH, JAPANESE, SPANISH, INDONESIAN, EXAMPLE (KO), EXAMPLE (EN), EXAMPLE (JA), EXAMPLE (ES), EXAMPLE (ID)]","[numeric, string, string, string, string, string, string, string, string, string, string, string]",Context A few years ago I investigated a Korean corpus in order to find the most frequent 1000 words. Subsequently I asked native speakers to translate those words and their example sentences into English Japanese Spanish and Indonesian. I've totally forgotten this data since then but it flashed on me this might be helpful for some people. Undoubtedly 1000 sentences are a pretty small corpus but it is also true that parallel corpora are hard to get. Content It's a csv file. As you expect the first line is the heading.  ID Id of the headword. It is arranged by alphabetical order. HEADWORD 1000 most frequent Korean words. POS Part of speech. ENGLISH English meaning or equivalent. JAPANESE Japanese meaning or equivalent. SPANISH Spanish meaning or equivalent. INDONESIAN Indonesian meaning or equivalent. EXAMPLE (KO) An example sentence EXAMPLE (EN) English translation EXAMPLE (JA) Japanese translation EXAMPLE (ES) Spanish translation EXAMPLE (ID) Indonesian translation  Inspiration For now I'm not sure how this small corpus can be used. Hopefully this will be helpful for some pilot linguistic project.,CSV,,"[languages, linguistics]",CC4,,,115,1956,0.263671875,"1000 parallel sentences of Korean, English, Japanese, Spanish, and Indonesian",1000 parallel sentences,https://www.kaggle.com/bryanpark/parallelsents,Sun Dec 25 2016
189,,Reason Foundation,"[InventNumb, PolRptNumb, INumber, SeizeDate, SeizeAddress, SeizeCity, SeizeState, SeizeZip, InvItemNumb, Descr, EstValue, Vin, Findings, ForfDate, ForfValue, CaseNumb, AD, ADDate]","[numeric, string, string, dateTime, string, string, string, string, numeric, string, numeric, string, string, dateTime, numeric, string, string, string]","Context ""Law enforcement in Cook County which includes Chicago seized items from residents ranging from a cashier's check for 34 cents to a 2010 Rolls Royce Ghost with an estimated value of more than $200000. They also seized Xbox controllers televisions nunchucks 12 cans of peas a pair of rhinestone cufflinks and a bayonet."" -Reason.com Content There wasn't much documentation on the dataset but the data fields are somewhat explanatory.  InventNumb Inventory number PolRptNumb Police report number INumber ? SeizeDate Date siezed SeizeAddress Address where it was siezed SeizeCity City where it was siezed SeizeState State where it was siezed SeizeZip Zip code for where it was siezed InvItemNumb Invenotry Item number? Descr Description of item EstValue estimated value of item Vin VIN number? (vehicles) Findings ? ForfDate Forfieture date? ForfValue Value of forfieture? CaseNumb Case number AD ? ADDate Date?  Acknowledgements This dataset is the result of FOIA request by Lucy Parsons Labs a Chicago-based transparency non-profit. It contains information about to the seizures of assets in Cook County (Chicago IL). These data were presented in a Reason.com article showing the disparity of Police seizures between poor and wealthier parts of Cook County IL. http//reason.com/blog/2017/06/13/poor-neighborhoods-hit-hardest-by-asset The original excel file was converted to CSV Inspiration This dataset comes from a Freedom of Information Act request. I love datasets like these because of the potential for finding new socioeconomic insights and improving government accountability.",CSV,,[],CC0,,,19,448,3,"A FOIA request for asset forfeiture in Cook County, IL","Cook County Asset Forfeiture (Chicago, IL)",https://www.kaggle.com/reason-foundation/cook-county-asset-forfeiture-chicago-il,Fri Jul 21 2017
190,,Nelson Chu,"[time, station, AMB_TEMP, CH4, CO, NMHC, NO, NO2, NOx, O3, PH_RAIN, PM10, PM2.5, RAINFALL, RAIN_COND, RH, SO2, THC, UVB, WD_HR, WIND_DIREC, WIND_SPEED, WS_HR]","[dateTime, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This data is from Environmental Protection Administration Executive Yuan R.O.C. (Taiwan). There is air quality data and meteorological monitoring data for research and analysis (only include northern Taiwan 2015). Content 25 observation stations data in the 2015_Air_quality_in_northern_Taiwan.csv The columns in csv file are  time - The first column is observation time of 2015 station - The second column is station name there is 25 observation stations [Banqiao Cailiao Datong Dayuan Guanyin Guting Keelung Linkou Longtan Pingzhen Sanchong Shilin Songshan Tamsui Taoyuan Tucheng Wanhua Wanli Xindian Xinzhuang Xizhi Yangming Yonghe Zhongli Zhongshan] items - From the third column to the last one item - unit - description SO2 - ppb - Sulfur dioxide CO - ppm - Carbon monoxide O3 - ppb - ozone PM10 - μg/m3 - Particulate matter PM2.5 - μg/m3 - Particulate matter NOx - ppb - Nitrogen oxides NO - ppb - Nitric oxide NO2 - ppb - Nitrogen dioxide THC - ppm - Total Hydrocarbons NMHC - ppm - Non-Methane Hydrocarbon CH4 - ppm - Methane UVB - UVI - Ultraviolet index AMB_TEMP - Celsius - Ambient air temperature RAINFALL - mm RH - % - Relative humidity WIND_SPEED - m/sec - The average of last ten minutes per hour WIND_DIREC - degress - The average of last ten minutes per hour WS_HR - m/sec - The average of hour WD_HR - degress  - The average of hour PH_RAIN - PH - Acid rain RAIN_COND - μS/cm - Conductivity of acid rain  Data mark  # indicates invalid value by equipment inspection * indicates invalid value by program inspection x indicates invalid value by human inspection NR indicates no rainfall blank indicates no data  License Open Government Data License version 1.0 http//data.gov.tw/license,CSV,,[environment],Other,,,840,7312,18,"Air quality monitoring data from northern Taiwan, 2015",Air quality in northern Taiwan,https://www.kaggle.com/nelsonchu/air-quality-in-northern-taiwan,Fri Jul 29 2016
191,,LiamLarsen,"[Date, Time, Tweet_Text, Type, Media_Type, Hashtags, Tweet_Id, Tweet_Url, twt_favourites_IS_THIS_LIKE_QUESTION_MARK, Retweets, , ]","[dateTime, dateTime, string, string, string, string, numeric, string, numeric, numeric, string, string]",Context Unlike This dataset (which proved to be unusable). And This one which was filled with unnecessary columns; This Donald trump dataset has the cleanest usability and consists of over 7000 tweets no nonsense You may need to use a decoder other than UTF-8 if you want to see the emojis Content Data consists of  -Date -Time -Tweet_Text -Type -Media_Type -Hashtags -Tweet_Id -Tweet_Url -twt_favourites_IS_THIS_LIKE_QUESTION_MARK -Retweets  I scrapped this from someone on reddit,CSV,,"[politics, internet]",Other,,,824,9967,2,A collection of all of Donald Trump tweets--better than its predecessors,(Better) - Donald Trump Tweets!,https://www.kaggle.com/kingburrito666/better-donald-trump-tweets,Sun Apr 16 2017
192,,姜上（Integ）,"[#, Good Old Plain JavaScript, ES6, CoffeeScript, TypeScript, Elm, ClojureScript, On a scale of one to five dogs, how happy are you with your current flavor of JavaScript?, No Front-End Framework, React, Angular, Angular 2, Ember, Vue, Backbone, Other Front-End Frameworks, On a scale of one to five cats, how happy are you with your current solution for the front-end?, Redux, MobX, Relay, Other State Management Libraries, On a scale of one to five thunderbolts, how happy are you with your current solution for state management?, Custom REST API, Firebase, GraphQL, Apollo, Falcor, Horizon, Other API layer solutions, On a scale of one to five crowns, how happy are you with your current solution for the API layer?, Meteor, FeathersJS, DoneJS, MERN, MEAN, Other stacks, On a scale of one to five trophies, how happy are you with your current full-stack solution?, Mocha, Jasmine, Enzyme, Jest, Cucumber, Ava, Other testing frameworks, On a scale of one to five severed hands, how happy are you with the current state of JavaScript testing?, Plain CSS, SASS/SCSS, LESS, CSS Modules, Aphrodite, Other CSS solutions, On a scale of one to five lightbulbs, how happy are you with the current state of CSS?, Webpack, Grunt, Gulp, Browserify, Bower, Other build tools, On a scale of one to five droplets, how happy are you with the current state of build tools?, Native Apps, React Native, Cordova, PhoneGap, NativeScript, Other mobile apps solutions, On a scale of one to five pencils, how happy are you with the current state of mobile apps?, Server-Side Rendering, Code Splitting, Optimistic Updates, Hot Module Reloading, Time-Travel Debugging, Real-Time Operations, Dead Code Elimination, Progressive Enhancement, Other Features, JavaScript is moving in the right direction, Building JavaScript apps is overly complex right now, JavaScript is over-used online, I enjoy building JavaScript apps, I would like JavaScript to be my main programming language, The JavaScript ecosystem is changing too fast, This survey is too damn long!, Years of Experience, Company Size, Yearly Salary, Favorite Text Editor, Other, Tabs or Spaces?, Other Comments, Start Date (UTC), Submit Date (UTC)]","[string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, numeric, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, dateTime, dateTime]",Context Over nine thousand developers took part in the first edition of the State Of JavaScript survey. They answered questions on topics ranging from front-end frameworks and state management to build tools and testing libraries. You'll find out which libraries developers most want to learn next and which have the highest satisfaction ratings. And hopefully this data will help you make sense of the ever-changing JavaScript ecosystem. Content http//stateofjs.com/2016/introduction/ Acknowledgements Thanks to http//stateofjs.com/ open the data for download.,CSV,,"[programming languages, programming]",CC0,,,140,2718,20,Responses to the State of JavaScript survey,"The State of JavaScript, 2016",https://www.kaggle.com/integjs/state-of-javascript-2016,Mon Dec 05 2016
193,,SEPTA,[],[],SEPTA - Southeastern Pennsylvania Transportation Authority The SEPTA Regional Rail system consists of commuter rail service on 13 branches to more than 150 active stations in Philadelphia Pennsylvania and its suburbs and satellite cities.   SEPTA uses On-Time Performance (OTP)  to measure service reliability. OTP identifies the number of trains for all rail lines that arrive at their scheduled destination at the scheduled time.  However by industry standard  a train may arrive up to 5 minutes and 59 seconds after its scheduled time and still be considered on-time.   SEPTA has established an annual goal of 91% for Regional Rail On-Time Performance.  How well are they doing?  Is it even a meaningful measure? Data Description otp.csv  train_id direction ('N' or 'S'   direction is demarcated as either Northbound or Southbound)^1 origin (See map below - you'll see 'Warminster' 'Glenside'...'Airport Terminal..') next_station (Think of this as the station stop at timeStamp)  date status ('On Time' '5 min' '10 min'.  This is a status on train lateness. 999 is a suspended train) timeStamp   trainView.csv - GPS Train data (early release) Most GPS coordinates are based on track telemetry; however cars are being equipped with GPS units.   train_id status next_station service dest lon lat source track_change track date timeStamp0   First timeStamp at coordinates. timeStamp1    Last timeStamp at coordinates.  You can look at the example here on how to track a single train. What To Look For... Ah as a commuter you care more about the performance of the train(s) you plan to take.  OTP maybe 91% or above; but if the train you take runs late you'll spend extra time on the tracks.  If it consistently runs late maybe the schedule should be changed. Look for patterns. For example during the Tuesday morning rush do some trains run consistently late?   How long does it take to get to from point A to point B in the system?  Performance is VERY important to SEPTA.   Below is a map of the system and station stops.  This dataset contains data on the Regional Rail Lines.  SEPTA train schedules can be found here.  Note different Weekday Saturday and Sunday schedules.   ,CSV,,[rail transport],CC0,,,1803,25078,775,Predict  Arrival Times of Philadelphia's Regional Trains.,SEPTA - Regional Rail,https://www.kaggle.com/septa/on-time-performance,Sun Nov 06 2016
194,,zjf,"[id, huml, humw, ulnal, ulnaw, feml, femw, tibl, tibw, tarl, tarw, type]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context There are many kinds of birds pigeons ducks ostriches penguins... Some are good at flying others can't fly but run fast. Some swim under water others wading in shallow pool.  According to their living environments and living habits birds are classified into different ecological groups. There are 8 ecological groups of birds  Swimming Birds Wading Birds Terrestrial Birds Raptors Scansorial Birds Singing Birds Cursorial Birds (not included in dataset) Marine Birds (not included in dataset)  First 6 groups are main and are covered by this dataset. Apparently birds belong to different ecological groups have different appearances flying birds have strong wings and wading birds have long legs. Their living habits are somewhat reflected in their bones' shapes. As data scientists we may think of examining the underlying relationship between sizes of bones and ecological groups  and recognising birds' ecological groups by their bones' shapes. Content There are 420 birds contained in this dataset. Each bird is represented by 10 measurements (features)  Length and Diameter of Humerus Length and Diameter of Ulna Length and Diameter of Femur Length and Diameter of Tibiotarsus Length and Diameter of Tarsometatarsus   All measurements are continuous float numbers (mm) with missing values represented by empty strings. The skeletons of this dataset are collections of Natural History Museum of Los Angeles County. They belong to 21 orders 153 genera 245 species. Each bird has a label for its ecological group  SW Swimming Birds W Wading Birds T Terrestrial Birds R Raptors P Scansorial Birds SO Singing Birds  Acknowledgements This dataset is provided by Dr. D. Liu of Beijing Museum of Natural History. Inspiration This dataset is a 420x10 size continuous values unbalanced multi-class dataset. What can be done include  Data Visualisation Statical Analysis Supervised Classification Unsupervised Clustering  License Please do not publish or cite this dataset in research papers or other public publications.,CSV,,"[biology, ecology]",Other,,,529,4567,0.0244140625,Measurements of bones and ecological groups of birds,Birds' Bones and Living Habits,https://www.kaggle.com/zhangjuefei/birds-bones-and-living-habits,Wed Jan 18 2017
195,,Free Code Camp,"[Age, AttendedBootcamp, BootcampFinish, BootcampFullJobAfter, BootcampLoanYesNo, BootcampMonthsAgo, BootcampName, BootcampPostSalary, BootcampRecommend, ChildrenNumber, CityPopulation, CodeEventBootcamp, CodeEventCoffee, CodeEventConferences, CodeEventDjangoGirls, CodeEventGameJam, CodeEventGirlDev, CodeEventHackathons, CodeEventMeetup, CodeEventNodeSchool, CodeEventNone, CodeEventOther, CodeEventRailsBridge, CodeEventRailsGirls, CodeEventStartUpWknd, CodeEventWomenCode, CodeEventWorkshop, CommuteTime, CountryCitizen, CountryLive, EmploymentField, EmploymentFieldOther, EmploymentStatus, EmploymentStatusOther, ExpectedEarning, FinanciallySupporting, Gender, HasChildren, HasDebt, HasFinancialDependents, HasHighSpdInternet, HasHomeMortgage, HasServedInMilitary, HasStudentDebt, HomeMortgageOwe, HoursLearning, ID.x, ID.y, Income, IsEthnicMinority, IsReceiveDiabilitiesBenefits, IsSoftwareDev, IsUnderEmployed, JobApplyWhen, JobPref, JobRelocateYesNo, JobRoleInterest, JobRoleInterestOther, JobWherePref, LanguageAtHome, MaritalStatus, MoneyForLearning, MonthsProgramming, NetworkID, Part1EndTime, Part1StartTime, Part2EndTime, Part2StartTime, PodcastChangeLog, PodcastCodeNewbie, PodcastCodingBlocks, PodcastDeveloperTea, PodcastDotNetRocks, PodcastHanselminutes, PodcastJSJabber, PodcastJsAir, PodcastNone, PodcastOther, PodcastProgrammingThrowDown, PodcastRubyRogues, PodcastSEDaily, PodcastShopTalk, PodcastTalkPython, PodcastWebAhead, ResourceBlogs, ResourceBooks, ResourceCodeWars, ResourceCodecademy, ResourceCoursera, ResourceDevTips, ResourceEdX, ResourceEggHead, ResourceFCC, ResourceGoogle, ResourceHackerRank, ResourceKhanAcademy, ResourceLynda, ResourceMDN, ResourceOdinProj, ResourceOther]","[numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, numeric, numeric, string, numeric, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, numeric, string, numeric, string, string, numeric, string, string, numeric, string]",Context Free Code Camp is an open source community where you learn to code and build projects for nonprofits. CodeNewbie.org is the most supportive community of people learning to code. Together we surveyed more than 15000 people who are actively learning to code. We reached them through the twitter accounts and email lists of various organizations that help people learn to code. Our goal was to understand these people's motivations in learning to code how they're learning to code their demographics and their socioeconomic background. We've written in depth about this dataset. In May 2017 we just released an even bigger open dataset with our 2017 survey results.,CSV,,"[employment, computing and society]",ODbL,,,3288,31149,10,"A survey of 15,000+ people who are new to software development",2016 New Coder Survey,https://www.kaggle.com/freecodecamp/2016-new-coder-survey-,Fri Jun 03 2016
196,,Victor Genin,"[Code,  Description,  CodeType,  Deprecated,  Created,  Modified,  ]","[string, string, string, string, dateTime, dateTime, string]",The datasets provides data of annual nominal catches of more than 200 species of fish and shellfish in the Northeast Atlantic region which are officially submitted by 20 International Council for the Exploration of the Sea (ICES) member countries between 2006 and 2014.,CSV,,[fishing],CC0,,,980,7144,4,Explore the impact of overfishing in the Northeast Atlantic region.,Annual Nominal Fish Catches,https://www.kaggle.com/victorgenin/ices-fish-catch,Sun May 29 2016
197,,Jake Waitze,"[oxidant, reductant, potential]","[string, string, numeric]","Context Observations of particles much smaller than us and various understandings of those particles have propelled mankind forward in ways once impossible to imagine. ""The elements"" are what we call the sequential patterns in which some of these particles manifest themselves. As a chemistry student and a coder I wanted to do what came naturally to me and make my class a bit easier by coding/automating my way around some of the tedious work involved with calculations. Unfortunately it seems that chemical-related datasets are not yet a thing which have been conveniently formatted into downloadable databases (as far as my research went). I decided that the elements would be a good place to start data collection so I did that and I'd like to see if this is useful to others as well. Other related data sets I'd like to coalesce are some large amount of standard entropies and enthalpies of various compounds and many of the data sets from the CRC Handbook of Chemistry and Physics. I also think as many diagrams as possible should be documented in a way that can be manipulated and read via code. Content Included here are three data sets. Each data set I have included is in three different formats (CSV JSON Excel) for a total of nine files. Table of the Elements  This is the primary data set. 118 elements in sequential order 72 features  Reactivity Series  33 rows (in order of reactivity - most reactive at the top) 3 features (symbol name ion)  Electromotive Potentials  284 rows (in order from most negative potential to most positive) 3 features (oxidant reductant potential)  Acknowledgements All of the data was scraped from 120 pages on Wikipedia using scripts. The links to those scripts are available in the dataset descriptions. Extra If you are interested in trying the chemistry calculations code I made for completing some of my repetitive class work it's publicly available on my GitHub. (Chemistry Calculations Repository) I plan to continue updating that as time goes on.",CSV,,"[chemistry, physics]",CC0,,,357,4125,0.6845703125,"Elements, activity series, and electromotive potentials",Periodic Table of the Elements,https://www.kaggle.com/jwaitze/tablesoftheelements,Thu Feb 16 2017
198,,Eibriel,[],[],"Context I have the idea to build a virtual companion capable of holding long and interesting conversations. But the lack of a good technique and good datasets apparently is holding the advances of AI in that sense. Chatbots don't have personality nor context awareness and datasets used to train them are just pair of question/answers or IT conversations. This dataset is being built using rDany bot for Telegram Kik and Messenger. If you want to see this dataset grow please use and share it.  Telegram https//t.me/rDanyBot Kik https//kik.me/rDanyBot Messenger https//m.me/rDanyBot  You can also support the development on Patreon https//www.patreon.com/rDanyBot This bot have a personality  Candid True Fun Optimistic Empathic Gender neutral Likes art  And knows a very limited word its room Wikipedia and a schematic view of the world. And speaks Spanish (native) English (with some errors) and other languages (using automatic translation). You can learn more about it on rDany's Telegram channel https//t.me/rDany  Content The dataset consists on 157 anonymized (modified personal information) conversations between a human and other human acting as a companion bot. Each conversation and messages are labeled with hashed IDs. {     ""2059a7bf16436f39b3e713f7b5fe756776cd5e5a601186a1ba17c017027781d9"" [         {             ""date"" 0             ""hashed_message_id"" ""0fd2e5cf87ae0f148db113f06ab746e0c76a55de04819e1a45c9454a34ba8a97""             ""source"" ""human""             ""text"" ""[START]""         }         {             ""date"" 108             ""hashed_message_id"" ""a8c5a80334c8177f07913a192f048d05cd5ad5cc77752eb0abb8d0705eccedfb""             ""source"" ""human""             ""text"" ""hello""         }         {             ""date"" 15097             ""hashed_message_id"" ""73e9765c0d0eab4dfd6f9be2d665e32cc97c5fc3e0fd9c2d12ef920d18ecf349""             ""source"" ""robot""             ""text"" ""Hi! How are you?!""         }     ] }   date Seconds since first message hashed_message_id Message ID source human if the message is from the user and robot if is from rDany text text of the message or ""[START]"" for the start command or ""[photo]"" ""[document]"" ""[audio]"" ""[voice]"" ""[unknown]"" for other types of messages.  Acknowledgements Thanks to all the amazing people that spent time speaking to a crazy human pretending to be a robot ) Inspiration Can you train your own virtual companion that says hello using this dataset?",Other,,"[linguistics, artificial intelligence]",CC4,,,862,8483,3,157 chats & 6300+ messages with a (fake) virtual companion,rDany Chat,https://www.kaggle.com/eibriel/rdany-conversations,Wed Mar 01 2017
199,,DebdootSheet,[],[],Context BDRW is a real-world image dataset for developing machine learning and vision algorithms with minimal requirement on data pre-processing and formatting to identify digits of the decimal number system appearing in Bengali script. It can be seen as similar in flavor to SVHN (e.g. the images are of small cropped digits) but incorporates higher visual heterogeneity and comes from a significantly harder unsolved real world problem (recognizing digits and numbers in natural scene images). BDRW is obtained from numbers appearing in photographs printed materials sign boards wall writings calendar or book pages etc. File BDRW_train.zip (contains BDRW_train_1.zip BDRW_train_2.zip) The data in the two zip files are to be used together and together contain a set of .jpg images of different sized which are cropped from different photographs magazine prints wall writing images etc. Each image represents a digit from the decimal number system written in Bengali (https//en.wikipedia.org/wiki/Bengali_numerals). The file labels.xls contains the number represented in each image which can be used as the ground truth labels for training a learning based system to recognize the Bengali numbers. Inspiration This dataset is released for a machine vision challenge being hosted at IEEE TechSym 2016. The challenge will also include a testing set which includes samples not present in the training set released here and would be released after the challenge is closed.,Other,,"[writing, image data, multiclass classification]",CC4,,,384,4073,1,BDRW is a real-world image dataset for recognizing digits in Bengali,Bengali Digit Recognition in the Wild (BDRW),https://www.kaggle.com/debdoot/bdrw,Thu Aug 18 2016
200,,Sohier Dane,"[#, Organization Name, City/State, Organization Type, Position Held, From, To, OpenCorporates URL, Foreign Interest]","[numeric, string, string, string, string, dateTime, string, string, string]",Context All US presidential candidates are required to fill out form 278e a general disclosure of their assets debts and sources of income. This is an unpacked version of the pdf of Trump's form that was made available by the Federal Election Commission in mid 2016. It contains some information about his financial interests but not enough to paint a complete picture of his net worth. It may be possible to use some of these forms to identify his foreign business partners. Acknowledgements This dataset unpacked from the original pdf and kindly made available by Quartz. Please see their original article for the full background on what this form does and does not contain. You might also like  Trump's World Trump's Tweets Trump Campaign Expenditures ,CSV,,"[presidents, finance, politics]",Other,,,62,955,0.1923828125,Donald Trump's 2016 Form 278e,Trump Financial Disclosure 2016,https://www.kaggle.com/sohier/trump-financial-disclosure-2016,Tue Jul 11 2017
201,,World Bank,"[Country Name, Country Code, Indicator Name, Indicator Code, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Latest poverty and inequality indicators compiled from officially recognized international sources. Poverty indicators include the poverty headcount ratio poverty gap and number of poor at both international and national poverty lines. Inequality indicators include the Gini index and income or consumption distributions. The database includes national regional and global estimates. Content This dataset contains country names and indicator variables from 1974 until 2015. Additional materials and detailed descriptions of the datasets can be downloaded from here. Acknowledgement The original datasets and data dictionaries can be found here. Inspiration Some ideas for exploring the dataset  How does the poverty headcount ratio differ across countries? Can you visualize the temporal trend? Which countries have the highest or lowest GINI index as estimated by the World Bank? Is this indicator correlated with other indicators such as urban/rural poverty or income status? ,CSV,,"[income, international relations]",Other,,,729,6264,1,Poverty and Inequality Indicators from International Sources,Poverty and Equity Database,https://www.kaggle.com/theworldbank/poverty-and-equity-database,Fri Nov 25 2016
202,,LiamLarsen,"[Date(UTC), UnixTimeStamp, Value]","[dateTime, numeric, numeric]",Context The Ethereum blockchain gives a revolutionary way of decentralized applications and provides its own cryptocurrency. Ethereum is a  decentralized platform that runs smart contracts applications that run exactly as programmed without any possibility of downtime censorship fraud or third party interference. These apps run on a custom built blockchain an enormously powerful shared global infrastructure that can move value around and represent the ownership of property. This enables developers to create markets store registries of debts or promises move funds in accordance with instructions given long in the past (like a will or a futures contract) and many other things that have not been invented yet all without a middle man or counterparty risk. Content What you may see in the CSVs are just numbers but there is more to this. Numbers make machine learning easy. I've labeled each column the first in all of them is the day; it may look weird but it makes sense if you look closely.  Note TIMESTAMP FORMAT How to convert timestamp in python import datetime as dt # The (would-be) timestamp value is below timestamp = 1339521878.04  # Technechly you would iterate through and change them all if you were graphing timeValue = dt.datetime.fromtimestamp(timestamp) #Year month day hour minute second print(timeValue.strftime('%Y-%m-%d %H%M%S'))  Acknowledgements MR. Vitalik Buterin. co-founder of Ethereum and as a co-founder of Bitcoin Magazine. Hit a brother up 0x767e8b211f70c5b8b4caa38c2efe05bf8eac0da7 Will be updating every month with new Ethereum history!,CSV,,"[history, finance]",Other,,,2198,22374,0.4267578125,All Ethereum data from the start to Dec 2017,Ethereum Historical Data,https://www.kaggle.com/kingburrito666/ethereum-historical-data,Fri Feb 02 2018
203,,SeanLahman,"[player_id, year, game_num, game_id, team_id, league_id, gp, starting_pos]","[string, numeric, numeric, string, string, string, numeric, numeric]",Baffled why your team traded for that 34-year-old pitcher? Convinced you can create a new and improved version of WAR? Wondering what made the 1907 Cubs great and if can they do it again?  The History of Baseball is a reformatted version of the famous Lahman’s Baseball Database. It contains Major League Baseball’s complete batting and pitching statistics from 1871 to 2015 plus fielding statistics standings team stats park stats player demographics managerial records awards post-season data and more. Scripts Kaggle’s free in-browser analytics tool makes it easy to share detailed sabermetrics predict the next hall of fame inductee illustrate how speed scores runs or publish a definitive analysis on why the Los Angeles Dodgers will never win another World Series.  We have more ideas for analysis than games in a season but here are a few we’d really love to see   Is there a most error-prone position? When do players at different positions peak? Are the best performers selected for all-star game? How many walks does it take for a starting pitcher to get pulled? Do players with a high ground into double play (GIDP) have a lower batting average? Which players are the most likely to choke during the post-season? Why should or shouldn’t the National League adopt the designated hitter rule?  See the full SQLite schema.,CSV,,"[baseball, history]",CC3,,,4761,27857,66,A complete history of major league baseball stats from 1871 to 2015,The History of Baseball,https://www.kaggle.com/seanlahman/the-history-of-baseball,Thu Sep 08 2016
204,,Rishi Sankineni,"[test_id, description_x, description_y, same_security]","[numeric, string, string, string]",Context Natural Language Processing(NLP) Text Similarity(lexical and semantic) Content  In each row of the included datasets(train.csv and test.csv) products X(description_x) and Y(description_y) are considered to refer to the same security(same_security) if they have the same ticker(ticker_xticker_y) even if the descriptions don't exactly match. You can make use of these descriptions to predict whether each pair in the test set also refers to the same security. Dataset info Train - description_x description_y ticker_x ticker_y same_security. Test - description_x description_y same_security(to be predicted) Past Research  This dataset is pretty similar to the Quora Question Pairs . You can also check out my kernel for dataset exploration and n-gram analysis N-gram analysis on stock data. How to Approach  There are several good ways to approach this check out this algorithm and see how far you can go with it https//en.wikipedia.org/wiki/Tf–idf http//scikit learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html. You can also try doing n-gram analysis(check out my kernel). I would suggest using log-loss as your evaluation metric since it gives you a number between 0 and 1 instead of binary classification which is not so effective in this case. Acknowledgements Quovo stock data.,CSV,,"[languages, finance, linguistics]",ODbL,,,870,11447,0.197265625,Natural Language Processing on Stock data,Text Similarity,https://www.kaggle.com/rishisankineni/text-similarity,Sun Mar 19 2017
205,,Simon Fraser University - Summit,"[GameID, LeagueIndex, Age, HoursPerWeek, TotalHours, APM, SelectByHotkeys, AssignToHotkeys, UniqueHotkeys, MinimapAttacks, MinimapRightClicks, NumberOfPACs, GapBetweenPACs, ActionLatency, ActionsInPAC, TotalMapExplored, WorkersMade, UniqueUnitsMade, ComplexUnitsMade, ComplexAbilityUsed, MaxTimeStamp]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset is an aggregate of the screen-fixations from screen movements of StarCraft 2 replay files. Content This dataset contains 21 variables  GameID Unique ID for each game LeagueIndex 1-8 for Bronze Silver Gold Diamond Master GrandMaster Professional leagues Age Age of each player HoursPerWeek Hours spent playing per week TotalHours Total hours spent playing APM Action per minute SelectByHotkeys Number of unit selections made using hotkeys per timestamp AssignToHotkeys Number of units assigned to hotkeys per timestamp UniqueHotkeys Number of unique hotkeys used per timestamp MinimapAttacks Number of attack actions on minimal per timestamp MinimapRightClicks Number of right-clicks on minimal per timestamp NumberOfPACs Number of PACs per timestamp GapBetweenPACs Mean duration between PACs (milliseconds) ActionLatency Mean latency from the onset of PACs to their first action (milliseconds) ActionsInPAC Mean number of actions within each PAC TotalMapExplored Number of 24x24 game coordinate grids viewed by player per timestamp WorkersMade Number of SCVs drones probes trained per timestamp UniqueUnitsMade Unique units made per timestamp ComplexUnitsMade Number of ghosts investors and high templars trained per timestamp ComplexAbilityUsed Abilities requiring specific targeting instructions used per timestamp MaxTimeStamp Time stamp of game's last recorded event  Inspiration Questions worth exploring  How do the replay attributes differ by level of player expertise?  What are significant predictors of a player's league?  Acknowledgements This dataset is from Simon Fraser University - Summit and can be found here. You must give attribution to the work; You may not use this work for commercial purposes; You may not alter transform or build upon this work. Any further uses require the permission of the rights holder.,CSV,,[video games],CC4,,,447,7551,0.51953125,In-Depth Look at StarCraft II Replays,StarCraft II Replay Analysis,https://www.kaggle.com/sfu-summit/starcraft-ii-replay-analysis,Fri Nov 04 2016
206,,Anna Montoya,[],[],"What influences love at first sight? (Or at least love in the first four minutes?) This dataset was compiled by Columbia Business School professors Ray Fisman and Sheena Iyengar for their paper Gender Differences in Mate Selection Evidence From a Speed Dating Experiment. Data was gathered from participants in experimental speed dating events from 2002-2004. During the events the attendees would have a four minute ""first date"" with every other participant of the opposite sex.  At the end of their four minutes participants were asked if they would like to see their date again. They were also asked to rate their date on six attributes Attractiveness Sincerity Intelligence Fun Ambition and Shared Interests.  The dataset also includes questionnaire data gathered from participants at different points in the process. These fields include demographics dating habits self-perception across key attributes beliefs on what others find valuable in a mate and lifestyle information. See the Speed Dating Data Key document below for details. For more analysis from Iyengar and Fisman read Racial Preferences in Dating. Data Exploration Ideas  What are the least desirable attributes in a male partner? Does this differ for female partners?  How important do people think attractiveness is in potential mate selection vs. its real impact?  Are shared interests more important than a shared racial background? Can people accurately predict their own perceived value in the dating market?  In terms of getting a second date is it better to be someone's first speed date of the night or their last?  ",Other,,[sociology],Other,,,24389,119467,5,What attributes influence the selection of a romantic partner?,Speed Dating Experiment,https://www.kaggle.com/annavictoria/speed-dating-experiment,Mon May 09 2016
207,,Gibs,[],[],"From Wikipedia   ""The 500 Greatest Albums of All Time"" is a 2003 special issue of American magazine Rolling Stone and a related book published in 2005. The lists presented were compiled based on votes from selected rock musicians critics and industry figures and predominantly feature American and British music from the 1960s and the 1970s. In 2012 Rolling Stone published a revised edition of the list drawing on the original and a later survey of albums in the 2000s. It was made available in ""bookazine"" format on newsstands in the US from April 27 to July 25. The new list contained 38 albums not present in the previous one 16 of them released after 2003.   I took the albums from MusicBrainz but the genres weren't listed. I wrote a Python script to get the genres and subgenres of each album from the Discogs API. The data collected are  Position on the list Year of release Album name Artist name Genre name Subgenre name  Some of the genres/subgenres may not be entirely correct - Discogs seems to not consider some of the smaller genres. Let me know if there are any glaring issues and I'll try to fix them.",CSV,,"[critical theory, music]",CC0,,,612,6078,0.0361328125,Data about Rolling Stone magazine's (2012) top 500 albums of all time list,Rolling Stone's 500 Greatest Albums of All Time,https://www.kaggle.com/notgibs/500-greatest-albums-of-all-time-rolling-stone,Fri Jan 06 2017
208,,Rachael Tatman,"[, tweet, clap, word]","[numeric, numeric, string, numeric]",Context This dataset was collected to answer questions about where in tweets users put clap emoji especially when clap emoji are used 👏  between 👏 every 👏  word. 👏 Content This dataset is made of up information on 27035 unique tweets continaing at least on clap emoji collected on July 7th 2017. Tweets were collected through Fireant  using the Twitter streaming API. For each tweet every word in the tweet is marked as either the clap emoji or a different word. Acknowledgements This dataset was collected by Rachael Tatman during the process of linguistic research on the clap emoji. A blog post on an analysis of this data can be found here. The dataset here is released to the public domain.  Inspiration While this dataset was originally collected to look specific at the clap-word-clap-word pattern it can also be used to investigate other problems. -Do most tweets which contain the tweet emoji contain more than one? -When multiple claps are used together (👏👏👏)  are they more likely to show up at the beginning or the end of the tweet? -Can you predict the distribution of claps over the tweet? (Perhaps by using a Bernoulli distribution?) -How can you visualize where tweet emoji are used?,CSV,,"[linguistics, human-computer interaction]",CC0,,,47,1040,0.6962890625,Where do clap emojis show up in tweets?,Clap Emoji in Tweets,https://www.kaggle.com/rtatman/clap-emoji-in-tweets,Fri Jul 14 2017
209,,DataSF,"[business_id, business_name, business_address, business_city, business_state, business_postal_code, business_latitude, business_longitude, business_location, business_phone_number, inspection_id, inspection_date, inspection_score, inspection_type, violation_id, violation_description, risk_category]","[numeric, string, string, string, string, numeric, numeric, numeric, string, numeric, string, dateTime, numeric, string, string, string, string]",Context The SF Health Department has developed an inspection report and scoring system. After conducting an inspection of the facility the Health Inspector calculates a score based on the violations observed. Violations can fall intohigh risk category records specific violations that directly relate to the transmission of food borne illnesses the adulteration of food products and the contamination of food-contact surfaces.moderate risk category records specific violations that are of a moderate risk to the public health and safety.low risk category records violations that are low risk or have no immediate risk to the public health and safety.The score card that will be issued by the inspector is maintained at the food establishment and is available to the public in this dataset. Potential question(s) to get started with!  What are some predictors of health scores?  What relevant outside data can you bring to bear on the question including restaurant reviews sentiment analysis demographic data etc?  Fields San Francisco's LIVES restaurant inspection data leverages the LIVES Flattened Schema (https//goo.gl/c3nNvr) which is based on LIVES version 2.0 cited on Yelp's website (http//www.yelp.com/healthscores).   Please refer to https//goo.gl/c3nNvr for detailed data dictionary.   Further info on the Food Safety Program can be found here. We have included the following commonly used geographic shapefiles  Analysis Neighborhoods Supervisor Districts as of April 2012  Acknowledgements Data provided by the San Francisco Health Department via the San Francisco Open Data Portal at https//data.sfgov.org/d/pyih-qa8i License PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL)  Photo via Flickr Rob Hyndman Attribution-NonCommercial-ShareAlike 2.0 Generic (CC BY-NC-SA 2.0),CSV,,[food and drink],Other,,,355,4392,12,SF Health Department records for restaurant inspections,SF Restaurant Inspection Scores,https://www.kaggle.com/datasf/sf-restaurant-inspection-scores,Sat Jan 07 2017
210,,Kumar Nityan Suman,[],[],Context Keystroke dynamics is the study of whether people can be distinguished by their typing rhythms much like handwriting is used to identify the author of a written text. Possible applications include acting as an electronic fingerprint or in an access-control mechanism. A digital fingerprint would tie a person to a computer-based crime in the same manner that a physical fingerprint ties a person to the scene of a physical crime. Access control could incorporate keystroke dynamics both by requiring a legitimate user to type a password with the correct rhythm and by continually authenticating that user while they type on the keyboard.  Content The data are arranged as a table with 34 columns. Each row of data corresponds to the timing information for a single repetition of the password by a single subject. The first column subject is a unique identifier for each subject (e.g. s002 or s057). Even though the data set contains 51 subjects the identifiers do not range from s001 to s051; subjects have been assigned unique IDs across a range of keystroke experiments and not every subject participated in every experiment. For instance Subject 1 did not perform the password typing task and so s001 does not appear in the data set. The second column sessionIndex is the session in which the password was typed (ranging from 1 to 8). The third column rep is the repetition of the password within the session (ranging from 1 to 50). The remaining 31 columns present the timing information for the password. The name of the column encodes the type of timing information. Column names of the form H.key designate a hold time for the named key (i.e. the time from when key was pressed to when it was released). Column names of the form DD.key1.key2 designate a key down-key down time for the named digraph (i.e. the time from when key1 was pressed to when key2 was pressed). Column names of the form UD.key1.key2 designate a key up-key down time for the named digraph (i.e. the time from when key1 was released to when key2 was pressed). Note that UD times can be negative and that H times and UD times add up to DD times. Consider the following one-line example of what you will see in the data    subject  sessionIndex  rep      H.period   DD.period.t   UD.period.t     ...      s002             1    1        0.1491        0.3979        0.2488     ...  The example presents typing data for subject 2 session 1 repetition 1. The period key was held down for 0.1491 seconds (149.1 milliseconds); the time between pressing the period key and the t key (key down-key down time) was 0.3979 seconds; the time between releasing the period and pressing the t key (key up-key down time) was 0.2488 seconds; and so on Acknowledgements Kevin S. Killourhy and Roy A. Maxion Inspiration To make measurable progress in the field of keystroke dynamics i shared data. The anomaly-detection task was to discriminate between the typing of a genuine user trying to gain legitimate access to his or her account and the typing of an impostor trying to gain access illegitimately to that same account. Our intent with this is  to share our resources—the typing data with the research community and to answer questions that they (or you) might have.  For starters 1. The typing patterns of different users. 2. The changing typing styles of a user over different attempts. 3. The difference in typing of left-side keys and right-side keys on the keyboard. and so on . . . *TEMPORARY NOTE* Some people are having problem with the main download button please try downloading from the bottom of the page rather than the main button if issues observed.  Happy Machine Learning!,Other,,[computer security],CC0,,,735,4291,4,Typing patterns for keystroke authentication,Keystroke Dynamics,https://www.kaggle.com/knityansuman/keystroke-dynamics,Fri Mar 10 2017
211,,Megan Risdal,"[PlanetIdentifier, TypeFlag, PlanetaryMassJpt, RadiusJpt, PeriodDays, SemiMajorAxisAU, Eccentricity, PeriastronDeg, LongitudeDeg, AscendingNodeDeg, InclinationDeg, SurfaceTempK, AgeGyr, DiscoveryMethod, DiscoveryYear, LastUpdated, RightAscension, Declination, DistFromSunParsec, HostStarMassSlrMass, HostStarRadiusSlrRad, HostStarMetallicity, HostStarTempK, HostStarAgeGyr, ListsPlanetIsOn]","[string, numeric, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, numeric, string, string]",Our first glimpse at planets outside of the solar system we call home came in 1992 when several terrestrial-mass planets were detected orbiting the pulsar PSR B1257+12. In this dataset you can become a space explorer too by analyzing the characteristics of all discovered exoplanets (plus some familiar faces like Mars Saturn and even Earth). Data fields include planet and host star attributes discovery methods and (of course) date of discovery. Data was originally collected and continues to be updated by Hanno Rein at the Open Exoplanet Catalogue Github repository. If you discover any new exoplanets please submit a pull request there. Constants  Jupiter mass 1.8991766e+27 kg Solar mass 1.9891e+30 kg Jupiter radius 69911000 m Solar radius 6.96e+08 m  License The database is licensed under an MIT license. If you use it for a scientific publication please include a reference to the Open Exoplanet Catalogue on GitHub or to this arXiv paper.,CSV,,"[astronomy, space]",Other,,,1543,15699,0.4443359375,Characteristics of all discovered extrasolar planets,Open Exoplanet Catalogue,https://www.kaggle.com/mrisdal/open-exoplanet-catalogue,Fri Jun 09 2017
212,,TadashiNagao,[],[],Because of memory limitationsdata format change csv -> db (sqlite format) This dataset includes yearly and monthly versions of Japan's international trading data (segmented by country  the type of good  and local custom ). Japan trade statistics is searchable here,CSV,,"[business, finance]",CC4,,,4360,46258,241,Japan's international trade by country and type of good,Japan Trade Statistics,https://www.kaggle.com/zanjibar/japan-trade-statistics,Sun Feb 04 2018
213,,sammy123,"[Col1, Col2, Col3, Col4, Col5, Col6, Col7, Col8, Col9, Col10, Col11, Col12, Class_att, ]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",310 Observations 13 Attributes (12 Numeric Predictors 1 Binary Class Attribute - No Demographics) Lower back pain can be caused by a variety of problems with any parts of the complex interconnected network of spinal muscles nerves bones discs or tendons in the lumbar spine. Typical sources of low back pain include  The large nerve roots in the low back that go to the legs may be irritated The smaller nerves that supply the low back may be irritated The large paired lower back muscles (erector spinae) may be strained The bones ligaments or joints may be damaged An intervertebral disc may be degenerating  An irritation or problem with any of these structures can cause lower back pain and/or pain that radiates or is referred to other parts of the body. Many lower back problems also cause back muscle spasms which don't sound like much but can cause severe pain and disability. While lower back pain is extremely common the symptoms and severity of lower back pain vary greatly. A simple lower back muscle strain might be excruciating enough to necessitate an emergency room visit while a degenerating disc might cause only mild intermittent discomfort. This data set is about to identify a person is abnormal or normal using collected physical spine details/data.,CSV,,[healthcare],Other,,,3214,26903,0.041015625,Collection of physical spine data,Lower Back Pain Symptoms Dataset,https://www.kaggle.com/sammy123/lower-back-pain-symptoms-dataset,Fri Aug 19 2016
214,,Alberto Barradas,"[Pokemon No., Name, Type 1, Type 2, Max CP, Max HP, Image URL]","[numeric, string, string, string, numeric, numeric, string]",This is a database of the first 151 pokemon; the ones you can find in the PokemonGO game. The stats include Pokemon Number Name First and Second Type Max CP Max HP and a url from the bulbagarden.net gallery.  Pokemon No Number or ID of the pokemon. Name The original name of the pokemon. First Type What type of pokemon it is. Second Type Some pokemon can have two types if they don't this cell is empty. Max CP This is the maximum amount of damage a pokemon can infringe. Max HP The maximum amount of damage a pokemon can receive. URL This is a link to the pokemon's image on bulbagarden.  This database presents a great way of helping new generations of pokemon players learn about data science and pokemon at the same time. This data was scrapped from http//handbooks.bulbagarden.net/pokemongo/pokemon-index,CSV,,[video games],CC0,,,1278,17180,0.0166015625,151 Pokemon and battle stats,PokemonGO,https://www.kaggle.com/abcsds/pokemongo,Fri Aug 26 2016
215,,Open Food Facts,"[code, url, creator, created_t, created_datetime, last_modified_t, last_modified_datetime, product_name, generic_name, quantity, packaging, packaging_tags, brands, brands_tags, categories, categories_tags, categories_en, origins, origins_tags, manufacturing_places, manufacturing_places_tags, labels, labels_tags, labels_en, emb_codes, emb_codes_tags, first_packaging_code_geo, cities, cities_tags, purchase_places, stores, countries, countries_tags, countries_en, ingredients_text, allergens, allergens_en, traces, traces_tags, traces_en, serving_size, no_nutriments, additives_n, additives, additives_tags, additives_en, ingredients_from_palm_oil_n, ingredients_from_palm_oil, ingredients_from_palm_oil_tags, ingredients_that_may_be_from_palm_oil_n, ingredients_that_may_be_from_palm_oil, ingredients_that_may_be_from_palm_oil_tags, nutrition_grade_uk, nutrition_grade_fr, pnns_groups_1, pnns_groups_2, states, states_tags, states_en, main_category, main_category_en, image_url, image_small_url, energy_100g, energy-from-fat_100g, fat_100g, saturated-fat_100g, -butyric-acid_100g, -caproic-acid_100g, -caprylic-acid_100g, -capric-acid_100g, -lauric-acid_100g, -myristic-acid_100g, -palmitic-acid_100g, -stearic-acid_100g, -arachidic-acid_100g, -behenic-acid_100g, -lignoceric-acid_100g, -cerotic-acid_100g, -montanic-acid_100g, -melissic-acid_100g, monounsaturated-fat_100g, polyunsaturated-fat_100g, omega-3-fat_100g, -alpha-linolenic-acid_100g, -eicosapentaenoic-acid_100g, -docosahexaenoic-acid_100g, omega-6-fat_100g, -linoleic-acid_100g, -arachidonic-acid_100g, -gamma-linolenic-acid_100g, -dihomo-gamma-linolenic-acid_100g, omega-9-fat_100g, -oleic-acid_100g, -elaidic-acid_100g, -gondoic-acid_100g, -mead-acid_100g, -erucic-acid_100g, -nervonic-acid_100g, trans-fat_100g]","[numeric, string, string, numeric, dateTime, numeric, dateTime, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]", A food products database Open Food Facts is a free open collbarative database of food products from around the world with ingredients allergens nutrition facts and all the tidbits of information we can find on product labels.  Made by everyone Open Food Facts is a non-profit association of volunteers. 5000+ contributors like you have added 100 000+ products from 150 countries using our Android iPhone or Windows Phone app or their camera to scan barcodes and upload pictures of products and their labels. For everyone Data about food is of public interest and has to be open. The complete database is published as open data and can be reused by anyone and for any use. Check-out the cool reuses or make your own! Dataset structure The dataset contains a single table FoodFacts in CSV form in FoodFacts.csv and in SQLite form in database.sqlite. The columns in Open Food Facts are as follows  code (text) url (text) creator (text) created_t (text) created_datetime (text) last_modified_t (text) last_modified_datetime (text) product_name (text) generic_name (text) quantity (text) packaging (text) packaging_tags (text) brands (text) brands_tags (text) categories (text) categories_tags (text) categories_en (text) origins (text) origins_tags (text) manufacturing_places (text) manufacturing_places_tags (text) labels (text) labels_tags (text) labels_en (text) emb_codes (text) emb_codes_tags (text) first_packaging_code_geo (text) cities (text) cities_tags (text) purchase_places (text) stores (text) countries (text) countries_tags (text) countries_en (text) ingredients_text (text) allergens (text) allergens_en (text) traces (text) traces_tags (text) traces_en (text) serving_size (text) no_nutriments (numeric) additives_n (numeric) additives (text) additives_tags (text) additives_en (text) ingredients_from_palm_oil_n (numeric) ingredients_from_palm_oil (numeric) ingredients_from_palm_oil_tags (text) ingredients_that_may_be_from_palm_oil_n (numeric) ingredients_that_may_be_from_palm_oil (numeric) ingredients_that_may_be_from_palm_oil_tags (text) nutrition_grade_uk (numeric) nutrition_grade_fr (text) pnns_groups_1 (text) pnns_groups_2 (text) states (text) states_tags (text) states_en (text) main_category (text) main_category_en (text) image_url (text) image_small_url (text) energy_100g (numeric) energy_from_fat_100g (numeric) fat_100g (numeric) saturated_fat_100g (numeric) butyric_acid_100g (numeric) caproic_acid_100g (numeric) caprylic_acid_100g (numeric) capric_acid_100g (numeric) lauric_acid_100g (numeric) myristic_acid_100g (numeric) palmitic_acid_100g (numeric) stearic_acid_100g (numeric) arachidic_acid_100g (numeric) behenic_acid_100g (numeric) lignoceric_acid_100g (numeric) cerotic_acid_100g (numeric) montanic_acid_100g (numeric) melissic_acid_100g (numeric) monounsaturated_fat_100g (numeric) polyunsaturated_fat_100g (numeric) omega_3_fat_100g (numeric) alpha_linolenic_acid_100g (numeric) eicosapentaenoic_acid_100g (numeric) docosahexaenoic_acid_100g (numeric) omega_6_fat_100g (numeric) linoleic_acid_100g (numeric) arachidonic_acid_100g (numeric) gamma_linolenic_acid_100g (numeric) dihomo_gamma_linolenic_acid_100g (numeric) omega_9_fat_100g (numeric) oleic_acid_100g (numeric) elaidic_acid_100g (numeric) gondoic_acid_100g (numeric) mead_acid_100g (numeric) erucic_acid_100g (numeric) nervonic_acid_100g (numeric) trans_fat_100g (numeric) cholesterol_100g (numeric) carbohydrates_100g (numeric) sugars_100g (numeric) sucrose_100g (numeric) glucose_100g (numeric) fructose_100g (numeric) lactose_100g (numeric) maltose_100g (numeric) maltodextrins_100g (numeric) starch_100g (numeric) polyols_100g (numeric) fiber_100g (numeric) proteins_100g (numeric) casein_100g (numeric) serum_proteins_100g (numeric) nucleotides_100g (numeric) salt_100g (numeric) sodium_100g (numeric) alcohol_100g (numeric) vitamin_a_100g (numeric) beta_carotene_100g (numeric) vitamin_d_100g (numeric) vitamin_e_100g (numeric) vitamin_k_100g (numeric) vitamin_c_100g (numeric) vitamin_b1_100g (numeric) vitamin_b2_100g (numeric) vitamin_pp_100g (numeric) vitamin_b6_100g (numeric) vitamin_b9_100g (numeric) vitamin_b12_100g (numeric) biotin_100g (numeric) pantothenic_acid_100g (numeric) silica_100g (numeric) bicarbonate_100g (numeric) potassium_100g (numeric) chloride_100g (numeric) calcium_100g (numeric) phosphorus_100g (numeric) iron_100g (numeric) magnesium_100g (numeric) zinc_100g (numeric) copper_100g (numeric) manganese_100g (numeric) fluoride_100g (numeric) selenium_100g (numeric) chromium_100g (numeric) molybdenum_100g (numeric) iodine_100g (numeric) caffeine_100g (numeric) taurine_100g (numeric) ph_100g (numeric) fruits_vegetables_nuts_100g (numeric) collagen_meat_protein_ratio_100g (numeric) cocoa_100g (numeric) chlorophyl_100g (numeric) carbon_footprint_100g (numeric) nutrition_score_fr_100g (numeric) nutrition_score_uk_100g (numeric) ,Other,,"[food and drink, nutrition]",ODbL,,,18852,131669,963,Explore nutrition facts from foods around the world,Open Food Facts,https://www.kaggle.com/openfoodfacts/world-food-facts,Mon Sep 18 2017
216,,boyofans,"[gameID, game_datetime, blackID, blackELO, redID, redELO, winner]","[numeric, dateTime, string, numeric, string, numeric, string]",Content Xiangqi also known as Chinese Chess is one of the most popular board game in China and Southeastern Asia that is played by millions of people every single day. More information on the rules and the history of Xiangqi can be found from the Xiangqi wikipedia page The dataset contains 10000 game logs of Blitz xiangqi played on playOK.com scraped off playOK API with Python. In particular the games in the dataset have ID numbers between 57380690 and 57390689. The game records are stored in two separate files  gameinfo.csv which contains players information and game result in gameID game_datetime blackID blackELO redID redELO winner moves.csv which contains game moves in gameID turn A number denoting at which turn of the game the move was made. side move Moves are recorded with the WXF notation. Explainations can be found at XQinEnglish.com  Acknowledgements Data is scraped from the playOK.com game logs API. Cover photo is from Rosino under CC BY-SA 2.0. Misc. There are millions of game logs on playOK.com but I decided to cut the data off at 10000 games due to file size. If you need more games check the GitHub repository of my online xiangqi scraper.,CSV,,[board games],CC4,,,151,3447,15,"10,000 games of Blitz xiangqi",Online Chinese Chess (Xiangqi),https://www.kaggle.com/boyofans/onlinexiangqi,Wed Feb 15 2017
217,,Open Source Sports,"[Type, Code, Fullname]","[string, string, string]","The Hockey Database is a collection of historical statistics from men's professional hockey teams in North America. Note that as of v1 this dataset is missing a few files due to Kaggle restrictions on the number of individual files that can be uploaded. The missing files will be noted in the description below. The Data The dataset contains the following tables (all are csv)  Master Names and biographical information Scoring Scoring statistics ScoringSup Supplemental scoring statistics. Missing in v1 ScoringSC Scoring for Stanley Cup finals 1917-18 through 1925-26 ScoringShootout Scoring statistics for shootouts Goalies Goaltending statistics GoaliesSC Goaltending for Stanley Cup finals 1917-18 through 1925-26 GoaliesShootout Goaltending statistics for shootouts AwardsPlayers Player awards trophies postseason all-star teams AwardsCoaches Coaches awards trophies postseason all-star teams AwardsMisc Miscellaneous awards. Missing in v1 Coaches Coaching statistics Teams Team regular season statistics TeamsPost Team postseason statistics TeamsSC Team Stanley Cup finals statistics 1917-18 through 1925-26 TeamsHalf First half / second half standings 1917-18 through 1920-21 TeamSplits Team home/road and monthly splits TeamVsTeam Team vs. team results SeriesPost Postseason series CombinedShutouts List of combined shutouts. abbrev Abbreviations used in Teams and SeriesPost tables HOF Hall of Fame information  Descriptions of the individual fields in each file can be found in the file's description. Copyright Notice The Hockey Databank project allows for free usage of its data including the production of a commercial product based upon the data subject to the terms outlined below.  1) In exchange for any usage of data in whole or in part you agree to display the following statement prominently and in its entirety on your end product ""The information used herein was obtained free of charge from and is copyrighted by the Hockey Databank project.  For more information about the Hockey Databank project please visit http//sports.groups.yahoo.com/group/hockey-databank"" 2) Your usage of the data constitutes your acknowledgment acceptance and agreement that the Hockey Databank project makes no guarantees regarding the accuracy of the data supplied and will not be held responsible for any consequences arising from the use of the information presented.  Acknowledgments This dataset was downloaded from the hockey database at Open Source Sports. The original acknowledgments are as follows A variety of sources were consulted while constructing this database.  These are listed below in no particular order. Books  National Hockey League Guide (various years) National Hockey League Official Record Book (1982-83 and 1983-84) National Hockey League Official Guide & Record Book (1984-85 to present) The Stanley Cup Records and Statistics (various years) World Hockey Association Media Guide (various years) WHA Schedule & Statistics (1974-75) The Sporting News Hockey Guide (various years) Official NHL Record Book 1917-64 The Complete Historical and Statistical Reference to the World Hockey Association 1972-1979 by Scott Surgent; Xaler Press (7th edition 2004; 8th edition 2008) Total Hockey; Total Sports Publishing (1st edition 1998; 2nd edition 2000) The Encyclopedia of Hockey by Robert A. Styer; A.S. Barnes (2nd edition 1973) The Hockey Encyclopedia by Stan Fischler and Shirley Walton Fischler; Macmillan (1983) The Trail of the Stanley Cup (Vol. 1 2 and 3) by Charles L. Coleman  Periodicals  The Sporting News  On-line sources  ESPN.com http//www.espn.com/nhl/statistics Find A Grave http//www.findagrave.com The Goaltender Home Page (Doug Norris)        http//hockeygoalies.org History Of NHL Trades         http//nhltradeshistory.blogspot.com Hockey Research Association        http//www.hockeyresearch.com/stats Hockey-Reference.com (Justin Kubatko)        http//www.hockey-reference.com Hockey Summary Project        http//sports.groups.yahoo.com/group/hockey_summary_project/         http//hsp.flyershistory.com        (previously at http//www.shrpsports.com/hsp) Internet Hockey Database (Ralph Slate)         http//www.hockeydb.com Legends of Hockey.net (Hockey Hall of Fame)        http//www.legendsofhockey.net/html/search.htm LostHockey.com        http//www.losthockey.com National Hockey League        http//www.nhl.com NHL Hockey Shootout Statistics        http//jeays.net/shootout/index.htm NHL Shootouts        http//www.nhlshootouts.com North American Pro Hockey        http//www.ottawavalleyonline.com/sites/tomking_01/index.html Puckerings        http//www.puckerings.com Society for International Hockey Research        http//www.sihrhockey.org The Sports Network        http//www.sportsnetwork.com USA Today hockey stats archive        http//www.usatoday.com/sports/hockey         http//www.usatoday.com/sports/hockey/archive.htm Yahoo Sports        http//sports.yahoo.com/nhl  Thanks to the following individuals  Ralph Dinger (NHL Publishing / Dan Diamond and Associates) has confirmed a number of corrections to errors found in the NHL's official statistics. Thanks also to Justin Kubatko of hockey-reference.com for a number of discussions in this area. Morey Holzman provided information on Lloyd Cook's 1921-22 goaltending appearance. Stu McMurray provided correct 1917-18 scoring statistics including GWG. Doug Norris provided corrected 1984-85 statistics for Rick St. Croix. Paul Reeths created the Hall of Fame table and provided updates for the Coaches table  Other contributors include Roger Brewer Mike Burton Eric Hornick and Claude Paradis. An acknowledgement is also given to the team led by Sean Forman and Sean Lahman that has developed and maintained the Lahman baseball database.  This database follows the same general design.",CSV,,[],Other,,,1253,8912,5,"Data on hockey players, teams, and coaches from 1909 to 2011",Professional Hockey Database,https://www.kaggle.com/open-source-sports/professional-hockey-database,Sun Nov 27 2016
218,,DucThanhNguyen,[],[],Context The ability to monitor species in their natural habitat is useful when determining how the species respond to changes in environment. This may be particularly important when the species being studied are endangered and a population decline will trigger management action. The Eastern Ground Parrot (Pezoporous wallicus wallicus) is found only in Australia where it is officially listed as a rare and threatened species. Furthermore it is difficult to directly observe as it lives in dense vegetation and rarely flies. As such the chance of successful monitoring depends on locating the vocalisation or calls. One of the techniques that has been commonly used to monitor Ground Parrots (GPs) in their natural habitat is for experienced ecologists to listen to the calls and manually locate and plot the perceived distribution of the birds. More recently surveys have also used techniques based on audio signal processing and pattern recognition where records of acoustic signals are analysed and specific vocalisations are detected. Such detection can be used for the purpose of recognition and estimation of the number of GPs in the habitat. To facilitate research on bird call detection using pattern recognition techniques we provide here a dataset of GP vocalisation. The dataset contains 4 audio sequences of different lengths sampled at 16kHz using Song Meter SM2 devices. The dataset was recorded at dusk and dawn times on different days and at four different locations in the Barren Grounds Nature Reserve a protected nature park located in the Southern Highlands region of New South Wales Australia. The sequences include overlapping GP calls and sounds from sources other than GP (e.g. wind spurious noise and vocalisations of other species). All the GP calls in the dataset are manually annotated and used as the ground truth for evaluation of detection algorithms.  Content This is the dataset used for Ground Parrot Call detection project. The folder Data includes 1) 4 audio sequences formatted in wav files and named Sq1 Sq2 Sq3 and Sq4. 2) Training. This folder contains the training data and includes 2 sub-folders GroundParrot (ground parrot calls) and Others (sounds other than ground parrot calls). Each folder contains 25 wav files and each file is 1 second length. 3) GroundTruth. This folder includes 4 files. Each file contains the ground-truth of the ground parrot calls of one of the 4 sequences e.g. Sq1_gt.txt is the ground-truth of the sequence Sq1.wav. The ground-truth files are written in the following format - The first line contains the number of ground parrot calls. - Each following line describes a call with the start and end time stamp. The time stamps are represented in hoursminutesseconds Acknowledgements This work was supported by funding from the Office of Environment and Heritage NSW Australia.,Other,,"[australia, animals, acoustics]",CC0,,,42,850,2,University of Wollongong ground parrot vocalisation dataset,Ground Parrot Vocalisation Dataset,https://www.kaggle.com/ducthanhnguyen/ground-parrot-vocalisation-dataset,Fri Jul 07 2017
219,,University of Michigan,"[war_id, war_name, war_type, side1_code, side1_name, side2_code, side2_name, start_year1, start_month1, start_day1, end_year1, end_month1, end_day1, start_year2, start_month2, start_day2, end_year2, end_month2, end_day2, previous_war, initiation, intervention, combat_location, state_fatalities, nonstate_fatalities, outcome, next_war]","[numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The Correlates of War (COW) Project has utilized a classification of wars that is based upon the status of territorial entities in particular focusing on those that are classified as members of the interstate system. Wars have been categorized by whether they primarily take place between/among states between/among a state and a non-state actor and within states. Within the COW war typology an interstate intrastate or extrastate war must meet same definitional requirements of all wars in that the war must involve sustained combat involving organized armed forces resulting in a minimum of 1000 battle-related combatant fatalities within a twelve month period. For a state to be considered a war participant the minimum requirement is that it has to either commit 1000 troops to the war or suffer 100 battle-related deaths. When Correlates of War scholars J. David Singer and Melvin Small first extended their study of war to include intrastate wars in Resort to Arms they established the requisite condition that for a conflict to be a war it must involve armed forces capable of “effective resistance” on both sides. They then developed two alternative criteria for defining effective resistance “both sides had to be initially organized for violent conflict and prepared to resist the attacks of their antagonists or the weaker side although initially unprepared is able to inflict upon the stronger opponents at least five percent of the number of fatalities it sustains.” The effective resistance criteria were specifically utilized to differentiate wars from massacres one-sided state killings or general riots by unorganized individuals. Acknowledgements The dataset was created by Meredith Reid Sarkees American University and Professor Frank Wayman University of Michigan-Dearborn and published by the Correlates of War Project.,CSV,,"[war, international relations]",Other,,,291,2663,0.1025390625,"Countries, dates, and fatalities of all wars between 1816 and 2007",Correlates of War: Interstate Wars,https://www.kaggle.com/umichigan/interstate-wars,Fri Feb 03 2017
220,,GIANT: Machine learning for smart environments,[],[],Context This data set is focused on WLAN fingerprint positioning technologies and methodologies (also know as WiFi Fingerprinting). It was the official database used in the IPIN2015 competition.  Many real world applications need to know the localization of a user in the world to provide their services. Therefore automatic user localization has been a hot research topic in the last years. Automatic user localization consists of estimating the position of the user (latitude longitude and altitude) by using an electronic device usually a mobile phone. Outdoor localization problem can be solved very accurately thanks to the inclusion of GPS sensors into the mobile devices. However indoor localization is still an open problem mainly due to the loss of GPS signal in indoor environments. Although there are some indoor positioning technologies and methodologies this database is focused on WLAN fingerprint-based ones (also know as WiFi Fingerprinting). Although there are many papers in the literature trying to solve the indoor localization problem using a WLAN fingerprint-based method there still exists one important drawback in this field which is the lack of a common database for comparison purposes. So UJIIndoorLoc database is presented to overcome this gap.  The UJIIndoorLoc database covers three buildings of Universitat Jaume I (http//www.uji.es) with 4 or more floors and almost 110.000m2. It can be used for classification e.g. actual building and floor identification or regression e.g. actual longitude and latitude estimation. It was created in 2013 by means of more than 20 different users and 25 Android devices. The database consists of 19937 training/reference records (trainingData.csv file) and 1111 validation/test records (validationData.csv file) The 529 attributes contain the WiFi fingerprint the coordinates where it was taken and other useful information. Each WiFi fingerprint can be characterized by the detected Wireless Access Points (WAPs) and the corresponding Received Signal Strength Intensity (RSSI). The intensity values are represented as negative integer values ranging -104dBm (extremely poor signal) to 0dbM. The positive value 100 is used to denote when a WAP was not detected. During the database creation 520 different WAPs were detected. Thus the WiFi fingerprint is composed by 520 intensity values. Then the coordinates (latitude longitude floor) and Building ID are provided as the attributes to be predicted. The particular space (offices labs etc.) and the relative position (inside/outside the space) where the capture was taken have been recorded. Outside means that the capture was taken in front of the door of the space. Information about who (user) how (android device & version) and when (timestamp) WiFi capture was taken is also recorded.  Content  Attributes 001 to 520 (WAP001-WAP520) Intensity value for WAP001. Negative integer values from -104 to 0 and +100. Positive value 100 used if WAP001 was not detected. Attribute 521 (Longitude) Longitude. Negative real values from -7695.9387549299299000 to -7299.786516730871000 Attribute 522 (Latitude) Latitude. Positive real values from 4864745.7450159714 to 4865017.3646842018. Attribute 523 (Floor) Altitude in floors inside the building. Integer values from 0 to 4.  Attribute 524 (BuildingID) ID to identify the building. Measures were taken in three different buildings. Categorical integer values from 0 to 2. Attribute 525 (SpaceID) Internal ID number to identify the Space (office corridor classroom) where the capture was taken.  Categorical integer values. Attribute 526 (RelativePosition) Relative position with respect to the Space (1 - Inside 2 - Outside in Front of the door). Categorical integer values. Attribute 527 (UserID) User identifier (see below). Categorical integer values. Attribute 528 (PhoneID) Android device identifier (see below). Categorical integer values. Attribute 529 (Timestamp) UNIX Time when the capture was taken.  Integer value.  Relevent Paper More information can be found in this paper Joaquín Torres-Sospedra Raúl Montoliu Adolfo Martínez-Usó Tomar J. Arnau Joan P. Avariento Mauri Benedito-Bordonau Joaquín Huerta. UJIIndoorLoc A New Multi-building and Multi-floor Database for WLAN Fingerprint-based Indoor Localization Problems. In Proceedings of the Fifth International Conference on Indoor Positioning and Indoor Navigation 2014. Available at http//www.ipin2014.org/wp/pdf/4A-3.pdf If your are going to use this dataset in your research please cite this paper Acknowledgements The dataset was created by  Joaquín Torres-Sospedra Raul Montoliu Adolfo Martínez-Usó Tomar J. Arnau Joan P. Avariento Mauri Benedito-Bordonau Joaquín Huerta Yasmina Andreu óscar Belmonte Vicent Castelló Irene Garcia-Martí Diego Gargallo Carlos Gonzalez Nadal Francisco Josep López Ruben Martínez Roberto Mediero Javier Ortells Nacho Piqueras Ianisse Quizán David Rambla Luis E. Rodríguez Eva Salvador Balaguer Ana Sanchís Carlos Serra and Sergi Trilles. Inspiration The objective is to estimate the building floor and coordinates (latitude and longitude) of the 1111 samples included in the validation set. Since the real values of the building floor and coordinates are also included it is posible to determine the localization error. The formula used in the IPIN2015 competition was the mean of the localization error of each sample. The localization error of each sample can be estimated as follows Error = building_penality * building_error + floor_penality * floor_error + coordinates_error where  building_error is 1 if the estimated building is not equal to the real one. 0 otherwise  floor_error is 1 if the estimated floor is not equal to the real one. 0 otherwise  coordinates_error is sqrt(   (estimated_latitude - real_latitude)^2 +  (estimated_longitude-real_longitude)^2)  In the IPIN2015 competition building_penalty and floor_penalty where set to 50 and 4 meters respectively.,CSV,,"[geography, computing and society]",CC4,,,285,5391,43,Compare WiFi fingerprinting indoor localization algorithms,UjiIndoorLoc: An indoor localization dataset,https://www.kaggle.com/giantuji/UjiIndoorLoc,Wed Dec 21 2016
221,,QadeemKhan,"[Timestamp, Your Good Name?, Organization?, Last Degree?, Job Experience ?, 1. How Many Repository Architectural Styles have you used for a particular project?, 2. How Many Client Server Styles you have used for a particular project?, 3. How Many Abstract Machine Styles you have used for a particular project?, 4. How Many Object Oriented Styles you have used for a particular project?, 5. How Many Function Oriented Styles you have used for a particular project?, 6. How Many Event Driven Styles you have used for a particular project?, 7. How Many Layered Styles you have used for a particular project?, 8. How Many  Pipes & Filters Architectural Styles  have you used for a particular project?, 9. How Many  Data centeric Architectural Styles have you used for a particular project?, 10. How Many  Blackboard  Architectural Styles have you used for a particular project?, 11. How Many  Rule Based Architectural Styles have you used for a particular project?, 12. How Many Publish Subscribe Architectural Styles have you used for a particular project?, 13. How Many  Asynchronous Messaging Architectural Styles have you used for a particular project?, 14. How Many  Plug-ins Architectural Styles have you used for a particular project?, 15. How Many  Micro-kernel Architectural Styles have you used for a particular project?, 16. How Many  Peer-to-Peer Architectural Styles have you used for a particular project?, 17. How Many  Domain Driven Architectural Styles have you used for a particular project?, 18. How Many  Shared Nothing Architectural Styles have you used for a particular project?]","[string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Software systems are composed of one or more software architectural styles. These styles define the usage patterns of a programmer in order to develop a complex project. These architectural styles are required to analyze for pattern similarity in the structure of multiple groups of projects. The researcher can apply different types of data mining algorithms to analyze the software projects through architectural styles used. The dataset is obtained from an online questionnaire delivered to the world 's best academic and software industry.  Content The content of this dataset are multiple architectural styles utilized by the system. He attributes are  Repository Client Server Abstract MachineObject OrientedFunction OrientedEvent DrivenLayered Pipes & Filters Data centeric Blackboard Rule Based Publish Subscribe Asynchronous Messaging Plug-ins Microkernel Peer-to-Peer Domain Driven Shared Nothing. Acknowledgements Thanks to my honorable teacher Prof.Dr Usman Qamar for guiding me to accomplish this wonderful task. Inspiration The dataset is capable of updating and refinements.Any researcher who want to contribute plz feel free to ask.,CSV,,[programming],CC0,,,244,3773,0.1748046875,The pattern analysis of software development by statistical/datamining methods,Software Architectural Styles,https://www.kaggle.com/qadeemkhan/dataset-of-software-architectural-styles,Mon Mar 27 2017
222,,Fifth Tribe,"[name, username, description, location, followers, numberstatuses, time, tweets]","[string, string, string, string, numeric, numeric, dateTime, string]","We scraped over 17000 tweets from 100+ pro-ISIS fanboys from all over the world since the November 2015 Paris Attacks. We are working with content producers and influencers to develop effective counter-messaging measures against violent extremists at home and abroad. In order to maximize our impact we need assistance in quickly analyzing message frames.  The dataset includes the following  Name Username Description Location Number of followers at the time the tweet was downloaded Number of statuses by the user when the tweet was downloaded Date and timestamp of the tweet The tweet itself  Based on this data here are some useful ways of deriving insights and analysis   Social Network Cluster Analysis Who are the major players in the pro-ISIS twitter network? Ideally we would like this visualized via a cluster network with the biggest influencers scaled larger than smaller influencers.  Keyword Analysis Which keywords derived from the name username description location and tweets were the most commonly used by ISIS fanboys? Examples include ""baqiyah"" ""dabiq"" ""wilayat"" ""amaq"" Data Categorization of Links Which websites are pro-ISIS fanboys linking to? Categories include Mainstream Media Altermedia Jihadist Websites Image Upload Video Upload  Sentiment Analysis Which clergy do pro-ISIS fanboys quote the most and which ones do they hate the most? Search the tweets for names of prominent clergy and classify the tweet as positive negative or neutral and if negative include the reasons why.  Examples of clergy they like the most ""Anwar Awlaki"" ""Ahmad Jibril"" ""Ibn Taymiyyah"" ""Abdul Wahhab"". Examples of clergy that they hate the most ""Hamza Yusuf"" ""Suhaib Webb"" ""Yaser Qadhi"" ""Nouman Ali Khan"" ""Yaqoubi"".  Timeline View Visualize all the tweets over a timeline and identify peak moments  Further Reading ""ISIS Has a Twitter Strategy and It is Terrifying [Infographic]"" About Fifth Tribe Fifth Tribe is a digital agency based out of DC that serves businesses non-profits and government agencies. We provide our clients with product development branding web/mobile development and digital marketing services. Our client list includes Oxfam Ernst and Young Kaiser Permanente Aetna Innovation Health the U.S. Air Force and the U.S. Peace Corps. Along with Goldman Sachs International and IBM we serve on the Private Sector Committee of the Board of the Global Community Engagement and Resilience Fund (GCERF) the first global effort to support local community-level initiatives aimed at strengthening resilience against violent extremism. In December 2014 we won the anti-ISIS ""Hedaya Hack"" organized by Affinis Labs and hosted at the ""Global Countering Violent Extremism (CVE) Expo "" in Abu Dhabi. Since then we've been actively involved in working with the open-source community and community content producers in developing counter-messaging campaigns and tools. ",CSV,,"[crime, twitter, internet]",CC0,,,4810,52548,6,Analyze how ISIS fanboys have been using Twitter since 2015 Paris Attacks,How ISIS Uses Twitter,https://www.kaggle.com/fifthtribe/how-isis-uses-twitter,Wed May 18 2016
223,,IbrahimAljarah,"[gender, NationalITy, PlaceofBirth, StageID, GradeID, SectionID, Topic, Semester, Relation, raisedhands, VisITedResources, AnnouncementsView, Discussion, ParentAnsweringSurvey, ParentschoolSatisfaction, StudentAbsenceDays, Class]","[string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string]",Students' Academic Performance Dataset (xAPI-Edu-Data) Data Set Characteristics Multivariate Number of Instances 480 Area E-learning Education Predictive models Educational Data Mining Attribute Characteristics Integer/Categorical  Number of Attributes 16 Date 2016-11-8 Associated Tasks Classification Missing Values? No File formats xAPI-Edu-Data.csv Source Elaf Abu Amrieh Thair Hamtini and Ibrahim Aljarah The University of Jordan Amman Jordan http//www.Ibrahimaljarah.com www.ju.edu.jo Dataset Information This is an educational data set which is collected from learning management system (LMS) called Kalboard 360. Kalboard 360 is a multi-agent LMS which has been designed to facilitate learning through the use of leading-edge technology. Such system provides users with a synchronous access to educational resources from any device with Internet connection.  The data is collected using a learner activity tracker tool which called experience API (xAPI). The xAPI is a component of the training and learning architecture (TLA) that enables to monitor learning progress and learner’s actions like reading an article or watching a training video. The experience API helps the learning activity providers to determine the learner activity and objects that describe a learning experience. The dataset consists of 480 student records and 16 features. The features are classified into three major categories (1) Demographic features such as gender and nationality. (2) Academic background features such as educational stage grade Level and section. (3) Behavioral features such as raised hand on class opening resources answering survey by parents and school satisfaction. The dataset consists of 305 males and 175 females. The students come from different origins such as 179 students are from Kuwait 172 students are from Jordan 28 students from Palestine 22 students are from Iraq 17 students from Lebanon 12 students from Tunis 11 students from Saudi Arabia 9 students from Egypt 7 students from Syria 6 students from USA Iran and Libya 4 students from Morocco and one student from Venezuela. The dataset is collected through two educational semesters 245 student records are collected during the first semester and 235 student records are collected during the second semester.  The data set includes also the school attendance feature such as the students are classified into two categories based on their absence days 191 students exceed 7 absence days and 289 students their absence days under 7. This dataset includes also a new category of features; this feature is parent parturition in the educational process. Parent participation feature have two sub features Parent Answering Survey and Parent School Satisfaction. There are 270 of the parents answered survey and 210 are not 292 of the parents are satisfied from the school and 188 are not.  (See the related papers for more details). Attributes 1 Gender - student's gender (nominal 'Male' or 'Female’)  2 Nationality- student's nationality (nominal’ Kuwait’’ Lebanon’’ Egypt’’ SaudiArabia’’ USA’’ Jordan’’  Venezuela’’ Iran’’ Tunis’’ Morocco’’ Syria’’ Palestine’’ Iraq’’ Lybia’) 3 Place of birth- student's Place of birth (nominal’ Kuwait’’ Lebanon’’ Egypt’’ SaudiArabia’’ USA’’ Jordan’’  Venezuela’’ Iran’’ Tunis’’ Morocco’’ Syria’’ Palestine’’ Iraq’’ Lybia’) 4 Educational Stages- educational level student belongs (nominal ‘lowerlevel’’MiddleSchool’’HighSchool’)  5 Grade Levels- grade student belongs (nominal ‘G-01’ ‘G-02’ ‘G-03’ ‘G-04’ ‘G-05’ ‘G-06’ ‘G-07’ ‘G-08’ ‘G-09’ ‘G-10’ ‘G-11’ ‘G-12 ‘)  6 Section ID- classroom student belongs (nominal’A’’B’’C’) 7 Topic- course topic (nominal’ English’’ Spanish’ ‘French’’ Arabic’’ IT’’ Math’’ Chemistry’ ‘Biology’ ‘Science’’ History’’ Quran’’ Geology’) 8 Semester- school year semester (nominal’ First’’ Second’) 9 Parent responsible for student (nominal’mom’’father’) 10 Raised hand- how many times the student raises his/her hand on classroom (numeric0-100) 11- Visited resources- how many times the student visits a course content(numeric0-100) 12 Viewing announcements-how many times the student checks the new announcements(numeric0-100) 13 Discussion groups- how many times the student participate on discussion groups (numeric0-100) 14 Parent Answering Survey- parent answered the surveys which are provided from school or not  (nominal’Yes’’No’) 15 Parent School Satisfaction- the Degree of parent satisfaction from school(nominal’Yes’’No’) 16 Student Absence Days-the number of absence days for each student (nominal above-7 under-7) The students are classified into three numerical intervals based on their total grade/mark Low-Level interval includes values from 0 to 69  Middle-Level interval includes values from 70 to 89  High-Level interval includes values from 90-100. Relevant Papers  Amrieh E. A. Hamtini T. & Aljarah I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application 9(8) 119-136. Amrieh E. A. Hamtini T. & Aljarah I. (2015 November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT) 2015 IEEE Jordan Conference on (pp. 1-5). IEEE.  Citation Request Please include these citations if you plan to use this dataset  Amrieh E. A. Hamtini T. & Aljarah I. (2016). Mining Educational Data to Predict Student’s academic Performance using Ensemble Methods. International Journal of Database Theory and Application 9(8) 119-136. Amrieh E. A. Hamtini T. & Aljarah I. (2015 November). Preprocessing and analyzing educational data set using X-API for improving student's performance. In Applied Electrical Engineering and Computing Technologies (AEECT) 2015 IEEE Jordan Conference on (pp. 1-5). IEEE. ,CSV,,[education],CC4,,,8883,74895,0.0361328125,xAPI-Educational Mining Dataset,Students' Academic Performance Dataset,https://www.kaggle.com/aljarah/xAPI-Edu-Data,Sun Nov 27 2016
224,,National Archives,[],[],Content Executive orders are official documents numbered consecutively through which the President of the United States manages the operations of the federal government. The text of executive orders appears in the daily Federal Register as each executive order is signed by the President and received by the Office of the Federal Register. The total number of Executive orders issued by each administration includes number-and-letter designated orders such as 9577-A 9616-A etc. Acknowledgements The data was compiled and published by the National Archives as executive order disposition tables available for Franklin D. Roosevelt and later presidents.,CSV,,"[history, politics]",CC0,,,327,4565,0.00390625,"Name, years in office, and executive orders signed by every American president","Executive Orders, 1789-2016",https://www.kaggle.com/nationalarchives/executive-orders,Thu Feb 02 2017
225,,Rachael Tatman,[],[],Context Some words like “the” or “and” in English are used a lot in speech and writing. For most Natural Language Processing applications you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand. Content This project uses the source texts provided by the African Storybook Project as a corpus and provides a number of tools to extract frequency lists and lists of stopwords from this corpus for the 60+ languages covered by ASP. Included in this dataset are the following languages  Afrikaans stoplist and word frequency Hausa stoplist and word frequency Lugbarati word frequency only Lugbarati (Official) word frequency only Somali stoplist and word frequency Sesotho stoplist and word frequency Kiswahili stoplist and word frequency Yoruba stoplist and word frequency isiZulu stoplist and word frequency  Files are named using the language’s ISO code. For each language code.txt is the list of stopwords and code_frequency_list.txt is word frequency information. A list of ISO codes the the languages associated with them may be found in ISO_codes.csv. Acknowledgements This project therefore attempts to fill in the gap in language coverage for African language stoplists by using the freely-available and open-licensed ASP Source project as a corpus. Dual-licensed under CC-BY and Apache-2.0 license. Compiled by Liam Doherty. More information and the scripts used to generate these files are available here. Inspiration This dataset is mainly helpful for use during NLP analysis however there may some interesting insights in the data.  What qualities do stopwords share across languages? Given a novel language could you predict what its stopwords should be? What stopwords are shared across languages? Often related languages will have words with the same meaning and similar spellings. Can you automatically identify any of these pairs of words?  You may also like  Stopword Lists for 19 Languages (mainly European and South Asian) ,CSV,,"[languages, africa, linguistics]",Other,,,103,726,0.2041015625,Stopword Lists & Frequency Information for 9 African Languages,Stopword Lists for African Languages,https://www.kaggle.com/rtatman/stopword-lists-for-african-languages,Fri Jul 28 2017
226,,World Bank,"[CountryCode, ShortName, TableName, LongName, Alpha2Code, CurrencyUnit, SpecialNotes, Region, IncomeGroup, Wb2Code, NationalAccountsBaseYear, NationalAccountsReferenceYear, SnaPriceValuation, LendingCategory, OtherGroups, SystemOfNationalAccounts, AlternativeConversionFactor, PppSurveyYear, BalanceOfPaymentsManualInUse, ExternalDebtReportingStatus, SystemOfTrade, GovernmentAccountingConcept, ImfDataDisseminationStandard, LatestPopulationCensus, LatestHouseholdSurvey, SourceOfMostRecentIncomeAndExpenditureData, VitalRegistrationComplete, LatestAgriculturalCensus, LatestIndustrialData, LatestTradeData, LatestWaterWithdrawalData]","[string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric]",The World Development Indicators from the World Bank contain over a thousand annual indicators of economic development from hundreds of countries around the world. Here's a list of the available indicators along with a list of the available countries. For example this data includes the life expectancy at birth from many countries around the world  The dataset hosted here is a slightly transformed verion of the raw files available here to facilitate analytics.,CSV,,"[economics, international relations]",Other,,,20418,134341,2048,Explore country development indicators from around the world,World Development Indicators,https://www.kaggle.com/worldbank/world-development-indicators,Mon May 01 2017
227,,Jordan Goblet,"[ATP, Location, Tournament, Date, Series, Court, Surface, Round, Best of, Winner, Loser, WRank, LRank, W1, L1, W2, L2, W3, L3, W4, L4, W5, L5, Wsets, Lsets, Comment, CBW, CBL, GBW, GBL, IWW, IWL, SBW, SBL, B365W, B365L, B&WW, B&WL, EXW, EXL, PSW, PSL, WPts, LPts, UBW, UBL, LBW, LBL, SJW, SJL, MaxW, MaxL, AvgW, AvgL]","[numeric, string, string, dateTime, string, string, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Results for the men's ATP tour date back to January 2000 including Grand Slams Masters Series Masters Cup and International Series competitions. Historical head-to-head betting odds go back to 2001. See here for more details about the metadata  http//www.tennis-data.co.uk/notes.txt Source - http//www.tennis-data.co.uk/data.php There is a lot you can do with this data set. The ultimate goal is obviously to predict the outcome of the game or to build an efficient betting strategy based on your model(s).  You can also  compare the betting agencies (Bet365 Bet&Win Ladbrokes Pinnacles...) in terms of predictions quality or measure the progress of these betting agencies over the years discover if it's possible to predict specific events (3 sets match retirement walk over...) or the evolution of players. ,CSV,,[tennis],Other,,,1796,15362,9,Results of the ATP tour competitions since 2000,ATP Men's Tour,https://www.kaggle.com/jordangoblet/atp-tour-20002016,Tue Sep 27 2016
228,,Central Bureau of Statistics,"[Name, Hebrew Name, Year Established, Regional Council, Latitude, Longitude, West Bank Barrier, 2015 Population, 2010 Population, 2005 Population, 2000 Population]","[string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric]",Content This dataset includes a record for each Israeli neighborhood in East Jerusalem and settlement in the West Bank recognized by Israeli authorities; therefore Israeli outposts constructed without government authorization and Nahal settlements established by Israel Defense Forces and thus regarded as military bases are excluded. Each row includes the settlement name in English and Hebrew year established regional council location in latitude/longitude coordinates and relative to the West Bank Barrier and population estimates for the past fifteen years. Acknowledgements The settlement population statistics were collected by the Israel Central Bureau of Statistics.,CSV,,[politics],CC0,,,159,2548,0.01171875,Is the population of Israeli settlements rising or falling?,Israeli Settlements in the West Bank,https://www.kaggle.com/ilcbs/israeli-settlements,Fri Feb 03 2017
229,,xWang,"[Season, Series, City, Country, Year, Month, Day, Distance, Round, Group, Num_Skater, Name, Nationality, Rank_In_Group, Start_Position, Time, Qualification, rank_lap1, time_lap1, rank_lap2, time_lap2, rank_lap3, time_lap3, rank_lap4, time_lap4, rank_lap5, time_lap5, Time_Event]","[string, string, string, string, numeric, numeric, numeric, string, string, numeric, numeric, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Short Track Speed Skating Database for Sports Data Analysis What is short track speed skating? Maybe some people have never heard of this sport. Short track is a competitive and strategic game in which skaters race on ice. Sometimes the smartest or the luckiest guy rather than the strongest wins the game (for example). What's in the data? The database covers all the international short track games in the last 5 years. Currently it contains only men's 500m but I will keep updating it.  Detailed lap data including personal time and ranking in each game from seasons 2012/2013 to present . The final time results ranking starting position qualified or penalized information of each athlete in each game. All series of World Cup World Championship European Championship and Olympic Games.  Original data source The data is collected from the ISU's (International Skating Union) official website. I have already done the cleaning procedure.  Please make sure that the data are only for personal and non-commercial use.  Explore the data Interesting questions may be like  What will happen in a game when there are more than one athlete from the same team? Are there performance all improved? How does the performance of athletes change within a season and over seasons? Do some athletes have special patterns in terms of time allocation and surpassing opportunity? What is the influence of the implementation of ""no toe starts"" rules on athletes since July 2015?  Is there also home advantage like in other sports? Who are the most ""dangerous"" guys that always get penalty?  ",CSV,,[sports],Other,,,180,2281,0.8203125,Detailed lap data of each game in the last 5 seasons,Short Track Speed Skating Database,https://www.kaggle.com/seniorwx/shorttrack,Wed Dec 28 2016
230,,Megan Risdal,"[Code, State, Sector, Obs, Employment, Members, Covered, PctMem, PctCov, Year]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","The United States Department of Labor tells us that ""Labor Day the first Monday in September is a creation of the labor movement and is dedicated to the social and economic achievements of American workers. It constitutes a yearly national tribute to the contributions workers have made to the strength prosperity and well-being of our country."" This database of state-level union membership and coverage from 1983 to 2015 was originally compiled by Barry Hirsch (Andrew Young School of Policy Studies Georgia State University) and David Macpherson (Department of Economics Trinity University). The database available at unionstats.com provides private and public sector labor union membership coverage and density estimates compiled from the monthly household Current Population Survey (CPS) using BLS methods. Use of this data requires citation of the following paper which also includes a description of how the database was created Barry T. Hirsch and David A. Macpherson ""Union Membership and Coverage Database from the Current Population Survey Note"" Industrial and Labor Relations Review Vol. 56 No. 2 January 2003 pp. 349-54. (PDF).",CSV,,"[government, economics]",Other,,,205,2885,0.4296875,The state of unions in the United States (1983 to 2015),Union Membership & Coverage,https://www.kaggle.com/mrisdal/union-membership-coverage,Mon Sep 05 2016
231,,UCI Machine Learning,"[age, workclass, fnlwgt, education, education.num, marital.status, occupation, relationship, race, sex, capital.gain, capital.loss, hours.per.week, native.country, income]","[numeric, string, numeric, string, numeric, string, string, string, string, string, numeric, numeric, numeric, string, string]","This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization Silicon Graphics). A set of reasonably clean records was extracted using the following conditions ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year. Description of fnlwgt (final weight) The weights on the Current Population Survey (CPS) files are controlled to independent estimates of the civilian noninstitutional population of the US.  These are prepared monthly for us by Population Division here at the Census Bureau. We use 3 sets of controls. These are   A single cell estimate of the population 16+ for each state. Controls for Hispanic Origin by age and sex. Controls by Race age and sex.  We use all three sets of controls in our weighting program and ""rake"" through them 6 times so that by the end we come back to all the controls we used. The term estimate refers to population totals derived from CPS by creating ""weighted tallies"" of any specified socio-economic characteristics of the population. People with similar demographic characteristics should have similar weights. There is one important caveat to remember about this statement. That is that since the CPS sample is actually a collection of 51 state samples each with its own probability of selection the statement only applies within state. Relevant papers Ron Kohavi ""Scaling Up the Accuracy of Naive-Bayes Classifiers a Decision-Tree Hybrid"" Proceedings of the Second International Conference on Knowledge Discovery and Data Mining 1996. (PDF)",CSV,,"[employment, demographics]",CC0,,,5033,42616,4,Predict whether income exceeds $50K/yr based on census data,Adult Census Income,https://www.kaggle.com/uciml/adult-census-income,Sat Oct 08 2016
232,,Kaggle,[],[],"One way to understand how a city government works is by looking at who it employs and how its employees are compensated. This data contains the names job title and compensation for San Francisco city employees on an annual basis from 2011 to 2014.  Exploration Ideas To help get you started here are some data exploration ideas  How have salaries changed over time between different groups of people? How are base pay overtime pay and benefits allocated between different groups? Is there any evidence of pay discrimination based on gender in this dataset? How is budget allocated based on different groups and responsibilities?  Have other ideas you're curious for someone else to explore? Post them in this forum thread. Data Description sf-salaries-release-*.zip (downloadable via the ""Download Data"" link in the header above) contains a CSV table and a SQLite database (with the same data as the CSV file). Here's the code that creates this data release. The original source for this data is here. We've taken the raw files here and combined/normalized them into a single CSV file as well as a SQLite database with an equivalently-defined table.",SQLite,,[income],CC0,,,14950,95514,33,Explore San Francisco city employee salary data,SF Salaries,https://www.kaggle.com/kaggle/sf-salaries,Thu Oct 06 2016
233,,Dryad Digital Repository,"[female_id, male_id, cycle_id, consort, conceptive, female_hybridscore, male_hybridscore, female_gendiv, male_gendiv, gen_distance, female_age, male_rank, female_rank, males_present, females_present, male_rank_transform, gen_distance_transform, rank_interact, assort_index, female_age_transform]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","This dataset contains over 12k observations of male-female baboon pairs from a population of baboons that has recently seen genetic admixture from a different (but closely-related) taxon. The data contains genetic and social information for the male and female baboons whether they mated and whether the mating resulted in conception of offspring. Acknowledgements The original journal article that this data was collected for  Tung J Charpentier MJE Mukherjee S Altmann J Alberts SC (2012) Genetic effects on mating success and partner choice in a social mammal. The American Naturalist 180(1) 113-129. http//dx.doi.org/10.1086/665993 The Data Dryad page that this data was downloaded from  Tung J Charpentier MJE Mukherjee S Altmann J Alberts SC (2012) Data from Genetic effects on mating success and partner choice in a social mammal. Dryad Digital Repository. http//dx.doi.org/10.5061/dryad.4r9h61v8 Abstract (from the original paper) Mating behavior has profound consequences for two phenomena—individual reproductive success and the maintenance of species boundaries—that contribute to evolutionary processes. Studies of mating behavior in relation to individual reproductive success are common in many species but studies of mating behavior in relation to genetic variation and species boundaries are less commonly conducted in socially complex species. Here we leveraged extensive observations of a wild yellow baboon (Papio cynocephalus) population that has experienced recent gene flow from a close sister taxon the anubis baboon (Papio anubis) to examine how admixture-related genetic background affects mating behavior. We identified novel effects of genetic background on mating patterns including an advantage accruing to anubis-like males and assortative mating among both yellow-like and anubis-like pairs. These genetic effects acted alongside social dominance rank inbreeding avoidance and age to produce highly nonrandom mating patterns. Our results suggest that this population may be undergoing admixture-related evolutionary change driven in part by nonrandom mating. However the strength of the genetic effect is mediated by behavioral plasticity and social interactions emphasizing the strong influence of social context on mating behavior in socially complex species. The Data This dataset contains over 12000 observations of the following variables  female_id three letter ""short name"" ID for the female in a potentially consorting pair; each female has a unique ID male_id three letter ""short name"" ID for the male in a potentially consorting pair; each male has a unique ID cycle_id a unique number assigned to each female-estrus cycle combination consort whether the female-male pair consorted (1) or not (0) given the opportunity to do so conceptive whether the estrus cycle resulted in a conception (1) or not (0) female_hybridscore an estimate of the proportion of the female's genome that represents anubis baboon ancestry; for details of the estimation procedure see Materials and Methods and Tung et al (2008) male_hybridscore an estimate of the proportion of the male's genome that represents anubis baboon ancestry; for details of the estimation procedure see Materials and Methods and Tung et al (2008) female_gendiv an estimate of the female's genetic diversity; for details of the estimation procedure see Materials and Methods male_gendiv an estimate of the male's genetic diversity; for details of the estimation procedure see Materials and Methods gen_distance an estimate of the genetic distance (Queller-Goodnight r) between the male and female of a potentially consorting pair female_age the age of the female in a potentially consorting pair male_rank the ordinal rank of the male in a potentially consorting pair female_rank the ordinal rank of the female in a potentially consorting pair males_present the number of adult males present in the group of the potentially consorting pair females_present the number of adult females present in the group of the potentially consorting pair male_rank_transform ordinal male rank transformed to reflect fit (given number of males in a group) to the priority-of-access model; see Materials and Methods and Appendix for more details gen_distance_transform genetic distance estimate transform to test whether consortship probabilities decrease with genetic distance as well as genetic similarity rank_interact the multiplicative interaction of male rank and female rank in the potentially consorting pair assort_index assortative mating index calculated from the hybrid scores of the male and female of a potentially consorting pair; see Materials and Methods for additional detail female_age_transform female age transformed to test for a higher probability of consortship behavior for maximally fertile (middle-aged) females  Inspiration Here are a few ideas for things to look at in this dataset  does genetic distance affect mating probability? does age of the female baboon affect conception probability? does social rank of the male affect mating probability? ",CSV,,[animals],CC0,,,186,2853,2,Data on genetic admixture and likelihood of successful mating between baboons,Baboon Mating and Genetic Admixture,https://www.kaggle.com/dryad/baboon-mating,Mon Nov 07 2016
234,,Mike Chirico,"[gid, pid, cid, timeStamp, id, name, rid, msg]","[numeric, string, numeric, dateTime, numeric, string, string, string]",Facebook is becoming an essential tool for more than just family and friends.  Discover how Cheltenham Township (USA) a diverse community just outside of Philadelphia deals with major issues such as the Bill Cosby trial everyday traffic issues sewer I/I problems and lost cats and dogs.  And yes theft.   Communities work when they're connected and exchanging information.  What and who are the essential forces making a positive impact and when and how do conversational threads get directed or misdirected?      Use Any Facebook Public Group You can leverage the examples here for any public Facebook group.  For an example of the source code used to collect this data and a quick start docker image take a look at the following project facebook-group-scrape.   Data Sources There are 4 csv files in the dataset with data from the following 5 public Facebook groups  Unofficial Cheltenham Township Elkins Park Happenings! Free Speech Zone Cheltenham Lateral Solutions Cheltenham Township Residents  post.csv These are the main posts you will see on the page.  It might help to take a quick look at the page.  Commas in the msg field have been replaced with {COMMA} and apostrophes have been replaced with {APOST}.   gid Group id (5 different Facebook groups)  pid  Main Post id id    Id of the user posting name  User's name timeStamp shares url msg  Text of the message posted.  likes Number of likes  comment.csv These are comments to the main post. Note Facebook postings have comments and comments on comments.    gid  Group id pid  Matches Main Post identifier in post.csv cid  Comment Id. timeStamp  id  Id of user commenting name  Name of user commenting rid   Id of user responding to first comment  msg  Message  like.csv These are likes and responses.  The two keys in this file (pidcid) will join to post and comment respectively.  gid Group id pid  Matches Main Post identifier in post.csv cid  Matches Comments id. response  Response such as LIKE ANGRY etc. id  The id of user responding name Name of the user responding  member.csv These are all the members in the group.  Some members never or rarely post or comment.  You may find multiple entries in this table for the same person.  The name of the individual never changes but they change their profile picture. Each profile picture change is captured in this table.  Facebook gives users a new id in this table when they change their profile picture.  gid Group id  id Id of the member name Name of the member url  URL of the member ,CSV,,[internet],GPL,,,862,12879,59,Discover How a Community Leverages Facebook,Cheltenham's Facebook Groups,https://www.kaggle.com/mchirico/cheltenham-s-facebook-group,Sat Dec 23 2017
235,,Paulo Henrique Vasconcellos,"[, aircraft_id, occurrence_id, registration, operator_id, equipment, manufacturer, model, engine_type, engines_amount, takeoff_max_weight (Lbs), seatings_amount, year_manufacture, registration_country, registration_category, registration_aviation, origin_flight, destination_flight, operation_phase, type_operation, damage_level, fatalities_amount, extraction_day]","[numeric, numeric, numeric, string, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, dateTime]",Context For many years airplanes have been considered the second safest transport mean in the world - losing just to elevators. Traveling great distances in short time those aircrafts have brought several advantaged for the world both in commercial and regular application. Unfortunately as any transport mean aircrafts have their own count of tragedies. The last event envolving airplanes - to the publication date - was the accident envolving the brazilian soccer team Chapecoense and a LAMIA's aircraft which was transporting them to Colombia for a Championship. This tragedy brought back discussions and controversies about aircraft's security and human capacity during aeronautics occurrences. Content This dataset was available by CENIPA - Centro de Investigação e Prevenção de Acidentes aeronáuticos - or Aeronautical Accidents Investigation and Prevention Center. Such files contains informations about occurrences which envolved aircrafts in the last 10 years. You may access more updated data by visiting Brazilian Open Data's official website or clicking in the download links below.  Acknowledgements This dataset is available for studies and analysis thanks to CENIPA.,CSV,,"[brazil, aviation]",Other,,,202,2450,0.6005859375,Occurrences involving aircrafts from the last 10 years in Brazil,Brazilian Aeronautics Accidents,https://www.kaggle.com/paulovasconcellos/aeronautics-accidents-in-brazil,Wed Feb 08 2017
236,,Gabriele Angeletti,"[Time, Latitude, Longitude, Depth/Km, Magnitude]","[dateTime, numeric, numeric, numeric, numeric]",Context This dataset contains data about the earthquakes that hit the center of Italy between August and November 2016. For some simple visualizations of this dataset you can checkout this post. Content The dataset contains events from 2016-08-24 to 2016-11-30. It's a single .csv file with the following header  TimeLatitudeLongitudeDepth/KmMagnitude  The dataset contains 8087 rows (8086 of data + 1 of header) Acknowledgements The dataset was collected from this real-time updated list from the Italian Earthquakes National Center. Inspiration I hope that someone in the kaggle community will find this dataset interesting to analyze and/or visualize.,CSV,,[earth sciences],CC0,,,615,5882,0.376953125,Data about the earthquakes that hit Italy between August and November 2016.,Italy's Earthquakes,https://www.kaggle.com/blackecho/italy-earthquakes,Tue Dec 06 2016
237,,US Senate,"[Position, President, Nominee, Announced, Received, Withdrawn, Confirmed, Rejected, Vote Type, Votes For, Votes Against]","[string, string, string, dateTime, dateTime, string, dateTime, string, string, numeric, numeric]","Context The United States Constitution provides that the president ""shall nominate and by and with the Advice and Consent of the Senate shall appoint Ambassadors other public Ministers and Consuls Judges of the Supreme Court and all other Officers of the United States whose Appointments are not herein otherwise provided for..."" (Article II section 2). This provision like many others in the Constitution was born of compromise and over the more than two centuries since its adoption has inspired widely varying interpretations. The president nominates all federal judges in the judicial branch and specified officers in cabinet-level departments independent agencies the military services the Foreign Service and uniformed civilian services as well as U.S. attorneys and U.S. marshals. The importance of the position the qualifications of the nominee and the prevailing political climate influence the character of the Senate's response to each nomination. Views of the Senate's role range from a narrow construction that the Senate is obligated to confirm unless the nominee is manifestly lacking in character and competence to a broad interpretation that accords the Senate power to reject for any reason a majority of its members deems appropriate. Just as the president is not required to explain why he selected a particular nominee neither is the Senate obligated to give reasons for rejecting a nominee. Acknowledgements The confirmation vote records were recorded compiled and published by the Office of the Secretary of the Senate.",CSV,,[politics],CC0,,,60,827,0.0224609375,Senate confirmation vote records for cabinet nominees since 1976,Presidential Cabinet Nominations,https://www.kaggle.com/senate/confirmation-votes,Tue Feb 07 2017
238,,California Environmental Protection Agency,"[August 2016 Non-filers (The following suppliers did not submit July 2016 data by September 19,  2016)]","[string, string]",Context California has been dealing with the effects of an unprecedented drought. September 2016 marks the 16th month since the state’s 400-plus urban water suppliers were directed to be in compliance with the emergency conservation standards that followed the Governor’s April 1 2015 Executive Order. The State Water Board has been requiring water delivery information from urban water suppliers for 28 consecutive months following the historic July 2014 board action to adopt emergency water conservation regulations. On May 18 following the Governor’s May 9 Executive Order the Board adopted a statewide water conservation approach that replaces the prior percentage reduction-based water conservation standard with a localized “stress test” approach that mandates urban water suppliers act now to ensure at least a three-year supply of water to their customers under drought conditions. Content This fact sheet contains more details on how water conservation is monitored by the California EPA. This dataset describes the cumulative savings and compliance of water suppliers from June 2015 - August 2016.  Inspiration  What percentage of suppliers are actually meeting water conservation compliance standards? Which hydrologic regions of California are in most danger of not meeting their residents' water needs? Which city has the most residential gallons per capita per day (R-GPCD)? The least?  Are any cities outliers within their hydrological regions? Why might they be more or less successful in their water conservation efforts?  Acknowledgement This dataset is part of the CalEPA Water Boards Water Conservation Reporting and the original source can be found here.,CSV,,"[ecology, agriculture]",CC0,,,213,2318,0.0400390625,Reporting compliance by water suppliers in drought-stricken California,Water Conservation Supplier Compliance,https://www.kaggle.com/calepa/water-conservation-supplier-compliance,Thu Nov 17 2016
239,,Kory Becker,"[meanfreq, sd, median, Q25, Q75, IQR, skew, kurt, sp.ent, sfm, mode, centroid, meanfun, minfun, maxfun, meandom, mindom, maxdom, dfrange, modindx, label]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Voice Gender Gender Recognition by Voice and Speech Analysis This database was created to identify a voice as male or female based upon acoustic properties of the voice and speech. The dataset consists of 3168 recorded voice samples collected from male and female speakers. The voice samples are pre-processed by acoustic analysis in R using the seewave and tuneR packages with an analyzed frequency range of 0hz-280hz (human vocal range). The Dataset The following acoustic properties of each voice are measured and included within the CSV  meanfreq mean frequency (in kHz) sd standard deviation of frequency median median frequency (in kHz) Q25 first quantile (in kHz) Q75 third quantile (in kHz) IQR interquantile range (in kHz) skew skewness (see note in specprop description) kurt kurtosis (see note in specprop description) sp.ent spectral entropy sfm spectral flatness mode mode frequency centroid frequency centroid (see specprop) peakf peak frequency (frequency with highest energy) meanfun average of fundamental frequency measured across acoustic signal minfun minimum fundamental frequency measured across acoustic signal maxfun maximum fundamental frequency measured across acoustic signal meandom average of dominant frequency measured across acoustic signal mindom minimum of dominant frequency measured across acoustic signal maxdom maximum of dominant frequency measured across acoustic signal dfrange range of dominant frequency measured across acoustic signal modindx modulation index. Calculated as the accumulated absolute difference between adjacent measurements of fundamental frequencies divided by the frequency range label male or female  Accuracy Baseline (always predict male) 50% / 50% Logistic Regression 97% / 98% CART 96% / 97% Random Forest 100% / 98% SVM 100% / 99% XGBoost 100% / 99% Research Questions An original analysis of the data-set can be found in the following article  Identifying the Gender of a Voice using Machine Learning The best model achieves 99% accuracy on the test set. According to a CART model it appears that looking at the mean fundamental frequency might be enough to accurately classify a voice. However some male voices use a higher frequency even though their resonance differs from female voices and may be incorrectly classified as female. To the human ear there is apparently more than simple frequency that determines a voice's gender. Questions  What other features differ between male and female voices? Can we find a difference in resonance between male and female voices? Can we identify falsetto from regular voices? (separate data-set likely needed for this) Are there other interesting features in the data?  CART Diagram  Mean fundamental frequency appears to be an indicator of voice gender with a threshold of 140hz separating male from female classifications. References The Harvard-Haskins Database of Regularly-Timed Speech Telecommunications & Signal Processing Laboratory (TSP) Speech Database at McGill University Home VoxForge Speech Corpus Home Festvox CMU_ARCTIC Speech Database at Carnegie Mellon University,CSV,,"[gender, linguistics]",CC4,,,8487,83452,1,Identify a voice as male or female,Gender Recognition by Voice,https://www.kaggle.com/primaryobjects/voicegender,Fri Aug 26 2016
240,,Bianca Kramer,[],[],Many new websites and online tools have come into existence to support scholarly communication in all phases of the research workflow. To what extent researchers are using these and more traditional tools has been largely unknown. This 2015-2016 survey aimed to fill that gap.   The survey captured information on tool usage for 17 research activities stance towards open access and open science and expectations of the most important development in scholarly communication. Respondents’ demographics included research roles country of affiliation research discipline and year of first publication. The online survey employed an open non-probability sample. A largely self-selected group of 20663 researchers librarians editors publishers and other groups involved in research took the survey which was available in seven languages. The survey was open from May 10 2015 to February 10 2016. This data set contains  Full raw (anonymized) and cleaned data files (csv each file containing 20663 records and 178 variables) Variable lists for raw and cleaned data files (csv) Readme file (txt)  The dataset is also deposited in Zenodo http//dx.doi.org/10.5281/zenodo.49583 The full description of survey methodology is in a data publication in F1000 Research http//dx.doi.org/10.12688/f1000research.8414.1 More information on the project this survey is part of can be found here http//101innovations.wordpress.com [edited to add] For quick visual exploration of the data check out the interactive dashboard on Silk http//dashboard101innovations.silk.co/ Contact  Jeroen Bosman http//orcid.org/0000-0001-5796-2727 / j.bosman@uu.nl Bianca Kramer http//orcid.org/0000-0002-5965-6560 / b.m.r.kramer@uu.nl ,CSV,,[research],CC0,,,1062,16639,27,Explore global research practices and opinions on scholarly communication,101 Innovations - Research Tools Survey,https://www.kaggle.com/bmkramer/101-innovations-research-tools-survey,Wed May 18 2016
241,,samdeeplearning,"[response_id, class, response_text, , , , , ]","[string, string, string, string, string, string, string, string]",What's In The Deep-NLP Dataset? Sheet_1.csv contains 80 user responses in the response_text column to a therapy chatbot. Bot said 'Describe a time when you have acted as a resource for someone else'.  User responded. If a response is 'not flagged' the user can continue talking to the bot. If it is 'flagged' the user is referred to help.   Sheet_2.csv contains 125 resumes in the resume_text column. Resumes were queried from Indeed.com with keyword 'data scientist' location 'Vermont'. If a resume is 'not flagged' the applicant can submit a modified resume version at a later date. If it is 'flagged' the applicant is invited to interview. What Do I Do With This? Classify new resumes/responses as flagged or not flagged.  There are two sets of data here - resumes and responses. Split the data into a train set and a test set to test the accuracy of your classifier. Bonus points for using the same classifier for both problems.  Good luck. Acknowledgements Thank you to Parsa Ghaffari (Aylien) without whom these visuals (cover photo is in Parsa Ghaffari's excellent LinkedIn article on English Spanish and German postive v. negative sentiment analysis) would not exist. There Is A 'deep natural language processing' Kernel. I will update it. I Hope You Find It Useful. You can use any of the code in that kernel anywhere on or off Kaggle. Ping me at @_samputnam for questions.,CSV,,"[languages, linguistics]",Other,,,1671,25437,0.6474609375,natural language processing,Deep-NLP,https://www.kaggle.com/samdeeplearning/deepnlp,Wed Mar 01 2017
242,,Myles O'Neill,"[gene, transcript, protein]","[string, string, string]",Drosophila Melanogaster Drosophila Melanogaster the common fruit fly is a model organism which has been extensively used in entymological research. It is one of the most studied organisms in biological research particularly in genetics and developmental biology.  When its not being used for scientific research D. melanogaster is a common pest in homes restaurants and anywhere else that serves food. They are not to be confused with Tephritidae flys (also known as fruit flys).  https//en.wikipedia.org/wiki/Drosophila_melanogaster About the Genome This genome was first sequenced in 2000. It contains four pairs of chromosomes (234 and X/Y). More than 60% of the genome appears to be functional non-protein-coding DNA.   The genome is maintained and frequently updated at FlyBase. This dataset is sourced from the UCSC Genome Bioinformatics download page. It uses the August 2014 version of the D. melanogaster genome (dm6 BDGP Release 6 + ISO1 MT). http//hgdownload.soe.ucsc.edu/downloads.html#fruitfly Files were modified by Kaggle to be a better fit for analysis on Scripts. This primarily involved turning files into CSV format with a header row as well as converting the genome itself from 2bit format into a FASTA sequence file. Bioinformatics Genomic analysis can be daunting to data scientists who haven't had much experience with bioinformatics before. We have tried to give basic explanations to each of the files in this dataset as well as links to further reading on the biological basis for each. If you haven't had the chance to study much biology before some light reading (ie wikipedia) on the following topics may be helpful to understand the nuances of the data provided here Genetics Genomics (Sequencing/Genome Assembly) Chromosomes DNA RNA (mRNA/miRNA) Genes Alleles Exons Introns Transcription Translation Peptides Proteins Gene Regulation Mutation Phylogenetics and SNPs.  Of course if you've got some idea of the basics already - don't be afraid to jump right in!  Learning Bioinformatics There are a lot of great resources for learning bioinformatics on the web. One cool site is Rosalind - a platform that gives you bioinformatic coding challenges to complete. You can use Kaggle Scripts on this dataset to easily complete the challenges on Rosalind (and see Myles' solutions here if you get stuck). We have set up Biopython on Kaggle's docker image which is a great library to help you with your analyses. Check out their tutorial here and we've also created a python notebook with some of the tutorial applied to this dataset as a reference. Files in this Dataset  Drosophila Melanogaster Genome  genome.fa  The assembled genome itself is presented here in FASTA format. Each chromosome is a different sequence of nucleotides. Repeats from RepeatMasker and Tandem Repeats Finder (with period of 12 or less) are show in lower case; non-repeating sequence is shown in upper case.  Meta Information There are 3 additional files with meta information about the genome.  meta-cpg-island-ext-unmasked.csv  This file contains descriptive information about CpG Islands in the genome. https//en.wikipedia.org/wiki/CpG_site  meta-cytoband.csv  This file describes the positions of cytogenic bands on each chromosome. https//en.wikipedia.org/wiki/Cytogenetics  meta-simple-repeat.csv  This file describes simple tandem repeats in the genome. https//en.wikipedia.org/wiki/Repeated_sequence_(DNA) https//en.wikipedia.org/wiki/Tandem_repeat  Drosophila Melanogaster mRNA Sequences Messenger RNA (mRNA) is an intermediate molecule created as part of the cellular process of converting genomic information into proteins. Some mRNA are never translated into proteins and have functional roles in the cell on their own. Collectively organism mRNA information is known as a Transcriptome. mRNA files included in this dataset give insight into the activity of genes in the organism. https//en.wikipedia.org/wiki/Messenger_RNA  mrna-genbank.fa  This file includes all mRNA sequences from GenBank associated with Drosophila Melanogaster. http//www.ncbi.nlm.nih.gov/genbank/  mrna-refseq.fa  This file includes all mRNA sequences from RefSeq associated with Drosophila Melanogaster. http//www.ncbi.nlm.nih.gov/refseq/  Gene Predictions A gene is a segment of DNA on the genome which through mRNA is used to create proteins in the organism. Knowing which parts of DNA are coding (genes) or non-coding is difficult and a number of different systems for prediction exist. This dataset includes a number of different gene prediction systems applied to the drosophila melanogaster genome. https//en.wikipedia.org/wiki/Gene_prediction  genes-augustus.csv  AUGUSTUS is a piece of software that predicts genes ab initio using Hidden Markov Models. http//www.ncbi.nlm.nih.gov/pmc/articles/PMC441517/  genes-genscan.csv  GENSCAN is an older ab initio software for predicting genes.  http//genes.mit.edu/GENSCANinfo.html  genes-ensembl.csv ensembl-gtp.csv ensembl-pep.csv ensembl-source.csv ensembl-to-gene-name.csv  Ensembl provides gene annotation generated by their software Genebuild. This process combines automatic annotation alongside manual curation.  http//uswest.ensembl.org/info/genome/genebuild/genome_annotation.html We have also included some supplementary files for these including predicted protein peptide sequences for each predicted gene.  genes-refseq.csv genes-xeno-refseq.csv refseq-link.csv refseq-summary.csv  We have included two RefSeq gene predictions in this dataset. The first is based solely on information from the drosophila melanogaster genome. The second (genes-xeno-refseq.csv) uses genes from other organisms as a basis for predicting genes in drosophila melanogaster. RefSeq RNAs were aligned against the D. melanogaster genome using blat; those with an alignment of less than 15% were discarded. When a single RNA aligned in multiple places the alignment having the highest base identity was identified. Only alignments having a base identity level within 0.1% of the best and at least 96% base identity with the genomic sequence were kept. We have also included supplementary files for these which include information about the genes that have been identified. http//www.ncbi.nlm.nih.gov/refseq/  What can you do with this data? Genomic data is the foundation of bioinformatics and there is an incredible array of things you can do with this data. A good place to start is to look at some of the meta supplementary files alongside the genomic sequence itself.  We have a number of different gene prediction systems in the dataset how do they compare to each other? How do they compare to the mRNA data?  Working back from the refseq-summary.csv file you can look at genes that code for particular proteins - can you find these genes in the genome? How much of the genome codes for the mRNA's found in our mRNA data? Of the mRNA's we have how many map to the predicted genes and the predicted peptided sequence data? How much of the mRNA seems to be protein-coding vs how much looks like it is miRNA? Can you find pre-mRNA or splice variants within the mRNA data? Does meta information like cytogenic bands or CpG sites correspond with splice variants or a lack of mRNA altogether? Those are just some of many ideas that could get you started. Looking for Feedback This is the first genomic dataset on Kaggle and we are looking for feedback from our community about how interesting this dataset is to them or if there are ways we could improve it to better suit analysis. Please post suggestions for supplementary data future genomes we could host bioinformatics packages we should include on scripts and any other feedback on the dataset forum.,CSV,,"[biology, medicine]",CC0,,,668,12392,460,Explore the annotated genome of the common fruit fly,Drosophila Melanogaster Genome,https://www.kaggle.com/mylesoneill/drosophila-melanogaster-genome,Thu May 26 2016
243,,Kaggle,[],[],"US Social Security applications are a great way to track trends in how babies born in the US are named. Data.gov releases two datasets that are helplful for this one at the national level and another at the state level. Note that only names with at least 5 babies born in the same year (/ state) are included in this dataset for privacy.  I've taken the raw files here and combined/normalized them into two CSV files (one for each dataset) as well as a SQLite database with two equivalently-defined tables. The code that did these transformations is available here. New to data exploration in R? Take the free interactive DataCamp course ""Data Exploration With Kaggle Scripts"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way.",Other,,[children],CC0,,,10889,94565,173,Explore naming trends from babies born in the US,US Baby Names,https://www.kaggle.com/kaggle/us-baby-names,Wed Nov 22 2017
244,,BoltzmannBrain,[],[],"The Numenta Anomaly Benchmark (NAB) is a novel benchmark for evaluating algorithms for anomaly detection in streaming online applications. It is comprised of over 50 labeled real-world and artificial timeseries data files plus a novel scoring mechanism designed for real-time applications. All of the data and code is fully open-source with extensive documentation and a scoreboard of anomaly detection algorithms github.com/numenta/NAB. The full dataset is included here but please go to the repo for details on how to evaluate anomaly detection algorithms on NAB. NAB Data Corpus The NAB corpus of 58 timeseries data files is designed to provide data for research in streaming anomaly detection. It is comprised of both real-world and artifical timeseries data containing labeled anomalous periods of behavior. Data are ordered timestamped single-valued metrics. All data files contain anomalies unless otherwise noted. The majority of the data is real-world from a variety of sources such as AWS server metrics Twitter volume advertisement clicking metrics traffic data and more. All data is included in the repository with more details in the data readme. We are in the process of adding more data and actively searching for more data. Please contact us at nab@numenta.org if you have similar data (ideally with known anomalies) that you would like to see incorporated into NAB. The NAB version will be updated whenever new data (and corresponding labels) is added to the corpus; NAB is currently in v1.0. Real data  realAWSCloudwatch/ AWS server metrics as collected by the AmazonCloudwatch service. Example metrics include CPU Utilization Network Bytes In and Disk Read Bytes. realAdExchange/ Online advertisement clicking rates where the metrics are cost-per-click (CPC) and cost per thousand impressions (CPM). One of the files is normal without anomalies. realKnownCause/ This is data for which we know the anomaly causes; no hand labeling. ambient_temperature_system_failure.csv The ambient temperature in an office setting. cpu_utilization_asg_misconfiguration.csv From Amazon Web Services (AWS) monitoring CPU usage – i.e. average CPU usage across a given cluster. When usage is high AWS spins up a new machine and uses fewer machines when usage is low. ec2_request_latency_system_failure.csv CPU usage data from a server in Amazon's East Coast datacenter. The dataset ends with complete system failure resulting from a documented failure of AWS API servers. There's an interesting story behind this data in the Numenta blog. machine_temperature_system_failure.csv Temperature sensor data of an internal component of a large industrial mahcine. The first anomaly is a planned shutdown of the machine. The second anomaly is difficult to detect and directly led to the third anomaly a catastrophic failure of the machine. nyc_taxi.csv Number of NYC taxi passengers where the five anomalies occur during the NYC marathon Thanksgiving Christmas New Years day and a snow storm. The raw data is from the NYC Taxi and Limousine Commission. The data file included here consists of aggregating the total number of taxi passengers into 30 minute buckets. rogue_agent_key_hold.csv Timing the key holds for several users of a computer where the anomalies represent a change in the user. rogue_agent_key_updown.csv Timing the key strokes for several users of a computer where the anomalies represent a change in the user. realTraffic/ Real time traffic data from the Twin Cities Metro area in Minnesota collected by the Minnesota Department of Transportation. Included metrics include occupancy speed and travel time from specific sensors. realTweets/ A collection of Twitter mentions of large publicly-traded companies such as Google and IBM. The metric value represents the number of mentions for a given ticker symbol every 5 minutes.  Artificial data  artificialNoAnomaly/ Artifically-generated data without any anomalies. artificialWithAnomaly/ Artifically-generated data with varying types of anomalies.  Acknowledgments We encourage you to publish your results on running NAB and share them with us at nab@numenta.org. Please cite the following publication when referring to NAB Lavin Alexander and Ahmad Subutai. ""Evaluating Real-time Anomaly Detection Algorithms – the Numenta Anomaly Benchmark"" Fourteenth International Conference on Machine Learning and Applications December 2015. [PDF]",Other,,"[computer science, information technology]",Other,,,871,7846,9,Dataset and scoring for detecting anomalies in streaming data,Numenta Anomaly Benchmark (NAB),https://www.kaggle.com/boltzmannbrain/nab,Fri Aug 19 2016
245,,Ed King,"[game, cycle, unit, losses, observable-units, observed-losses, production, scouting, vision]","[string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset contains information on player reconnaissance in over 500 professional-level Starcraft games. From the perspective of one player (the Terran) it contains information on how many enemy (Protoss) units the player has observed can observe has seen destroyed etc. along with an overall measure of how much enemy territory the player can see.  Acknowledgements This dataset was downloaded from this webpage. It was the basis for the following paper Hostetler J. Dereszynski E. Dietterich T. and Fern A. (2012). Inferring strategies from limited reconnaissance in real-time strategy games. Proc. 28th Conference on Uncertainty in Artificial Intelligence (UAI 2012) (to appear). The Data Games are divided into 30 second chunks with the first 7 minutes of each game being represented in this dataset. Values of variables at any given time cycle represent their values over the entire chunk that ends at that time. This dataset contains the following fields  game a unique identifier for the game being played cycle the cycle (in game frames which are typically 24 fps) unit the enemy unit that this row gives info for losses how many of this enemy unit were lost during this time chunk? observable-units how many of this enemy unit could the player see during this time chunk? observed-losses how many of this enemy did the player observe being lost during this time chunk? production how many of this enemy unit became observable (i.e. was produced or -- in the case of buildings -- was under construction) during this time chunk? scouting how many of this enemy unit did the player scout during this time chunk? vision what proportion of the total enemy territory could the player observe during this time chunk? -- NOTE that vision appears once per unit; however the vision variable is not linked to any one unit. Its value spans the time chunk and is identical in every row that represents a given time chunk ,CSV,,[video games],CC0,,,218,3906,11,Limited reconnaissance in a real-time strategy game,Starcraft: Scouting The Enemy,https://www.kaggle.com/kinguistics/starcraft-scouting-the-enemy,Mon Nov 07 2016
246,,Zeeshan-ul-hassan Usmani,"[S#, Date, Islamic Date, Blast Day Type, Holiday Type, Time, City, Latitude, Longitude, Province, Location, Location Category, Location Sensitivity, Open/Closed Space, Influencing Event/Event, Target Type, Targeted Sect if any, Killed Min, Killed Max, Injured Min, Injured Max, No. of Suicide Blasts, Explosive Weight (max), Hospital Names, Temperature(C), Temperature(F)]","[numeric, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, string, string, numeric, numeric]",Context Pakistan Suicide Bombing Attacks (1995-2016) Suicide bombing is an operational method in which the very act of the attack is dependent upon the death of the perpetrator. Though only 3% of all terrorist attacks around the world can be classified as suicide bombing attacks these account for 48% of the casualties. Explosions and suicide bombings have become the modus operandi of terrorist organizations throughout the world. The world is full of unwanted explosives brutal bombings accidents and violent conflicts and there is a need to understand the impact of these explosions on one’s surroundings the environment and most importantly on human bodies. From 1980 to 2001 (excluding 9/11/01) the average number of deaths per incident for suicide bombing attacks was 13. This number is far above the average of less than one death per incident across all types of terrorist attacks over the same time period. Suicide bombers unlike any other device or means of destruction can think and therefore detonate the charge at an optimal location with perfect timing to cause maximum carnage and destruction. Suicide bombers are adaptive and can quickly change targets if forced by security risk or the availability of better targets.  Suicide attacks are relatively inexpensive to fund and technologically primitive as IEDs can be readily constructed.  World has seen more than 3600 suicide bombing attacks in over 40 countries since 1982. Suicide Bombing has wreaked havoc in Pakistan in the last decade or so. From only a couple of attacks before 2000 it kept escalating after the US Operation Enduring Freedom in Afghanistan promiscuously killing hundreds of people each year towering as one of the most prominent security threats that every single Pakistani faces today. The conundrum of suicide bombing in Pakistan has obliterated 6982 clean-handed civilians and injured another 17624 in a total of 475 attacks since 1995. More than 94% of these attacks have taken place after year 2006. From 2007 to 2013 the country witnessed a suicide bombing attack on every 6th day that increased to every 4th day in 2013. Counting the dead and the injured each attack victimizes 48 people in Pakistan.  Pakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of suicide bombings in Pakistan. The given database (PakistanSuicideAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count and building up on it by canvassing open source newspapers media reports think tank analyses and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in suicide attacks including the ones who died later in hospitals or homes due to injuries caused or aggravated by these attacks (second and tertiary blast injuries) making it the most authentic source for suicide attacks related data in this region. We will keep releasing the updates every quarter at this page. Content Geography Pakistan Time period 1995-2016  Unit of analysis Attack Dataset The dataset contains detailed information of 475 suicide bombing attacks in Pakistan that killed an estimated 6982 and injured 17624 people.  Variables The dataset contains Serial No Incident Date Islamic Date (based on Islamic lunar calendar) approximate Time Long-Lat City Province Location Location Sensitivity & Type Target type and Sect Open/Close Space (as it will change the impact of blast waves due to reflection) min and max number of people killed and injured number of suicide bombers amount of explosive being used and the name of hospitals where victims went for treatment. Sources Unclassified media articles hospital reports think tank analysis and reports and government official press releases. Acknowledgements & References Pakistan Body Count has been leveraged extensively in scholarly publications reports media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani.  Users are allowed to use copy distribute and cite the dataset as follows “Zeeshan-ul-hassan Usmani Pakistan Body Count Pakistan Suicide Bombing Attacks Dataset Kaggle Dataset Repository Jan 25 2017.” Past Work  Zeeshan-ul-hassan Usmani and Daniel Kirk “Simulation of Suicide Bombing – Using Computers to Save Lives” I-Universe New York NY April 2011 Zeeshan-ul-hassan Usmani and Daniel Kirk “Modeling and Simulation of Explosion Effectiveness as a Function of Blast and Crowd Characteristics” The Journal of Defense Modeling and Simulation Applications Methodology Technology Sage Publications with Society of Simulation Vol. 6 No. 2 pp. 79-95 Vista CA USA October 2009 Muhammad Irfan and Zeeshan-ul-hassan Usmani “Suicide Terrorism and its New Target –Pakistan” in Wars Insurgencies and Terrorist Attacks A Psychosocial Perspective from The Muslim World by Unaiza Niaz Oxford University Press Canada July 2010  Sana Rasheed Data Science for Suicide Bombings I-Universe New York NY December 2016  Zeeshan-ul-hassan Usmani and Sana Rasheed “Terrorism What Data Sciences Can Do?” 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD 2014 Data Framework Track) at Bloomberg  New York NY USA (August 24-27 2014) Zeeshan-ul-hassan Usmani “Suicide Bombing Forecaster – Novel Techniques to Predict Patterns of Suicide Bombing in Pakistan” 2012 Conference on Homeland Security part of 2012 Autumn Simulation Multi-Conference San Diego CA USA October 28 – 31 2012 Zeeshan-ul-hassan Usmani “BlastSim – Simulation to Save Lives” IEEE/SIC Winter Simulation Conference PhD Colloquium Austin Texas December 13-16 2009 Zeeshan-ul-hassan Usmani Fawzi Alghamdi and Daniel Kirk “BlastSim – Multi-agent Simulation of Suicide Bombing“ IEEE Symposium Computational Intelligence for Security and Defense Applications (CISDA) Ottawa Canada July 8-10 2009 Zeeshan-ul-hassan Usmani Eyosias Imana and Daniel Kirk “Virtual Iraq – Simulation of Insurgent Attacks” IEEE Workshop on Computational Intelligence in Virtual Environments (CIVE) March 30-April 2 2009  Zeeshan-ul-hassan Usmani Eyosias Imana and Daniel Kirk “Random Walk in Extreme Conditions – An Agent Based Simulation of Suicide Bombing” IEEE Symposium on Intelligent Agents March 2009 Zeeshan-ul-hassan Usmani Eyosias Imana and Daniel Kirk “Escaping Death – Geometrical Recommendations for High Value Targets” IEEE International Joint Conferences on Computer Information and Systems Sciences and Engineering (CIS2E 08) Bridgeport CT December 5–13 2008 Zeeshan-ul-hassan Usmani and Daniel Kirk “Extreme Conditions for Intelligent Agents” IEEE 2008 WI-IAT Doctoral Workshop Sydney Australia December 9-12 2008  Zeeshan-ul-hassan Usmani Andrew English & Richard Griffith “The Effects of a Suicide Bombing Crowd Formations” Inter-service/Industry Training Simulation and Education Conference (I/ITSEC) Orlando FL Nov 26-29 2007  Inspiration Some ideas worth exploring  How many people got killed and injured per year? Visualize suicide attacks on timeline Find out any correlation with number of suicide bombing attacks with drone attacks Find out any correlation with suicide bombing attacks with influencing events given in the dataset Can we predict the next suicide bombing attack? Find the correlation between blast/explosive weight and number of people killed and injured Find the impact of holiday type on number of blast victims Find the correlation between Islamic date and blast day/time/size/number of victims Find the Top 10 locations of blasts Find the names of hospitals sorted by number of victims   Questions? For detailed visit www.PakistanBodyCount.org  Or contact Pakistan Body Count staff at info@pakistanbodycount.org ,CSV,,[crime],CC0,,,944,11949,0.220703125,Most Authentic Count of Suicide Bombing Attacks in Pakistan (1995-2016),Pakistan Suicide Bombing Attacks,https://www.kaggle.com/zusmani/pakistansuicideattacks,Fri Dec 01 2017
247,,Ed King,"[Line, Speaker, Text, Date, Party, Location, URL]","[numeric, string, string, dateTime, string, string, string]","Overview Only two major-party candidates are competing in the 2016 US presidential election but there was tough competition to get to the general election. This dataset contains transcripts of every Democratic Republican and Republican Undercard debate held during the 2016 primary season. This dataset is meant to be a complement to Megan Risdal's transcripts of the 2016 US Presidential (General Election) Debates.  So you can now take all of the questions (who talks the most? who has a wider vocabulary?) that you answered in the general election debates and apply the same procedures to see how your favorite (or least favorite) candidate has changed over time. The column names (and order) in this dataset are a superset of those found in the general election dataset and non-speech annotations (such as ""(applause)"") in this dataset are also a superset. Kernels uploaded for the general election dataset should be compatible with this dataset as well; please let me know if you have any compatibility issues. What in the world is an ""Undercard"" debate? The field of Republicans running for President in the primaries was (yuuuuge!) pretty big 17 candidates threw their hat in the ring at one point or another. Debate organizers realized that having 17 people on stage (each with a set amount of time to answer a question / respond to a criticism / interrupt each other) would in the best case lead to a three-to-four-hour-long debate (and in the worst case lead to a chaotic shout-fest as candidates tried to talk over each other for three to four hours). To alleviate this issue many of the Republican debate nights were split into two separate debates the main debate with the top party contenders aired live during primetime; and the Undercard debate which typically aired a few hours earlier than the main debate. The criteria for a candidate to be allowed into the main debate (rather than the ""kids' table"" debate as some pundits derisively called the Undercard event) varied a bit by event. Typically a poll showing of 1% in one of several specified polls was sufficient to gain admission to the Undercard. To get into the main debate candidates had to either (a) be polling above a different higher percentage in specific polls or (b) be among the top n Republican candidates in said polls. The details get a little thorny (certain debates had multiple criteria of which candidates had to meet at least one) so I refer questions to the individual Undercard debate pages at the American Presidency Project for detailed criteria. In this dataset the split between Republican debates is made in the Party column Republican is used for the primetime main events and Republican Undercard is used for the Undercard. Acknowledgements All transcripts were scraped from the presidential debates page of the American Presidency Project at the University of California Santa Barbara. Individual lines in the dataset contain links to the particular page for that debate. Updates v2 contains a few minor changes related to normalizing parenthesized elements (non-speech) within the Text field and adding a few lines of interruptions that persisted in the Text field The Data The fields are described more fully in the file description. This section describes the particular elements that can appear in the Speaker and Text fields. Who's Who? (aka the Speaker column) The primary debates had a ton of participants. This dataset contains utterances from 22 politicians and 49 moderators not to mention the occasional audience laughter or 2-minute timer.  Almost all Speaker columns contain either a single title-case name (listed below in the Participants and Moderators subsections) or a single upper-case word indicating non-speaker noise (listed below in the None-speaker Turns subsection); the exceptions to this are cases where a name is concatenated with a space and a parenthesized tag as follows  spkr (VIDEO) transcriptions of pre-recorded material of any of the candidates or moderators spkr (TRANSLATED) in the Univision/Telemundo debate some questions and answers are translated into English from the original Spanish; the transcript reflects the translations as spoken by a translator  Non-speaker Turns The non-speaker turns in the Speaker column are  AUDIENCE any laughter booing applause etc. from the audience CANDIDATES cross-talk between candidates OTHER non-speaker non-audience noise (such as commercial break timer bell national anthem etc.) QUESTION a question from an audience member (or a prerecorded question) UNKNOWN cases where the transcriber could hear a phrase but could not determine who said it  Here are the various speakers who appear in the dataset Candidates Democratic  Chafee Former Governor Lincoln Chafee (RI) Clinton Former Secretary of State Hillary Clinton O'Malley Former Governor Martin O'Malley (MD) Sanders Senator Bernie Sanders (VT) Webb Former Senator Jim Webb (VA)  Republican  Bush Former Governor Jeb Bush (FL) Carson Ben Carson Cruz Senator Ted Cruz (TX) Kasich Governor John Kasich (OH) Paul Senator Rand Paul (KY) Rubio Senator Marco Rubio (FL) Trump Donald Trump Walker Governor Scott Walker (WI)  Republican (Undercard ONLY)  Gilmore Former Governor Jim Gilmore (VA) Graham Senator Lindsey Graham (SC) Jindal Governor Bobby Jindal (LA) Pataki Former Governor George Pataki (NY) Perry Former Governor Rick Perry (TX) Santorum Former Senator Rick Santorum (PA)  Republican (Main AND Undercard)  Christie Governor Chris Christie (NJ) Fiorina Carly Fiorina Huckabee Former Governor Mike Huckabee (AR)  All candidates in a Python list for easy copy/paste ['Bush' 'Carson' 'Chafee' 'Christie' 'Clinton' 'Cruz' 'Fiorina' 'Gilmore' 'Graham' 'Huckabee' 'Jindal' 'Kasich' ""O'Malley"" 'Pataki' 'Paul' 'Perry' 'Rubio' 'Sanders' 'Santorum' 'Trump' 'Walker' 'Webb'] Moderators NOTE Some moderators are seen across various debates; in particular the Republican main debates and undercard debates on a given day tend to share the same moderators. Some moderators are public figures who are seen only in videos (with the (VIDEO) tag). Moderators  Arrarás María Celeste Arrarás (Telemundo) Baier Bret Baier (Fox News) Baker Gerard Baker (The Wall Street Journal) Bartiromo Maria Bartiromo (Fox Business Network) Bash Dana Bash (CNN) Blitzer Wolf Blitzer (CNN) Cavuto Neil Cavuto (Fox Business Network) Cooney Kevin Cooney (CBS News) Cooper Anderson Cooper (CNN) Cordes Nancy Cordes (CBS News) Cramer Jim Cramer (CNBC) Cuomo Governor Andrew Cuomo (NY) Dickerson John Dickerson (CBS News) Dinan Stephen Dinan (Washington Times) Epperson Sharon Epperson (CNBC) Garrett Major Garrett (CBS News) Ham Mary Katharine Ham (Hot Air) Hannity Sean Hannity (Fox News) Harwood John Harwood (CNBC) Hemmer Bill Hemmer (Fox News) Hewitt Hugh Hewitt (Salem Radio Network) Holt Lester Holt (NBC News) Ifill Gwen Ifill (PBS) Kelly Megyn Kelly (Fox News) Lemon Don Lemon (CNN) Levesque Neil Levesque (New Hampshire Institute of Politics) Lopez Juan Carlos Lopez (CNN en Espanol) Louis Errol Louis (NY1) MacCallum Martha MacCallum (Fox News) Maddow Rachel Maddow (MSNBC) Mcelveen Josh McElveen (WMUR-TV) Mitchell Andrea Mitchell (NBC News) Muir David Muir (ABC News) O'Reilly Bill O'Reilly (Fox News) Obradovich Kathie Obradovich (The Des Moines Register) Quick Becky Quick (CNBC) Quintanilla Carl Quintanilla (CNBC) Raddatz Martha Raddatz (ABC News) Ramos Jorge Ramos (Univision) Regan Trish Regan (Fox Business Network) Salinas María Elena Salinas (Univision) Santelli Rick Santelli (CNBC) Seib Gerald Seib (The Wall Street Journal) Strassel Kimberly Strassel (The Wall Street Journal) Tapper Jake Tapper (CNN) Todd Chuck Todd (MSNBC) Tumulty Karen Tumulty (The Washington Post) Wallace Chris Wallace (Fox News) Woodruff Judy Woodruff (PBS)  All moderators in a Python list for easy copy/paste ['Arrarás' 'Baier' 'Baker' 'Bartiromo' 'Bash' 'Blitzer' 'Cavuto' 'Cooney' 'Cooper' 'Cordes' 'Cramer' 'Cuomo' 'Dickerson' 'Dinan' 'Epperson' 'Garrett' 'Ham' 'Hannity' 'Harwood' 'Hemmer' 'Hewitt' 'Holt' 'Ifill' 'Kelly' 'Lemon' 'Levesque' 'Lopez' 'Louis' 'MacCallum' 'Maddow' 'Mcelveen' 'Mitchell' 'Muir' ""O'Reilly"" 'Obradovich' 'Quick' 'Quintanilla' 'Raddatz' 'Ramos' 'Regan' 'Salinas' 'Santelli' 'Seib' 'Strassel' 'Tapper' 'Todd' 'Tumulty' 'Wallace' 'Woodruff'] What's What? (aka the Text column) In general the Text column contains fully punctuated and appropriately capitalized speech transcriptions. Most parenthesized elements are non-speech elements with the following exceptions  (c) and (4) are spoken in reference to 501(c)(4)s (tax-exempt lobbying groups) (k) spoken in reference to 401(k)s (individual pension accounts)  The non-speech elements that can be found in parentheses in the Text column are  (ANTHEM) the national anthem is played (APPLAUSE) the audience expresses approval (BELL) a bell or buzzer indicating that a candidate's time has expired (BOOING) the audience expresses disapproval (COMMERCIAL) the televised debate breaks for a commercial advertisement (CROSSTALK) more than one candidate or moderator are speaking at the same time (LAUGHTER) the audience expresses a sense of humor (MOMENT.OF.SILENCE) the debate pauses for a moment of silence (SPANISH) the utterance is in Spanish (for the Democrats' Univision-hosted debate on 3/9/16 in Miami) (VIDEO.END) a video clip ends (VIDEO.START) a video clip begins (inaudible) the utterance was inaudible off-mike or too indecipherable to transcribe ",CSV,,[politics],CC0,,,592,6565,4,Full transcripts of the debates among all the 2016 contenders for US President,2016 US Presidential Primary Debates,https://www.kaggle.com/kinguistics/2016-us-presidential-primary-debates,Wed Oct 19 2016
248,,Institute for Computing Education at Georgia Tech,"[state, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, Pop]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The datasets contain all the data for the number of CS AP A exam taken in each state from 1998 to 2013 and detailed data on pass rates race and gender from 2006-2013. The data was complied from the data available at http//research.collegeboard.org/programs/ap/data. This data was originally gathered by the CSTA board but Barb Ericson of Georgia Tech keeps adding to it each year. Content historical.csv contains data for the number of CS AP A exam taken in each state from 1998 to 2013  state US states 1998-2013 Pop population  pass_06_13.csv contains exam pass rates race and gender data from 2006 to 2013 for selected states. pass_12_13.csv contains exam pass rates race and gender information for every state for 2012 and 2013.  Acknowledgements The original datasets can be found here and here. Inspiration Using the datasets can you examine the temporal trends in the exam pass rates by race gender and geographical location?,CSV,,"[education, computer science]",Other,,,259,3853,0.029296875,AP CS A Exam Pass Rates Across States,AP Computer Science A Exam Dataset,https://www.kaggle.com/iceatgt/ap-computer-science-a-exam-dataset,Sun Nov 13 2016
249,,Hossein Banki Koshki,"[feature1, feature2, feature3, feature4, feature5, feature6, feature7, feature8, feature9, feature10, feature11, feature12, feature13, feature14, feature15, feature16, feature17, feature18, feature19, feature20, feature21, feature22, feature23, feature24, feature25, feature26, feature27, feature28, label]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset includes SP1 transcription factor binding and non-binding sites on human chromosome1. It can be used for binary classification tasks in bioinformatics. There are 1200 sequences for binding sites (BS) and 1200 sequences for non-biding sites (nBS)  We have labeled sequences with 1 for BS and 0 for nBS. Each sequence is 14 nucleobase length which is converted to numeric string using codes below assigned to each nucleobase 00 for A 01 for T 10 for C 11 for G,CSV,,[human genetics],Other,,,267,2901,0.2060546875,SP1 factor binding and non-binding sites on Ch1 for classification tasks,SP1 factor binding sites on Chromosome1,https://www.kaggle.com/hobako1993/sp1-factor-binding-sites-on-chromosome1,Sat Nov 12 2016
250,,Kaggle,[],[],Throughout 2015 Hillary Clinton has been embroiled in controversy over the use of personal email accounts on non-government servers during her time as the United States Secretary of State. Some political experts and opponents maintain that Clinton's use of personal email accounts to conduct Secretary of State affairs is in violation of protocols and federal laws that ensure appropriate recordkeeping of government activity. Hillary's campaign has provided their own four sentence summary of her email use here.  There have been a number of Freedom of Information lawsuits filed over the State Department's failure to fully release the emails sent and received on Clinton's private accounts. On Monday August 31 the State Department released nearly 7000 pages of Clinton's heavily redacted emails (its biggest release of emails to date).  The documents were released by the State Department as PDFs. We've cleaned and normalized the released documents and are hosting them for public analysis. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent.  Here's the code that creates this data release.,CSV,,"[politics, telecommunications]",CC0,,,11480,115379,51,Uncover the political landscape in Hillary Clinton's emails,Hillary Clinton's Emails,https://www.kaggle.com/kaggle/hillary-clinton-emails,Thu Oct 06 2016
251,,Myles O'Neill,[],[],"Overview Game of Thrones is a hit fantasy tv show based on the equally famous book series ""A Song of Fire and Ice"" by George RR Martin. The show is well known for its vastly complicated political landscape large number of characters and its frequent character deaths. Data Sources This dataset combines three sources of data all of which are based on information from the book series.  Firstly there is battles.csv which contains Chris Albon's ""The War of the Five Kings"" Dataset which can be found here https//github.com/chrisalbon/war_of_the_five_kings_dataset . Its a great collection of all of the battles in the series. Secondly we have character-deaths.csv from Erin Pierce and Ben Kahle. This dataset was created as a part of their Bayesian Survival Analysis which can be found here http//allendowney.blogspot.com/2015/03/bayesian-survival-analysis-for-game-of.html Finally we have a more comprehensive character dataset with character-predictions.csv. This comes from the team at A Song of Ice and Data who scraped it from  http//awoiaf.westeros.org/ . It also includes their predictions on which character will die the methodology of which can be found here https//got.show/machine-learning-algorithm-predicts-death-game-of-thrones  What insights about the complicated political landscape of this fantasy world can you find in this data? Of course it goes without saying that this dataset contains spoilers ;)",CSV,,"[literature, social groups, war]",CC0,,,13552,114123,0.2509765625,Explore deaths and battles from this fantasy world,Game of Thrones,https://www.kaggle.com/mylesoneill/game-of-thrones,Fri May 20 2016
252,,William Cukierski,[],[],"Kaggle’s March Machine Learning Mania competition challenged data scientists to predict winners and losers of the men's 2016 NCAA basketball tournament. This dataset contains the 1070 selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament. How can this data be used? You can pivot it to look at both Kaggle and NCAA teams alike. You can look at who will win games which games will be close which games are hardest to forecast or which Kaggle teams are gambling vs. sticking to the data.  The NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games usually called the “play-in round” before the traditional bracket action starts. Due to competition timing these games are included in the prediction files but should not be used in analysis as it’s possible that the prediction was submitted after the play-in round games were over. Data Description Each Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the 'predictions' folder and named according to  TeamName_TeamId_SubmissionId.csv  The file format contains a probability prediction for every possible game between the 68 teams. This is necessary to cover every possible tournament outcome. Each team has a unique numerical Id (given in Teams.csv). Each game has a unique Id column created by concatenating the year and the two team Ids. The format is the following  IdPred    2016_1112_11140.6    2016_1112_11220    ...    The team with the lower numerical Id is always listed first. “Pred” represents the probability that the team with the lower Id beats the team with the higher Id. For example ""2016_1112_11140.6"" indicates team 1112 has a 0.6 probability of beating team 1114. For convenience we have included the data files from the 2016 March Mania competition dataset in the Scripts environment (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups see the documentation). However the focus of this dataset is on Kagglers' predictions.",CSV,,"[basketball, artificial intelligence]",CC4,,,1485,12501,27,Forecasting the 2016 NCAA Basketball Tournament,2016 March ML Mania Predictions,https://www.kaggle.com/wcukierski/2016-march-ml-mania,Thu Nov 16 2017
253,,Kaggle,"[Id, Name, Title, OrderId]","[numeric, string, string, numeric]",We aren't saying this dataset is the Rosetta Stone of machine learning competitions but we do think there is a lot to learn from (and a lot of fun to be had by) releasing some of our most interesting tables on Kaggle community and competition activity. Strategizing to become a Master? Wondering who where and what goes in to a winning team? Deciding between evaluation metrics for your next data science project? We hope the scripts published here will enrich and entertain Kagglers spark some lively conversations and act as a resource for the larger machine learning community.  This data (available through Kaggle Scripts as CSV files and a SQLite database) contains the tables listed below. Note that this data is not a complete dump rows columns and tables have been filtered out and it is a small subset of the data that we can release publicly. Over time we'll add more of the tables that we can release publicly to it. ,CSV,,"[statistics, telecommunications]",CC4,,,3441,45518,2048,"Kaggle's public data on competitions, users, submission scores, and kernels",Meta Kaggle,https://www.kaggle.com/kaggle/meta-kaggle,Thu Jul 21 2016
254,,MANOJKUMAR PARMAR,"[gameNo, team, oppTeam, matchStage, tossResult, alloutRec, alloutGiv, sTackleRec, sTackleGiv, touchPntsRec, touchPntsGiv, bonusPntsRec, bonusPntsGiv, raidPntsRec, raidPntsGiv, tacklePntsRec, tacklePntsGiv, alloutPntsRec, alloutPntsGiv, extraPntsRec, extraPntsGiv, totalPntsRec, totalPntsGiv, touchPntsDiff, bonusPntsDiff, raidPntsDiff, tacklePntsDiff, alloutPntsDiff, extraPntsDiff, totalPntsDiff, matchResult]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Kabaddi is a contact sport that originated in ancient India. more information The standard style Kabaddi World Cup is an indoor international kabaddi competition conducted by the International Kabaddi Federation (IKF)contested by men's and women's national teams. The competition has been previously contested in 2004 2007 and 2016. All the tournaments have been won by India. more information The 2016 Kabaddi World Cup the third standard-style Kabaddi World Cup was an international kabaddi tournament governed by the International Kabaddi Federation contested from 7 to 22 October 2016 in Ahmedabad India. Twelve countries had competed in the tournament. more information 30 league matches played between teams. teams were deivided in 2 pools with 6 team in each pool. Top 2 teams from each team were qualifid for semifinals and winner of semifianls played in finals. This dataset contains data for all 33 matches at granualirity level of attack defense allout and extra points. Data set also includes toss results super tackle count and all out count along with match results. Content This dataset was manually prepared from taking necessary statistics from Kabaddi world cup site. Points acquired as per rules are main statistics.   This dataset contains necessary statistics  in today format and details of all variables are as per following.  gameNo  Match number. Sequential {Integer} team  Team name {Factor} oppTeam  Opposition team name {Factor} matchStage  Tournament stage at which match was played. (0 - League 1 - SemiFinal 2 - Final ) {Factor} tossResult  Results of toss to select either side or raid (0 - Loss 1 - Win) {Factor} alloutRec  No. of time team was all out yielding 2 point {Integer} alloutGiv  No. of time opposition team was all out yielding 2 point {Integer} sTackleRec  No. of times super tackle by team yielding 2 point {Integer} sTackleGiv  No. of times super tackle by opposition team yielding 2 point {Integer} touchPntsRec  No. of times player in raid touched opposition team player yiedling 1 point for every touch {Integer} touchPntsGiv  No. of times opposition player in raid touched team player yiedling 1 point for every touch {Integer} bonusPntsRec  No. of times player in raid crossed bonus line yiedling 1 point for every raid {Integer} bonusPntsGiv  No. of times opposition player in raid crossed bonus line yiedling 1 point for every raid {Integer} raidPntsRec  No. of total raid (attack) points by team sum of touch points and bonus points {Integer} raidPntsGiv  No. of total raid (attack) points by opposition team sum of touch points and bonus points {Integer} tacklePntsRec  No. of tackle (defense) points received by team yielding 1 point for normal tackle and 2 points for super tackle {Integer} tacklePntsGiv  No. of tackle (defense) points received by opposition team yielding 1 point for normal tackle and 2 points for super tackle {Integer} alloutPntsRec  No. of all out points received by team yielding 2 points per allout  {Integer} alloutPntsGiv  No. of all out points received by opposition team yielding 2 points per allout {Integer} extraPntsRec  No. of extra (technical penalty) points received by team {Integer} extraPntsGiv  No. of extra (technical penalty) points received by opposition team  {Integer} totalPntsRec  No. of total points received by team sum of raid points tackle points allout points & extra points {Integer} totalPntsGiv  No. of total points received by opposition team sum of raid points tackle points allout points & extra points {Integer} touchPntsDiff  No. of touch points difference from opposition team {Integer} bonusPntsDiff  No. of bonus points difference from opposition team {Integer} raidPntsDiff  No. of raid points difference from opposition team {Integer} tacklePntsDiff  No. of tackle points difference from opposition team {Integer} alloutPntsDiff  No. of allout points difference from opposition team {Integer} extraPntsDiff  No. of extra points difference from opposition team {Integer} totalPntsDiff  No. of total points difference from opposition team {Integer} matchResults  Results of the match (0 - Loss 1 - Win) {Factor}  Acknowledgements I would like to thank Kabaddi World Cup site for providing this data. Inspiration This dataset was prepared for my research paper which aims to answer following questions  Is attack is better than defence? Does bonus point lead to victory? What is the role of all out points on determining strength of attack? Can we build predictive model for winning? How strong establish teams are compared to new teams? ,CSV,,[sports],CC4,,,156,3189,0.005859375,"2016 Kabaddi World Cup, the third standard-style Kabaddi World Cup dataset",Kabaddi World Cup 2016,https://www.kaggle.com/parmarmanojkumar/kabaddi-world-cup-2016,Mon Oct 02 2017
255,,Monika Munjal,"[Bug ID, Product, Component, Assignee, Status, Resolution, Summary, Changed, Assignee Real Name, Classification, Flags, Hardware, Keywords, Number of Comments, Opened, OS, Priority, QA Contact, QA Contact Real Name, Reporter, Reporter Real Name, Severity, Summary, Tags, Target Milestone, URL, Version, Votes, Whiteboard, Alias]","[numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric]",Bug triage.,CSV,,[programming],Other,,,377,4992,3,"Bug triaging...contains details bug prioritization or assign, defect analysis.",Bug Triaging,https://www.kaggle.com/monika11/bug-triagingbug-assignment,Wed Sep 28 2016
256,,Myles O'Neill,"[world_rank, institution, country, national_rank, quality_of_education, alumni_employment, quality_of_faculty, publications, influence, citations, broad_impact, patents, score, year]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",Of all the universities in the world which are the best? Ranking universities is a difficult political and controversial practice. There are hundreds of different national and international university ranking systems many of which disagree with each other. This dataset contains three global university rankings from very different places. University Ranking Data The Times Higher Education World University Ranking is widely regarded as one of the most influential and widely observed university measures. Founded in the United Kingdom in 2010 it has been criticized for its commercialization and for undermining non-English-instructing institutions. The Academic Ranking of World Universities also known as the Shanghai Ranking is an equally influential ranking. It was founded in China in 2003 and has been criticized for focusing on raw research power and for undermining humanities and quality of instruction. The Center for World University Rankings is a less well know listing that comes from Saudi Arabia it was founded in 2012.  How do these rankings compare to each other? Are the various criticisms levied against these rankings fair or not? How does your alma mater fare against the world?  Supplementary Data To further extend your analyses we've also included two sets of supplementary data.  The first of these is a set of data on educational attainment around the world. It comes from The World Data Bank and comprises information from the UNESCO Institute for Statistics and the Barro-Lee Dataset. How does national educational attainment relate to the quality of each nation's universities? The second supplementary dataset contains information about public and private direct expenditure on education across nations. This data comes from the National Center for Education Statistics. It represents expenditure as a percentage of gross domestic product. Does spending more on education lead to better international university rankings?,CSV,,[universities and colleges],Other,,,20237,105657,11,Investigate the best universities in the world,World University Rankings,https://www.kaggle.com/mylesoneill/world-university-rankings,Wed Sep 28 2016
257,,Anthony Goldbloom,[],[],This data set will contain the results from all the 2016 kitefoil races. This allows analysis to be done including the calculation of world rankings.,Other,,"[oceans, sports]",Other,,,166,6462,0.3740234375,This data set will contain the results from all the 2016-2017 kitefoil races.,2016 and 2017 Kitefoil Race Results,https://www.kaggle.com/antgoldbloom/2016-kitefoil-race-results,Mon Oct 09 2017
258,,Lumin,"[gameNum, player, points, me, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, settlement1, , , , , , settlement2, , , , , , production, tradeGain, robberCardsGain, totalGain, tradeLoss, robberCardsLoss, tribute, totalLoss, totalAvailable]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The strategic board game The Settlers of Catan is a modern classic. Introduced in 1995 it has sold over 22 million copies worldwide. Learning how to play the game well requires an inherent understanding of probability economics game theory and social interactions.  This is my personal dataset of 50 4-player games I played on playcatan.com in 2014. Using the ingame statistics page and a spreadsheet I logged starting position choices the distribution of dice rolls and how each player spent the resources they acquired by the end of the game. Note of course because this dataset only consists of my games any analysis done is most relevant for games involving me... My personal analysis of this dataset consisted of a best subsets regression and resulted a 4-variable model that likely overfitted but managed to ascertain the winner correctly in 40 of 50 games. Questions to possibly consider How much luck is involved in winning a Catan game? Does starting position matter? If so what starting settlements lead to success from each position? How much information on the eventual winner can be gained from starting position/settlements alone? By looking at postgame stats what leads to a win? Can these statistics be a guide for ingame strategy? Data details/guide gameNum - each game I played has 4 corresponding rows 1 per player. player - the starting position corresponding to each row points - how many points the player ended the game with (the game is won with 10 or more) me - the position I played during the game 2 3 ... 12 - how many rolls of each value occurred during the game (game is played with 2 dice) settlement1 settlement2 - each starting settlement is logged as 3 pairs of [number resource] L = lumber C = clay S = sheep W = wheat O = ore 3G = 31 general port 2(X) = 21 port for resource X D = desert EX in game 1 player 1's first settlement was on a 6-lumber 3-clay and 11-clay. production - total cards gained from settlements and cities during game tradeGain - total cards gained from peer AND bank trades during game robberCardsGain - total cards gained from stealing with the robber plus cards gained with non-knight development  cards. A road building card is +4 resources. totalGain - sum of previous 3 columns. tradeLoss - total cards lost from peer AND bank trades during game robberCardsLoss - total cards lost from robbers knights and other players' monopoly cards tribute - total cards lost when player had to discard on a 7 roll (separate from previous column.) totalLoss - sum of previous 3 columns. totalAvailable - totalGain minus totalLoss. I only ask that if you produce a good model you share it with me! Please don't hesitate to ask any clarifying questions.,CSV,,[board games],CC4,,,555,9059,0.015625,"Starting positions, roll distributions, postgame statistics of 50 4-player games",My Settlers of Catan Games,https://www.kaggle.com/lumins/settlers-of-catan-games,Tue Aug 30 2016
259,,gunner38,"[UID, ID, Tipster, Date, Track, Horse, Bet Type, Odds, Result, TipsterActive]","[numeric, numeric, string, string, string, string, string, numeric, string, string]",Horse Racing - A different and profitable approach The traditional approach in attempting to make a profit from horse-racing using machine learning techniques is to use systems involving dozens and dozens of variables. These systems include the following types of variables Horse - Name Sex Age Pedigree Weight Speed over various distances race data with finishing times and positions - etc. Trainer info. Jockey info. Track info - Track track conditions - etc. And a whole lot more. Finding compiling maintaining and updating this data is a massive task for the individual. Unless you have access to a database of such data - where would you even start? We have a different approach. We collect maintain and use data from various 'Tipsters'. The tipsters use their skill to study the horses and make a prediction - that they think a particular horse will win a particular race. We take those tipsters predictions and put them through a machine learning algorithm (microsoft azure) asking it to predict a 'win' or 'lose' based upon the tipsters performance history. We have a database of approx. 39000 bets using 31 odd tipsters. Fifteen tipsters are active and sixteen tipsters are inactive The betting history for the inactive tipsters is used in the dataset as it appears to add 'weight' to the system when considering active tips. We have been using this system live for three months now and although it has it's ups and downs - it makes money! One bookmaker has already closed our account. We are looking to further optimize the system to reach it's maximum efficiency coupled with a betting strategy  to increase profitability. We ask for your help. If you can produce an 'Azure' system more efficient than ours - then further information will be shared with you. Questions  Are bets from inactive tipsters critical to performance? Is it better to have all the tipsters 'stacked on top of each other' in one large dataset or is the system better served by separating them out?  Predicting Bets When we ask the system to predict if a bet will Win or Lose for say Tipster A - we take the last ID number for that Tipster and add one to it - making it a new ID - outside the systems experience. That ID is used for all that Tipsters bets until the system is updated. The system is updated once a week. Good hunting. Gunner38,CSV,,[horse racing],Other,,,1061,16648,3,Thirty nine thousand bets from thirty one tipsters,Horse Racing - Tipster Bets,https://www.kaggle.com/gunner38/horseracing,Wed Sep 14 2016
260,,Kaggle,"[title, link, publication_date, content]","[string, string, string, string]",In 2010 Kaggle launched its first competition which was won by Jure Zbontar who used a simple linear model. Since then a lot has changed. We've seen the rebirth of neural networks the rise of Python the creation of powerful libraries like XGBoost Keras and Tensorflow.  This is data set is a dump of all winners' posts from the Kaggle blog starting with Jure Zbontar. It allows us to track trends in the techniques tools and libraries that win competitions.  This is a simple dump. If there's demand I can upload more detail (including comments and tags).,CSV,,[artificial intelligence],CC0,,,294,7865,2,Examine trends in machine learning by analyzing winners' posts on No Free Hunch,Kaggle Blog: Winners' Posts,https://www.kaggle.com/kaggle/kaggle-blog-winners-posts,Wed Sep 21 2016
261,,Ben Hamner,[],[],Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.   This year Kaggle is hosting the NIPS 2015 paper dataset to facilitate and showcase exploratory analytics on the NIPS data. We've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. Here's a quick script that gives an overview of what's included in the data. We encourage you to explore this data and share what you find through Kaggle Scripts! Data Description Overview of the data in Kaggle Scripts. nips-2015-papers-release-*.zip (downloadable from the link above) contains the below files/folders. All this data's available through Kaggle Scripts as well and you can create a new script to immediately start exploring the data in R Python Julia or SQLite. This dataset is available in two formats three CSV files and a single SQLite database (consisting of three tables with content identical to the CSV files). You can see the code used to create this dataset on Github. Papers.csv This file contains one row for each of the 403 NIPS papers from this year's conference. It includes the following fields  Id - unique identifier for the paper (equivalent to the one in NIPS's system) Title - title of the paper EventType - whether it's a poster oral or spotlight presentation PdfName - filename for the PDF document Abstract - text for the abstract (scraped from the NIPS website) PaperText - raw text from the PDF document (created using the tool pdftotext)  Authors.csv This file contains id's and names for each of the authors on this year's NIPS papers.  Id - unique identifier for the author (equivalent to the one in NIPS's system) Name - author's name  PaperAuthors.csv This file links papers to their corresponding authors.  Id - unique identifier PaperId - id for the paper AuthorId - id for the author  database.sqlite This SQLite database contains the tables with equivalent data and formatting as the Papers.csv Authors.csv and PaperAuthors.csv files. pdfs This folder contains the raw pdf files for each of the papers.,CSV,,"[writing, linguistics, artificial intelligence]",ODbL,,,2328,45374,28,Explore and analyze this year's NIPS papers,NIPS 2015 Papers,https://www.kaggle.com/benhamner/nips-2015-papers,Tue May 02 2017
262,,US Census Bureau,[],[],"The American Community Survey is an ongoing survey from the US Census Bureau. In this survey approximately 3.5 million households per year are asked detailed questions about who they are and how they live. Many topics are covered including ancestry education work transportation internet use and residency. The responses reveal a fascinating granular snapshot into the lives of many Americans.  We''re publishing this data on scripts to make it easy for you to explore this rich dataset share your work and collaborate with other data scientists. No data download or local environment needed! We''ve also added shapefiles to simplify publishing maps. What surprising insights can you find in this data? We look forward to seeing and sharing what you discover on scripts! Data Description Here''s a data dictionary. There are two types of survey data provided housing and population. For the housing data each row is a housing unit and the characteristics are properties like rented vs. owned age of home etc. For the population data each row is a person and the characteristics are properties like age gender whether they work method/length of commute etc. Each data set is divided in two pieces ""a"" and ""b"" (where ""a"" contains states 1 to 25 and ""b"" contains states 26 to 50). Both data sets have weights associated with them. Weights are included to account for the fact that individuals are not sampled with equal probably (people who have a greater chance of being sampled have a lower weight to reflect this).  Weight variable for the housing data WGTP Weight variable for the population data PWGTP  In Kaggle Scripts these files can be accessed at  ../input/pums/ss13husa.csv (housing a) ../input/pums/ss13husb.csv (housing b) ../input/pums/ss13pusa.csv (population a) ../input/pums/ss13pusb.csv (population b)  You can download the data from the census website  housing population  In scripts they are accessed at  ../input/shapefiles/pums/tl_2013_[state]_puma10.[extension].  The shapefiles can also be downloaded here.  DataCamp and Kaggle have teamed up to bring you the basics of Data Exploration With Kaggle Scripts. Take the free interactive course here and start building your data science portfolio.",CSV,,"[social groups, demographics, sociology]",CC0,,,3197,39336,4096,Find insights in the 2013 American Community Survey,2013 American Community Survey,https://www.kaggle.com/census/2013-american-community-survey,Tue May 02 2017
263,,Crowdflower,[],[],"This data originally came from Crowdflower's Data for Everyone library. As the original source says  A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive negative and neutral tweets followed by categorizing negative reasons (such as ""late flight"" or ""rude service"").  The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub For example it contains whether the sentiment of the tweets in this set was positive neutral or negative for six US airlines ",SQLite,,"[linguistics, twitter, internet, aviation]",CC4,,,12794,92292,8,Analyze how travelers in February 2015 expressed their feelings on Twitter,Twitter US Airline Sentiment,https://www.kaggle.com/crowdflower/twitter-airline-sentiment,Thu Oct 06 2016
264,,Crowdflower,[],[],This data originally came from Crowdflower's Data for Everyone library. As the original source says  We looked through tens of thousands of tweets about the early August GOP debate in Ohio and asked contributors to do both sentiment analysis and data categorization. Contributors were asked if the tweet was relevant which candidate was mentioned what subject was mentioned and then what the sentiment was for a given tweet. We've removed the non-relevant messages from the uploaded dataset.  The data we're providing on Kaggle is a slightly reformatted version of the original source. It includes both a CSV file and SQLite database. The code that does these transformations is available on GitHub,SQLite,,"[politics, internet]",CC4,,,4006,29971,8,Analyze tweets on the first 2016 GOP Presidential Debate,First GOP Debate Twitter Sentiment,https://www.kaggle.com/crowdflower/first-gop-debate-twitter-sentiment,Thu Oct 06 2016
265,,Kaggle,"[NAME OF DATA ELEMENT, Year, dev-category, developer-friendly name, VARIABLE NAME, API data type, label, VALUE, LABEL, SCORECARD? Y/N, SOURCE, NOTES]","[string, string, string, string, string, string, string, string, string, string, string, string]","It's no secret that US university students often graduate with debt repayment obligations that far outstrip their employment and income prospects. While it's understood that students from elite colleges tend to earn more than graduates from less prestigious universities the finer relationships between future income and university attendance are quite murky. In an effort to make educational investments less speculative the US Department of Education has matched information from the student financial aid system with federal tax returns to create the College Scorecard dataset. Kaggle is hosting the College Scorecard dataset in order to facilitate shared learning and collaboration. Insights from this dataset can help make the returns on higher education more transparent and in turn more fair. Data Description Here's a script showing an exploratory overview of some of the data. college-scorecard-release-*.zip contains a compressed version of the same data available through Kaggle Scripts. It consists of three components  All the raw data files released in version 1.40 of the college scorecard data Scorecard.csv a single CSV file with all the years data combined. In it we've converted categorical variables represented by integer keys in the original data to their labels and added a Year column database.sqlite a SQLite database containing a single Scorecard table that contains the same information as Scorecard.csv  New to data exploration in R? Take the free interactive DataCamp course ""Data Exploration With Kaggle Scripts"" to learn the basics of visualizing data with ggplot. You'll also create your first Kaggle Scripts along the way.",CSV,,"[education, finance]",CC0,,,9248,56726,4096,Raise the curtain on the true cost of higher education,US Dept of Education: College Scorecard,https://www.kaggle.com/kaggle/college-scorecard,Thu Nov 09 2017
266,,CWILOC,"[RecID, InstAbbr, InstName, InstPlace, InstLand, NumberEntry, NameArchiveSet, ArchivePart, Specification, LogbookIdent, LogbookLanguage, EnteredBy, DASnumber, ImageNumber, VoyageFrom, VoyageTo, ShipName, ShipType, Company, OtherShipInformation, Nationality, Name1, Rank1, Name2, Rank2, Name3, Rank3, ZeroMeridian, StartDay, TimeGen, ObsGen, ReferenceCourse, ReferenceWindDirection, DistUnits, DistToLandmarkUnits, DistTravelledUnits, LongitudeUnits, VoyageIni, UnitsOfMeasurement, Calendar, Year, Month, Day, DayOfTheWeek, PartDay, TimeOB, Watch, Glasses, UTC, CMG, ShipSpeed, Distance, drLatDeg, drLatMin, drLatSec, drLatHem, drLongDeg, drLongMin, drLongSec, drLongHem, LatDeg, LatMin, LatSec, LatHem, LongDeg, LongMin, LongSec, LongHem, Lat3, Lon3, LatInd, LonInd, PosCoastal, EncName, EncNat, EncRem, Anchored, AnchorPlace, LMname1, LMdirection1, LMdistance1, LMname2, LMdirection2, LMdistance2, LMname3, LMdirection3, LMdistance3, EstError, ApplError, WindDirection, AllWindDirections, WindForce, WindForceScale, AllWindForces, WindScale, Weather, ShapeClouds, DirClouds, Clearness, PrecipitationDescriptor]","[numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","In the mid-eighteenth to nineteenth centuries navigating the open ocean was an imprecise and often dangerous feat. In order to calculate their daily progress and avoid running/sailing into the unknown a ship's crew kept a detailed logbook with data on winds waves and any remarkable weather. Handwritten in archived logbooks these rich datasets were nearly impossible to study until the European Union funded their digitization in 2001. You can visit the EU project website for detailed information on the countries and ships included. We're hosting the full 1750-1850 dataset on Kaggle to promote the exploration of this unique and invaluable climatology resource.  Data Description This data comes from the Climatological Database for the World's Oceans 1750-1850 (CLIWOC) version 1.5 data release. The primary data file is CLIWOC15.csv. The columns in this table are described on this page (scroll down to the table that starts with ""Field abbreviation""). It includes 280280 observational records of ship locations weather data and other associated information. The ancillary data files are described on the above site.",CSV,,"[environment, climate, shipping]",CC0,,,2814,23846,19,Explore changing climatology with data from early shipping logs,Ocean Ship Logbooks (1750-1850),https://www.kaggle.com/cwiloc/climate-data-from-ocean-ships,Thu Nov 16 2017
267,,Project Jupyter,"[Time Started, Date Submitted, Status, How often do you use Jupyter Notebook?, What, if anything, hinders you from making Jupyter Notebook an even more regular part of your workflow?, Roughly how long have you been using Jupyter Notebook?, Tool / Application #1:What tools and applications, if any, would you like to see more tightly integrated with Jupyter Notebook?    , Tool / Application #2:What tools and applications, if any, would you like to see more tightly integrated with Jupyter Notebook?    , Tool / Application #3:What tools and applications, if any, would you like to see more tightly integrated with Jupyter Notebook?    , How do you run the Jupyter Notebook?, Other - Write In:How do you run the Jupyter Notebook?, Workflow Need #1:What needs in your workflow does Jupyter Notebook address?, Workflow Need #2:What needs in your workflow does Jupyter Notebook address?, Workflow Need #3:What needs in your workflow does Jupyter Notebook address?, Workflow Need #1:What needs in your workflow does Jupyter Notebook not address?, Workflow Need #2:What needs in your workflow does Jupyter Notebook not address?, Workflow Need #3:What needs in your workflow does Jupyter Notebook not address?, Aspect #1:What aspects of Jupyter Notebook make it pleasant to use in your workflow?, Aspect #2:What aspects of Jupyter Notebook make it pleasant to use in your workflow?, Aspect #3:What aspects of Jupyter Notebook make it pleasant to use in your workflow?, Aspect #1:What aspects of Jupyter Notebook make it difficult to use in your workflow?, Aspect #2:What aspects of Jupyter Notebook make it difficult to use in your workflow?, Aspect #3:What aspects of Jupyter Notebook make it difficult to use in your workflow?, Feature / Change #1:What new features or changes would you like to see in Jupyter Notebook? (Please list anything that comes to mind that helps you in your workflow, big or small.), Feature / Change #2:What new features or changes would you like to see in Jupyter Notebook? (Please list anything that comes to mind that helps you in your workflow, big or small.), Feature / Change #3:What new features or changes would you like to see in Jupyter Notebook? (Please list anything that comes to mind that helps you in your workflow, big or small.), Enhancement #1:Thinking back to when you first started using Jupyter Notebook, what enhancements would have made your initial experience better?, Enhancement #2:Thinking back to when you first started using Jupyter Notebook, what enhancements would have made your initial experience better?, Enhancement #3:Thinking back to when you first started using Jupyter Notebook, what enhancements would have made your initial experience better?, Select all the words that best describe Jupyter Notebook., Other word(s)::Select all the words that best describe Jupyter Notebook., What is your primary role when using Jupyter Notebook (e.g., student, astrophysicist, financial modeler, business manager, etc.)?, How many years have you been in this role? , Industry #1:What industries does your role and analytical work support (e.g., Journalism, IT, etc.)?, Industry #2:What industries does your role and analytical work support (e.g., Journalism, IT, etc.)?, Industry #3:What industries does your role and analytical work support (e.g., Journalism, IT, etc.)?, How many people typically see and/or interact with the results of your work in Jupyter Notebook? (Consider people who view your notebooks on nbviewer, colleagues who rerun your notebooks, developers who star your notebook repos on GitHub, audiences who se]","[dateTime, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",At the end of 2015 the Jupyter Project conducted a UX Survey for Jupyter Notebook users. This dataset Survey.csv contains the raw responses. See the Google Group Thread for more context around this dataset. ,CSV,,[human-computer interaction],CC4,,,813,12076,0.73046875,Understand user perspectives on Jupyter Notebooks,2015 Notebook UX Survey,https://www.kaggle.com/jupyter/2015-notebook-ux-survey,Mon May 01 2017
268,,Reddit,[],[],Recently Reddit released an enormous dataset containing all ~1.7 billion of their publicly available comments. The full dataset is an unwieldy 1+ terabyte uncompressed so we've decided to host a small portion of the comments here for Kagglers to explore. (You don't even need to leave your browser!) You can find all the comments from May 2015 on scripts for your natural language processing pleasure. What had redditors laughing bickering and NSFW-ing this spring? Who knows? Top visualizations may just end up on Reddit. Data Description The database has one table May2015 with the following fields  created_utc ups subreddit_id link_id name score_hidden author_flair_css_class author_flair_text subreddit id removal_reason gilded downs archived author score retrieved_on body distinguished edited controversiality parent_id ,Other,,"[linguistics, reddit, internet]",Other,,,4632,46213,0,Get personal with a dataset of comments from May 2015,May 2015 Reddit Comments,https://www.kaggle.com/reddit/reddit-comments-may-2015,Wed Aug 05 2015
269,,franky07724,[],[],Context ShapeQA is a simple dataset for Visual Question Answering where a machine learning program will take an image and a question as input and create a answer in yes or no as output. An introduction of Visual Question Answering could be found in the following blog.  https//medium.com/@franky07724_57962/deep-learning-and-visual-question-answering-c8c8093941bc Content ShapeQA is a supervised learning problem with four files in total two files as the training dataset and two files as the testing dataset. The training dataset consists of two files  shapeqa_dataset_train_image_v1.zip ~ 5000 images shapeqa_dataset_train_list_v1.csv ~ an excel file consists of 5000 records where each record has four fields index the name of the image file the question corresponding to the image the label ~ 1 as yes and 0 as no  The testing dataset consists of two files  shapeqa_dataset_test_image_v1.zip ~ 1000 images shapeqa_dataset_test_list_v1.csv ~ an excel file cosnsits of 1000 records where each record has four fileds  index the name of the image file the question corresponding to the image  the label ~ 1 as yes and 0 as no  Acknowledgements The dataset is inspired by two previous works.  Object Detection with Neural Networks — a simple tutorial using keras the blog ~ https//towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491 the code ~ https//github.com/jrieke/shape-detection Cornell Natual Language Visual Reasoning  the web ~ http//lic.nlp.cornell.edu/nlvr/ the paper ~ http//alanesuhr.com/suhr2017.pdf  Inspiration  Will a simple multimodal deep learning model i.e. using CNN for image recognition using RNN for natural language processing and combing the results of these two models produce a good result? How to correctly capture the relations (e.g. left right above and below) explored in questions? ,CSV,,[],CC0,,,0,65,14,Dataset for visual question answering,ShapeQA Version 1.0,https://www.kaggle.com/franky07724/shapeqa-v10-for-visual-question-answering,Wed Feb 14 2018
270,,Canggih P Wibowo,"[Anime_ID, Genres]","[numeric, string]",Context Japanese animation which is known as anime has become internationally widespread nowadays. This dataset provides data on anime taken from Anime News Network.  Content This dataset consists of 4029 anime data in 5 files. All of the csv files use '|' delimiter.   - Anime Title (datatitle-all-share-new.csv)   - Anime Synopsis (datasynopsis-all-share-new.csv)   - Anime Genre (datagenre-all-share-new.csv)   - Anime Staff (datastaff-all-share-new.csv)   - Anime Scores (datascorehist-all-share-new.csv)  Anime ID and Staff were taken as what they seen on Anime News Network system. While the scores are taken based on the histogram of scores on each anime page and normalized. Acknowledgements The dataset was collected from http//www.animenewsnetwork.com on 10 May 2016. If you use this dataset in publications please cite  Wibowo C. P. (2016). A Minimum Spanning Tree Representation of Anime Similarities. arXiv preprint arXiv1606.03048  Inspiration This dataset can be used to build recommendation systems predict a score visualize anime similarity etc.,CSV,,"[popular culture, film, animation, entertainment]",CC4,,,9,181,2,Consists of 4029 anime data,"Anime Data (Score, Staff, Synopsis, and Genre)",https://www.kaggle.com/canggih/anime-data-score-staff-synopsis-and-genre,Thu Feb 22 2018
271,,Arizona Secretary of State,[],[],Context Complete statewide mapping files of a state's precincts can be difficult to come by. This file has been compiled to fill that need. Content This is the most recent statewide precinct file for Arizona. The properties have been generated for efficiently slicing and dicing up counties so that the file can be merged with Legislative and Congressional districts as well as other political boundaries. Acknowledgements The precinct file is recompiled using data requested from one of Arizona's 15 county GIS departments whenever a county re-precincts. Inspiration Gain insight into Arizona's political demographics by combining this with election results files census tracts and other publicly available geographic information.,{}JSON,,[],CC0,,,2,31,0.35546875,JSON map file of all of Arizona's 1495 Voting Tabulation Districts,Map of Arizona Voting Precincts,https://www.kaggle.com/azsecretaryofstate/arizona-voting-precincts,Fri Feb 23 2018
272,,Canggih P Wibowo,"[Title, Subtitle, Owner, Votes, Versions, Tags, Data Type, Size, License, Views, Download, Kernels, Topics, URL, Description]","[string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string]",Context Kaggle dataset becomes a popular growing place to share datasets. Almost every day there will be new datasets uploaded. I am curious to explore what can be extracted from the information of each dataset. Content This dataset consists 2150 datasets information in 15 columns  Title Subtitle Owner Vote Version History Tags Datatype Size License Views Downloads Kernels Topics URL Description  Acknowledgements All data were taken from Kaggle website. Collected on 26 Feb 2018 Inspiration With this dataset we may try to predict the upcoming datasets uploaded including its topics number of votes number of downloads etc. Data visualization involving clustering may be performed also.,CSV,,"[databases, information]",CC0,,,10,235,1,2885 Kaggle datasets with at least one vote,Upvoted Kaggle Dataset,https://www.kaggle.com/canggih/voted-kaggle-dataset,Thu Feb 22 2018
273,,Nick Brooks,[jigaboo],[string],Context The purpose of this dataset is to support the Toxic Comment Classification Competition. The goal is to help Jigsaw create a model detecting language toxicity levels. Building of the this dataset is available on my Github. Content This dataset contains a lot of bad words. Acknowledgements Carnegie Mellon University user2592414 on StackExchange Inspiration Use this for good.,CSV,,"[languages, crime, linguistics, internet]",CC0,,,41,251,0.0126953125,1617 Unique Bad Words,Bad Bad Words,https://www.kaggle.com/nicapotato/bad-bad-words,Thu Feb 22 2018
274,,Karan,[],[],Context There's a story behind every dataset and here's your opportunity to share yours. IMDB work done during office hours.  Content What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents too. IMDB review dataset.  Acknowledgements We wouldn't be here without the help of others. If you owe any attributions or thanks include them here along with any citations of past research. Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?,Other,,[],CC0,,,2,13,0.447265625,IMDb Analytics using python,IMDB office,https://www.kaggle.com/karan1990/imdb-office,Thu Feb 22 2018
275,,kail,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,9,10, ,best_gru_submmision,https://www.kaggle.com/kagglelzj/best-gru-submmision,Fri Feb 23 2018
276,,TerenceLiu,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,127,309,131, ,glove.6B.100d.txt,https://www.kaggle.com/terenceliu4444/glove6b100dtxt,Sat Jan 13 2018
277,,piupiu,"[the, ðə; ði, art. 这；那,adv. 更加（用于比较级，最高级前）]","[string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,23,14,1, ,dictcn,https://www.kaggle.com/pureheart/dictcn,Fri Feb 23 2018
278,,Gonzalo Barrientos,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,74,47,259, ,glove6b-200d,https://www.kaggle.com/gonzalo90/glove6b200d,Wed Feb 14 2018
279,,Pat P,"[year, round, pick, player, position, school]","[numeric, numeric, numeric, string, string, string]",Context Created for March Madness competition. To be used as a measure of talent on each team. Content Self-explanatory Acknowledgements Pulled from wikipedia using BeautifulSoup 2018 mock from http//www.nbadraft.net/2018mock_draft,Other,,[],CC0,,,19,58,0.08203125, ,NBA Draft 1980-2017,https://www.kaggle.com/pmp5kh/nba-draft-19802017,Fri Feb 23 2018
280,,Ashish Gupta,[],[],Context Consist 28x28 handwritten Alphabet images Content There are total 785 columns each row consists an image of alphabets. The first coloumn represents the alphabet numbering from 0-25 as A-Z.,CSV,,"[cnn, image data]",CC0,,,33,188,92,MNIST like dataset for alphabets(A-Z),Handwritten A-Z ,https://www.kaggle.com/ashishguptajiit/handwritten-az,Fri Jan 26 2018
281,,Tigan2,"[school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3, sex, age, address, famsize, Pstatus, Medu, Fedu, Mjob, Fjob, reason, guardian, traveltime, studytime, failures, schoolsup, famsup, paid, activities, nursery, higher, internet, romantic, famrel, freetime, goout, Dalc, Walc, health, absences, G1, G2, G3]","[string, string, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,23,0.0400390625, ,students-dataset,https://www.kaggle.com/lopata/studentsdataset,Wed Feb 21 2018
282,,NagenderReddy,"[Film, Genre, Rotten Tomatoes Ratings %, Audience Ratings %, Budget (million $), Year of release]","[string, string, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],GPL,,,1,11,0.0205078125, ,Movie Ratings,https://www.kaggle.com/nagenderp/movie-ratings,Thu Feb 22 2018
283,,Bletchley Bootcamp,"[Weekly_Sales, id]","[numeric, string]",Context This dataset originally stems from a Walmart recruiting challenge. It is used here for educational purposes only. Content The dataset contains anonymized sales by department for 45 Walmart stores as well as supporting features. Acknowledgements The dataset belongs to Walmart and is used here only for educational purposes Inspiration Try to predict weekly sales.,CSV,,[],Other,,,56,368,14,A processed version of an old recruitment challenge,Course Material: Walmart Challenge,https://www.kaggle.com/bletchley/course-material-walmart-challenge,Fri Feb 02 2018
284,,Kyle,"[User_ID, Product_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Product_Category_1, Product_Category_2, Product_Category_3, Marital_Status, Female, Male, Age_0-17, Age_18-25, Age_26-35, Age_36-45, Age_46-50, Age_51-55, Age_55+, 0_Occupation, 1_Occupation, 2_Occupation, 3_Occupation, 4_Occupation, 5_Occupation, 6_Occupation, 7_Occupation, 8_Occupation, 9_Occupation, 10_Occupation, 11_Occupation, 12_Occupation, 13_Occupation, 14_Occupation, 15_Occupation, 16_Occupation, 17_Occupation, 18_Occupation, 19_Occupation, 20_Occupation, City_Category_A, City_Category_B, City_Category_C, Stay_0yr, Stay_1yr, Stay_2yr, Stay_3yr, Stay4+yr, 1_Product_Category, 2_Product_Category, 3_Product_Category, 4_Product_Category, 5_Product_Category, 6_Product_Category, 7_Product_Category, 8_Product_Category, 9_Product_Category, 10_Product_Category, 11_Product_Category, 12_Product_Category, 13_Product_Category, 14_Product_Category, 15_Product_Category, 16_Product_Category, 17_Product_Category, 18_Product_Category, 19_Product_Category, 20_Product_Category, AgeNew_0-17_55+, AgeNew_26-35, AgeNew_18-25_36_45, AgeNew_51+, OccupationNew_0-2, OccupationNew_3-5, OccupationNew_6-8, OccupationNew_9-11, OccupationNew_12-14, OccupationNew_15-17, OccupationNew_18-20, Unqiue_Cat_Count, 2-3_Cat_Count, 4-5_Cat_Count, 6+_Cat_Count, AvgPurchaseAmount, NumberPurchasesUser]","[numeric, string, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,14,15, ,Black Friday Big,https://www.kaggle.com/bostonian92/black-friday-big,Thu Feb 22 2018
285,,Leonardo Ferreira ,"[, Age, Sex, Job, Housing, Saving accounts, Checking account, Credit amount, Duration, Purpose, Risk]","[numeric, numeric, string, numeric, string, string, string, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,80,161,0.05078125, ,german_credit_data_with_risk,https://www.kaggle.com/kabure/german-credit-data-with-risk,Tue Jan 09 2018
286,,Aperonofsorts,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,8,0.0224609375, ,DATA DICTIONARY,https://www.kaggle.com/jonchan2003/data-dictionary,Thu Feb 22 2018
287,,Zhe Song,"[sale_date, class_id, sale_quantity, brand_id, compartment, type_id, level_id, department_id, TR, gearbox_type, displacement, if_charging, price_level, price, driven_type_id, fuel_type_id, newenergy_type_id, emission_standards_id, if_MPV_id, if_luxurious_id, power, cylinder_number, engine_torque, car_length, car_width, car_height, total_quality, equipment_quality, rated_passenger, wheelbase, front_track, rear_track]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[business],CC0,,,4,38,0.3642578125,Dataset for Yancheng City Big Data Competition 2018,Car Sales of Yancheng City,https://www.kaggle.com/zhesong1/yanchengcarsales,Thu Feb 22 2018
288,,Krishna Tej,"[PassengerId, Survived]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,10,0.0888671875, ,"Train, Test and Gender",https://www.kaggle.com/krishnatejat/train-test-and-gender,Wed Feb 21 2018
289,,Manoj Chitteti,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,2,14,0.541015625, ,movie-metadata,https://www.kaggle.com/manojchitteti/moviemetadata,Thu Feb 22 2018
290,,KK16,[],[],Context Introduction The dataset used for this experiment is real and authentic. The dataset is acquired from UCI machine learning repository website [13]. The title of the dataset is ‘Crime and Communities’. It is prepared using real data from socio-economic data from 1990 US Census law enforcement data from the 1990 US LEMAS survey and crimedata from the 1995 FBI UCR [13]. This dataset contains a total number of 147 attributes and 2216 instances. The per capita crimes variables were calculated using population values included in the 1995 FBI data (which differ from the 1990 Census values).  Content The variables included in the dataset involve the community such as the percent of the population considered urban and the median family income and involving law enforcement such as per capita number of police officers and percent of officers assigned to drug units. The crime attributes (N=18) that could be predicted are the 8 crimes considered 'Index Crimes' by the FBI)(Murders Rape Robbery .... ) per capita (actually per 100000 population) versions of each and Per Capita Violent Crimes and Per Capita Nonviolent Crimes) predictive variables  125 non-predictive variables  4 potential goal/response variables  18 Acknowledgements http//archive.ics.uci.edu/ml/datasets/Communities%20and%20Crime%20Unnormalized U. S. Department of Commerce Bureau of the Census Census Of Population And Housing 1990 United States Summary Tape File 1a & 3a (Computer Files) U.S. Department Of Commerce Bureau Of The Census Producer Washington DC and Inter-university Consortium for Political and Social Research Ann Arbor Michigan. (1992) U.S. Department of Justice Bureau of Justice Statistics Law Enforcement Management And Administrative Statistics (Computer File) U.S. Department Of Commerce Bureau Of The Census Producer Washington DC and Inter-university Consortium for Political and Social Research Ann Arbor Michigan. (1992) U.S. Department of Justice Federal Bureau of Investigation Crime in the United States (Computer File) (1995) Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered? Data available in the dataset  may not act as a complete source of information for identifying factors that contribute to more violent and non-violent crimes as many relevant factors may still be missing. However I would like to try and answer the following questions answered.  Analyze if number of vacant and occupied houses and the period of time the houses were vacant had contributed to any significant change in violent and non-violent crime rates in communities How has unemployment changed crime rate(violent and non-violent) in the communities? Were people from a particular age group more vulnerable to crime? Does ethnicity play a role in crime rate? Has education played a role in bringing down the crime rate? ,{}JSON,,[],CC4,,,20,136,0.6650390625, ,UCI Communities and Crime Unnormalized Data Set,https://www.kaggle.com/kkanda/communities and crime unnormalized data set,Wed Feb 21 2018
291,,bigdatarecommendation,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,11,97,1024, ,YouTube_popular,https://www.kaggle.com/maxrmit/youtube-popular,Fri Feb 23 2018
292,,Ruta Sakalauskaite,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,18,26, ,submission-ensemble-NN-LG,https://www.kaggle.com/goomba16/submissionensemblennlg,Thu Feb 22 2018
293,,Alexander Kireev,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,8,10, ,sub_Vl,https://www.kaggle.com/alexanderkireev/sub-vl,Thu Feb 22 2018
294,,Aswin Ash,"[V1, V2, V3, V4, V5, Class]","[numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,12,0.00390625, ,newdata,https://www.kaggle.com/aswinps22/newdata,Sat Feb 17 2018
295,,Victor Paslay,"[id, target]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2009,411,89, ,kaggle-porto-seguro-submissions,https://www.kaggle.com/vpaslay/kaggleportosegurosubmissions,Sun Nov 12 2017
296,,SHAIK RESHMA,"[ID, Gender, Age, City_Code, City_Category, Employer_Code, Employer_Category1, Employer_Category2, Monthly_Income, Customer_Existing_Primary_Bank_Code, Primary_Bank_Type, Contacted, Source, Source_Category, Existing_EMI, Loan_Amount, Loan_Period, Interest_Rate, EMI, Var1]","[string, string, numeric, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,20,2, ,LOAN PREDICTION,https://www.kaggle.com/shaikreshma/loan-prediction,Wed Feb 21 2018
297,,Jokingpoet,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,2,15,0.9658203125, ,arrhythmia,https://www.kaggle.com/jokingpoet/arrhythmia,Tue Feb 20 2018
298,,areeves87,"[id, target]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,734,114,367, ,Output for 20 kernels porto seguro,https://www.kaggle.com/areeves87/kernel-census,Tue Nov 07 2017
299,,yliu,[],[],This dataset does not have a description yet.,Other,,[],ODbL,,,542,338,68, ,glove.6B.50d,https://www.kaggle.com/yliu9999/glove6b50d,Tue Dec 05 2017
300,,Siddharth,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,14,0.0087890625, ,dolphin,https://www.kaggle.com/sidpath/dolphin,Mon Feb 19 2018
301,,Manoj Chitteti,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,4,20,0.541015625, ,movie_metadata,https://www.kaggle.com/manojchitteti/movie-metadata,Wed Feb 21 2018
302,,Submarineering,"[id, is_iceberg]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,552,798,8, ,submission38 LB-0.1448,https://www.kaggle.com/submarineering/submission38-lb01448,Fri Feb 02 2018
303,,Eran Reuveni,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,33,0.712890625, ,Semiconductors top 10,https://www.kaggle.com/ereuveni/semiconductors-top-10,Fri Feb 23 2018
304,,Shaksham Kapoor,"[Product Name, Brand Name, Price, Rating, Reviews, Review Votes]","[string, string, numeric, numeric, string, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,7,47,33, ,Amazon Mobile Dataset,https://www.kaggle.com/deadshot9520/amazon-mobile-dataset,Wed Feb 14 2018
305,,JorgeMartinez,[],[],This dataset does not have a description yet.,Other,,[],Other,,,5,40,2, ,data students,https://www.kaggle.com/coke680/data-students,Fri Feb 09 2018
306,,Vojtas,"[market, binance, bitfinex, bitstamp, bittrex, gdax, minV, minE, maxV, maxE, diff, diffP]","[string, string, string, string, string, string, numeric, string, numeric, string, numeric, string]",This dataset does not have a description yet.,CSV,,[],GPL,,,1,9,0.00390625, ,cryptoprices,https://www.kaggle.com/vojtas/cryptoprices,Sun Feb 18 2018
307,,Conner Brown,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,38,3, ,sounds,https://www.kaggle.com/connerbrown/sounds,Tue Feb 13 2018
308,,DanB,"[, Suburb, Address, Rooms, Type, Price, Method, SellerG, Date, Distance, Postcode, Bedroom2, Bathroom, Car, Landsize, BuildingArea, YearBuilt, CouncilArea, Lattitude, Longtitude, Regionname, Propertycount]","[numeric, string, string, numeric, string, numeric, string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, string, numeric]",Context Melbourne real estate is BOOMING.  Can you find the insight or predict the next big trend to become a real estate mogul... or even harder to snap up a reasonably priced 2-bedroom unit? Content This is a snapshot of a dataset created by Tony Pino.  It was scraped from publicly available results posted every week from Domain.com.au. He cleaned it well and now it's up to you to make data analysis magic. The dataset includes Address Type of Real estate Suburb Method of Selling Rooms Price Real Estate Agent Date of Sale and distance from C.B.D. Notes on Specific Variables Rooms Number of rooms Price Price in dollars Method S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available. Type br - bedroom(s); h - housecottagevilla semiterrace; u - unit duplex; t - townhouse; dev site - development site; o res - other residential. SellerG Real Estate Agent Date Date sold Distance Distance from CBD Regionname General Region (West North West North North east ...etc) Propertycount Number of properties that exist in the suburb. Bedroom2  Scraped # of Bedrooms (from different source) Bathroom Number of Bathrooms Car Number of carspots Landsize Land Size BuildingArea Building Size CouncilArea Governing council for the area Acknowledgements This is intended as a static (unchanging) snapshot of https//www.kaggle.com/anthonypino/melbourne-housing-market. It was created in September 2017. Additionally homes with no Price have been removed.,CSV,,"[australia, housing, real estate, demographics]",CC4,,,1283,1118,3,Snapshot of Tony Pino's Melbourne Housing Dataset,Melbourne Housing Snapshot,https://www.kaggle.com/dansbecker/melbourne-housing-snapshot,Thu Sep 28 2017
309,,Lingzhi,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,319,172,47, ,Public Kernel Results from Favorita Forecasting,https://www.kaggle.com/vrtjso/ensemble,Mon Dec 18 2017
310,,kairos,"[ImageId, Label]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,22,15, ,MINST Image Recognition,https://www.kaggle.com/kairosart/minst-image-recognition,Thu Feb 15 2018
311,,zark muckerberg,[],[],This dataset does not have a description yet.,Other,,"[cricket, india, sports]",CC0,,,47,155,0.52734375,Indian premier league data up to year 2017,IPL Players 2017,https://www.kaggle.com/akshay35c/ipl-players-2017,Wed Jan 17 2018
312,,eoveson,"[rev_id, comment, year, logged_in, ns, sample, split]","[numeric, string, numeric, string, string, string, string]",This dataset does not have a description yet.,Other,,[],CC0,,,386,151,80, ,ConversationAIDataset,https://www.kaggle.com/eoveson/conversationaidataset,Sat Dec 30 2017
313,,Kostiantyn Isaienkov,"[id, target]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,72,115,14, ,NN ensemble,https://www.kaggle.com/isaienkov/nn-ensemble,Sun Nov 12 2017
314,,SteveZheng,"[id, property_type, room_type, amenities, accommodates, bathrooms, bed_type, cancellation_policy, cleaning_fee, city, description, first_review, host_has_profile_pic, host_identity_verified, host_response_rate, host_since, instant_bookable, last_review, latitude, longitude, name, neighbourhood, number_of_reviews, review_scores_rating, thumbnail_url, zipcode, bedrooms, beds]","[numeric, string, string, string, numeric, numeric, string, string, boolean, string, string, string, string, string, string, dateTime, string, string, numeric, numeric, string, string, numeric, string, string, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,31,10, ,airbnb_test,https://www.kaggle.com/stevezhenghp/airbnb-test,Tue Feb 13 2018
315,,Lisa,"[id, target]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1371,179,42, ,Input datasets,https://www.kaggle.com/arpitajena/input-datasets,Fri Nov 24 2017
316,,G R Navaneesh Kumar,"[id, log_price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,13,85,42, ,AirBnB,https://www.kaggle.com/navaneesh/airbnb,Tue Feb 20 2018
317,,takuoko,[],[],This dataset does not have a description yet.,Other,,[],Other,,,83,274,2048, ,glove.840B.300d.txt,https://www.kaggle.com/takuok/glove840b300dtxt,Sun Dec 31 2017
318,,donghwe,"[number, air_pressure_9am, air_temp_9am, avg_wind_direction_9am, avg_wind_speed_9am, max_wind_direction_9am, max_wind_speed_9am, rain_accumulation_9am, rain_duration_9am, relative_humidity_9am, relative_humidity_3pm]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,18,0.1640625, ,daily_weather,https://www.kaggle.com/donghwe90/daily-weather,Thu Feb 15 2018
319,,Siddharth Yadav,"[Date, Open, High, Low, Close, Adj Close, Volume]","[dateTime, string, string, string, string, string, string]",Wipro Limited (Western India Palm Refined Oils Limited or more recently Western India Products Limited) is an Indian Information Technology Services corporation headquartered in Bengaluru India. In 2013 Wipro demerged its non-IT businesses into separate companies to bring in more focus on independent businesses.,CSV,,"[finance, information technology]",CC0,,,2,42,0.4345703125,Ideal for time series analysis,Wipro Complete stocks data,https://www.kaggle.com/thebrownviking20/wipro,Wed Feb 21 2018
320,,Abdullah Karimi,[],[],This dataset does not have a description yet.,Other,,[],Other,,,4,40,1024, ,fasttext_toxic,https://www.kaggle.com/abdullahkarimi/fasttext-toxic,Mon Feb 12 2018
321,,Jezz D.,"[Customer ID, network_age, Customer tenure in month, Total Spend in Months 1 and 2 of 2017, Total SMS Spend, Total Data Spend, Total Data Consumption, Total Unique Calls, Total Onnet spend , Total Offnet spend, Total Call centre complaint calls, Network type subscription in Month 1, Network type subscription in Month 2, Most Loved Competitor network in in Month 1, Most Loved Competitor network in in Month 2, Churn Status]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,34,0.1103515625, ,Tele Churn Train data,https://www.kaggle.com/zamine/tele-churn-train-data,Mon Feb 12 2018
322,,Miri Choi,"[age, sex, bmi, children, smoker, region, charges]","[numeric, string, numeric, numeric, string, string, numeric]",Context Machine Learning with R by Brett Lantz is a book that provides an introduction to machine learning using R. As far as I can tell Packt Publishing does not make its datasets available online unless you buy the book and create a user account which can be a problem if you are checking the book out from the library or borrowing the book from a friend. All of these datasets are in the public domain but simply needed some cleaning up and recoding to match the format in the book. Content Columns  - age age of primary beneficiary   - sex insurance contractor gender female male   - bmi Body mass index providing an understanding of body weights that are relatively high or low relative to height             objective index of body weight (kg / m ^ 2) using the ratio of height to weight ideally 18.5 to 24.9   - children Number of    children covered by health insurance / Number of dependents  - smoker Smoking  - region the beneficiary's residential area in the US northeast southeast southwest northwest.  - charges Individual medical costs billed by health insurance Acknowledgements The dataset is available on GitHub here. Inspiration Can you accurately predict insurance costs?,CSV,,"[healthcare, finance]",ODbL,,,165,947,0.052734375,Insurance Forecast by using Linear Regression, Medical Cost Personal Datasets,https://www.kaggle.com/mirichoi0218/insurance,Wed Feb 21 2018
323,,Gfan,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,21,8, ,justatest,https://www.kaggle.com/ggggfan/justatest,Tue Feb 13 2018
324,,Khaiser,"[PassengerId, Survived]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,9,19,0.0888671875, ,Titanic survival prediction,https://www.kaggle.com/khaiser/titanic-survival-prediction,Tue Feb 20 2018
325,,DevjyotiChandra,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,132,487,68, ,glove.6B.50d.txt,https://www.kaggle.com/devjyotichandra/glove6b50dtxt,Tue Dec 26 2017
326,,Jayavardhan Reddy,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,44,133,131, ,Glove Vectors,https://www.kaggle.com/jayavardhanr/glove-vectors,Fri Dec 22 2017
327,,sudhi,"[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,33,66, ,creditcard,https://www.kaggle.com/sudheeshna189/creditcard,Sun Feb 18 2018
328,,Jonatan Cisneros,[],[],"Context Airports from https//openflights.org Content Airport ID  Unique OpenFlights identifier for this airport. Name    Name of airport. May or may not contain the City name. City    Main city served by airport. May be spelled differently from Name. Country Country or territory where airport is located. See countries.dat to cross-reference to ISO 3166-1 codes. IATA    3-letter IATA code. Null if not assigned/unknown. ICAO    4-letter ICAO code. Null if not assigned. Latitude    Decimal degrees usually to six significant digits. Negative is South positive is North. Longitude   Decimal degrees usually to six significant digits. Negative is West positive is East. Altitude    In feet. Timezone    Hours offset from UTC. Fractional hours are expressed as decimals eg. India is 5.5. DST Daylight savings time. One of E (Europe) A (US/Canada) S (South America) O (Australia) Z (New Zealand) N (None) or U (Unknown). See also Help Time Tz database time zone   Timezone in ""tz"" (Olson) format eg. ""America/Los_Angeles"". Type    Type of the airport. Value ""airport"" for air terminals ""station"" for train stations ""port"" for ferry terminals and ""unknown"" if not known. In airports.csv only type=airport is included. Source  Source of this data. ""OurAirports"" for data sourced from OurAirports ""Legacy"" for old data not matched to OurAirports (mostly DAFIF) ""User"" for unverified user contributions. In airports.csv only source=OurAirports is included. The data is UTF-8 (Unicode) encoded. Note Rules for daylight savings time change from year to year and from country to country. The current data is an approximation for 2009 built on a country level. Most airports in DST-less regions in countries that generally observe DST (eg. AL HI in the USA NT QL in Australia parts of Canada) are marked incorrectly. Acknowledgements https//openflights.org Inspiration I imported this data set to be able to perform analytics on Airport data combined with other large data sets.",Other,,"[business, demographics]",Other,,,4,63,0.3134765625, ,Airports,https://www.kaggle.com/jonatancr/airports,Sat Feb 17 2018
329,,DSEverything,"[id, is_iceberg]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1369,236,1, ,Statoil Iceberg Submissions,https://www.kaggle.com/dongxu027/statoil-iceberg-submissions,Mon Nov 27 2017
330,,Aleksey Bilogur,"[points, Bordeaux-style Red Blend, Cabernet Sauvignon, Chardonnay, Pinot Noir, Red Blend]","[numeric, numeric, numeric, numeric, numeric, numeric]",This is a supplemental dataset derived from the Wine Reviews dataset.,CSV,,"[food and drink, alcohol]",CC4,,,251,258,0.3671875,Review scores for five common wines,Most Common Wine Scores,https://www.kaggle.com/residentmario/most-common-wine-scores,Mon Nov 06 2017
331,,Fagun,"[Id, MSSubClass, MSZoning, LotFrontage, LotArea, Street, Alley, LotShape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, MasVnrArea, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinSF1, BsmtFinType2, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, Heating, HeatingQC, CentralAir, Electrical, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, KitchenQual, TotRmsAbvGrd, Functional, Fireplaces, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond, PavedDrive, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, PoolQC, Fence, MiscFeature, MiscVal, MoSold, YrSold, SaleType, SaleCondition, SalePrice, Age]","[numeric, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, numeric, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, string, string, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,2,50,0.4453125, ,housing prices,https://www.kaggle.com/fgnmittal/housing-prices,Sun Feb 11 2018
332,,Aswin Ash,"[V1;""V2"";""V3"";""V4"";""V5"";""Class"", V2, V3, V4, V5, Class]","[string, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,15,0.0048828125, ,train2,https://www.kaggle.com/aswinps22/train2,Sat Feb 17 2018
333,,Nagarjuna Challa,[],[],This dataset does not have a description yet.,Other,,[],CC4,,,3,16,1, ,default of credit card clients.xls,https://www.kaggle.com/nagarjunac/default-of-credit-card-clientsxls,Mon Feb 19 2018
334,,Elen Vardanyan,[],[],This dataset does not have a description yet.,Other,,[],Other,,,57,213,19, ,Football Players,https://www.kaggle.com/lnvardanyan/football-players,Tue Dec 12 2017
335,,Danan Dio,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,1,30,0.38671875, ,Summer_Olympic_Medals,https://www.kaggle.com/danandio/summer-olympic-medals,Tue Feb 13 2018
336,,puneet,[],[],This dataset does not have a description yet.,Other,,[],Other,,,260,1003,23, ,Online Retail,https://www.kaggle.com/puneetbhaya/online-retail,Fri Nov 10 2017
337,,Amit Maurya,"[ID, y, X0, X1, X2, X3, X4, X5, X6, X8, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99, X100, X101]","[numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,19,0.28515625, ,Random dataset,https://www.kaggle.com/akm5160/random-dataset,Mon Feb 12 2018
338,,shivam rishishwar,[],[],Introduction A car company has the data for all the cars that are present in the market. They are planning to introduce some new ones of their own but first they want to find out what would be the popularity of the new cars in the market based on each car's attributes. We will provide you a dataset of cars along with the attributes of each car along with its popularity. Your task is to train a model that can predict the popularity of new cars based on the given attributes. Dataset You are given a training dataset train.csv. The file is a comma separated file with useful information for this task train.csv contains the information about a car along with its popularity level. Each row provides information on each car. Information such as buying_price maintenance_cost number_of_doors number_of_seats etc. The definition of each attribute is as follows buying_price The buying_price denotes the buying price of the car and it ranges from [1...4] where buying_price equal to 1 represents the lowest price while buying_price equal to 4 represents the highest price. maintenance_cost The maintenance_cost denotes the maintenance cost of the car and it ranges from [1...4] where maintenance_cost equal to 1 represents the lowest cost while maintenance_cost equal to 4 represents the highest cost. number_of_doors The number_of_doors denotes the number of doors in the car and it ranges from [2...5] where each value of number_of_doors represents the number of doors in the car. number_of_seats The number_of_seats denotes the number of seats in the car and it consists of [2 4 5] where each value of number_of_seats represents the number of seats in the car. luggage_boot_size The luggage_boot_size denotes the luggage boot size and it ranges from [1...3] where luggage_boot_size equal to 1 represents smallest luggage boot size while luggage_boot_size equal to 3 represents largest luggage boot size. safety_rating The safety_rating denotes the safety rating of the car and it ranges from [1...3] where safety_rating equal to 1 represents low safety while safety_rating equal to 3 represents high safety. popularity The popularity denotes the popularity of the car and it ranges from [1...4] where popularity equal to 1 represents an unacceptable car popularity equal to 2 represents an acceptable car popularity equal to 3 represents a good car and popularity equal to 4 represents the best car. We also provide a test set of  car along with the above attributes excluding popularity in test.csv. The goal is to predict the popularity of the car based on its attributes.,Other,,[],CC0,,,8,49,0.0048828125, ,car prediction,https://www.kaggle.com/rishishwar123/car-prediction,Sun Feb 11 2018
339,,Ananth Reddy,"[, price, lotsize, bedrooms, bathrms, stories, driveway, recroom, fullbase, gashw, airco, garagepl, prefarea]","[numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string]",This dataset does not have a description yet.,CSV,,"[housing, linear regression]",CC0,,,12,57,0.029296875, ,Problem on Housing DataSet,https://www.kaggle.com/ananthreddy/housing,Tue Feb 13 2018
340,,QuantScientist,[],[],This dataset does not have a description yet.,Other,,[],GPL,,,462,273,0.0888671875, ,Statoil Iceberg Classifier Challenge LB 0.1690,https://www.kaggle.com/solomonk/statoil-01690,Wed Nov 29 2017
341,,Pak Shing Ho,"[Country, Year, SavingsRate, RealGDP, LaborForce, HumanCapital, PhysicalCapital]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,15,0.22265625, ,Cross Country,https://www.kaggle.com/shinggg/crosscountry,Wed Feb 14 2018
342,,Karan Saluja,"[id, species, margin1, margin2, margin3, margin4, margin5, margin6, margin7, margin8, margin9, margin10, margin11, margin12, margin13, margin14, margin15, margin16, margin17, margin18, margin19, margin20, margin21, margin22, margin23, margin24, margin25, margin26, margin27, margin28, margin29, margin30, margin31, margin32, margin33, margin34, margin35, margin36, margin37, margin38, margin39, margin40, margin41, margin42, margin43, margin44, margin45, margin46, margin47, margin48, margin49, margin50, margin51, margin52, margin53, margin54, margin55, margin56, margin57, margin58, margin59, margin60, margin61, margin62, margin63, margin64, shape1, shape2, shape3, shape4, shape5, shape6, shape7, shape8, shape9, shape10, shape11, shape12, shape13, shape14, shape15, shape16, shape17, shape18, shape19, shape20, shape21, shape22, shape23, shape24, shape25, shape26, shape27, shape28, shape29, shape30, shape31, shape32, shape33, shape34]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,24,0.3896484375, ,Leaves_Train,https://www.kaggle.com/karsal/leaves-train,Thu Feb 15 2018
343,,ivanloginov,"[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",This dataset is about accumulated production machine data. There are about 60 different indicators and all of them unnamed. And there is of course data on machine breakdowns given in ytrain.csv file.  Thus the topic of this dataset is to create a classification model to predict breakdowns. And if there is need to reveal the indicators with the greatest impact.,CSV,,[classification],CC0,,,6,43,233,Predict the machine breakdown with more than 50 indicators,The broken machine,https://www.kaggle.com/ivanloginov/the-broken-machine,Thu Feb 22 2018
344,,Abdullah Karimi,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,8,34,10, ,sub-5f_2_9,https://www.kaggle.com/abdullahkarimi/sub5f-2-9,Sat Feb 10 2018
345,,KumarRajarshi,"[Country, Year, Status, Life expectancy , Adult Mortality, infant deaths, Alcohol, percentage expenditure, Hepatitis B, Measles ,  BMI , under-five deaths , Polio, Total expenditure, Diphtheria ,  HIV/AIDS, GDP, Population,  thinness  1-19 years,  thinness 5-9 years, Income composition of resources, Schooling]","[string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Although there have been lot of studies undertaken in the past on factors affecting life expectancy considering                 demographic variables income composition and mortality rates. It was found that affect of immunization and human                development index was not taken into account in the past. Also some of the past research was done considering multiple                    linear regression based on data set of one year for all the countries. Hence this gives motivation to resolve both the                     factors stated previously by formulating a regression model based on mixed effects model and multiple linear regression                 while considering data from a period of 2000 to 2015 for all the countries. Important immunization like Hepatitis B Polio                    and Diphtheria will also be considered. In a nutshell this study will focus on immunization factors mortality factors                  economic factors social factors and other health related factors as well. Since the observations this dataset are based on                   different countries it will be easier for a country to determine the predicting factor which is contributing to lower value                    of life expectancy. This will help in suggesting a country which area should be given importance in order to efficiently                    improve the life expectancy of its population.  Content The project relies on accuracy of data. The Global Health Observatory (GHO) data repository under World Health                 Organization (WHO) keeps track of the health status as well as many other related factors for all countries The data-sets                    are made available to public for the purpose of health data analysis. The data-set related to life expectancy health factors                    for 193 countries has been collected from the same WHO data repository website and its corresponding economic                 data was collected from United Nation website. Among all categories of health-related factors only those critical factors were chosen which are more representative. It has been observed that in the past 15 years  there has been a huge development in health sector resulting in improvement of human mortality rates especially in the developing  nations in comparison to the past 30 years.  Therefore in this project we have considered data from year 2000-2015 for 193 countries for further analysis. The                  individual data files have been merged together into a single data-set. On initial visual inspection of the data showed some                    missing values. As the data-sets were from WHO we found no evident errors. Missing data was handled in R software by                     using Missmap command. The result indicated that most of the missing data was for population Hepatitis B and GDP. The                    missing data were from less known countries like Vanuatu Tonga Togo Cabo Verde etc. Finding all data for these                  countries was difficult and hence it was decided that we exclude these countries from the final model data-set. The final                    merged file(final dataset) consists of 22 Columns and 2938 rows which meant 20 predicting variables. All predicting                 variables was then divided into several broad categories​Immunization related factors Mortality factors Economical              factors and Social factors. Acknowledgements The data was collected from WHO and United Nations website with the help of Deeksha Russell and Duan Wang. Inspiration The data-set aims to answer the following key questions  1. Does various predicting factors which has been chosen initially really affect the Life expectancy? What are the                predicting variables actually affecting the life expectancy?  2. Should a country having a lower life expectancy value(<65) increase its healthcare expenditure in order to improve its average lifespan?  3. How does Infant and Adult mortality rates affect life expectancy?  4. Does Life Expectancy has positive or negative correlation with eating habits lifestyle exercise smoking drinking               alcohol etc.  5. What is the impact of schooling on the lifespan of humans?  6. Does Life Expectancy have positive or negative relationship with drinking alcohol?  7. Do densely populated countries tend to have lower life expectancy?  8. What is the impact of Immunization coverage on life Expectancy? ,CSV,,"[countries, data cleaning, regression analysis, multiple regression]",Other,,,72,309,0.318359375,Statistical Analysis on factors influencing Life Expectancy,Life Expectancy (WHO),https://www.kaggle.com/kumarajarshi/life-expectancy-who,Sat Feb 10 2018
346,,Umakant Jena,"[1, 14.23, 1.71, 2.43, 15.6, 127, 2.8, 3.06, .28, 2.29, 5.64, 1.04, 3.92, 1065]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,40,0.0107421875, ,Naive_bayes,https://www.kaggle.com/umakantjena2015/naive-bayes,Tue Feb 13 2018
347,,HamzaMassaoudi,"[, text, polarity]","[numeric, string, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,1,20,25, ,IMDB_data,https://www.kaggle.com/hamzamassaoudi/imdb-data,Tue Feb 20 2018
348,,S.Day,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,20,0.0302734375, ,Paroles_PNL,https://www.kaggle.com/saxinou/paroles-pnl,Wed Feb 14 2018
349,,DaK'anWei,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,28,0.0283203125, ,a cat pic,https://www.kaggle.com/darkenwei/a-cat-pic,Mon Feb 12 2018
350,,Miri Choi,"[checking_balance, months_loan_duration, credit_history, purpose, amount, savings_balance, employment_duration, percent_of_income, years_at_residence, age, other_credit, housing, existing_loans_count, job, dependents, phone, default]","[string, numeric, string, string, numeric, string, string, numeric, numeric, numeric, string, string, numeric, string, numeric, string, string]","First of all this dataset is not mine! I just want to use this dataset to approve my machine learning skills. So I upload this one! -) Hope you like it! ======================================================================================== This dataset classifies people described by a set of attributes as good or bad credit risks. Comes in two formats (one all numeric).  Also comes with a cost matrix. Source Professor Dr. Hans Hofmann  Institut f""ur Statistik und ""Okonometrie  Universit""at Hamburg  FB Wirtschaftswissenschaften  Von-Melle-Park 5  2000 Hamburg 13  ============================================================================================ Data Set Information Two datasets are provided. the original dataset in the form provided by Prof. Hofmann contains categorical/symbolic attributes and is in the file ""german.data"".  For algorithms that need numerical attributes Strathclyde University produced the file ""german.data-numeric"".  This file has been edited and several indicator variables added to make it suitable for algorithms which cannot cope with categorical variables.  Several attributes that are ordered categorical (such as attribute 17) have been coded as integer. This was the form used by StatLog.  This dataset requires use of a cost matrix (see below)  ..... 1 2  1 0 1  2 5 0  (1 = Good 2 = Bad)  The rows represent the actual classification and the columns the predicted classification.  It is worse to class a customer as good when they are bad (5) than it is to class a customer as bad when they are good (1).  ========================================================================================  UCI  German Credit Data original version You can find more details about variance etc. >  https//archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)  UCI German Credit Data Modify version Here is an modify version of the original one. > https//github.com/stedy/Machine-Learning-with-R-datasets/blob/master/credit.csv",CSV,,"[business, finance]",ODbL,,,22,125,0.08984375,classifies people described by a set of attributes as good or bad credit risks,Statlog (German Credit Data) Data Set,https://www.kaggle.com/mirichoi0218/statlog-german-credit-data-data-set,Tue Feb 13 2018
351,,N Saravana,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,3,15,0.08203125, ,Weather Report,https://www.kaggle.com/nsaravana/weather-report,Sat Feb 17 2018
352,,George Bofysil,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,2,28,65, ,Lead Score,https://www.kaggle.com/gbofysil/lead-score,Sun Feb 18 2018
353,,zi0n,[],[],Content Technical Information  Dates range from 2017-09-11 to 2018-02-16 and the time interval is 1 minute.  This is a MultiIndex CSV file to load in pandas use  dataset = pd.read_csv('dataset.csv' index_col=0 header=[0 1]).sort_index(axis=1)  Stocks that entered or exited the Index during the dataset time range are omitted. Collection & Processing  These are the scripts used for collecting the data and also utilities to clean & scale the dataset & convert it to a numpy array  https//github.com/nickdl/alpha,Other,,"[time series, finance]",Other,,,6,57,201,S&P 500 Index Intraday Data with 1min Interval,S&P 500 Intraday Data,https://www.kaggle.com/nickdl/snp-500-intraday-data,Sat Feb 17 2018
354,,Boris Marjanovic,[],[],Context Stock market data -- and particularly intraday price data -- can be very expensive to buy. To help more people gain access to it here I provide daily as well as intraday price and volume data for all U.S.-based stocks and ETFs trading on the NYSE NASDAQ and NYSE MKT.  Content The dataset (last updated 12/06/2017) is presented in CSV format as follows   Intraday data DateTimeOpenHighLowCloseVolumeOpenInt Daily data DateOpenHighLowCloseVolumeOpenInt   Acknowledgements The dataset belongs to me. I’m sharing it here for free. You may do with it as you wish. Inspiration Many have tried but most have failed to predict the stock market's ups and downs. Can you do any better? ,Other,,"[business, finance, economics, artificial intelligence]",CC0,,,77,416,418,Daily and Intraday Price + Volume Data For All U.S. Stocks & ETFs,Daily and Intraday Stock Price Data,https://www.kaggle.com/borismarjanovic/daily-and-intraday-stock-price-data,Sat Dec 09 2017
355,,Maxim Gritsenia,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,1,13,0.724609375, ,mlbootcamp5_train1,https://www.kaggle.com/maximgritsenia/mlbootcamp5-train1,Mon Feb 19 2018
356,,Max Stanford-Taylor,"[Surname, Forename, Score, Gum Learning, Gum Reciting]","[string, string, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,11,495, ,Barker Chewing Gum Test,https://www.kaggle.com/m0ongg/barker-chewing-gum-test,Mon Feb 19 2018
357,,AshwiniSarode,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,27,0.123046875, ,Dictonary,https://www.kaggle.com/ashwinirs/dictonary,Fri Feb 09 2018
358,,N Saravana,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,18,0.0419921875, ,Housing Data ,https://www.kaggle.com/subramanyamsaravana/housing-data,Sat Feb 17 2018
359,,LE PALLEC Clément,[],[],Context Contains leaderboards of 2017 PGA Tour season. Content Location Course Yardage Purse Pos Player Earnings To Par THRU R1 R2 R3 R4 Total Score Acknowledgements Scrapped on Cssports.com Inspiration Best players regarding yardage ?,Other,,[sports],Other,,,48,190,0.9189453125, ,PGA Tour 2016/2017 Leaderboards,https://www.kaggle.com/clementlepallec/pga-tour-20162017-leaderboards,Wed Dec 20 2017
360,,Mauro Pelucchi,[],[],This dataset does not have a description yet.,Other,,[],GPL,,,3,13,0.0078125, ,Stopwords-En,https://www.kaggle.com/mauropelucchi/stopwordsen,Thu Feb 22 2018
361,,Deeba,"[userId, movieId, rating, timestamp]","[numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,36,129, ,ratings,https://www.kaggle.com/deebakaz/ratings,Wed Feb 14 2018
362,,Chris Bartel,"[In, -0.3125]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,638,170,0.001953125, ,elemental_properties,https://www.kaggle.com/cbartel/elemental-properties,Sat Dec 23 2017
363,,Divya Varun,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,20,127,26, ,imdb Dataset,https://www.kaggle.com/varun08/imdb-dataset,Fri Jan 19 2018
364,,Rituraj Singh,"[id, game, white, black, white_elo, black_elo, white_rd, black_rd, whiteiscomp, blackiscomp, timecontrol, date, time, white_clock, black_clock, eco, plycount, commentaries, moves]","[numeric, string, string, string, numeric, numeric, numeric, numeric, boolean, boolean, string, string, dateTime, string, string, string, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],ODbL,,,1,21,7, ,trainingData,https://www.kaggle.com/ritzdevp/trainingdata,Tue Feb 13 2018
365,,Ibrahim,"[Data Source, World Development Indicators, ]","[string, string, numeric]","Null Hypotheses (H-not/H0) - Are religious people more happy and does it contribute to a better experience of life?  ----------------------AND in the same vein ---------------------------------------------------------------------- Is the increasing trend of Atheism directly related to increasing reported levels of ADHD depression and suicide rates around the world? The research - A slew of research suggests that religious people are happier are better at keeping family ties contribute to society more by being involved in the community report better life experience and are better able to cope with life's setbacks like Divorce. Is this true? Below is a random list of research I found from googling -  (1) https//www.psychologytoday.com/blog/more-mortal/201212/are-religious-people-happier-non-religious-people (2) https//www.psychologytoday.com/blog/curious/201510/does-being-religious-make-us-happy (3) http//www.huffingtonpost.co.uk/2016/02/02/office-for-national-statistics-well-being-data_n_9138076.html (4) https//www.unilad.co.uk/news/new-research-shows-religious-people-are-happier-than-atheists/ (5) https//www.christiantoday.com/article/why-religious-people-are-happier-and-how-to-share-the-joy/78581.htm (6) http//www.pewforum.org/2016/04/12/religion-in-everyday-life/ What the Quran says - Having graduated from the London School of Economics (2004 Bsc Hons) and having been greatly influenced by Richard Dawkins books like ""The God Delusion"" etc. for about 7 years and seeking extensively through the various religious/self development traditions including Judaism Christianity Buddhism Hinduism The Landmark Forum and Tai Chi I converted to Islam 4 years ago. I can personally attest to having a much greater experience of life and feeling peace and tranquility and calmness in my heart. In the Sufi tradition the heart is the kernel of connecting to God (Allah) and the seat of God consciousness - https//www.youtube.com/watch?v=nqNPVP6GerM&index=1&list=PLwFLXkJiBtuza1uSJHsB8MJCfQ9l7h8jf Allah says in the Quran - ""And whoever turns away from My remembrance - indeed he will have a depressed life...."" [Quran 20124] And Allah also says in the Quran - ""Those who have believed and whose hearts are assured by the remembrance of Allah. Unquestionably by the remembrance of Allah hearts are assured."" [Quran 1328] Dataset -  The data set regarding population is the gross population by country taken from the World Bank Data Site link here - https//data.worldbank.org/indicator/SP.POP.TOTL?locations=US&view=chart Can you -  Look at populations around the world using the dataset and look at suicide levels depression levels reported ADHD levels and anxiety levels and find a correlation between the increasing trend of atheism in the world and these reported markers.",CSV,,[],CC0,,,2,52,0.1630859375,Quran - Aetheism - Statistic,Quran- are religious people happier?,https://www.kaggle.com/ibrahimmukherjee/quran-religious-people-more-happy,Sun Feb 18 2018
366,,Manoel Ribeiro,[],[],"Context This dataset contains a network of 100k users out of which ~5k were annotated as hateful or not.  For each user several content-related network-related and activity related features were provided.  Check this repo for analysis and a straightforward classification approach and this repo where we employed GraphSage a network embedding method; Hint Try to use not only the content associated with each user but also Twitter's network structure. Content  users_anon_neighborhood.csv  file with several features for each user as well as the avg for some features for their 1-neighborhood (ppl they tweeted). Notice that c_ are attributes calculated for the 1-neighborhood of a user in the retweet network (averaged out). users_clean.graphml networkx compatible file with retweet network. User id's correspond to those in  users_anon_neighborhood.csv!  If you're keen on the original tweets contact me ). For reproducibility purposes This are the files used by GraphSage here. They come in a special format ); I've added ""_""  _users_(hate|suspended)_(glove|all).content files with the feature vector for each user and their classes the ones with hate label users as either hateful normal or other whereas the ones with suspended label users as either suspended or active. The ones with glove have only the glove vectors as features the ones with all have other attributes related to users activity and network centrality. This is only for the GraphSage algorithm. _user.edges file with all the (directed) edges in the retweet graph.  Attributes description   hate (""hateful""|""normal""|""other"")   if user was annotated as hateful normal or not annotated.    (is_50|is_50_2) bool   whether user was deleted up to 12/12/17 or 14/01/18.     (is_63|is_63_2) bool   whether user was suspended up to 12/12/17 or 14/01/18.     (hate|normal)_neigh bool   is the user on the neighborhood of a (hateful|normal) user?     [c_] (statuses|follower|followees|favorites)_count int   number of (tweets|follower|followees|favorites) a user has.    [c_] listed_countint   number of lists a user is in.    [c_] (betweenness|eigenvector|in_degree|outdegree) float   centrality measurements for each user in the retweet graph.    [c_] *_empath float   occurrences of empath categories in the users latest 200 tweets.    [c_] *_glove float             glove vector calculated for users latest 200 tweets.    [c_] (sentiment|subjectivity) float   average sentiment and subjectivity of users tweets.    [c_] (time_diff|time_diff_median) float   average and median time difference between tweets.    [c_] (tweet|retweet|quote) number float   percentage of direct tweets retweets and quotes of an user.    [c_] (number urls|number hashtags|baddies|mentions) float   number of bad words|mentions|urls|hashtags per tweet in average.    [c_] status length float   average status length.    hashtags string   all hashtags employed by the user separated by spaces. ",Other,,"[crime, internet, network analysis]",CC0,,,6,105,2048,Detecting hate speech with context,Hateful Users on Twitter,https://www.kaggle.com/manoelribeiro/hateful-users-on-twitter,Fri Feb 23 2018
367,,DataMoose,"[car.id, buying, maint, doors, persons, lug_boot, safety, class]","[numeric, string, string, numeric, numeric, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,39,0.0322265625, ,cars-train,https://www.kaggle.com/datamoose/carstrain,Fri Feb 16 2018
368,,Frédéric Kosmowski,"[rally, serve, hitpoint, speed, net.clearance, distance.from.sideline, depth, outside.sideline, outside.baseline, player.distance.travelled, player.impact.depth, player.impact.distance.from.center, player.depth, player.distance.from.center, previous.speed, previous.net.clearance, previous.distance.from.sideline, previous.depth, opponent.depth, opponent.distance.from.center, same.side, previous.hitpoint, previous.time.to.net, server.is.impact.player, id, train, outcome, gender]","[numeric, numeric, string, numeric, numeric, numeric, numeric, boolean, boolean, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, boolean, string, numeric, boolean, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,"[tennis, sports]",CC4,,,3,49,0.99609375, ,Hawkeye Tennis Matches ,https://www.kaggle.com/fkosmowski/hawkeye-tennis-matches,Tue Feb 13 2018
369,,Faizunnabi,[],[],1. Title Contraceptive Method Choice 2. Sources (a) Origin  This dataset is a subset of the 1987 National Indonesia                 Contraceptive Prevalence Survey (b) Creator Tjen-Sien Lim (limt@stat.wisc.edu) (c) Donor   Tjen-Sien Lim (limt@stat.wisc.edu) (d) Date    June 7 1997 3. Past Usage Lim T.-S. Loh W.-Y. & Shih Y.-S. (1999). A Comparison of    Prediction Accuracy Complexity and Training Time of Thirty-three    Old and New Classification Algorithms. Machine Learning. Forthcoming.    (ftp//ftp.stat.wisc.edu/pub/loh/treeprogs/quest1.7/mach1317.pdf or    (http//www.stat.wisc.edu/~limt/mach1317.pdf) 4. Relevant Information This dataset is a subset of the 1987 National Indonesia Contraceptive    Prevalence Survey. The samples are married women who were either not     pregnant or do not know if they were at the time of interview. The     problem is to predict the current contraceptive method choice     (no use long-term methods or short-term methods) of a woman based     on her demographic and socio-economic characteristics. 5. Number of Instances 1473 6. Number of Attributes 10 (including the class attribute) 7. Attribute Information  Wife's age                     (numerical) Wife's education               (categorical)      1=low 2 3 4=high Husband's education            (categorical)      1=low 2 3 4=high Number of children ever born   (numerical) Wife's religion                (binary)           0=Non-Islam 1=Islam Wife's now working?            (binary)           0=Yes 1=No Husband's occupation           (categorical)      1 2 3 4 Standard-of-living index       (categorical)      1=low 2 3 4=high Media exposure                 (binary)           0=Good 1=Not good Contraceptive method used     (class attribute)  1=No-use 2=Long-term3=Short-term  8. Missing Attribute Values None,Other,,[],CC0,,,3,43,0.029296875,Predict the current contraceptive method choice of a woman ,Contraceptive Method Choice,https://www.kaggle.com/faizunnabi/contraceptive-method-choice,Tue Feb 13 2018
370,,Moghazy,[],[],This dataset does not have a description yet.,Other,,[],Other,,,2,55,11, ,mnist dataset,https://www.kaggle.com/moghazy/mnist,Thu Jan 11 2018
371,,monkeyking,"[id, is_iceberg]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,401,147,0.197265625, ,LB 0.1400,https://www.kaggle.com/supersp1234/lb-01400,Fri Jan 12 2018
372,,SHUBHAM KARANDE,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,30,0.2177734375, ,HR DATA,https://www.kaggle.com/shubham17mcb1015/hr-data,Sun Feb 18 2018
373,,Rajath Chidananda,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,3,23,53,IMDB Data set for NLP analysis, Bag of Words Meets Bags of Popcorn :),https://www.kaggle.com/rajathmc/bag-of-words-meets-bags-of-popcorn-,Tue Feb 13 2018
374,,Maxim Gritsenia,"[State, Account length, Area code, International plan, Voice mail plan, Number vmail messages, Total day minutes, Total day calls, Total day charge, Total eve minutes, Total eve calls, Total eve charge, Total night minutes, Total night calls, Total night charge, Total intl minutes, Total intl calls, Total intl charge, Customer service calls, Churn]","[string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, boolean]",This dataset does not have a description yet.,CSV,,[],CC0,,,11,53,0.2666015625, ,Telecom_churn,https://www.kaggle.com/maximgritsenia/telecom-churn,Sun Feb 18 2018
375,,Ai-LongZheng,"[id, log_price, property_type, room_type, amenities, accommodates, bathrooms, bed_type, cancellation_policy, cleaning_fee, city, description, first_review, host_has_profile_pic, host_identity_verified, host_response_rate, host_since, instant_bookable, last_review, latitude, longitude, name, neighbourhood, number_of_reviews, review_scores_rating, thumbnail_url, zipcode, bedrooms, beds]","[numeric, numeric, string, string, string, numeric, numeric, string, string, boolean, string, string, string, string, string, string, dateTime, string, string, numeric, numeric, string, string, numeric, string, string, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,16,31, ,airbnb train,https://www.kaggle.com/ailongzheng/airbnb-train,Mon Feb 19 2018
376,,Vlad Golubev,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,83,299,4, ,C++ submission,https://www.kaggle.com/golubev/c-submission,Fri Dec 15 2017
377,,Shreya Sahu,[],[],This dataset does not have a description yet.,Other,,[],ODbL,,,1,23,155, ,Berlin EMODB- numpy files,https://www.kaggle.com/shreya22/berlin-emodb-numpy-files,Thu Feb 15 2018
378,,Bletchley Bootcamp,"[Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline, Cultivar 1, Cultivar 2, Cultivar 3]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This is a normalized and preprocessed version of the UCI Wine Classification dataset. The input data was normalized and the output data was encoded into one hot. This dataset is used for the first weeks challenge of our deep learning course. Feel free to experiment with it. Acknowledgements This dataset was originally published by UCI and was donated by Stefan Aeberhard. Inspiration This is a small supervised learning dataset we use for understanding simple neural networks. Feel free to try out other strategies with it. Why not use evolutionary strategies to train it?,Other,,[],CC0,,,113,206,0.23828125,A normalized version of the classic UCI set,Course Material: Normalized Wine Classification,https://www.kaggle.com/bletchley/normalized-wine-classification,Sun Jan 28 2018
379,,YalamatiSandeep,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,9,41,0.2158203125, ,HR DATASETS,https://www.kaggle.com/mohan006/hr-datasets,Wed Feb 14 2018
380,,vsmolyakov,[],[],"Context FastText word embeddings trained on English wikipedia  FastText embeddings are enriched with sub-word information useful in dealing with misspelled and out-of-vocabulary words. Content Each line contains a word followed by 300-dimensional embedding Acknowledgements P. Bojanowski E. Grave A. Joulin T. Mikolov ""Enriching Word Vectors with Subword Information"" arXiv 2016  FastText Embeddings https//github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md Inspiration Q1 How does FastText compare with Glove and word2vec embeddings?  Q2 What are the different approaches for learning embeddings with sub-word information?  Q3 How does FastText compare with character-level n-gram representation of words?  ",Other,,[linguistics],CC0,,,93,394,107,embeddings with sub-word information,FastText,https://www.kaggle.com/vsmolyakov/fasttext,Sun Dec 31 2017
381,,yogender singh,[],[],This dataset does not have a description yet.,CSV,,[],ODbL,,,3,29,15, ,handwritten images,https://www.kaggle.com/yogirj/handwritten-images,Mon Feb 12 2018
382,,Arnasca1965,"[PRODUCT_ID, MANUFACTURER, DEPARTMENT, BRAND, COMMODITY_DESC, SUB_COMMODITY_DESC, CURR_SIZE_OF_PRODUCT]","[numeric, numeric, string, string, string, string, string]",Context Grocery's shop Content It's a dataset representing retails of a grocery shop. It is very useful for Market Basket Analysis,CSV,,[],Other,,,12,58,1,Grocery's shop Dataset,Dataset - MBA,https://www.kaggle.com/arnasca1965/dataset-mba,Fri Feb 16 2018
383,,Yash Prakash,[],[],This Dataset is comprised of the Chest X-Rays of 5 Different diseases.  They are Atelectasis Infiltration Effusion Cardiomegaly Fibrosis All of the training sets contain 150+ images.  All of the Validation/Test sets contain 30+ images. It contains folder wise arranged images for all diseases in both the training and Validation sets. The aim is to train  a Neural Network to classify a given Chest X-Ray image into one of these categories.,Other,,"[healthcare, diseases, medicine]",CC0,,,28,179,127,Contains folder wise arranged images for 5 diseases.,Chest X-Rays Dataset,https://www.kaggle.com/yashprakash13/chest-xrays-dataset,Mon Jan 29 2018
384,,Krid Jin,[],[],"Context Clustering benchmark datasets published by School of Computing University of Eastern Finland Content 2D scatter points and label which need to process the formatting first. find more in https//cs.joensuu.fi/sipu/datasets/ Acknowledgements @misc{ClusteringDatasets     author = {Pasi Fr\""anti et al}     title = {Clustering datasets}     year = {2015}     url = {http//cs.uef.fi/sipu/datasets/} } Inspiration With standard and famous benchmark various clustering algorithm can be performed and compared though a number of kernels.",Other,,[clustering],ODbL,,,2,52,0.220703125,2D dataset with label,Clustering benchmark datasets,https://www.kaggle.com/vasopikof/clustering-benchmark-datasets,Mon Feb 12 2018
385,,QuantScientist,"[id, is_iceberg]","[string, numeric]",This dataset does not have a description yet.,Other,,[],CC0,,,366,157,0.2529296875, ,Statiol LB 0.1538,https://www.kaggle.com/solomonk/0.1538,Tue Nov 28 2017
386,,Tomato,"[encounter_id, patient_nbr, race, gender, age, weight, admission_type_id, discharge_disposition_id, admission_source_id, time_in_hospital, payer_code, medical_specialty, num_lab_procedures, num_procedures, num_medications, number_outpatient, number_emergency, number_inpatient, diag_1, diag_2, diag_3, number_diagnoses, max_glu_serum, A1Cresult, metformin, repaglinide, nateglinide, chlorpropamide, glimepiride, acetohexamide, glipizide, glyburide, tolbutamide, pioglitazone, rosiglitazone, acarbose, miglitol, troglitazone, tolazamide, examide, citoglipton, insulin, glyburide-metformin, glipizide-metformin, glimepiride-pioglitazone, metformin-rosiglitazone, metformin-pioglitazone, change, diabetesMed, readmitted]","[numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,20,137,3, ,Diabetic,https://www.kaggle.com/kebabdk400/diabetic,Thu Jan 11 2018
387,,MOSABR027,"[train_id, name, item_condition_id, category_name, brand_name, price, shipping, item_description]","[numeric, string, numeric, string, string, numeric, numeric, string]",This dataset does not have a description yet.,Other,,[],CC0,,,3,10,129, ,dataset_train,https://www.kaggle.com/msabr027/dataset-train,Thu Feb 22 2018
388,,Hendy Irawan,"[length, area, label]","[numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,26,110, ,SimpleOCR,https://www.kaggle.com/hendyirawan/simpleocr,Fri Feb 16 2018
389,,Idan Erez,"[symbol, rank, price_usd, price_btc, market_cap_usd, update_time(EST)]","[string, numeric, numeric, numeric, numeric, dateTime]",Context The past two weeks were crazy in the crypto market. The goal is to allow analyze correlations between Bitcoin and other Crypto Currencies in order to do smarter day-trading. Content This data set was updated every 15 min using Coin Market Cap API and includes the top 100 coins market cap price in USD and price in BTC. Every row has its update time in EST Time zone Acknowledgements Coin Market Cap API Inspiration Who are the followers and leaders in the crypto market? When BTC goes down - what coins should be bought and when? When it goes up - which coins start to rise following it but still giving us enough time to buy them?,CSV,,[finance],CC0,,,7,54,2,Date range 1/26/18 - 2/10/18,Top 100 Cryptos - 15 min cycles,https://www.kaggle.com/idanerez/top-100-cryptos-updates-every-15-min,Sun Feb 11 2018
390,,InfiniteWing,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,30,144,57, ,KKBOX churn scala label,https://www.kaggle.com/infinitewing/kkbox-churn-scala-label,Mon Dec 18 2017
391,,sjlyle,"[v1, v2, , , ]","[string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,24,0.48046875, ,sms_spam_collection,https://www.kaggle.com/sjlyle/sms-spam-collection,Wed Feb 14 2018
392,,Srinivas Paturu,"[Buisness Service, Asset, Date, Incident Count, Net cool event count, Exception Messages Count from log excel, Health rule voilation Count, Application Event Count, Slow Transaction Count(slow and very slow transaction from transcatio snap), Error Transaction Count error_transaction snap, Exception, SQL Exception]","[string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,37,0.044921875, ,MasterCard,https://www.kaggle.com/asvvisb/mastercard,Sun Feb 11 2018
393,,max201712,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,2,55,0.375, ,YouTube_popular ,https://www.kaggle.com/dongqin201712/youtube-popular,Mon Feb 19 2018
394,,Jordan Nietzel,"[state, POP, GEONAME]","[numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,7,0.0009765625, ,state_populations,https://www.kaggle.com/jnietzel/state-populations,Wed Feb 21 2018
395,,Alex Achterberg,"[PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]","[numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,5,19,0.0859375, ,Titanic,https://www.kaggle.com/alexbariloche/titanic,Mon Feb 12 2018
396,,Anvesh Tummala,"[air_store_id, visit_datetime, reserve_datetime, reserve_visitors]","[string, dateTime, dateTime, numeric]",This dataset does not have a description yet.,CSV,,[],CC3,,,48,121,28,Including Weather Data ,Recruit Restaurant Visitor Forecasting Data,https://www.kaggle.com/anvesh525/recruit-restaurant-visitor-forecasting-data,Sat Dec 30 2017
397,,EZENG,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,9,0.005859375, ,badwords,https://www.kaggle.com/ericzengyi/badwords,Thu Feb 22 2018
398,,Mathieu Goutay,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,46,271,9, ,train.csv,https://www.kaggle.com/mgoutay/traincsv,Tue Dec 05 2017
399,,bob-li,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,31,168,93, ,12306 captcha image,https://www.kaggle.com/libowei/12306-captcha-image,Tue Dec 26 2017
400,,Ai-LongZheng,"[id, property_type, room_type, amenities, accommodates, bathrooms, bed_type, cancellation_policy, cleaning_fee, city, description, first_review, host_has_profile_pic, host_identity_verified, host_response_rate, host_since, instant_bookable, last_review, latitude, longitude, name, neighbourhood, number_of_reviews, review_scores_rating, thumbnail_url, zipcode, bedrooms, beds]","[numeric, string, string, string, numeric, numeric, string, string, boolean, string, string, string, string, string, string, dateTime, string, string, numeric, numeric, string, string, numeric, string, string, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,9,68,42, ,airbnb,https://www.kaggle.com/ailongzheng/airbnb,Mon Feb 19 2018
401,,Conner Brown,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,16,3, ,subset_msd_summary_file.h5,https://www.kaggle.com/connerbrown/subset-msd-summary-fileh5,Tue Feb 13 2018
402,,sihao di,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,24,137, ,cnn_model,https://www.kaggle.com/sihaodi/cnn-model,Tue Feb 13 2018
403,,Shawn Xin,"[datetime, price]","[dateTime, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,24,2, ,Stocks_time_series,https://www.kaggle.com/zxin1989/stocks,Sun Feb 11 2018
404,,yogender singh,"[CustomerID, Genre, Age, Annual Income (k$), Spending Score (1-100)]","[numeric, string, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,49,0.00390625, ,spending of customer ,https://www.kaggle.com/yogirj/spending-of-customer,Sat Feb 10 2018
405,,Tanmay Salunkhe,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,52,0.2177734375, ,Attrition Rate Analysis,https://www.kaggle.com/tanmaysalunkhe/attrition-rate-analysis,Mon Jan 29 2018
406,,CSSYoda,[],[],This dataset does not have a description yet.,Other,,[],Other,,,3,21,0.00390625, ,iris dataset,https://www.kaggle.com/celeonha/iris-dataset,Fri Feb 16 2018
407,,Kenneth Collins,"[CrimeDate, CrimeTime, CrimeCode, Location, Description, Inside/Outside, Weapon, Post, District, Neighborhood, Longitude, Latitude, Location 1, Premise, Total Incidents]","[dateTime, dateTime, string, string, string, string, string, numeric, string, string, numeric, numeric, string, string, numeric]",https//www.kaggle.com/sohier/crime-in-baltimore This is where I got the data. R wouldn't load it in because of an unentered datapoint on line 61009. I added an NA there.,CSV,,[],CC0,,,1,24,9, ,"""Crime in Baltimore"" edit",https://www.kaggle.com/duds00/crime-in-baltimore-edit,Mon Feb 19 2018
408,,kvpratama,[],[],Context I collected this dataset for my school project. The project is to train GAN to generate new Pokemon. I had a difficult time to find training dataset that is complete and clean. So I gather this collection of image and publish it here in hope that it will help others who need similar dataset. You can find my project on my Github Content 819 transparent Pokemon images in png format size 256x256. Acknowledgements I collected the image mostly from this website https//veekun.com/dex/downloads Banner image is taken from https//viking011.deviantart.com/art/Pokemon-Poster-436455502 Inspiration Since I failed to generate new Pokemon with clarity (I can only generate the shape) I wish there will be others that could do it with this dataset. If you managed to please share it!,Other,,[],CC0,,,54,385,39,Dataset of 819 Pokemon images,Pokemon Images Dataset,https://www.kaggle.com/kvpratama/pokemon-images-dataset,Tue Dec 12 2017
409,,Onofrio_BIScience,[],[],This dataset does not have a description yet.,Other,,[],ODbL,,,150,566,815, ,Dogs vs Cats,https://www.kaggle.com/biaiscience/dogs-vs-cats,Tue Dec 05 2017
410,,AAKASH AGRAWAL,"[PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]","[numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,31,0.0859375, ,THE TITANIC,https://www.kaggle.com/aakash2016/the-titanic,Sun Feb 11 2018
411,,Mirantha Jayathilaka,[],[],700 annotations of Kumar Sangakkara's face Context Recently I have been working on some object localization problems using Convolutional Nets and I wanted to try train the model on a new dataset other than the very common COCO or PASCAL VOC datasets. While pondering on what object to compile a small dataset around I thought of pushing the challenge a bit more to see if the same model can be trained to localize faces. Having this in mind I wanted a dataset of a person's face annotations. As you may know with Deep Learning models the more data you have the more accuracy you reach. So considering the challenge to detect a face I wanted a considerable number of images of the same face that the model should be trained on.  Hence I needed many pictures of the same person. So the person had to be famous so I could easily find many pictures.  So being in Sri Lanka where else to look other than our Cricket stars. So I chose the living legend in Sri Lankan Cricket Kumar Sangakkara. Content I downloaded around 1000 images from google images and after manual cleaning ended up with 704 which are contained here. I manually annotated all the pictures using a python script to generate the xml files. (Yeah I couldn't find a better thing to do in that 2 hours.) Now here is the dataset for anyone to make use of.  The zip file attached contains two folders images and annotations.  Inspiration So as I mentioned in the above description my goal with this dataset was to see if an object localization model can be used to detect a face of a person. Even though I have the pipeline I couldn't still thoroughly test its performance using a GPU. So anyone whose interested can use this dataset to test those results. Also if these annotations can be useful for any other application feel free to use it and share it. Have fun! ,Other,,"[cricket, sports, image processing]",ODbL,,,1,68,6,Annotations of the face of sporting star Kumar Sangakkara,700 Kumar Sangakkara Face Annotations ,https://www.kaggle.com/mirantha/sangaface,Tue Feb 13 2018
412,,Udacity,[],[],Context The online job market is a good indicator of overall demand for labor in an economy. This dataset consists of 19000 job postings from 2004 to 2015 posted on CareerCenter an Armenian human resource portal. Since postings are text documents and tend to have similar structures text mining can be used to extract features like posting date job title company name job description salary and more. Postings that had no structure or were not job-related were removed. The data was originally scraped from a Yahoo! mailing group. Inspiration Students job seekers employers career advisors policymakers and curriculum developers use online job postings to explore the nature of today's dynamic labor market. This dataset can be used to  Understand the demand for certain professions job titles or industries Identify skills that are most frequently required by employers and how the distribution of necessary skills changes over time Help education providers with curriculum development  Acknowledgements The data collection and initial research were funded by the American University of Armenia’s research grant (2015).  Habet Madoyan CEO at Datamotus compiled this dataset and has granted us permission to republish. The republished dataset is identical to the original dataset which can be found here. Datamotus also published a report detailing the text mining techniques used plus analyses and visualizations of the data.,Other,,[],Other,,,648,1796,92,"19,000 online job postings from 2004 to 2015 from Armenia's CareerCenter",Armenian Online Job Postings,https://www.kaggle.com/udacity/armenian-online-job-postings,Sun Aug 06 2017
413,,JonathanPhoon,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,70,146,0.1083984375, ,BTCUSDKRAKEN,https://www.kaggle.com/jphoon/btcusdkraken,Wed Oct 18 2017
414,,DanB,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,209,471,0.0029296875,Used in the course kaggle.com/deep-learning,Python Utility Code for Deep Learning Exercises,https://www.kaggle.com/dansbecker/python-utility-code-for-deep-learning-exercises,Thu Jan 11 2018
415,,AvtanshSharma,[],[],This dataset does not have a description yet.,Other,,[],Other,,,2,25,381, ,CARPARTS,https://www.kaggle.com/avtanshsharma/carparts,Mon Feb 12 2018
416,,Abdullah Karimi,"[id, formation_energy_ev_natom, bandgap_energy_ev]","[numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,26,0.0224609375, ,datase_nomad_2,https://www.kaggle.com/abdullahkarimi/datase-nomad-2,Sun Feb 11 2018
417,,rmtmtr,"[Category, Item, Serving Size, Calories, Calories from Fat, Total Fat, Total Fat (% Daily Value), Saturated Fat, Saturated Fat (% Daily Value), Trans Fat, Cholesterol, Cholesterol (% Daily Value), Sodium, Sodium (% Daily Value), Carbohydrates, Carbohydrates (% Daily Value), Dietary Fiber, Dietary Fiber (% Daily Value), Sugars, Protein, Vitamin A (% Daily Value), Vitamin C (% Daily Value), Calcium (% Daily Value), Iron (% Daily Value)]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,19,0.0283203125, ,menu.csv,https://www.kaggle.com/rmtmtr/menucsv,Mon Feb 12 2018
418,,Rohan.Pote,"[lat, lng, desc, zip, title, timeStamp, twp, addr, e]","[numeric, numeric, string, string, string, dateTime, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,37,4, ,911 data,https://www.kaggle.com/rohanpote/911-data,Mon Feb 12 2018
419,,Kun Hao Yeh,[],[],This dataset does not have a description yet.,Other,,[],CC3,,,2,50,1024,fasttext pretrained word vectors,FastText Pretrained Wordvec,https://www.kaggle.com/khyeh0719/fasttext-pretrained-wordvec,Sun Feb 04 2018
420,,ChamberUnderground,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,56,195,0.541015625, ,movie_metadata.csv,https://www.kaggle.com/karrrimba/movie-metadatacsv,Tue Jan 09 2018
421,,AnilKumarPallekonda,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,15,149,11, ,Keras-MNIST,https://www.kaggle.com/apallekonda/keras-mnist,Sat Dec 09 2017
422,,dingo,"[X.2, X.1, X, Id, Date, ID, TurtleExactCountSC, turtleexactdiscretizeSC, TurtleExactCountNC, TurtleExactCombined, TurtleexactdiscretizeNC, TurtleAttackActivity, TurtleAttackActivityDiscretized, Area, Location, Time, Species, Attack, Timeofattack, Beach, County, MoonPhaseExtended, MoonPhase, Precipitation_Value, StationPressure, WindSpeed, Salinity, Turbidity, Temperature, DissovedO2, PrecipitationValueMod, StationPressureMod, WindSpeedMod, SalinityMod, TurbidityMod, TemperatureMod, DissovedO2Mod, DissolvedO2discretize, salinitydiscretize, turbiditydiscretize, temperaturediscretize, precipitationdiscretize, pressurediscretize, windspeeddiscretize, prepmovingaverage, precipitationmvadiscretize, CrabLandings, CrabLandingsnormalised, CrabLandingsDisc, Degree, Direction, MoonPhase3daysextended, MoonPhase4daysextended, zscorewatertemp, changetemp, Precipitation_Normalised, StationPressure_Normalised, WindSpeed_Normalised, Salinity_Normalised, Turbidity_Normalised, Dissolved02_Normalised, Precipitation_minmax, StationPressure_minmax, WindSpeed_minmax, Salinity_minmax, Turbidity_minmax, Dissolved02_minmax, WaterTemp_minmax, Turtle_minmax, Crablandings_minmax, turbidity_kmeans_binning, turbidty_domain]","[numeric, numeric, numeric, numeric, dateTime, numeric, numeric, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, numeric, string, numeric, numeric, string, numeric, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context It would be pretty awesome if you can predict shark attacks ...right??? well this is your chance now . Content This dataset has 187 rows and 72 columns . Some of the variables are  SalinityTurbidityDissovedO2Temperatureprecipitation state etc..  Acknowledgements This dataset was obtained from AKDD Research Inspiration Use the dataset and implement Machine Learning Algorithms for prediction of shark attacks. ,CSV,,"[classification, random forest, xgboost, svm]",Other,,,5,58,0.0966796875,Prediction of Shark Attacks,Shark Attack,https://www.kaggle.com/dingo1694/shark-attack,Tue Feb 20 2018
423,,Carlos Rafael,[],[],Context A zip file from SWIFTKEY in partnership with the Johns Hopkins Data Science Specialization. This Natural Language Processing dataset include millions of tweets blog posts and news articles in multiple languages.  Content This RAW data is zipped. It is readable as a large character data type. Select which file (based on language) that you wish to download and the source. Here is the list of files  Name    Length                Date 1                         final/         0 2014-07-22 101000 2                   final/de_DE/         0 2014-07-22 101000 3  final/de_DE/de_DE.twitter.txt  75578341 2014-07-22 101100 4    final/de_DE/de_DE.blogs.txt  85459666 2014-07-22 101100 5     final/de_DE/de_DE.news.txt  95591959 2014-07-22 101100 6                   final/ru_RU/         0 2014-07-22 101000 7    final/ru_RU/ru_RU.blogs.txt 116855835 2014-07-22 101200 8     final/ru_RU/ru_RU.news.txt 118996424 2014-07-22 101200 9  final/ru_RU/ru_RU.twitter.txt 105182346 2014-07-22 101200 10                  final/en_US/         0 2014-07-22 101000 11 final/en_US/en_US.twitter.txt 167105338 2014-07-22 101200 12    final/en_US/en_US.news.txt 205811889 2014-07-22 101300 13   final/en_US/en_US.blogs.txt 210160014 2014-07-22 101300 14                  final/fi_FI/         0 2014-07-22 101000 15    final/fi_FI/fi_FI.news.txt  94234350 2014-07-22 101100 16   final/fi_FI/fi_FI.blogs.txt 108503595 2014-07-22 101200 17 final/fi_FI/fi_FI.twitter.txt  25331142 2014-07-22 101000 Acknowledgements Thank you to Swiftkey an the Johns Hopkins Bloomberg School of Public Health.  Inspiration How does the 140 character limit twitter cause language to change?  Can we predict the future development of innovative language (idk lol idr nbd...)? Is this enough data to make an accurate predictive text app for texting? tweeting? writing full articles?  ,Other,,"[languages, linguistics, twitter, internet]",CC0,,,17,135,548,"NLP - Tweets, Blogs, and News Articles 4 million text entries",Tweets Blogs News - Swiftkey Dataset 4million ,https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million,Wed Dec 06 2017
424,,MirrorLu,"[test_id, name, item_condition_id, category_name, brand_name, shipping, item_description]","[numeric, string, numeric, string, numeric, numeric, string]",This dataset does not have a description yet.,Other,,[],CC0,,,38,157,188, ,net shopping,https://www.kaggle.com/mirrorlu/net-shopping,Wed Dec 27 2017
425,,Juanu,"[English short name lower case, Alpha-2 code, Alpha-3 code, Numeric code, ISO 3166-2]","[string, string, string, numeric, string]",Context This dataset was uploadedto be able to link the Countries ISO codes to any data in a better way than just names. This Dataset can give the opportunity to improve current and new Notebooks as well as other datasets. Libraries like plotly use country codes to easily identify the data linked to the country. This dataset can help with that task. Content The dataset contains a list of ALL the states and their codes. Columns  - Alpha-2 code The alpha-2 code of the country (2 characters)  - Alpha-3 code The alpha-3 code of the country (3 characters)  - Numeric code The numeric code of the country (int)  - ISO 3166-2 The ISO 3166-2 code. Formatted as ISO 3166-2[2 characters] Acknowledgements https//gist.github.com/radcliff/f09c0f88344a7fcef373 Inspiration Any dataset that contains a country column can be linked to this dataset and be used to link other data as well as plotting MAPS. Libraries like plotly use country codes to easily identify the data linked to the country. This dataset can help with that task.,CSV,,[],CC0,,,42,308,0.0087890625,List of countries of the world with their ISO codes,Countries ISO Codes,https://www.kaggle.com/juanumusic/countries-iso-codes,Thu Aug 10 2017
426,,Michael Kazachok,[],[],This dataset does not have a description yet.,Other,,[],Other,,,25,48,32, ,Plant Weight,https://www.kaggle.com/miklgr500/plant-weight,Tue Jan 30 2018
427,,Henrietta,"[RT @BlossomBeautyEd: 💜 WAXING MODELS 💜 tomorrow at 5pm looking for legs • standard bikini • underarm • facial waxing. If you have hair in a…, 20, 23, 59, , ]","[string, numeric, numeric, numeric, string, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,5,0.6923828125, ,edinburgh-bigtweet,https://www.kaggle.com/henriettabaker/edinburghbigtweet,Thu Feb 22 2018
428,,Shahid Ali,"[, close, high, low, open, time, volumefrom, volumeto, timestamp]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime]",This dataset does not have a description yet.,CSV,,[],CC0,,,6,68,0.3447265625, ,Bitcoin,https://www.kaggle.com/appinventorpak1/bitcoin,Sun Feb 18 2018
429,,Max Baas,"[Store, Dept, Date, Weekly_Sales, IsHoliday, Temperature, Fuel_Price, MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5, CPI, Unemployment, Type, Size]","[numeric, numeric, dateTime, numeric, boolean, numeric, numeric, string, string, string, string, string, numeric, numeric, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,10,8, ,train.csv,https://www.kaggle.com/maxbaas/traincsv,Sun Feb 18 2018
430,,Mike Krisko,"[Year, Month, State, County, Rate]","[numeric, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,45,6, ,Unemployment by County,https://www.kaggle.com/mikekrisko/unemployment-by-county,Thu Feb 01 2018
431,,Eduardo Morelli,[],[],Context Starting with 20 years data scrapped from NUFORC (from 97 to 2017) plus Mr. Ajayrana UFO report dataset (1949 to 2000) my idea was since the beginning to be able to predict given sight data which shape the UFO would be. Therefore besides NUFORC data I captured data from Wunderground related to those occurrences. Content First of all I did some web scrapping over NUFORC website collecting data from 1997 to 2017 using R. Then I created another Python program running periodically (every 30 minutes) reading 10 rows at a time from Wunderground but only for those cities dates having sight records  Acknowledgements Thanks to Wunderground for letting me capture small amounts of historical data every day. And thanks to Alura one of the best online educating platforms I ever used and for letting me host some of my courses about Big Data. Also I must thank Mr. Ajayrana for his nice work Inspiration It is an ongoing work. Soon I will have more data and be able to create an awesome prediction model,CSV,,[weather],CC0,,,10,95,2,20 years UFO Sights data along with corresponding weather data,Consolidated UFO and Weather Data,https://www.kaggle.com/emorelli/consolidated-ufo-weather-data,Thu Feb 08 2018
432,,HiroyukiSHINODA,[brand_name],[string],This dataset does not have a description yet.,CSV,,[],CC0,,,1,18,330, ,brand_blacklist.csv,https://www.kaggle.com/mirandora/brand-blacklistcsv,Tue Feb 13 2018
433,,Rahul Bairathi,[],[],Context Mr. President has been using Twitter a lot to share his thoughts with the world. His posts have been receiving variety of responses from the world Content The dataset consists of post texts and stats of the tweets posted by Mr. Donald Trump since October 2016 which was the beginning of presidential elections The first thing I am doing with this data is to perform sentiment analysis of the post text.,Other,,[],GPL,,,5,38,0.38671875,Twitter feeds of Donald Trump since 2016,Donald Trump Twitter Feeds,https://www.kaggle.com/bairathirahul/donald-trump-twitter-feeds,Sun Feb 11 2018
434,,Aruna Jayasena,[],[],Context There are no any data-sets for distinguishing paddy plants from other weeds. This data-set was taken from different fields from Sri Lanka. Content This data-set contains 1200 images of paddy and other weeds. Acknowledgements This data can be be used for any sort of machine learning projects. Use this for learning purposes only. Inspiration this data-set was developed for Semester project of the university.,Other,,[],Other,,,4,43,52,this data-set contains over 1000 images of paddy plants and weeds.,Paddy-Grass Distinguisher,https://www.kaggle.com/archfx/paddygrass-distinguisher,Thu Feb 15 2018
435,,Tommy Wu,[],[],"Context I'm trying to make a Choropleth map over time of home sale prices by block in Brooklyn for the last 15 years to visualize gentrification. I have the entire dataset for all 5 boroughs of New York but am starting with Brooklyn. Content and Acknowledgements Primary dataset is the NYC Housing Sales Data Found in this Link http//www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page The data in all the separate excel spreadsheets for 2003-2017 was merged via VBA scripting in Excel and further cleaned & de-duped in R Additionally in my hunt for shapefiles I discovered these wonderful shapefiles from NYCPluto https//www1.nyc.gov/site/planning/data-maps/open-data/dwn-pluto-mappluto.page I left joined it by ""Block"" & ""Lot"" onto the primary data frame but 25% of the block/lot combo's ended up not having a corresponding entry in the Pluto shapefile and are NAs. Note that as in other uploaded datasets of NYC housing on Kaggle many of these transactions have a sale_price of $0 or only a nominal amount far less than market value. These are likely property transfers to relatives and should be excluded from any analysis of market prices. Inspiration Can you model Brooklyn home prices accurately?",Other,,[housing],CC0,,,78,365,77,Brooklyn New York housing and GIS data,"Brooklyn Home Sales, 2003 to 2017",https://www.kaggle.com/tianhwu/brooklynhomes2003to2017,Fri Feb 16 2018
436,,Miaomiao,[],[],This dataset does not have a description yet.,Other,,[],CC4,,,48,197,125, ,Loan_Forecast,https://www.kaggle.com/jinmm1992/loanforecast,Thu Nov 16 2017
437,,AntonioIvanovski,"[id, host_response_time, host_response_rate, host_is_superhost, host_has_profile_pic, neighbourhood_cleansed, latitude, longitude, is_location_exact, property_type, room_type, accommodates, bathrooms, bedrooms, beds, bed_type, amenities, square_feet, price, guests_included, minimum_nights, maximum_nights, calendar_updated, availability_30, number_of_reviews, review_scores_rating, instant_bookable, is_business_travel_ready, cancellation_policy, require_guest_profile_picture, reviews_per_month]","[numeric, string, string, string, string, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,7,56,2, ,NYC AirBnb Rental data October 2017,https://www.kaggle.com/ivanovskia1/nyc-airbnb-rental-data-october-2017,Thu Feb 15 2018
438,,Jon Doc,"[number, air_pressure_9am, air_temp_9am, avg_wind_direction_9am, avg_wind_speed_9am, max_wind_direction_9am, max_wind_speed_9am, rain_accumulation_9am, rain_duration_9am, relative_humidity_9am, relative_humidity_3pm]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,19,0.1640625, ,daily weather,https://www.kaggle.com/doc2lero/daily-weather,Wed Feb 14 2018
439,,EllyMandliel,[],[],This dataset does not have a description yet.,Other,,[],Other,,,2,41,8, ,Headlines,https://www.kaggle.com/yokra909/headlines,Tue Feb 06 2018
440,,Predictive modelling,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,4,89,261, ,Lending club loan data,https://www.kaggle.com/loknath2017/lending-club-loan-data,Fri Feb 02 2018
441,,RajeevkumarYadav,"[Date, Open, High, Low, Close, Adj Close, Volume]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,25,0.1357421875, ,Infosys ,https://www.kaggle.com/rajeev11430/infosys,Mon Feb 12 2018
442,,Keval M,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,1,23,422, ,Rediff-News,https://www.kaggle.com/kevalm/rediffnews,Mon Feb 12 2018
443,,toby2bc,"[stadium_name, stadium_location, stadium_open, stadium_close, stadium_type, stadium_address, stadium_weather_station_code, stadium_weather_type, stadium_capacity, stadium_surface, STATION, NAME, LATITUDE, LONGITUDE, ELEVATION]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context National Football League historic game and betting info Content National Football League (NFL) game results since 1966 with betting odds information since 1979. Dataset was created from a variety of sources including games and scores from a variety of public websites such as ESPN NFL.com and Pro Football Reference. Weather information is from NOAA data with NFLweather.com a good cross reference. Betting data was used from http//www.repole.com/sun4cast/data.html for 1978-2013 seasons. Pro-football-reference.com data was then cross referenced for betting lines and odds as well as weather data. From 2013 on betting data reflects lines available as sportsline.com. Acknowledgements Helpful sites with interest in football and sports betting include https//github.com/fivethirtyeight/nfl-elo-game http//www.repole.com/sun4cast/data.html https//www.pro-football-reference.com/ http//www.espn.com/nfl/ http//www.nflweather.com/ http//www.noaa.gov/weather https//www.sportsline.com/ https//github.com/jp-wright/nfl_betting_market_analysis Inspiration Can you build a predictive model to better predict NFL game outcomes and identify successful betting strategies?,CSV,,"[american football, sports]",CC4,,,12,76,0.291015625,Scores and descriptive game info for National Football League games,NFL scores and betting data,https://www.kaggle.com/tobycrabtree/nfl-scores-and-betting-data,Mon Feb 12 2018
444,,Akash Kumar,"[transaction_id, num_var_1, num_var_2, num_var_3, num_var_4, num_var_5, num_var_6, num_var_7, cat_var_1, cat_var_2, cat_var_3, cat_var_4, cat_var_5, cat_var_6, cat_var_7, cat_var_8, cat_var_9, cat_var_10, cat_var_11, cat_var_12, cat_var_13, cat_var_14, cat_var_15, cat_var_16, cat_var_17, cat_var_18, cat_var_19, cat_var_20, cat_var_21, cat_var_22, cat_var_23, cat_var_24, cat_var_25, cat_var_26, cat_var_27, cat_var_28, cat_var_29, cat_var_30, cat_var_31, cat_var_32, cat_var_33, cat_var_34, cat_var_35, cat_var_36, cat_var_37, cat_var_38, cat_var_39, cat_var_40, cat_var_41, cat_var_42, target]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,32,222,13, ,fraud_transaction,https://www.kaggle.com/akashkr/fraud-transaction,Mon Dec 18 2017
445,,XinHu,"[PID, CM_ID, GIS_ID, ST_NUM, ST_NAME, ST_NAME_SUF, UNIT_NUM, ZIPCODE, PTYPE, LU, OWN_OCC, OWNER, MAIL_ADDRESSEE, MAIL_ADDRESS, MAIL CS, MAIL_ZIPCODE, AV_LAND, AV_BLDG, AV_TOTAL, GROSS_TAX, LAND_SF, YR_BUILT, YR_REMOD, GROSS_AREA, LIVING_AREA, NUM_FLOORS, STRUCTURE_CLASS, R_BLDG_STYL, R_ROOF_TYP, R_EXT_FIN, R_TOTAL_RMS, R_BDRMS, R_FULL_BTH, R_HALF_BTH, R_BTH_STYLE, R_BTH_STYLE2, R_BTH_STYLE3, R_KITCH, R_KITCH_STYLE, R_KITCH_STYLE2, R_KITCH_STYLE3, R_HEAT_TYP, R_AC, R_FPLACE, R_EXT_CND, R_OVRALL_CND, R_INT_CND, R_INT_FIN, R_VIEW, S_NUM_BLDG, S_BLDG_STYL, S_UNIT_RES, S_UNIT_COM, S_UNIT_RC, S_EXT_FIN, S_EXT_CND, U_BASE_FLOOR, U_NUM_PARK, U_CORNER, U_ORIENT, U_TOT_RMS, U_BDRMS, U_FULL_BTH, U_HALF_BTH, U_BTH_STYLE, U_BTH_STYLE2, U_BTH_STYLE3, U_KITCH_TYPE, U_KITCH_STYLE, U_HEAT_TYP, U_AC, U_FPLACE, U_INT_FIN, U_INT_CND, U_VIEW]","[string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","This data is from https//data.boston.gov/dataset/property-assessment It is the property assessment for Boston in 2017. It is a more updated version of Boston Housing prices.  Gives property or parcel ownership together with value information which ensures fair assessment of Boston taxable and non-taxable property of all types and classifications. To preserve their integrity the identifiers PID CM_ID GIS_ID ZIPCODE and MAIL_ZIPCODE all are marked with an underscore (""_"") as the last character.",CSV,,[],CC0,,,2,48,9, ,Boston Property Assessment 2017 ,https://www.kaggle.com/mistyhx/boston-property-assessment-2017,Mon Feb 05 2018
446,,nailo,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,40,8, ,gru_ensemble_data_2,https://www.kaggle.com/nailo2c/gru-ensemble-data-2,Mon Jan 22 2018
447,,Resul CALISKAN,[],[],This dataset does not have a description yet.,Other,,[],Other,,,4,22,0.0205078125, ,GDP_growth,https://www.kaggle.com/resulcaliskan/gdp-growth,Sat Feb 17 2018
448,,psparks,"[aisle_id, aisle]","[numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,126,323,197, ,Instacart Market Basket Analysis,https://www.kaggle.com/psparks/instacart-market-basket-analysis,Mon Nov 20 2017
449,,kiweee,[],[],This dataset does not have a description yet.,{}JSON,,[],CC0,,,2,49,15, ,meta_data,https://www.kaggle.com/kiweee/meta-data,Wed Jan 17 2018
450,,Amit Maurya,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,20,0.1962890625, ,sentiment analysis 0-1,https://www.kaggle.com/akm5160/sentiment-analysis-01,Thu Feb 15 2018
451,,Lisette,[],[],File descriptions  train.csv - the training set test.csv - the test set data_description.txt - full description of each column originally prepared by Dean De Cock but lightly edited to match the column names used here sample_submission.csv - a benchmark submission from a linear regression on year and month of sale lot square footage and number of bedrooms  Data fields Here's a brief version of what you'll find in the data description file.  SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict. MSSubClass The building class MSZoning The general zoning classification LotFrontage Linear feet of street connected to property LotArea Lot size in square feet Street Type of road access Alley Type of alley access LotShape General shape of property LandContour Flatness of the property Utilities Type of utilities available LotConfig Lot configuration LandSlope Slope of property Neighborhood Physical locations within Ames city limits Condition1 Proximity to main road or railroad Condition2 Proximity to main road or railroad (if a second is present) BldgType Type of dwelling HouseStyle Style of dwelling OverallQual Overall material and finish quality OverallCond Overall condition rating YearBuilt Original construction date YearRemodAdd Remodel date RoofStyle Type of roof RoofMatl Roof material Exterior1st Exterior covering on house Exterior2nd Exterior covering on house (if more than one material) MasVnrType Masonry veneer type MasVnrArea Masonry veneer area in square feet ExterQual Exterior material quality ExterCond Present condition of the material on the exterior Foundation Type of foundation BsmtQual Height of the basement BsmtCond General condition of the basement BsmtExposure Walkout or garden level basement walls BsmtFinType1 Quality of basement finished area BsmtFinSF1 Type 1 finished square feet BsmtFinType2 Quality of second finished area (if present) BsmtFinSF2 Type 2 finished square feet BsmtUnfSF Unfinished square feet of basement area TotalBsmtSF Total square feet of basement area Heating Type of heating HeatingQC Heating quality and condition CentralAir Central air conditioning Electrical Electrical system 1stFlrSF First Floor square feet 2ndFlrSF Second floor square feet LowQualFinSF Low quality finished square feet (all floors) GrLivArea Above grade (ground) living area square feet BsmtFullBath Basement full bathrooms BsmtHalfBath Basement half bathrooms FullBath Full bathrooms above grade HalfBath Half baths above grade BedroomAbvGr Bedrooms above grade (does NOT include basement bedrooms) KitchenAbvGr Kitchens above grade KitchenQual Kitchen quality TotRmsAbvGrd Total rooms above grade (does not include bathrooms) Functional Home functionality rating Fireplaces Number of fireplaces FireplaceQu Fireplace quality GarageType Garage location GarageYrBlt Year garage was built GarageFinish Interior finish of the garage GarageCars Size of garage in car capacity GarageArea Size of garage in square feet GarageQual Garage quality GarageCond Garage condition PavedDrive Paved driveway WoodDeckSF Wood deck area in square feet OpenPorchSF Open porch area in square feet EnclosedPorch Enclosed porch area in square feet 3SsnPorch Three season porch area in square feet ScreenPorch Screen porch area in square feet PoolArea Pool area in square feet PoolQC Pool quality Fence Fence quality MiscFeature Miscellaneous feature not covered in other categories MiscVal $Value of miscellaneous feature MoSold Month Sold YrSold Year Sold SaleType Type of sale SaleCondition Condition of sale  Acknowledgments Using data from House Prices Advanced Regression Techniques  2 attributes corrected from the description KitchenAbvGr and BedroomAbvGr,CSV,,[],CC0,,,15,67,0.9130859375, ,House Prices dataset,https://www.kaggle.com/lespin/house-prices-dataset,Mon Feb 19 2018
452,,ShubhamGupta,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,29,4, ,sampleSubmission2,https://www.kaggle.com/shubham14gupta/samplesubmission2,Sun Feb 04 2018
453,,setuc,"[id, identity_hate, insult, obscene, severe_toxic, threat, toxic]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,12,10, ,logisticsubmission,https://www.kaggle.com/setuch/logisticsubmission,Tue Feb 20 2018
454,,Rajkiran Veldur,"[test_id, name, item_condition_id, category_name, brand_name, shipping, item_description]","[numeric, string, numeric, string, string, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,37,59, ,test.csv,https://www.kaggle.com/rveldur/testcsv,Wed Jan 17 2018
455,,Jan Nordin,"[eventid, iyear, imonth, iday, extended, country, country_txt, region, region_txt, provstate, city, latitude, longitude, specificity, vicinity, location, summary, crit1, crit2, crit3, doubtterr, alternative, alternative_txt, multiple, success, suicide, attacktype1, attacktype1_txt, targtype1, targtype1_txt, targsubtype1, targsubtype1_txt, corp1, target1, natlty1, natlty1_txt, gname, motive, guncertain1, individual, nperps, nperpcap, claimed, compclaim, weaptype1, weaptype1_txt, weapsubtype1, weapsubtype1_txt, nkill, nkillter, nwound, nwoundte, property, propextent, propextent_txt, ishostkid, nhostkid, dbsource]","[numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, string, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, string, numeric, string, string]",Context This is the data set of the 170.000+ global terrorist attacks during 1970-2016. I've removed many of the text columns since these can be found as categorized data as well. The number of columns in this data set is 58. In the original source file it was 135. Content The columns in this data set are the following eventid iyear imonth iday extended country country_txt region region_txt provstate city latitude longitude specificity vicinity location summary crit1 crit2 crit3 doubtterr alternative alternative_txt multiple success suicide attacktype1 attacktype1_txt targtype1 targtype1_txt targsubtype1 targsubtype1_txt corp1 target1 natlty1 natlty1_txt gname motive guncertain1 individual nperps nperpcap claimed compclaim weaptype1 weaptype1_txt weapsubtype1 weapsubtype1_txt nkill nkillter nwound nwoundte property propextent propextent_txt ishostkid nhostkid dbsource,CSV,,[terrorism],Other,,,7,76,17,"A slimmer version, 58 columns vs 135, of the GTD 1970-2016 dataset. ",Global Terrorism Database_compact version,https://www.kaggle.com/northon/globalterrorismdatabase-compact,Thu Feb 15 2018
456,,Andrei Dukhounik,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,40,4, ,try-ohoho12,https://www.kaggle.com/dukhovnik/try-ohoho12,Fri Jan 12 2018
457,,Munir,"[test_id, name, item_condition_id, category_name, brand_name, shipping, item_description]","[numeric, string, numeric, string, numeric, numeric, string]",This dataset does not have a description yet.,Other,,[],Other,,,3,27,188, ,Mercari_DataSet,https://www.kaggle.com/munirbd/mercari-dataset,Sun Feb 04 2018
458,,Peter Wu,"[id, identity_hate, insult, obscene, severe_toxic, threat, toxic]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,25,10, ,tox_loj,https://www.kaggle.com/peterwu1/tox-loj,Mon Feb 12 2018
459,,shivrajp,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,25,20, ,blend_toxic,https://www.kaggle.com/shivrajp/blend-toxic,Sun Feb 11 2018
460,,NaomiNguyen,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,34,10, ,LSTM32_0131,https://www.kaggle.com/naominguyen7/lstm320131,Tue Jan 30 2018
461,,Sudheer Sankar,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,1,10,0.541015625, ,movie_dataset,https://www.kaggle.com/sudheersankar/movie-dataset,Wed Feb 21 2018
462,,Rick Chen,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,38,8, ,final2,https://www.kaggle.com/ddongchen/final2,Tue Jan 09 2018
463,,Abdullah Karimi,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,2,30,8, ,data_wordls,https://www.kaggle.com/abdullahkarimi/data-wordls,Tue Feb 06 2018
464,,floser,"[Kilometres, Zone, Bonus, Make, Insured, Claims, Payment]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset is used in Ewards W. Frees' book ""Regression Modeling with Actuarial and Financial Applications""(Cambridge University Press 2010) Chapter 20.5 ""Case Study Swedish Automobile Claims"". It can be found on the book's Web site http//research.bus.wisc.edu/RegActuaries als well as related SAS- and R-Codes.  Content These data were compiled by the Swedish Committee on the Analysis of Risk Premium in Motor Insurance summarized in Hallin and Ingenbleek (1983) and Andrews and Herzberg (1985). The data are cross-sectional describing third party automobile insurance claims for the year 1977. The outcomes of interest are the number of claims (the frequency) and sum of payments (the severity) in Swedish kroners. Outcomes are based on 5 categories of distance driven by a vehicle broken down by 7 geographic zones 7 categories of recent driver claims experience and 9 types of automobile. Even though there are 2205 potential distance zone experience and type combinations (5 x 7 x 7 x 9 = 2205) only n = 2182 were realized in the 1977 data set. Variable names Kilometres Zone Bonus Make Insured Claims Payment Acknowledgements Pleas cite/acknowledge Marc Hallin & Jean-François Ingenbleek The Swedish automobile portfolio in 1977 Scandinavian Actuarial Journal Vol. 1983 Iss. 1 1983 Andrews David F. Herzberg A.M. Data A Collection of Problems from Many Fields for the Student and Research Worker Springer Series in Statistics 1985 Inspiration This upload shall enable actuarial kernels with R an Python",CSV,,[],Other,,,4,51,0.048828125, ,Swedish Motor Insurance,https://www.kaggle.com/floser/swedish-motor-insurance,Sun Feb 11 2018
465,,Nurrizky I,"[kode_provinsi, nama_provinsi, sektor_ekonomi, nilai]","[numeric, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,6,42,0.017578125, ,Indonesia Goverment Debt,https://www.kaggle.com/bocahrokok/indonesia-govdebt,Fri Feb 16 2018
466,,fabiolux,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,50,66, ,favorita 19,https://www.kaggle.com/fabioluciani/favorita-19,Mon Jan 08 2018
467,,Alexander Kireev,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,13,10, ,ch_9792,https://www.kaggle.com/alexanderkireev/ch-9792,Sun Feb 18 2018
468,,Ivan,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],GPL,,,17,127,13, ,toxic-xgboost,https://www.kaggle.com/demesgal/toxicxgboost,Fri Dec 29 2017
469,,Nitin,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,2,52,4, ,imputed_train,https://www.kaggle.com/tundraman/imputed-train,Thu Jan 04 2018
470,,Song WanG,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,39,237, ,everything used ,https://www.kaggle.com/swang215/everything-used,Thu Jan 11 2018
471,,PenPen,[],[],"Context I was doing some Reddit Prawing and just came across this.  Details and most of the code for prawing are here.  https//cuddle-cuddle.github.io/Tell-Me-A-Joke-(Part2)/ https//cuddle-cuddle.github.io/Tell-Me-A-Joke-(Part2)/ Content Jokes from 2010 to Feb.2018 with score 5+.  Rows  csv.writer(csvfile quoting=csv.QUOTE_ALL quotechar=""|"" delimiter="""") ... spamwriter.writerow([submission.id submission.scoreq a                 submission.created_utcsubmission.author.name submission.ups submission.upvote_ratio])  This should clear up the parsing format and content of the file.  Acknowledgements I'd like to thank Ginger else I'd never have thought of doing a joke scrape. Inspiration So many things to do What kind of jokes do people like when do people joke about what etc. etc. ",CSV,,"[languages, linguistics, text mining, text data]",Other,,,2,39,21,"all jokes from 2010 to 2018Feb, with score 5+",reddit /r/Jokes,https://www.kaggle.com/cuddlefish/reddit-rjokes,Thu Feb 15 2018
472,,R. Kukuh,"[YearStart, YearEnd, LocationAbbr, LocationDesc, DataSource, Topic, Question, Response, DataValueUnit, DataValueType, DataValue, DataValueAlt, DataValueFootnoteSymbol, DatavalueFootnote, LowConfidenceLimit, HighConfidenceLimit, StratificationCategory1, Stratification1, StratificationCategory2, Stratification2, StratificationCategory3, Stratification3, GeoLocation, ResponseID, LocationID, TopicID, QuestionID, DataValueTypeID, StratificationCategoryID1, StratificationID1, StratificationCategoryID2, StratificationID2, StratificationCategoryID3, StratificationID3]","[numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,6,51,8, ,U.S. Chronic Disease Indicators,https://www.kaggle.com/rkukuh/us-chronic-disease-indicators,Sun Feb 04 2018
473,,Abdullah Karimi,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,38,30, ,toxic_ensemble_3,https://www.kaggle.com/abdullahkarimi/toxic-ensemble-3,Sat Jan 27 2018
474,,Wei Chun Chang,"[test_id, price, Table]","[numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,46,6, ,test_price,https://www.kaggle.com/justjun0321/test-price,Fri Jan 05 2018
475,,YangLi,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,24,943, ,pretrainedModel,https://www.kaggle.com/careeryangli/pretrained,Tue Feb 13 2018
476,,John Doe,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,4,34,0.015625, ,height,https://www.kaggle.com/jdoejdoe/height,Mon Feb 12 2018
477,,Idan Erez,"[symbol, rank, price_usd, price_btc, 24h_volume_usd, market_cap_usd, available_supply, total_supply, max_supply, percent_change_1h, percent_change_24h, percent_change_7d, update_time(EST)]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, dateTime]",Context I started collecting data from Coin Market Cap of all crypto currencies and token on a 12 hours cycle. Plan to upload it every weekend. If you need it before for your analysis please PM me. Content All available crypto currency data from coin market cap - Symbol Rank Price USD Price BTC 24 hrs Volume Market Cap Supply Week Day and Hour % changes. Updated every 12 hours - update time for each row is on the last column. Acknowledgements Thanks coinmarketcap.com for the API access Inspiration What correlations are there? Is there a low-risk portfolio? What are the signals before crypto rise or crushes?,CSV,,[finance],CC0,,,9,41,2,Dates: 01/26/18 - 02/10/18,All Crypto Data - Every 12 hrs,https://www.kaggle.com/idanerez/all-cryoto-data-every-12-hrs,Sun Feb 11 2018
478,,shreyas,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,10,191, ,Artist-classifier1,https://www.kaggle.com/shreyaseshadri/artistclassifier1,Sun Feb 18 2018
479,,voronwe2007,[],[],This dataset does not have a description yet.,Other,,[],GPL,,,4,60,10, ,AIC Logistic Model,https://www.kaggle.com/voronwe2007/aic-logistic-model,Fri Dec 29 2017
480,,Sachin Patel,[],[],Context For recognising handwritten forms the very first step was to gather data in a considerable amount for training. Which I struggled to collect for weeks. Content The dataset contains 26 folders (A-Z) containing handwritten images in size 28*28 pixels each alphabet in the image is center fitted to 20*20 pixel box. Each image is stored as Gray-level Note Might contain some noisy image as well Acknowledgements The images are taken from NIST(https//www.nist.gov/srd/nist-special-database-19) and NMIST large dataset and few other sources which were then formatted as mentioned above. Inspiration The dataset would serve beginners in machine learning for there created predictive model to recognise handwritten characters.,Other,,"[writing, linguistics, machine learning]",CC0,,,109,549,82,3700000+ English Alphabets Image Data-set,A-Z Handwritten Alphabets in .csv format,https://www.kaggle.com/sachinpatel21/az-handwritten-alphabets-in-csv-format,Fri Feb 16 2018
481,,Kewal Kishan Gokuldas,"[id, comment_text]","[string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,37,27, ,MERCARI_SUB,https://www.kaggle.com/kewalkishang/mercari-sub,Wed Feb 14 2018
482,,avinash madasu,"[ItemID, Sentiment, SentimentSource, SentimentText]","[numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,4,74,56, ,Amazon reviews,https://www.kaggle.com/avinashsai/amazon-reviews,Sat Jan 20 2018
483,,ningenjanai,"[time_created, date_created, up_votes, down_votes, title, over_18, author, subreddit]","[numeric, dateTime, numeric, numeric, string, boolean, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,1,30,27, ,worldnews Reddit,https://www.kaggle.com/ningenjanai/worldnews-reddit,Sun Feb 04 2018
484,,Sharoon Saxena,"[RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,2,29,0.6533203125, ,Churn Modelling,https://www.kaggle.com/lonewolf95/churn-modelling,Sat Feb 10 2018
485,,ShivamGoel,"[Case Id, Case Status, JT_EXECUTIVE_FLAG, Prevailing Wage filter, Longitude, Latitude, SoC_COMPUTER_FLAG, Employer Name, SoC_UPPERCASE_FLAG, JT_PROFESSOR_FLAG, Full Time Position, Job Title, Number of Records, Prevailing Wage, JT_SENIOR_FLAG, JT_DATASCIENCE_FLAG, Emp_UNIVERSITY_FLAG, Soc Name, Worksite - Split 1, Worksite - Split 2, COAST_FLAG, Year]","[numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,6,65,46, ,H1B Visa data,https://www.kaggle.com/sg1791/h1b-visa-data,Thu Jan 11 2018
486,,Rishi Pandey,"[project_id, final_status]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,38,20, ,Funding Successful Projects,https://www.kaggle.com/ripand/funding-successful-projects,Sat Jan 20 2018
487,,Teresa,"[business_id, name, neighborhood, address, city, state, postal_code, latitude, longitude, stars, review_count, is_open, categories]","[string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],Other,,,1,39,13, ,yelp_business.csv,https://www.kaggle.com/teresayeung/yelp-businesscsv,Mon Feb 05 2018
488,,Abdullah Karimi,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,8,32,30, ,dataset_2_6_tox,https://www.kaggle.com/abdullahkarimi/dataset-2-6-tox,Wed Feb 07 2018
489,,Thawatchai Rangsihiranrat,"[longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],ODbL,,,6,36,0.390625, ,California Housing Dataset,https://www.kaggle.com/thawatchai2018/california-housing-dataset,Tue Feb 13 2018
490,,Ashley93,"[MeetID, MeetPath, Federation, Date, MeetCountry, MeetState, MeetTown, MeetName]","[numeric, string, string, dateTime, string, string, string, string]",Context These data come from the OpenPowerlifting project and can be found at http//www.openpowerlifting.org/data.html and https//github.com/sstangl/openpowerlifting. I hope to contribute to this project by providing compelling and informative visuals. I am also building visuals from these data and posting them to https//public.tableau.com/profile/ashley.e.bleggi#!/ Content These data track meets and lifters with over 300000 entries and over 8000 meets world-wide. They include information on lifter statistics wilks scores and date of competition. Acknowledgements This page uses data from the OpenPowerlifting project http//www.openpowerlifting.org. You may download a copy of the data at https//github.com/sstangl/openpowerlifting. Inspiration I am just starting my career as a powerlifter and hope to continue participating in the sport for as long as possible. I hope to explore and learn about the performance and relationships between those that came before me and are currently competing as recorded in these data. I don't currently have a question and am in the initial exploratory phase of my analysis.,CSV,,"[sports, weight training]",CC0,,,5,83,9,Contributions to the OpenPowerlifting Project,OpenPowerlifting,https://www.kaggle.com/ashley93/openpowerlifting,Tue Feb 06 2018
491,,Arun Godwin Patel,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,21,6,Graphics for my Titanic kernel,Titanic Images,https://www.kaggle.com/agodwinp/titanic-images,Fri Feb 09 2018
492,,Colby,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,2,12,0.013671875, ,data.csv,https://www.kaggle.com/comcgrath4/datacsv,Mon Feb 19 2018
493,,Alexander Kireev,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,5,85,13, ,knn_support,https://www.kaggle.com/alexanderkireev/knn-support,Sun Jan 07 2018
494,,Sergey,[],[],Introduction Explore the archive of relevant economic information relevant news on all indicators with explanations data on past publications on the economy of the United States Britain Japan and other developed countries volatility assessments and much more. For the construction of their forecast models the use of in-depth training is optimal with a learning model built on the basis of EU and Forex data.     The economic calendar is an indispensable assistant for the trader. Data set The data set is created in the form of an Excel spreadsheet (two files 2011-2013 2014-2018) which can be found at boot time. You can see the source of the data on the site https//www.investing.com/economic-calendar/   column - Event date column - Event time (time New York) column - Country of the event column - The degree of volatility (possible fluctuations in currency indices etc.) caused by this event column - Description of the event column - Evaluation of the event according to the actual data which came out better than the forecast worse or correspond to it column - Data format (% K x103 M x106 T x109) column - Actual event data column - Event forecast data column - Previous data on this event (with comments if there were any interim changes).  Inspiration  Use the historical EU in conjunction with the Forex data (exchange rates indices metals oil stocks) to forecast subsequent Forex data in order to minimize investment risks (combine fundamental market analysis and technical). Historical events of the EU used as a forecast of the subsequent (for example the calculation of the probability of an increase in the rate of the Fed). Investigate the impact of combinations of EC events on the degree of market volatility at different time periods. To trace the main trends in the economies of the leading countries (for example a decrease in the demand for unemployment benefits). Use the EU calendar together with the news background archive for this time interval for a more accurate forecast. ,Other,,"[learning, finance, money]",ODbL,,,66,431,3,"Archive of important events, economic news, volatility in a convenient format",Economic calendar (EC) Forex (2011-2017),https://www.kaggle.com/devorvant/economic-calendar,Sun Dec 31 2017
495,,Sylas,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,47,4, ,Santadata,https://www.kaggle.com/jsylas/santadata,Fri Jan 12 2018
496,,invalid username,"[Hotel_name, , Review_Text, Sentiment, Rating_Percentage]","[string, string, string, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,30,0.38671875, ,Trivago Data,https://www.kaggle.com/rohitanil/trivago-data,Thu Feb 15 2018
497,,Traveling Light,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC4,,,6,118,54, ,predict_data,https://www.kaggle.com/youyi1991/predict-data,Wed Feb 14 2018
498,,genexpres,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,4,65,4, ,Patient Characteristics Survey (PCS): 2015,https://www.kaggle.com/genexpres/patient-characteristics-survey-pcs-2015,Sat Jan 06 2018
499,,cpl,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,1,14,6, ,ccv_prb_1,https://www.kaggle.com/cdpilcol/ccv-prb-1,Thu Feb 15 2018
500,,ymtoo,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,54,2048, ,word2vec model,https://www.kaggle.com/ymtoo86/word2vec-model,Thu Jan 11 2018
501,,Vincy C,"[, id, name, rating, address, city, lat, long]","[numeric, string, string, numeric, string, string, numeric, numeric]",Context Boba is an increasingly popular drink in the SF Bay Area and I got bored one day over winter break and decided to make this in the hopes it would become useful later. Have fun! Data is scraped from the Yelp API.  Data Description  id Unique id of the boba shop name  Name of the boba shop rating  Yelp rating of the boba shop on a scale of 1-5 address  Address of the boba shop city  City the boba shop is in lat  Latitude of the boba shop long  Longitude of the boba shop  strong text,CSV,,[],CC0,,,1,51,0.060546875,Explore boba shops in the bay area,Boba Shops in the Bay Area,https://www.kaggle.com/vnxiclaire/bobabayarea,Tue Feb 20 2018
502,,Louis,"[RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,15,0.6533203125, ,Churn_Modelling,https://www.kaggle.com/louishgy/churn-modelling,Thu Feb 22 2018
503,,zyayoung,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,18,252, ,Dogs vs Cats features under resnet50,https://www.kaggle.com/zyayoung/dogs-vs-cats-features-under-resnet50,Sat Feb 10 2018
504,,zarak,[],[],This dataset does not have a description yet.,{}JSON,,[],CC0,,,1,21,0.44140625, ,testfile,https://www.kaggle.com/zaraks/testfile,Tue Feb 13 2018
505,,Abdullah Karimi,"[id, comment_text, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,37,29, ,toxic_train,https://www.kaggle.com/abdullahkarimi/toxic-train,Sun Feb 11 2018
506,,Dylan Rainwater,"[id, title, author, text]","[numeric, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,4,69,46, ,Fake News utkML,https://www.kaggle.com/dylanrainwater/fake-news-comp,Fri Jan 26 2018
507,,Rahulbenal,"[Store_Name, Broad_Category, Fine_Category]","[string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,57,3, ,Business case study: Shopping centre,https://www.kaggle.com/rahulbenal/business-case-study-shopping-centre,Wed Jan 31 2018
508,,dhruv,"[age;""job"";""marital"";""education"";""default"";""balance"";""housing"";""loan"";""contact"";""day"";""month"";""duration"";""campaign"";""pdays"";""previous"";""poutcome"";""y"", job, marital, education, default, balance, housing, loan, contact, day, month, duration, campaign, pdays, previous, poutcome, y]","[string, string, string, string, string, numeric, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,1,16,2, ,UCI_Bank_Marketing dataset,https://www.kaggle.com/dhruv007/ucibankmarketing,Fri Feb 16 2018
509,,RajeevkumarYadav,"[Date, Open, High, Low, Close, Adj Close, Volume]","[dateTime, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC4,,,2,54,2048, ,Infosys DataSet ( News Headlines + Historic Data ),https://www.kaggle.com/rajeev11430/infosys-dataset-news-headlines-historic-data-,Thu Feb 08 2018
510,,Seek4everd,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,7,53,0.025390625, ,olympics_2012,https://www.kaggle.com/everseek/olympics-2012,Tue Feb 13 2018
511,,Joe Young,"[Date, Location, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, WindGustDir, WindGustSpeed, WindDir9am, WindDir3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, Pressure9am, Pressure3pm, Cloud9am, Cloud3pm, Temp9am, Temp3pm, RainToday, RISK_MM, RainTomorrow]","[dateTime, string, numeric, numeric, numeric, string, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, string, numeric, string]",Context This is an easy-to-use binary classification dataset for predicting whether or not it will rain tomorrow.  Use for binary classification modeling. Content The content is daily weather observations from multiple Australian weather stations. The target variable is RainTomorrow which means Did it rain the next day? Yes or No.  So the dataset is already set up for prediction -- no need to change the Target with a lead function. Acknowledgements This dataset comes from the Rattle package. The Rattle package is currently not available to Kaggle's kernels.,CSV,,"[weather, binary classification]",CC0,,,34,211,4,Predict whether or not it will rain tomorrow in Australia,Weather in Australia,https://www.kaggle.com/jsphyg/weather-dataset-rattle-package,Mon Dec 04 2017
512,,Bella,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,2,43,59, ,deleting,https://www.kaggle.com/lbxyzz/textcnn-models,Wed Jan 24 2018
513,,BUPT划水运动员,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,4,44,129, ,Avazu Click-Through Rate Prediction,https://www.kaggle.com/xgdbigdata/avazu-clickthrough-rate-prediction,Sun Feb 11 2018
514,,Silogram,[],[],This dataset does not have a description yet.,Other,,[],Other,,,4,70,25, ,model_preds,https://www.kaggle.com/psilogram/model-preds,Mon Jan 08 2018
515,,Gunjan Pathak,"[, Suburb, Address, Rooms, Type, Price, Method, SellerG, Date, Distance, Postcode, Bedroom2, Bathroom, Car, Landsize, BuildingArea, YearBuilt, CouncilArea, Lattitude, Longtitude, Regionname, Propertycount]","[numeric, string, string, numeric, string, numeric, string, string, dateTime, numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,7,11,0.599609375, ,melb_data.csv,https://www.kaggle.com/gunjanpathak/melb-data,Sat Feb 17 2018
516,,Vitor R. F.,[],[],Context This dataset contains data acquired from a driver’s driving a vehicle through various conditions. The collected data were used in an attempt to predict driver's behaviour in order to improve gearbox control. Content What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents too. Acknowledgements About RAW DATA Contents of CSV files file names yyyy-mm-dd_hh-mm-ss.csv (timestamp of start of data collection) Column 1 Time vector in seconds Column 2 Engine RPM from OBD sensor Column 3 Car’s speed in km/h Column 4 Calculated engine load (in % of max power) Columns 5 - 7 Accelerometer data (XYZ) in G Columns 8 - 10 Gyroscope data (XYZ) in rad/s Columns 11 - 13 Magnetometer data (XYZ) file names yyyy-mm-dd_hh-mm-ss_ext.csv Contains data informed by user Column 1 Timestamp of parameter change Column 2 Passenger count (0 - 5) Column 3 Car’s load (0 - 10) Column 4 Air conditioning status (0 - 4) Column 5 Window opening (0 - 10) Column 6 Radio volume (0 - 10) Column 7 Rain intensity (0 - 10) Column 8 Visibility (0 - 10) Column 9 Driver’s wellbeing (0 - 10) Column 10 Driver’s rush (0 - 10) About PROCESSED DATA Contents of CSV files Column 1 Time (in seconds) Column 2 Vehicle’s speed (in m/s) Column 3 Shift number (0 = intermediate position) Column 4 Engine Load (% of max power) Column 5 Total Acceleration (m/s^2) Column 6 Engine RPM Column 7 Pitch Column 8 Lateral Acceleration (m/s^2) Column 9 Passenger count (0 - 5) Column 10 Car’s load (0 - 10) Column 11 Air conditioning status (0 - 4) Column 12 Window opening (0 - 10) Column 13 Radio volume (0 - 10) Column 14 Rain intensity (0 - 10) Column 15 Visibility (0 - 10) Column 16 Driver’s wellbeing (0 - 10) Column 17 Driver’s rush (0 - 10) Inspiration How efficiently can an automated gearbox be controlled with predictions based on these variables?,Other,,[automobiles],CC0,,,67,496,262,An attempt to predict driver's behaviour,Car trips data log,https://www.kaggle.com/vitorrf/cartripsdatamining,Fri Nov 17 2017
517,,Kamal raj,"[Sentence #, Word, POS, Tag]","[string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,59,3, ,ner_modified_encoding,https://www.kaggle.com/kamalkraj/ner-modified-encoding,Mon Dec 25 2017
518,,Natasha Zope,[],[],"This particular data zip contains all required files about various deodorant products information. This dataset is acquired from a data analysis competition.  Deodorant Liking Prediction Competition The client is one of the leading consumer packaged goods company. The client wants to monitor the performance of 10 popular Deodorants available in the market through a survey. The client has designed 2 survey questionnaires which are described as below  Screener Questionnaire  A screener question is a question that asks 'Are you a relevant person to answer this survey'. This questionnaire will recruit the audience who will be taking up the survey.  Main Questionnaire  The main questionnaire of the survey includes a set of questions which are specifically designed for evaluating a particular product. Following are the ways in which the measures are used by the target audience. (The questions described below are to be referred from the main questionnaire) A. Key Performance Measures Q1 (Instant Liking) Q3 (Strength of Deodorant) Q9 (purchase intent) Q10 (preference vs. usual) Q13 (Liking after 30 minutes) Q14 (overall rating). The difference between Q1 and Q13 is that a scent’s character changes over time. B. Fragrance Attributes Q2 (words) Q4 (scaled fragrance characteristics) Q6 (check all that apply olfactive attributes) Q7. These types of attributes are incredibly important for the client to diagnose the why behind a single fragrance's performance. These attributes help inspire our creative fragrance team and teach them the types of fragrances to create/which to avoid. C. Addiction Associations & Appropriateness Q5 (Addiction) Q8 (fabric/texture). These check all that apply attributes are helpful in terms of how to market a fragrance (thin packaging marketing messages and marketing copy) and also serve as another avenue for consumers to describe a scent. Q11 Q12 (Appropriateness) determine the appropriateness of the Deodorant. D. Respondent Demographics – This will tell us about the target population. All the questions present in the screener questionnaire can be showcased in one tab of the dashboard.  Develop a model that will predict the Instant Liking of the following products  Deodorant B  Deodorant G Deodorant J Deodorant F Deodorant H   FileName  Description Format  Size  Data-Dictionary.xlsx    Data’s metadata xlsx    0.02 MB Deodorant-B.csv Training data set for Deodorant B   csv 0.06 MB Deodorant-F.csv Training data for set Deodorant F   csv 0.06 MB Deodorant-G.csv Training data for set Deodorant G   csv 0.07 MB Deodorant-H.csv Training data for set Deodorant H   csv 0.07 MB Deodorant-J.csv Training data for set Deodorant J   csv 0.06 MB test-data.csv   Test data for all Deodorant B F G H J   csv 0.64 MB sample-submission.csv   Sample submission   csv 0.09 MB   Data Dictionary Here's a brief version of what you'll find in the data description file.  Variable Description Respondent.ID    Respondent ID Product  Unique Product Name (For e.g ""Product B"") Instant.Liking   First Impression about the scent  This dataset is very untidy and its a challenge to all beginners. So accept the challenge and start predicting the deodorant likings with the best accuracy.",CSV,,"[healthcare, product, survey analysis]",CC0,,,6,57,0.2978515625,For Product liking Prediction,Deodorant Liking Dataset,https://www.kaggle.com/nata009/deodorant-liking-dataset,Tue Feb 13 2018
519,,Çağatay ,"[AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4, class]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,16,0.404296875, ,EEG Eye Satte,https://www.kaggle.com/cmylmz/eeg-eye-satte,Thu Feb 22 2018
520,,Andrew Truman,"[, avg_n_pos_per_prev_tenure, avg_pos_len, avg_prev_tenure_len, c_name, m_urn, n_pos, n_prev_tenures, tenure_len, age, beauty, beauty_female, beauty_male, blur, blur_gaussian, blur_motion, emo_anger, emo_disgust, emo_fear, emo_happiness, emo_neutral, emo_sadness, emo_surprise, ethnicity, face_quality, gender, glass, head_pitch, head_roll, head_yaw, img, mouth_close, mouth_mask, mouth_open, mouth_other, skin_acne, skin_dark_circle, skin_health, skin_stain, smile, african, celtic_english, east_asian, european, greek, hispanic, jewish, muslim, nationality, nordic, south_asian, n_followers]","[numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",Context Anonymized data from profiles scraped on LinkedIn. Contains data from about 15000 profiles. Profiles came from people predominantly located in Australia. Includes all their work history as well as analysis of their photo and name. Content What's inside is more than just rows and columns. Make it easy for others to get started by describing how you acquired the data and what time period it represents too. Acknowledgements We wouldn't be here without the help of others. If you owe any attributions or thanks include them here along with any citations of past research. Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?,CSV,,[],CC0,,,67,497,5, ,LinkedIn Profile Data,https://www.kaggle.com/killbot/linkedin,Tue Jan 02 2018
521,,Praveen Murali,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,5,27,0.541015625, ,IMDB movies data,https://www.kaggle.com/praveenm1001/imdb-movies-data,Tue Feb 20 2018
522,,Amit Maurya,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,7,27,0.181640625, ,Dress Sales,https://www.kaggle.com/akm5160/dress-sales,Thu Feb 15 2018
523,,Yoga Wang,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,1,30,0.70703125, ,Economics of privacy,https://www.kaggle.com/raycosine/economics-of-privacy,Fri Feb 09 2018
524,,Nitin Reddy,"[price, test_id]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,19,15, ,rrn_input,https://www.kaggle.com/nitinr/rrn-input,Sun Feb 11 2018
525,,mpossi,[abuses],[string],This dataset does not have a description yet.,CSV,,[],ODbL,,,1,16,160, ,abusive words,https://www.kaggle.com/mmpossi/abusive-words,Mon Feb 19 2018
526,,Jose Cupertino Ruiz,[],[],This dataset does not have a description yet.,Other,,[],Other,,,3,18,0.080078125, ,Transactions from a restaurant,https://www.kaggle.com/jruizvar/transactions,Mon Feb 19 2018
527,,dreamteam,"[Id, MSSubClass, MSZoning, LotFrontage, LotArea, Street, Alley, LotShape, LandContour, Utilities, LotConfig, LandSlope, Neighborhood, Condition1, Condition2, BldgType, HouseStyle, OverallQual, OverallCond, YearBuilt, YearRemodAdd, RoofStyle, RoofMatl, Exterior1st, Exterior2nd, MasVnrType, MasVnrArea, ExterQual, ExterCond, Foundation, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinSF1, BsmtFinType2, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, Heating, HeatingQC, CentralAir, Electrical, 1stFlrSF, 2ndFlrSF, LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, KitchenQual, TotRmsAbvGrd, Functional, Fireplaces, FireplaceQu, GarageType, GarageYrBlt, GarageFinish, GarageCars, GarageArea, GarageQual, GarageCond, PavedDrive, WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, PoolQC, Fence, MiscFeature, MiscVal, MoSold, YrSold, SaleType, SaleCondition, SalePrice]","[numeric, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, numeric, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,37,0.439453125, ,House Prices: Advanced Regression Techniques,https://www.kaggle.com/ngee379k/house-prices-advanced-regression-techniques,Tue Feb 13 2018
528,,ysaz,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,39,4, ,180109_sub_1,https://www.kaggle.com/imanazas/180109-sub-1,Tue Jan 09 2018
529,,invalid username,[],[],This dataset does not have a description yet.,Other,,[],Other,,,3,48,68, ,glove.6B.50d.txt,https://www.kaggle.com/rohitanil/glove6b50dtxt,Tue Jan 23 2018
530,,Kun Hao Yeh,"[ftrl_pred, fm_ftrl_pred, km_0, km_1, km_2, km_3, km_4, km_5, km_6, km_7, km_8, km_9, km_10, km_11, km_12, km_13, km_14, km_15, km_16, km_17, km_18, km_19, km_20, km_21, km_22, km_23, km_24, km_25, km_26, km_27, km_28, km_29, km_30, km_31, price]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,34,35, ,ens local training,https://www.kaggle.com/khyeh0719/ens-local-training,Thu Feb 01 2018
531,,Ram Ramrakhya,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,36,11, ,Mnist Data,https://www.kaggle.com/axel81/mnistdata,Fri Jan 19 2018
532,,Ai-LongZheng,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,4,22,10, ,airbnb test,https://www.kaggle.com/ailongzheng/airbnb-test,Mon Feb 19 2018
533,,hb20007,"[Favorite Color, Favorite Music Genre, Favorite Beverage, Favorite Soft Drink, Gender]","[string, string, string, string, string]",Context Gender is a social construct. The way males and females are treated differently since birth moulds their behaviour and personal preferences into what society expects for their gender. This small dataset is designed to provide an idea about whether a person's gender can be predicted with an accuracy significantly above 50% based on their personal preferences. Content The data was collected and pre-processed in Fall 2015 from university students of 21 nationalities studying various majors in various countries using this form  https//docs.google.com/forms/d/e/1FAIpQLSduEjDURjTh7-a1ZjjlIYx75ScVETLp_gmoFszypz2J7E0LtQ/viewform Inspiration With the rise of feminism the difference between males and females in terms of their personal preferences has decreased in recent years. For instance historically in many cultures warm colors such as red and pink were thought of as feminine colors while cool colors such as blue were considered masculine. Today such ideas are considered outdated. Despite the decrease in the influence of gender on people’s personal preferences can a decent gender classifier be built given a dataset with people’s personal preferences? What does this small dataset suggest?,CSV,,"[gender, psychology]",CC4,,,14,63,0.001953125,Classifying gender based on personal preferences,Gender Classification,https://www.kaggle.com/hb20007/gender-classification,Wed Feb 21 2018
534,,Marco Boaretto,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,1,45,76,hum_of models with 511,subs_511,https://www.kaggle.com/mboaretto/subs-511,Fri Jan 12 2018
535,,manu chowdary,"[Price, Mileage, Make, Model, Trim, Type, Cylinder, Liter, Doors, Cruise, Sound, Leather]","[numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,20,0.0556640625, ,newdata,https://www.kaggle.com/manuchowdary/newdata,Mon Feb 12 2018
536,,Fellipe Gomes,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,23,0.22265625, ,base blocos de carnaval do rio de janeiro,https://www.kaggle.com/gomes555/base-blocos-de-carnaval-do-rio-de-janeiro,Sat Feb 17 2018
537,,Aleksey Bilogur,[],[],This dataset contains exercises for the Advanced Pandas Learn tutorial. It is not meant to be consumed separately.,Other,,[],CC0,,,4,51,0.015625,Dataset with exercises for the Advanced Pandas Learn tutorial,Advanced Pandas Exercises,https://www.kaggle.com/residentmario/advanced-pandas-exercises,Wed Feb 14 2018
538,,richard,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,41,397, ,glove.twitter.27B.100d,https://www.kaggle.com/richardfourd/glovetwitter27b100d,Wed Jan 17 2018
539,,greenet09,"[, test_id, price]","[numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,43,9, ,sub.csv,https://www.kaggle.com/greenet09/subcsv,Fri Jan 12 2018
540,,John Doe,[],[],This dataset does not have a description yet.,Other,,[medicine],Other,,,4,30,0.0224609375,"medicine review, Drug review, Patient review, reviews",Medicinal Reviews,https://www.kaggle.com/cybermed/medicinal-reviews,Sun Feb 18 2018
541,,shiMu,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,60,16, ,cat_out,https://www.kaggle.com/ddongjian0001/cat-out,Mon Jan 08 2018
542,,Prakriti Iyengar,"[action_taken, action_taken_name, agency_code, agency_abbr, agency_name, applicant_ethnicity, applicant_ethnicity_name, applicant_income_000s, applicant_race_1, applicant_race_2, applicant_race_3, applicant_race_4, applicant_race_5, applicant_race_name_1, applicant_race_name_2, applicant_race_name_3, applicant_race_name_4, applicant_race_name_5, applicant_sex, applicant_sex_name, application_date_indicator, as_of_year, census_tract_number, co_applicant_ethnicity, co_applicant_ethnicity_name, co_applicant_race_1, co_applicant_race_2, co_applicant_race_3, co_applicant_race_4, co_applicant_race_5, co_applicant_race_name_1, co_applicant_race_name_2, co_applicant_race_name_3, co_applicant_race_name_4, co_applicant_race_name_5, co_applicant_sex, co_applicant_sex_name, county_code, county_name, denial_reason_1, denial_reason_2, denial_reason_3, denial_reason_name_1, denial_reason_name_2, denial_reason_name_3, edit_status, edit_status_name, hoepa_status, hoepa_status_name, lien_status, lien_status_name, loan_purpose, loan_purpose_name, loan_type, loan_type_name, msamd, msamd_name, owner_occupancy, owner_occupancy_name, preapproval, preapproval_name, property_type, property_type_name, purchaser_type, purchaser_type_name, respondent_id, sequence_number, state_code, state_abbr, state_name, hud_median_family_income, loan_amount_000s, number_of_1_to_4_family_units, number_of_owner_occupied_units, minority_population, population, rate_spread, tract_to_msamd_income]","[numeric, string, numeric, string, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric]",440k rows of Home Mortgage Disclosure Data for state of NY 2015. Data on action applicant ethnicity income gender income and area demographics.,CSV,,[],CC0,,,7,70,28, ,HMDA dataset for New York,https://www.kaggle.com/prakriti73/hmda-dataset-for-new-york,Thu Dec 14 2017
543,,Will Koehrsen,"[row_id, Date, Holiday, site_id]","[numeric, dateTime, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,31,240, ,Data Driven Power Laws Anomaly Detection,https://www.kaggle.com/willkoehrsen/data-driven-power-laws-anomaly-detection,Wed Feb 14 2018
544,,adityapatil,"[div, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,10,183, ,mast_mob_score,https://www.kaggle.com/adityapatil673/mast-mob-score,Sat Feb 17 2018
545,,RDizzl3,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,8,96,68, ,glove6b50d,https://www.kaggle.com/rdizzl3/glove6b50d,Mon Jan 15 2018
546,,soojung,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,6,94,15,Fire-detection-model-Keras  for video ,Fire-detection-model-Keras ,https://www.kaggle.com/csjcsj7477/firedetectionmodelkeras,Fri Jan 12 2018
547,,Manik Soni,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,23,396,BreastCancer Histopathology images,breastCancer68579,https://www.kaggle.com/maniksoni/breastcancer68579,Wed Feb 14 2018
548,,Ana Galuzinskaya,"[attributes.Ambience.divey, attributes.RestaurantsDelivery, attributes.DogsAllowed, postal_code, hours.Thursday, attributes.HairSpecializesIn.coloring, attributes.BestNights.sunday, attributes.BYOB, attributes.AgesAllowed, hours.Friday, latitude, attributes.Alcohol, attributes.Ambience.classy, attributes.RestaurantsTableService, business_id, attributes.Ambience.touristy, attributes.RestaurantsCounterService, attributes.Corkage, attributes.RestaurantsGoodForGroups, categories, name, attributes.BusinessAcceptsBitcoin, attributes.HappyHour, attributes.WheelchairAccessible, attributes.Ambience.hipster, attributes.BusinessAcceptsCreditCards, is_open, attributes.Music.video, attributes.Music.live, attributes.Music.background_music, neighborhood, attributes.BusinessParking.lot, attributes.Music.karaoke, review_count, attributes.ByAppointmentOnly, attributes.NoiseLevel, attributes.HairSpecializesIn.perms, state, attributes.DriveThru, attributes.HasTV, attributes.GoodForMeal.dinner, attributes.BusinessParking.street, address, attributes.RestaurantsAttire, hours.Sunday, attributes.BestNights.tuesday, attributes.DietaryRestrictions.vegetarian, attributes.AcceptsInsurance, attributes.BestNights.wednesday, hours.Wednesday, attributes.HairSpecializesIn.kids, attributes.Open24Hours, attributes.Ambience.trendy, attributes.CoatCheck, hours.Monday, attributes.HairSpecializesIn.straightperms, city, attributes.HairSpecializesIn.curly, attributes.Music.no_music, hours.Tuesday, attributes.HairSpecializesIn.africanamerican, stars, attributes.RestaurantsPriceRange2, attributes.Ambience.intimate, attributes.GoodForMeal.latenight, attributes.GoodForMeal.dessert, attributes.BusinessParking.validated, attributes.GoodForMeal.lunch, attributes.GoodForKids, attributes.DietaryRestrictions.soy-free, attributes.GoodForMeal.brunch, attributes.BusinessParking.valet, longitude, attributes.DietaryRestrictions.gluten-free, attributes.BYOBCorkage, attributes.BusinessParking.garage, attributes.BestNights.friday, hours.Saturday, attributes.Music.dj, attributes.HairSpecializesIn.extensions, attributes.BestNights.saturday, attributes.Ambience.casual, attributes.BestNights.thursday, attributes.BestNights.monday, attributes.HairSpecializesIn.asian, attributes.DietaryRestrictions.kosher, attributes.WiFi, attributes.Smoking, attributes.DietaryRestrictions.halal, attributes.GoodForDancing, attributes.GoodForMeal.breakfast, attributes.Caters, attributes.RestaurantsReservations, attributes.DietaryRestrictions.dairy-free, attributes.DietaryRestrictions.vegan, attributes.Ambience.romantic, attributes.Music.jukebox, attributes.Ambience.upscale, attributes.RestaurantsTakeOut, attributes.BikeParking]","[string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC4,,,2,36,2048, ,Yelp_Data_Challenge,https://www.kaggle.com/agaluzin/yelp-data-challenge,Sun Feb 18 2018
549,,Avinash kumar,"[critic_rating, is_action, is_exclusive_to_us, is_portable, is_role_playing, is_sequel, is_sports, suitable_for_kids, total_earnings, unit_price]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,4,25,0.029296875, ,Sales Training Data,https://www.kaggle.com/avinash29/sales-training-data,Wed Feb 14 2018
550,,Prayan,[],[],This dataset does not have a description yet.,{}JSON,,[],CC0,,,1,38,4, ,test.json,https://www.kaggle.com/rabinandan/testjson,Sat Feb 10 2018
551,,weeliangng,"[ID, Gender, DOB, Lead_Creation_Date, City_Code, City_Category, Employer_Code, Employer_Category1, Employer_Category2, Monthly_Income, Customer_Existing_Primary_Bank_Code, Primary_Bank_Type, Contacted, Source, Source_Category, Existing_EMI, Loan_Amount, Loan_Period, Interest_Rate, EMI, Var1]","[string, string, string, dateTime, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,75,3, ,McKinsey Analytics Hackathon-Bank,https://www.kaggle.com/weeliangng/mckinsey-analytics-hackathonbank,Sat Jan 20 2018
552,,Aswin Ash,"[V1, V2, Class]","[numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,25,0.0009765625, ,mock data,https://www.kaggle.com/aswinps22/mock-data,Sat Feb 17 2018
553,,Akash Kumar,"[transaction_id, num_var_1, num_var_2, num_var_3, num_var_4, num_var_5, num_var_6, num_var_7, cat_var_1, cat_var_2, cat_var_3, cat_var_4, cat_var_5, cat_var_6, cat_var_7, cat_var_8, cat_var_9, cat_var_10, cat_var_11, cat_var_12, cat_var_13, cat_var_14, cat_var_15, cat_var_16, cat_var_17, cat_var_18, cat_var_19, cat_var_20, cat_var_21, cat_var_22, cat_var_23, cat_var_24, cat_var_25, cat_var_26, cat_var_27, cat_var_28, cat_var_29, cat_var_30, cat_var_31, cat_var_32, cat_var_33, cat_var_34, cat_var_35, cat_var_36, cat_var_37, cat_var_38, cat_var_39, cat_var_40, cat_var_41, cat_var_42]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,62,19, ,fraud_trans_testdata,https://www.kaggle.com/akashkr/fraud-trans-testdata,Wed Dec 20 2017
554,,Ching,"[1	Fed official says weak data caused by weather,  should not slow taper	http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310, 0, 1312750.story\?track=rss	Los Angeles Times	b	ddUyU0VZz0BRneMioxUPQVP6sIxvM	www.latimes.com	1394470370698, b, ddUyU0VZz0BRneMioxUPQVP6sIxvM, www.latimes.com, 1394470370698]","[string, string, string, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,7,67,28, ,news_corpora,https://www.kaggle.com/hchings/news-corpora,Thu Dec 28 2017
555,,Laurae,[],[],This dataset does not have a description yet.,Other,,[],Other,,,16,51,291, ,Santa 2017 Competition Lookup Tables,https://www.kaggle.com/laurae2/santa-2017-competition-lookup-tables,Mon Dec 25 2017
556,,qixiang109,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,37,6, ,qixiang109merge25,https://www.kaggle.com/qixiang109/qixiang109merge25,Thu Jan 11 2018
557,,DanB,[],[],Context Video Context Description Build your own version of the SeeFood App from the TV show Silicon Valley.  Whether you've seen the show or not you should watch a refresher on how it works.  This dataset has everything you need to build the SeeFood app. This data was extracted from the Food 101 dataset.  A full version of the dataset is available here. Acknowledgements Original data from this paper.,Other,,[],Other,,,51,354,45, ,Hot Dog - Not Hot Dog,https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog,Thu Jan 04 2018
558,,fabiolux,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,45,140, ,favorita 15,https://www.kaggle.com/fabioluciani/favorita-15,Fri Jan 05 2018
559,,yogender singh,"[Ad 1, Ad 2, Ad 3, Ad 4, Ad 5, Ad 6, Ad 7, Ad 8, Ad 9, Ad 10]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,1,42,0.2001953125, ,Reinforcement learning,https://www.kaggle.com/yogirj/reinforcement-learning,Sun Feb 11 2018
560,,Ankur Joshi,"[tract_to_msamd_income, rate_spread, population, minority_population, number_of_owner_occupied_units, number_of_1_to_4_family_units, loan_amount_000s, hud_median_family_income, applicant_income_000s, state_name, state_abbr, sequence_number, respondent_id, purchaser_type_name, property_type_name, preapproval_name, owner_occupancy_name, msamd_name, loan_type_name, loan_purpose_name, lien_status_name, hoepa_status_name, edit_status_name, denial_reason_name_3, denial_reason_name_2, denial_reason_name_1, county_name, co_applicant_sex_name, co_applicant_race_name_5, co_applicant_race_name_4, co_applicant_race_name_3, co_applicant_race_name_2, co_applicant_race_name_1, co_applicant_ethnicity_name, census_tract_number, as_of_year, application_date_indicator, applicant_sex_name, applicant_race_name_5, applicant_race_name_4, applicant_race_name_3, applicant_race_name_2, applicant_race_name_1, applicant_ethnicity_name, agency_name, agency_abbr, action_taken_name]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,44,194, ,HMDA Data,https://www.kaggle.com/joshiankur/hmda-data,Wed Dec 20 2017
561,,Sowjanya Ambati,"[, loan_amnt, funded_amnt, funded_amnt_inv, installment, annual_inc, dti, delinq_2yrs, fico_range_low, fico_range_high, inq_last_6mths, open_acc, pub_rec, revol_bal, total_acc, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_amnt, last_fico_range_high, last_fico_range_low, collections_12_mths_ex_med, acc_now_delinq, tot_coll_amt, tot_cur_bal, total_rev_hi_lim, acc_open_past_24mths, avg_cur_bal, bc_open_to_buy, bc_util, chargeoff_within_12_mths, delinq_amnt, mo_sin_old_il_acct, mo_sin_old_rev_tl_op, mo_sin_rcnt_rev_tl_op, mo_sin_rcnt_tl, mort_acc, mths_since_recent_bc, mths_since_recent_inq, num_accts_ever_120_pd, num_actv_bc_tl, num_actv_rev_tl, num_bc_sats, num_bc_tl, num_il_tl, num_op_rev_tl, num_rev_accts, num_rev_tl_bal_gt_0, num_sats, num_tl_120dpd_2m, num_tl_30dpd, num_tl_90g_dpd_24m, num_tl_op_past_12m, pct_tl_nvr_dlq, percent_bc_gt_75, pub_rec_bankruptcies, tax_liens, tot_hi_cred_lim, total_bal_ex_mort, total_bc_limit, total_il_high_credit_limit, term, int_rate, grade, sub_grade, emp_title, emp_length, home_ownership, verification_status, loan_status, purpose, addr_state, revol_util, debt_settlement_flag, issue_year, issue_month, earliest_cr_line_month, earliest_cr_line_year, last_pymnt_month, last_pymnt_year, last_credit_pull_month, last_credit_pull_year]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,22,184, ,Lending club,https://www.kaggle.com/sambati/lending-club,Mon Feb 19 2018
562,,Yair Bonastre,"[, Made Donation in March 2007]","[numeric, numeric]",Driven Data - https//www.drivendata.org/competitions/2/warm-up-predict-blood-donations/ We've all got to start somewhere. This is the smallest least complex dataset on DrivenData. That makes it a great place to dive into the world of data science competitions. Get your blood pumping and try your hand at predicting donations. Blood Donations Blood donation has been around for a long time. The first successful recorded transfusion was between two dogs in 1665 and the first medical use of human blood in a transfusion occurred in 1818. Even today donated blood remains a critical resource during emergencies. Red Cross 1943 Our dataset is from a mobile blood donation vehicle in Taiwan. The Blood Transfusion Service Center drives to different universities and collects blood as part of a blood drive. We want to predict whether or not a donor will give blood the next time the vehicle comes to campus.,CSV,,[medicine],Other,,,5,80,0.0146484375, ,Predicting Blood Analysis,https://www.kaggle.com/bonastreyair/predicting-blood-analysis,Sat Feb 17 2018
563,,Elif Demirtas,[],[],This dataset does not have a description yet.,Other,,[],Other,,,5,56,162, ,cifar10,https://www.kaggle.com/elfdmrtas/cifar10,Sun Feb 11 2018
564,,Abdullah Karimi,"[id, formation_energy_ev_natom, bandgap_energy_ev]","[numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,21,0.0244140625, ,nomad_dataset_rec,https://www.kaggle.com/abdullahkarimi/nomad-dataset-rec,Sun Feb 11 2018
565,,personalrealestate1,"[sum_amount, key]","[numeric, string]",Context There are many Property Tax Exemptions in New York City.  A map is a good way to see them.  This data summarizes the tax exemption totals. See https//tax.tidalforce.org/ to see your New York City property tax history. See https//tony.brooklyncoop.org/exemptions to see exemptions on a map. Content It is for the 2016-2017 Fiscal Year. Acknowledgements Thanks to John Krauss at http//taxbills.nyc and NYC Department of Finance. Inspiration How can we understand all these exemptions and make the system more equitable.,CSV,,[],Other,,,2,28,0.0078125,Understand your property tax,2017 New York City Tax Exemptions Summary,https://www.kaggle.com/personalrealestate1/2017-new-york-city-tax-exemptions-summary,Wed Feb 14 2018
566,,Jokingpoet,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,8,40,0.0029296875, ,logistic,https://www.kaggle.com/jokingpoet/logistic,Wed Feb 14 2018
567,,kirillformado,"[id;age;gender;height;weight;ap_hi;ap_lo;cholesterol;gluc;smoke;alco;active;cardio, age, gender, height, weight, ap_hi, ap_lo, cholesterol, gluc, smoke, alco, active, cardio]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,30,0.724609375, ,Cardio,https://www.kaggle.com/kirillformado/cardio,Wed Feb 14 2018
568,,YeongTaek Oh,"[id, comment_text]","[numeric, string]",This dataset does not have a description yet.,CSV,,[],Other,,,6,45,54, ,toxic old dataset,https://www.kaggle.com/dhznsdl/toxic-old-dataset,Sat Jan 20 2018
569,,amrrs,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,2,19,16, ,Udpipe English Model Pretrained,https://www.kaggle.com/nulldata/udpipe-english-model-pretrained,Fri Feb 16 2018
570,,Saiteja Tirunagari,"[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,18,66, ,CreditCard,https://www.kaggle.com/saiteja5662/creditcard,Sun Feb 18 2018
571,,mrinal,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,9,0.59375, ,cnnclassifier,https://www.kaggle.com/mrinalam/cnnclassifier,Thu Feb 22 2018
572,,Rashmi Arora,[],[],This dataset does not have a description yet.,Other,,[],Other,,,2,31,0.1337890625, ,Sample data for Apriori,https://www.kaggle.com/rashmi1735/sample-data-for-apriori,Thu Feb 22 2018
573,,DataMoose,"[car.id, buying, maint, doors, persons, lug_boot, safety]","[numeric, string, string, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,20,0.0126953125, ,cars-final-prediction,https://www.kaggle.com/datamoose/carsfinalprediction,Sun Feb 18 2018
574,,leeum,"[, Invoice/Item Number, Date, Store Number, Store Name, Address, City, Zip Code, Store Location, County Number, County, Category, Category Name, Vendor Number, Vendor Name, Item Number, Item Description, Pack, Bottle Volume (ml), State Bottle Cost, State Bottle Retail, Bottles Sold, Sale (Dollars), Volume Sold (Liters), Volume Sold (Gallons)]","[numeric, string, dateTime, numeric, string, string, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, numeric, string, string, numeric, string, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,7,38,162, ,Vodka Liquor Subset,https://www.kaggle.com/leeums/vodka-liquor-subset,Sun Jan 28 2018
575,,John Doe,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,40,160,1, ,Bitcoin Twitter Feed,https://www.kaggle.com/gwhittington/bitcointwitter,Wed Dec 13 2017
576,,SteveZheng,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,4,50,31, ,airbnb data,https://www.kaggle.com/stevezhenghp/airbnb-data,Tue Feb 13 2018
577,,WayneC,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,48,11, ,mnist_data,https://www.kaggle.com/chanfai514/mnist-data,Tue Jan 09 2018
578,,Hyun Ook Ryu,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,1,74,27, ,Data test,https://www.kaggle.com/ryutek/data-test,Thu Jan 11 2018
579,,Bibin Paul,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,53,4, ,Santa dataset1,https://www.kaggle.com/bibinpaul/santa-dataset1,Wed Jan 10 2018
580,,Rahul Jantwal,[],[],This dataset does not have a description yet.,Other,,[],CC3,,,1,26,107, ,FastText,https://www.kaggle.com/rahuljantwal/fasttext,Thu Feb 15 2018
581,,snehanshusengupta,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,10,42,5, ,traines,https://www.kaggle.com/snehanshu17/traines,Thu Jan 11 2018
582,,Elen Vardanyan,[],[],This dataset does not have a description yet.,Other,,[],Other,,,57,187,0.2265625, ,Customers Data,https://www.kaggle.com/lnvardanyan/customers-data,Tue Dec 12 2017
583,,Dheeraj,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,7,39,0.7265625, ,Telecom_dk,https://www.kaggle.com/dheerajetx/telecom-dk,Tue Feb 13 2018
584,,cyrilv,"[id;age;gender;height;weight;ap_hi;ap_lo;cholesterol;gluc;smoke;alco;active;cardio, age, gender, height, weight, ap_hi, ap_lo, cholesterol, gluc, smoke, alco, active, cardio]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,22,0.724609375, ,mlbootcamp5.train,https://www.kaggle.com/cyrilv3/mlbootcamp5train,Mon Feb 12 2018
585,,Feng Liu,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,24,8192, ,GoogleRecognitionPartial,https://www.kaggle.com/feng3245/googlerecognitionpartial,Sat Feb 17 2018
586,,Chtholly,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,50,6, ,gzt_Mercari2,https://www.kaggle.com/gzt940726/gzt-mercari2,Fri Dec 22 2017
587,,dgoke1,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, Application ID, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager, Employee Source]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,10,0.3173828125, ,IBM HR w/more Rows,https://www.kaggle.com/dgokeeffe/ibm-hr-wmore-rows,Tue Feb 20 2018
588,,Bo Shen,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,2,23,8, ,rossmann,https://www.kaggle.com/shenbo18/rossmann,Fri Feb 16 2018
589,,dhruv alexander,[],[],These queries are used to solve the hard problems for guest house database on sql zoo https//sqlzoo.net/wiki/Guest_House_Assessment_Hard,Other,,[],CC0,,,2,18,0.013671875, ,SQL ZOO Guest House Hard Problem Answers,https://www.kaggle.com/dhruviskalpen/sql-zoo-guest-house-hard-problem-answers,Sat Feb 17 2018
590,,Nikhil,"[Player Name, own_team, own_team_rating, opponent_team, opponent_team_rating, hight, position, PPG, Point, Class]","[string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,106,0.00390625, ,basketball,https://www.kaggle.com/nikhil1904/basketball,Sat Feb 10 2018
591,,Kuznetsova M.,[],[],This dataset does not have a description yet.,Other,,[],CC4,,,1,12,0.00390625, ,sample,https://www.kaggle.com/metlamanka/sample,Fri Feb 16 2018
592,,Lee Richards,[],[],Context I decided for a little hobby project I'm working on that I needed a dialog dataset which Cornell University kindly provided here. However as a database programmer I'm used to working with structured data not parsing and building lists from text-based files and I decided that life would be much easier for me if I had this data in an SQL-type database so I wrote me a little python script to chunk the whole thing into SQLite. Content Original Data Set https//www.kaggle.com/Cornell-University/movie-dialog-corpus As of this writing the original dataset was updated 7 months ago. The data is normalized with all of the code-breaking artifacts I ran into hand-corrected. If you're familiar with SQL and have a language/library that supports SQLite I hope you'll find this fairly easy to work with. All of the data from the original dataset is I believe present though I did remove some redundancies. For example in the original dataset movie_lines.tsv lists the character name along with the character id which is redundant because the name is listed in the movie_characters.tsv file. While this is a convenience when you have to process the file directly it can easily be obtained by a JOIN in a structured database. The raw_script_urls are included in the movies table. Acknowledgements Thank you to Cornell University for providing the original Corpus. Photo by Tobias Fischer on Unsplash Inspiration Do let me know if you find this useful. I will probably do similar conversions for other datasets as I need them and would happily upload them if anyone else finds them useful in that form.,Other,,"[film, linguistics]",ODbL,,,3,32,12,The original dialog corpus converted to an SQLite database,Cornell Movie Dialogs Corpus SQLite,https://www.kaggle.com/mrlarichards/cornell-movie-dialogs-corpus-sqlite,Tue Feb 13 2018
593,,Atanas Atanasov,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC4,,,2,59,4, ,blaaaa,https://www.kaggle.com/atanasova/blaaaa,Sat Jan 13 2018
594,,Mario Sabatino,"[id, identity_hate, insult, obscene, severe_toxic, threat, toxic]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,7,20, ,mysubmissions ,https://www.kaggle.com/ammario/mysubmissions,Wed Feb 21 2018
595,,fabiolux,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,45,36, ,favorita 6,https://www.kaggle.com/fabioluciani/favorita-6,Wed Dec 27 2017
596,,Kanksha Masrani,[],[],This dataset does not have a description yet.,CSV,,[],ODbL,,,4,29,0.5400390625, ,Employee Service time Prediction,https://www.kaggle.com/kankshamasrani/employee-service-time-prediction,Tue Feb 13 2018
597,,Sandro Marcelo Peirano Gozalvez,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,47,12, ,attrition de clientes,https://www.kaggle.com/ordnas/attrition-de-clientes,Sun Dec 24 2017
598,,Alex Meyer,[],[],Content This dataset is all of Hubway's ridership data and station information up to December 2017. License Hubways data license agreement can be found here https//www.thehubway.com/data-license-agreement,CSV,,[],Other,,,2,44,154, ,Hubway Data,https://www.kaggle.com/acmeyer/hubway-data,Tue Feb 13 2018
599,,Frédéric Kosmowski,[],[],Data capture experiment on sweet potato varietal identification in southern Ethiopia. Three household-based methods of identifying varietal adoption are tested against the benchmark of DNA fingerprinting A) Elicitation from farmers with basic questions for the most widely planted variety; B) Farmer elicitation on five sweet potato phenotypic attributes by showing a visual-aid protocol and C) Enumerator recording observations on five sweet potato phenotypic attributes using a visual-aid protocol and visiting the field. Kosmowski F Aragaw A Kilian A Ambel A Ilukor J Yigezu B et al. Varietal identification in household surveys results from three household-based methods against the benchmark of DNA fingerprinting in southern Ethiopia. Exp. Agri. 2018; In Press.,CSV,,"[africa, agriculture, survey analysis]",CC4,,,8,42,0.1005859375,A methodological experiment in southern Ethiopia,Sweet Potato Varietal Identification,https://www.kaggle.com/fkosmowski/sweet potato varietal identification,Mon Feb 12 2018
600,,Carlos E. Jimenez-Gomez,"[Nombre de la Entidad;""Nombre Alternativo"";""Historia"";""Año"";""Registros Afectados"";""Sector"";""Motivo de la brecha"";""1st source"";""2nd source"";""3rd source"";""Source name"", Nombre Alternativo, Historia, Año, Registros Afectados, Sector, Motivo de la brecha, 1st source, 2nd source, 3rd source, Source name]","[string, numeric, string, numeric, numeric, string, string, string, numeric, numeric, numeric]","Context Data breaches. Incidents in the world that compromised more than 30000 records between 2004 and 2017. Spanish version. I wanted to visualize the data including the possibility to compare numbers between variable levels. I did some improvements in levels of variables as well as data first based on the original dataset in English. Then I also translated it to Spanish and I did this version. I did the visualization with Tableau software. In this post in my blog your can read more about it Spanish version and English version. You can also see the visualization in this link Spanish version and English version.  Content The dataset has 270 observations and 11 variables. Most of them are categorical variables. Incidents happened between 2004 and 2017. Last updated February 2018. Format CSV2. Variables (columns) [ES]  Nombre de la Entidad name of the organization (public or private) that had the breach. String Nombre Alternativo other known names of the entity. String Historia String Año year of the breach. Date Registros Afectados number of records that the breach compromised. Integer Sector organization's main sector (or field of business). String Motivo de la brecha main cause of the breach. String 1st source (link) 1st. url with more info about the breach. String 2nd source (link) 2nd. url with more info about the breach. String 3rd source (link) 3rd. url with more info about the breach. String Source name name of the source of news official reports blog etc. included . Note that some of them have changed after I replaced some previous broken links that the original dataset had. String  Acknowledgements Informationisbeautiful.net. Before the improvements a first dataset was downloaded from this site by the end of 2017.  Inspiration The main question to be answered with the data visualization was ""What quantities of records were compromised by important data breaches in organizations and sectors between 2004 and 2017 and what was the reason?"". I wanted to have a visual answer that allows to compare numbers between year sector and method of leak. It would be great to improve the dataset adding new variables for data mining in the future in order to achieve a complete and exhaustive ""Data Breaches 2004-2017"" dataset. It would help to an in-depth analysis of incidents in this period. 2017 has been the worst year in the history.",CSV,,"[databases, computer security, computing and society]",CC4,,,3,44,0.0791015625,Spanish version of Data Breaches 2004-2017 greater than 30K records,Data Breaches 2004-2017 (ES),https://www.kaggle.com/estratic/brechas-datos-2004-2017-es-20180218,Tue Feb 20 2018
601,,ShubhamAgarwal,[],[],This dataset does not have a description yet.,Other,,[],Other,,,7,86,815, ,cat vs dog,https://www.kaggle.com/shubhamagarwal269/cat-vs-dog,Sat Jan 06 2018
602,,roxxxor,"[, Symbol, bv_ss, rv10, bv, open_price, nobs, open_to_close, rv10_ss, rk_th2, rsv, close_price, rv5, close_time, medrv, open_time, rk_twoscale, rsv_ss, rk_parzen, rv5_ss]","[dateTime, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,23,15, ,realizedvol,https://www.kaggle.com/roxxxor/realizedvol,Sat Feb 03 2018
603,,Sheng Guo,"[id, visitors]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,13,58,2, ,recruit - stacking - test,https://www.kaggle.com/anyezijue49/recruit-stacking-test,Mon Feb 05 2018
604,,Ray Dickenson,"[DateTime, AvgHourlyTempC, AvgHourlyRH]","[dateTime, numeric, numeric]",Context Using measurements from an indoor temperature sensor mounted over a cooktop can you help monitor the welfare of an elderly person who wants to live at home later in life? Content Indoor Temperature over the Cooktop The  dataset contains temperature measurements from an indoor sensor mounted 30cm over a combination cooktop-oven. Data were collected over a 100 day period. Measurements were recorded only when the temperature changed by 1 degree C or more and at a minimum of every 15 minutes. Outside Temperature and Relative Humidity Also included are outside air temperatures and relative humidity collected in the same region during the same period. Other factors include  The data include round-the-clock measurements including typical meal preparation times and long periods (overnight) when the kitchen is not being used. Ambient temperature in the house varies between 15C - 30C. The home may be under air conditioning or with the windows and sliding doors open when outside temperature and humidity allow it; use the associated climate data to help determine which state the house is in. When the air conditioning is in use the thermostat keeps the house between 24C - 25C. There is an incandescent light bulb about 20cm over the sensor that may be on or off for long periods. First use of the cooktop is typically to prepare coffee between 0530 and 0700; any absence of this event is likely an indicator of trouble.  Acknowledgements Indoor temperatures were collected in a home setting by the poster. Outdoor temperature and humidity are accessed from the National Climatic Data Center U.S. Climate Reference Network (USCRN/USRCRN) via anonymous ftp at ftp//ftp.ncdc.noaa.gov/pub/data/uscrn/products/hourly02 Inspiration Can you help monitor the welfare of elderly residents of a home so they can safely live independently later in life? Can you establish a profile of typical use of the oven and cooktop and then detect anomalies that signal the occupant(s) are in trouble and may need help? If so a simple monitoring system can alert family members to check on their elderly parents or grand parents. Can you detect these trouble events that exist in this dataset and possibly in future data?  - A saucepan was left on the cooktop too long creating a fire hazard  - Normal meal preparation patterns have been interrupted requiring a check on the occupant(s),CSV,,[],CC0,,,3,117,0.107421875,Monitoring usage of cooking appliance to identify risks,Indoor Temp Over an Oven and Cooktop,https://www.kaggle.com/rdickenson/cooktoptemp,Wed Feb 14 2018
605,,Fernando Lasso,"[Alcohol, Malic acid, Ash, Alcalinity of ash, Magnesium, Total phenols, Flavanoids, Nonflavanoid phenols, Proanthocyanins, Color intensity, Hue, OD280/OD315 of diluted wines, Proline, Cultivar 1, Cultivar 2, Cultivar 3]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,51,0.0283203125, ,Wine_set,https://www.kaggle.com/fernandol/wine-set,Mon Feb 12 2018
606,,salmanpathan,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,54,306,33, ,Fraud Transaction,https://www.kaggle.com/salmanasylum/fraud-transaction,Tue Dec 19 2017
607,,SourabhMittal,"[, id, date, store_nbr, item_nbr, unit_sales, onpromotion]","[numeric, numeric, dateTime, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,44,7, ,train_plus_test_store_45_900_items_MA,https://www.kaggle.com/srbhmitt/train-plus-test-store-45-900-items-ma,Fri Jan 05 2018
608,,Kuznetsova M.,[],[],This dataset does not have a description yet.,Other,,[],CC4,,,1,14,128, ,sample2,https://www.kaggle.com/metlamanka/sample2,Fri Feb 16 2018
609,,Joe Young,"[date, open, high, low, close, volume, adjusted]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric]",Context Some of the most challenging problems in machine learning for me have been in predicting stock market results. So I can use Kaggle's kernels on this data I'm adding the data to Kaggle. Content Acknowledgements This data was gathered using the tidyquant package.,CSV,,"[time series, finance]",CC0,,,14,199,1,"Includes Index ETF SPY, Inverse Index ETFs, and the Volatility Index",Stock Market Index ETF Daily Datasets,https://www.kaggle.com/jsphyg/spy-etf-stock-data,Mon Feb 19 2018
610,,Eliezer Bourchardt,"[DATA_GERACAO;""HORA_GERACAO"";""ANO_ELEICAO"";""DESCRICAO_ELEICAO"";""SIGLA_UF"";""SQ_CANDIDATO"";""CD_TIPO_BEM_CANDIDATO"";""DS_TIPO_BEM_CANDIDATO"";""DETALHE_BEM"";""VALOR_BEM"";""DATA_ULTIMA_ATUALIZACAO"";""HORA_ULTIMA_ATUALIZACAO"", HORA_GERACAO, ANO_ELEICAO, DESCRICAO_ELEICAO, SIGLA_UF, SQ_CANDIDATO, CD_TIPO_BEM_CANDIDATO, DS_TIPO_BEM_CANDIDATO, DETALHE_BEM, VALOR_BEM, DATA_ULTIMA_ATUALIZACAO, HORA_ULTIMA_ATUALIZACAO]","[string, string, numeric, string, string, numeric, numeric, string, string, numeric, dateTime, string]",Context Includes data for all candidates from all units of the federation and yours list of property declarations. Content Data of candidates brazilian national elections of 2014. Source http//www.tse.jus.br/eleitor-e-eleicoes/estatisticas/repositorio-de-dados-eleitorais-1/repositorio-de-dados-eleitorais,CSV,,"[brazil, government, politics]",CC0,,,9,104,29,Election results and candidate data.,Brazil Elections 2014,https://www.kaggle.com/eliezerfb/brazil-elections-2014,Fri Nov 10 2017
611,,Артем Лян,[],[],This dataset does not have a description yet.,Other,,[],GPL,,,7,96,2048, ,google_news,https://www.kaggle.com/arli2016/google-news,Thu Dec 14 2017
612,,gopisaran,"[air_store_id, hpg_store_id, visit_date, visitors, day_of_week, holiday_flg, air_genre_name, air_area_name, air_latitude, air_longitude, hpg_genre_name, hpg_area_name, hpg_latitude, hpg_longitude, month, year, min_visitor_by_day_of_week, max_visitor_by_day_of_week, median_visitor_by_day_of_week, avg_visitor_by_day_of_week, count_visitor_by_day_of_week, min_visitor_by_month_of_year, max_visitor_by_month_of_year, median_visitor_by_month_of_year, avg_visitor_by_month_of_year, count_visitor_by_month_of_year, min_visitor_by_air_genre_name, max_visitor_by_air_genre_name, median_visitor_by_air_genre_name, avg_visitor_by_air_genre_name, count_visitor_by_air_genre_name, min_visitor_by_air_area_name, max_visitor_by_air_area_name, median_visitor_by_air_area_name, avg_visitor_by_air_area_name, count_visitor_by_air_area_name, min_visitors_by_area_dow, max_visitors_by_area_dow, median_visitors_by_area_dow, avg_visitors_by_area_dow, count_visitors_by_area_dow, min_visitors_by_area_month, max_visitors_by_area_month, median_visitors_by_area_month, avg_visitors_by_area_month, count_visitors_by_area_month, min_visitors_by_genre_dow, max_visitors_by_genre_dow, median_visitors_by_genre_dow, avg_visitors_by_genre_dow, count_visitors_by_genre_dow, min_visitors_by_genre_month, max_visitors_by_genre_month, median_visitors_by_genre_month, avg_visitors_by_genre_month, count_visitors_by_genre_month]","[string, string, dateTime, numeric, string, numeric, string, string, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC4,,,2,42,0.6259765625, ,agg_data_3,https://www.kaggle.com/gopisaran/agg-data-3,Thu Feb 01 2018
613,,Naveen Kumar,[],[],This dataset does not have a description yet.,CSV,,[],ODbL,,,1,25,9, ,sampledigits,https://www.kaggle.com/naveen0342/sampledigits,Tue Feb 13 2018
614,,Nirajk18,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,29,0.2177734375, ,attrition,https://www.kaggle.com/nirajkalantri99/attrition,Tue Feb 13 2018
615,,Rajeev,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,94,67, ,rajeevdata,https://www.kaggle.com/rajeevmeda/preprocess,Sat Feb 10 2018
616,,Kun Yang,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,35,5, ,submission1,https://www.kaggle.com/rucyoung/submission1,Sun Feb 04 2018
617,,Faisal,"[parcelid, airconditioningtypeid, architecturalstyletypeid, basementsqft, bathroomcnt, bedroomcnt, buildingclasstypeid, buildingqualitytypeid, calculatedbathnbr, decktypeid, finishedfloor1squarefeet, calculatedfinishedsquarefeet, finishedsquarefeet12, finishedsquarefeet13, finishedsquarefeet15, finishedsquarefeet50, finishedsquarefeet6, fips, fireplacecnt, fullbathcnt, garagecarcnt, garagetotalsqft, hashottuborspa, heatingorsystemtypeid, latitude, longitude, lotsizesquarefeet, poolcnt, poolsizesum, pooltypeid10, pooltypeid2, pooltypeid7, propertycountylandusecode, propertylandusetypeid, propertyzoningdesc, rawcensustractandblock, regionidcity, regionidcounty, regionidneighborhood, regionidzip, roomcnt, storytypeid, threequarterbathnbr, typeconstructiontypeid, unitcnt, yardbuildingsqft17, yardbuildingsqft26, yearbuilt, numberofstories, fireplaceflag, structuretaxvaluedollarcnt, taxvaluedollarcnt, assessmentyear, landtaxvaluedollarcnt, taxamount, taxdelinquencyflag, taxdelinquencyyear, censustractandblock]","[numeric, string, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, string]",This dataset does not have a description yet.,CSV,,[],Other,,,2,45,50, ,properties_2016,https://www.kaggle.com/faisalhussainsabir/properties-2016,Wed Dec 13 2017
618,,Brunods10,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,4,47,27, ,Global Terrorism Data,https://www.kaggle.com/brunods10/global-terrorism-database,Thu Feb 15 2018
619,,Naveen Kumar,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,20,5, ,submission,https://www.kaggle.com/naveenkumar94/submission,Sat Feb 10 2018
620,,Sanjeet Kumar Yadav,[],[],"Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60000 examples and a test set of 10000 examples. Each example is a 28x28 grayscale image associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact MNIST is often the first dataset researchers try. ""If it doesn't work on MNIST it won't work at all"" they said. ""Well if it does work on MNIST it may still fail on others."" Zalando seeks to replace the original MNIST dataset",CSV,,[],CC0,,,8,87,34,Fashion-mnist_train -Data set,Fashion-mnist_train,https://www.kaggle.com/sanjeet41/fashionmnist-train,Fri Dec 15 2017
621,,Vijaykumar Ummadisetty,"[InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country]","[numeric, numeric, string, numeric, dateTime, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,108,689,43, ,Online Retail Data Set,https://www.kaggle.com/vijayuv/onlineretail,Thu Oct 26 2017
622,,ecodan,[],[],"Context Another week sadly another school shooting.   To better understand the facts I went looking for data and found it difficult to come by - often embedded in other datasets or fragmented and unusable.  I decided to create my own compilation based on a mashup of the Pah/Amaral/Hagan research on school shootings with the Wikipedia article from 1990 to present. Content pah_wikp file A list of all school shooting incidents from 1990 to present. Fields   Date date of incident  City location of incident  State location of incident  Area Type urban or suburban (only in Pah dataset)  School C = college HS = high school MS = middle school ES = elementary school - = unknown  Fatalities # killed  Wounded # wounded (only in Wikipedia dataset)  Dupe whether this incident appears in both datasets. Note only the ""Pah"" version of the incident is marked.  Source Pah or Wikp  Desc text description of incident (only in Wikipedia dataset)  cps file US census data on school populations.  Fields should be fairly self explanatory. Acknowledgements Thanks to the authors referenced above as well as the Wikipedia contributors! Inspiration  Why are school shootings (and death counts) increasing over time? How does the risk of being killed in a school shooting compare with other risks? Are some schools / cities / states at higher risk? Is there a correlation between countermeasures and a decrease in fatalities? What else correlates with school shooting risks?  In addition to firearms and the people who wield them is there any clear causality? ",Other,,[],CC0,,,32,178,0.1005859375,Record of all school shooting incidents since 1990,School Shootings US 1990-present,https://www.kaggle.com/ecodan/school-shootings-us-1990present,Sun Feb 18 2018
623,,xlang,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,18,0.4599609375, ,text classification,https://www.kaggle.com/gracethesnowboarder/text-classification,Mon Feb 19 2018
624,,Deep Jethwa,"[PassengerId, Pclass, Name, Sex, Age, SibSp, Parch, Ticket, Fare, Cabin, Embarked]","[numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,17,0.0859375, ,Titanic Data Problem,https://www.kaggle.com/jethwadeep/titanic,Mon Feb 19 2018
625,,N Saravana,"[Avg. Area Income, Avg. Area House Age, Avg. Area Number of Rooms, Avg. Area Number of Bedrooms, Area Population, Price, Address]","[numeric, numeric, numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,3,24,0.6923828125, ,USA Housing Data,https://www.kaggle.com/nsaravana/usa-housing-data,Sun Feb 18 2018
626,,Douglas Valério,[],[],This dataset does not have a description yet.,CSV,,[],ODbL,,,3,25,0.1630859375, ,GDP per-capita-worldBank,https://www.kaggle.com/dglvalerio/gdp-percapitaworldbank,Sun Feb 18 2018
627,,RodneyRick,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,73,8, ,sample_submission.csv,https://www.kaggle.com/rodneyrick/sample-submissioncsv,Wed Nov 29 2017
628,,Arjav Patel,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,10,98,8, ,Airline Dataset Mining,https://www.kaggle.com/arjhbholu/airline-dataset-mining,Mon Feb 12 2018
629,,ShaoqiangLiang,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,21,15, ,ftrl_fm,https://www.kaggle.com/liangshaoqiang/ftrl-fm,Sat Feb 10 2018
630,,N Saravana,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,5,16,0.08203125, ,Weather data Report,https://www.kaggle.com/nsaravana/weather-data-report,Sat Feb 17 2018
631,,nishantjain,"[transaction_id, target]","[string, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,6,60,38, ,frauldenttransactions,https://www.kaggle.com/nishantjain91/frauldenttransactions,Tue Dec 12 2017
632,,Mouhaned,"[price, test_id]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,40,15, ,Rnn_model,https://www.kaggle.com/mchebaan/rnn-model,Tue Jan 23 2018
633,,quoniammm,"[ChildId, GiftId]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,55,4, ,laaaaa,https://www.kaggle.com/quoniammm/laaaaa,Fri Jan 12 2018
634,,like,"[card, reports, age, income, share, expenditure, owner, selfemp, dependents, months, majorcards, active]","[string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,27,0.0703125, ,AER_credit_card_data.csv,https://www.kaggle.com/like1008/aer-credit-card-datacsv,Sun Feb 11 2018
635,,NikKonst,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,4,58,64, ,Plant Recognition Models,https://www.kaggle.com/nikkonst/plantrecomodels,Tue Feb 13 2018
636,,paultimothymooney,[],[],Context TXT files for Poetry Generation with Python Content TXT files of lyrics and poems Acknowledgements Free lyric hosting websites Inspiration TXT files for Poetry Generation with Python,Other,,"[music, text data]",CC0,,,25,263,6,Poems and Lyrics as TXT files,Poetry,https://www.kaggle.com/paultimothymooney/poetry,Fri Feb 09 2018
637,,VivekSingh,[],[],This dataset does not have a description yet.,Other,,[],Other,,,3,16,0.123046875, ,data dictionary,https://www.kaggle.com/viveksinghub/data-dictionary,Tue Feb 13 2018
638,,Daniel Franch,[],[],Context I am replicating the project shown in the City Class project by Roman Kuchokov. The objective is to classify a satellite picture of a small part of a city into one of its zones (such as residential industrial etc). I obtained the information about the zoning in Austin to work on the same problem but with a different dataset. Content There are 3666 small satellite images of the whole city of Austin and a csv file which relates the name of each of these images to the corresponding zoning tag. Acknowledgements This project was inspired by Roman Kuchukov's City https//towardsdatascience.com/cityclass-project-eng-15bc5fcd8e1 The zoning information was obtained in the official city of Austin data portal https//data.austintexas.gov/Locations-and-Maps/Zoning/5rzy-nm5e Inspiration I want to be able to train a classification model which can take a satellite picture of any city and create a grid where each cell is classified as a type of zone.  This may start with zoning classification but can also be tweaked for other urban geo-tagging projects.,Other,,[],GPL,,,6,99,569,How to classify the zoning of satellite pictures of the city of Austin?,Austin Zoning Satellite Images,https://www.kaggle.com/franchenstein/austin-zoning-satellite-images,Sun Jan 07 2018
639,,Alexis Arnould,"[sepallength, sepalwidth, petallength, petalwidth, classe]","[numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],Other,,,1,15,0.0048828125, ,IrisToutDur,https://www.kaggle.com/wargle/iristoutdur,Fri Feb 16 2018
640,,MartinBoyanov,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,4,61,13, ,Mercari fasttext vectors 64 v2,https://www.kaggle.com/mboyanov/mercari-fasttext-vectors-64-v2,Sun Dec 17 2017
641,,Aleix Dorca,"[fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, quality, good, color]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],Other,,,16,109,0.3876953125,Both Red and White wine data,Wine Quality,https://www.kaggle.com/aleixdorca/wine-quality,Sun Feb 11 2018
642,,Gaurav Sharma,"[year, month, day, dep_time, sched_dep_time, dep_delay, arr_time, sched_arr_time, arr_delay, carrier, flight, tailnum, origin, dest, air_time, distance, hour, minute, time_hour]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, dateTime]",This dataset does not have a description yet.,CSV,,[],GPL,,,7,90,8, ,NYC flight data 2013,https://www.kaggle.com/gauravsharma74/nyc-flight-data-2013,Wed Dec 27 2017
643,,Dheeraj,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,17,64,0.7265625, ,Telecom,https://www.kaggle.com/dheerajetx/telecom,Tue Feb 13 2018
644,,Dennis Antonyswamy,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,9,33,0.9130859375, ,Housing Prices- Regression techniques,https://www.kaggle.com/dennisndata/housing-prices-regression-techniques,Tue Feb 13 2018
645,,qxh5696,"[train_id, name, item_condition_id, category_name, brand_name, price, shipping, item_description]","[numeric, string, numeric, string, numeric, numeric, numeric, string]",This dataset does not have a description yet.,Other,,[],CC0,,,1,35,129, ,UnzippedTrain.tsv,https://www.kaggle.com/qxh5696/unzippedtraintsv,Mon Jan 22 2018
646,,Chester Cheng,[],[],This dataset does not have a description yet.,Other,,[],Other,,,3,70,2048, ,Google_news_w2v,https://www.kaggle.com/chez8990/google-news-w2v,Fri Dec 15 2017
647,,Nagendra Yadav,"[Year, Month, DayofMonth, DayOfWeek, UniqueCarrier, FlightNum, TailNum, Origin, Dest, Distance, DepDelayed, hour, hdays]","[numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, boolean, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,95,36,airline plus exotic options,CuratedDataSource,https://www.kaggle.com/nagendrayadav/curateddatasource,Thu Feb 22 2018
648,,Sameer Mahajan,[],[],Context This dataset has images of cats dogs birds and  automobiles. Content The data has raw pixels as well as deep features extracted from ImageNet winning  algorithm. Acknowledgements This data set is used by coursera Machine Learning Foundations course which is a part of Machine Learning Specialization. I have transformed and made it available in .csv so that it can be used with open  source  softwares like scikit-learn etc. Inspiration Learn working  with  images and evaluate how deep features help you with  transfer learning.,CSV,,[],CC0,,,2,84,54, ,Image Data with Deep Features,https://www.kaggle.com/sameersmahajan/image-data-with-deep-features,Wed Nov 22 2017
649,,Srinivas Paturu,"[Buisness Service, Asset, Date, Incident Count, Memory Alerts, Out Of Memory Exceptions, Application Heap Memory Alerts, CLR Crash Count, Slow Transactions, Error Transactions, 500 HTTP Error Code, Database Connection Issues]","[string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],GPL,,,5,51,0.044921875, ,ITSM_Prediction,https://www.kaggle.com/asvvisb/itsmds,Wed Feb 14 2018
650,,Prashant Kikani,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,54,80,20, ,Toxic-file,https://www.kaggle.com/prashantkikani/toxicfile,Tue Feb 13 2018
651,,Kalaba Chibale,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,19,2, ,Population estimates,https://www.kaggle.com/kalabachibale/population-estimates,Fri Feb 16 2018
652,,ningenjanai,"[Currency, Date, Open, High, Low, Close, Volume, Market Cap]","[string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,28,0.0546875, ,ETH history starting from 2016,https://www.kaggle.com/ningenjanai/eth-history-starting-from-2016,Sun Feb 11 2018
653,,Hasil Sharma,[],[],This dataset does not have a description yet.,Other,,[],Other,,,4,42,100, ,Wiki-Talk-Labels-Personal-Attacks,https://www.kaggle.com/hasilsharma/wikitalklabelspersonalattacks,Fri Jan 26 2018
654,,Kostya,[],[],This dataset does not have a description yet.,Other,,[],Other,,,19,110,189, ,movielens,https://www.kaggle.com/ktochylin/movielens,Sun Dec 10 2017
655,,adrien chevrier,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,3,46,15, ,mnist.pkl.gz,https://www.kaggle.com/adrienchevrier/mnist.pkl.gz,Fri Nov 24 2017
656,,Alexander Kireev,"[id, unit_sales]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,3,43,16, ,xgb_support_CFav,https://www.kaggle.com/alexanderkireev/xgb-support-cfav,Tue Jan 09 2018
657,,qixiang109,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,32,6, ,qixiang109merge26,https://www.kaggle.com/qixiang109/qixiang109merge26,Sat Jan 13 2018
658,,Vivek Kumar,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,46,8, ,submit,https://www.kaggle.com/viveknium/submit,Sun Nov 26 2017
659,,John Doe,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,29,12, ,DataTest,https://www.kaggle.com/tsetfacc/datatest,Thu Jan 18 2018
660,,Rick Chen,"[test_id, price]","[numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,40,8, ,first submission,https://www.kaggle.com/ddongchen/first-submission,Sat Jan 06 2018
661,,EasonZ,"[Weekly_Sales, id]","[numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,17,14, ,Walmart challenge material,https://www.kaggle.com/easonzeng1996/walmart-challenge-material,Sun Feb 18 2018
662,,Heitor Tomaz,[],[],Context There's a story behind every dataset and here's your opportunity to share yours. Context This dataset was downloaded from INEP a department from the Brazilian Education Ministry. It contains data from the applicants for the 2014 National High School Exam. Content Inside this dataset there are not only the exam results but the social and economic context of the applicants. Acknowledgements The original dataset is provided by INEP (http//portal.inep.gov.br/microdados). I removed some information from original files to fit the file size into the Kaggle constraints. Inspiration The objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.,Other,,[],CC0,,,3,60,1024, ,Microdados Enem 2014,https://www.kaggle.com/heitortomaz/enem2014,Mon Nov 27 2017
663,,yogender singh,"[RowNumber, CustomerId, Surname, CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary, Exited]","[numeric, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],ODbL,,,3,31,0.6533203125, ,Churn_Modelling,https://www.kaggle.com/yogirj/churn-modelling,Mon Feb 12 2018
664,,J.Jagadish Kumaran,"[test_id, name, item_condition_id, category_name, brand_name, shipping, item_description]","[numeric, string, numeric, string, numeric, numeric, string]",This dataset does not have a description yet.,Other,,[],CC0,,,1,25,188, ,traintest,https://www.kaggle.com/jug971990/traintest,Thu Feb 01 2018
665,,Abdullah Karimi,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,32,10, ,dataset_es,https://www.kaggle.com/abdullahkarimi/dataset-es,Fri Feb 02 2018
666,,genexpres,"[Survey Year, Program Category, Region Served, Age Group, Sex, Transgender, Sexual Orientation, Hispanic Ethnicity, Race, Living Situation, Household Composition, Preferred Language, Veteran Status, Employment Status, Number Of Hours Worked Each Week, Education Status, Special Education Services, Mental Illness, Intellectual Disability, Autism Spectrum, Other Developmental Disability, Alcohol Related Disorder, Drug Substance Disorder, Mobility Impairment Disorder, Hearing Visual Impairment, Hyperlipidemia, High Blood Pressure, Diabetes, Obesity, Heart Attack, Stroke, Other Cardiac, Pulmonary Asthma, Alzheimer or Dementia, Kidney Disease, Liver Disease, Endocrine Condition, Neurological Condition, Traumatic Brain Injury, Joint Disease, Cancer, Other Chronic Med Condition, No Chronic Med Condition, Unknown Chronic Med Condition, Smokes, Received Smoking Medication, Received Smoking Counseling, Serious Mental Illness, Principal Diagnosis Class, Additional Diagnosis Class, SSI Cash Assistance, SSDI Cash Assistance, Veterans Disability Benefits, Veterans Cash Assistance, Public Assistance Cash Program, Other Cash Benefits, Medicaid and Medicare Insurance, No Insurance, Unknown Insurance Coverage, Medicaid Insurance, Medicaid Managed Insurance, Medicare Insurance, Private Insurance, Child Health Plus Insurance, Other Insurance, Criminal Justice Status, Three Digit Residence Zip Code]","[numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,53,4, ,NY_mental_patient_survey,https://www.kaggle.com/genexpres/ny-mental-patient-survey,Sat Jan 06 2018
667,,Gaurav Bhat,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,15,71,9, ,Chatbot Data,https://www.kaggle.com/fungusamongus/chatbot-data,Sun Feb 18 2018
668,,John Doe,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,3,53,0.015625, ,animals,https://www.kaggle.com/jdoejdoe/animals,Tue Feb 13 2018
669,,Mainak kUNDU,"[msno, payment_method_id, payment_plan_days, plan_list_price, actual_amount_paid, is_auto_renew, transaction_date, membership_expire_date, is_cancel]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,68,53, ,transaction_version2,https://www.kaggle.com/mainakdatageek/transaction-version2,Mon Nov 27 2017
670,,Shivam Patel,[],[],Data Set The labeled data set consists of 50000 IMDB movie reviews specially selected for sentiment analysis. The sentiment of reviews is binary meaning the IMDB rating < 5 results in a sentiment score of 0 and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25000 review labeled training set does not include any of the same movies as the 25000 review test set. In addition there are another 50000 IMDB reviews provided without any rating labels. File descriptions  labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25000 rows containing an id sentiment and text for each review.   testData - The test set. The tab-delimited file has a header row followed by 25000 rows containing an id and text for each review. Your task is to predict the sentiment for each one.  unlabeledTrainData - An extra training set with no labels. The tab-delimited file has a header row followed by 50000 rows containing an id and text for each review.  Data fields  id - Unique ID of each review  sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews review - Text of the review  Acknowledgements The origin place is here.,Other,,[],ODbL,,,13,76,52, ,Bag of Words Meets Bags of Popcorn: Data,https://www.kaggle.com/spatel4140/bag-of-words-meets-bags-of-popcorn-data,Wed Dec 27 2017
671,,floser,[],[],"Context This dataset is part of the R-package ISLR and is used in the related book by G. James et al. (2013) ""An Introduction to Statistical Learning with applications in R"" to demonstrate how Ridge regression and the LASSO are performed using R. Content This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated April 20 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books Macmillan Publishing Company New York. Format A data frame with 322 observations of major league players on the following 20 variables. AtBat Number of times at bat in 1986 Hits Number of hits in 1986 HmRun Number of home runs in 1986 Runs Number of runs in 1986 RBI Number of runs batted in in 1986 Walks Number of walks in 1986 Years Number of years in the major leagues CAtBat Number of times at bat during his career CHits Number of hits during his career CHmRun Number of home runs during his career CRuns Number of runs during his career CRBI Number of runs batted in during his career CWalks Number of walks during his career League A factor with levels A and N indicating player’s league at the end of 1986 Division A factor with levels E and W indicating player’s division at the end of 1986 PutOuts Number of put outs in 1986 Assists Number of assists in 1986 Errors Number of errors in 1986 Salary 1987 annual salary on opening day in thousands of dollars NewLeague A factor with levels A and N indicating player’s league at the beginning of 1987 Acknowledgements Please cite/acknowledge Games G. Witten D. Hastie T. and Tibshirani R. (2013) An Introduction to Statistical Learning  with applications in R www.StatLearning.com Springer-Verlag New York. Inspiration This upload shall enable actuarial kernels with R and Python",CSV,,[],Other,,,2,47,0.01953125,Major League Baseball Data from the 1986 and 1987 seasons.,Hitters,https://www.kaggle.com/floser/hitters,Mon Feb 12 2018
672,,Resul CALISKAN,[],[],This dataset does not have a description yet.,Other,,[],Other,,,16,90,0.2177734375, ,Countries GDPs,https://www.kaggle.com/resulcaliskan/countries-gdps,Thu Feb 15 2018
673,,shengwei,[],[],This dataset does not have a description yet.,{}JSON,,[],Other,,,4,44,15, ,Descript_Meta,https://www.kaggle.com/syushengwei/descript-meta,Tue Jan 16 2018
674,,Gomathinayagam Velayutham,"[country, continent, year, lifeExpectancy, population, gdpPerCapita]","[string, string, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,2,32,0.080078125, ,countries,https://www.kaggle.com/aviyal/countries,Mon Feb 12 2018
675,,richinmind,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,10,204, ,Glove Twitter 27B 50d,https://www.kaggle.com/richinmind/glove-twitter-27b-50d,Wed Feb 21 2018
676,,Nat T,[],[],This dataset does not have a description yet.,CSV,,[],Other,,,1,62,0.958984375, ,Fire Emblem Heroes Survey,https://www.kaggle.com/natalieytan/fire-emblem-heroes-survey,Tue Nov 21 2017
677,,Brij,"[MacroTrends Data Download, , , , , ]","[string, string, string, string, string, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,29,0.44921875, ,Share Market Analysis,https://www.kaggle.com/brijbhushannanda1979/share-market-analysis,Sun Feb 11 2018
678,,Daniel Sobrado,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,1,48,14, ,GRU Glove Toxic,https://www.kaggle.com/danielsobrado/gru-glove-toxic,Wed Dec 27 2017
679,,Holly,"[State Name, State Code]","[string, numeric]",Content Useful for the US Traffic Fatality Records dataset Acknowledgements From gsa.gov,CSV,,[],GPL,,,1,18,668, ,Geographic Locator Codes for US States,https://www.kaggle.com/hollyg/glcs-for-us-states,Thu Feb 15 2018
680,,Loris CROCE,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,17,0.0068359375, ,TP3iris,https://www.kaggle.com/beaupitt/tp3iris,Fri Feb 16 2018
681,,FreekKlein,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,25,0.046875, ,iphone,https://www.kaggle.com/freekklein/iphone,Sun Feb 18 2018
682,,ymtoo,[],[],This dataset does not have a description yet.,Other,,[],Other,,,1,36,2048, ,word2vec_model,https://www.kaggle.com/ymtoo86/googlenews-vectors-negative300.bin,Fri Jan 12 2018
683,,yujack,[],[],This dataset does not have a description yet.,CSV,,[],CC3,,,1,50,7, ,output,https://www.kaggle.com/yuyijack/output,Tue Nov 28 2017
684,,Jaish K,[],[],This dataset does not have a description yet.,CSV,,[],CC0,,,3,23,0.5400390625, ,Human Resources Analytics ,https://www.kaggle.com/jaishofficial/human-resources-analytics,Sun Feb 18 2018
685,,Abdullah Karimi,"[id, toxic, severe_toxic, obscene, threat, insult, identity_hate]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,27,10, ,tox_aven_sub,https://www.kaggle.com/abdullahkarimi/tox-aven-sub,Sat Feb 03 2018
686,,Submarineering,"[Item, Year, Submarine, Incident]","[numeric, numeric, string, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,31,0.0009765625, ,Submarine incidents since 2000,https://www.kaggle.com/submarineering/submarine-incidents-since-2000,Thu Feb 15 2018
687,,WU Wuhui,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,17,112,106, ,roof images,https://www.kaggle.com/canonwu/roof-images,Tue Dec 12 2017
688,,Aalborg University,[],[],Context Simultaneous tracking of multiple people is still a very challenging computer vision problem. This is especially true for sports activities for which people often wear similar uniforms move quickly and erratically and have close interactions with each other.  This dataset is captured with thermal cameras which enables easier segmentation and ensures privacy of people in public facilities but at the same time we are left with no distinct appearance information to guide our tracking algorithms.  Content This dataset contains four 30-seconds video sequences of eight people playing soccer in an indoor arena (court size 40*20 metres). The video is captured by thermal cameras of type AXIS Q1922 with a resolution of 640*480 pixels and 25 fps. The three images are stitched to one image of 1920*480 pixels. The videos are manually annotated for tracking. Acknowledgements Gade R. & Moeslund T.B. Constrained multi-target tracking for team sports activities. IPSJ Transactions on Computer Vision and Applications (2018) 10 2. https//doi.org/10.1186/s41074-017-0038-z,Other,,"[sports, computer science]",CC4,,,6,81,823,Tracking sports players,Thermal soccer dataset,https://www.kaggle.com/aalborguniversity/thermal-soccer-dataset,Thu Feb 08 2018
689,,Oscar Dossa,[],[],Context Brexit opinions data  between 2016-06-15 and 2016-06-22 Content username date  retweets  favorites  text  geo  mentions   hashtags  id permalink Your data will be in front of the world's largest data science community. What questions do you want to see answered?,CSV,,[],CC0,,,9,56,23, ,Brexit_opinion_data,https://www.kaggle.com/natmonkey/brexit-opinion-data,Fri Feb 09 2018
690,,Notlir,"[sepallength, sepalwidth, petallength, petalwidth, class]","[numeric, numeric, numeric, numeric, string]",This dataset does not have a description yet.,CSV,,[],Other,,,3,13,0.0048828125, ,IRIS.csv,https://www.kaggle.com/notlir/iriscsv,Fri Feb 16 2018
691,,Mainak kUNDU,"[, msno, is_churn, city, bd, gender, registered_via, registration_init_time, expiration_date, total_order, plan_net_worth, mean_payment_each_transaction, total_actual_payment, cancel_times, len_tenure]","[numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,5,63,47, ,test_wo_usr_logs,https://www.kaggle.com/mainakdatageek/test-wo-usr-logs,Thu Nov 30 2017
692,,SohaibOmar,"[test_id, name, item_condition_id, category_name, brand_name, shipping, item_description]","[numeric, string, numeric, string, string, numeric, string]",This dataset does not have a description yet.,CSV,,[],CC0,,,1,42,298, ,mercari-5x-testset,https://www.kaggle.com/sohaibomar/mercari5xtestset,Tue Jan 23 2018
693,,Vladimir,[],[],This dataset does not have a description yet.,Other,,[],Other,,,4,37,0.056640625, ,US_traffic,https://www.kaggle.com/silversurf/us-traffic,Fri Feb 23 2018
694,,Sheng Guo,"[id, visitors, pred]","[string, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],CC0,,,2,64,19,train data for stacking,recruit - stacking - cv,https://www.kaggle.com/anyezijue49/recruit-stacking-cv,Mon Feb 05 2018
695,,Cenk Bircanoğlu,[],[],Context I am experimenting on this dataset various Deep Learning architectures. This dataset can be used in classification localization segmentation generative models and face detection/recognition/classification.  Content The dataset has 52156 rgb images. Train and test datasets are splitted for each 86 classes with ratio 0.8 . the original images has 1988x3056 dimension. It is reduced to 288x432 using OpenCV.   Acknowledgements I download the books from different webpages. In the futures I can add some new images if it needed. Inspiration Comic books have different images than standard images that are worked on. The characters images environments colors and more in this data set are much more challenging and confusing than the image data sets that have been worked on before. Besides the results for the use of GANs are much bigger and more complicated than those that have achieved successful results.,Other,,[animation],GPL,,,88,536,2048,Resized Comic Books Images with Train and Test Set,Comic Books Images,https://www.kaggle.com/cenkbircanoglu/comic-books-classification,Wed Nov 22 2017
696,,Stan Tyan,"[completed_at, status, ride_type, surge_multiplier, vehicle_make_model, driver_name, driver_gender, pickup_lat, pickup_long, dropoff_lat, dropoff_long, start_time, end_time, city, country, from_home, to_home, usd_rub, price_rub, price_usd, distance, trip_time, total_time, wait_time, service, ride_id, ride_uid, driver_uid, rider_uid, temp_time, temperature, feels_like, humidity, wind_speed, cloudness, weather_main, weather_desc, precipitation]","[string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, dateTime, dateTime, string, string, boolean, boolean, numeric, numeric, numeric, numeric, dateTime, dateTime, dateTime, string, string, string, string, string, dateTime, numeric, numeric, numeric, numeric, numeric, string, string, string]",My Uber Rides from 2015 to 2018 Almost all the data is automatically added my Uber Account to Google Sheets via Uber Riders API v1.2 upon each trip completion. Additional attributed and dimensions are added via formulas and functions. With the power of Tableau the data is then sliced and diced from different perspectives and contexts enabling to discover trends relationships and patterns. Tableau Public directly pulls the data from Google Sheets and auto-updates on a daily basis. https//public.tableau.com/profile/stantyan#!/vizhome/uber-rides/uber-rides,CSV,,[],Other,,,13,92,824,My Uber Rides from 2015 to 2018,Uber Rides,https://www.kaggle.com/stantyan/uber-rides,Sat Feb 17 2018
697,,Avinash kumar,"[critic_rating, is_action, is_exclusive_to_us, is_portable, is_role_playing, is_sequel, is_sports, suitable_for_kids, unit_price]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset does not have a description yet.,CSV,,[],Other,,,1,28,155, ,Proposed Sales,https://www.kaggle.com/avinash29/proposed-sales,Wed Feb 14 2018
698,,Trey Kollmer,[],[],This dataset does not have a description yet.,Other,,[],CC0,,,1,48,311, ,Glove_twitter_vecs,https://www.kaggle.com/treykollmer/glove-twitter-vecs,Sun Dec 03 2017
