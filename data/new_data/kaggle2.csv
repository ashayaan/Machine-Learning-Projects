Anonymised_Labels,Author,Column_Name,Column_Type,Description,File_Type,Image_Dimension,Keywords,License,No_Columns,No_Rows,Number_of_Downloads,Number_of_views,Size,Summary,Title,URL,Updated
,Google BigQuery,"[block_id, previous_block, merkle_root, timestamp, difficultyTarget, nonce, version, work_terahash, work_error, transactions]","[string, string, string, numeric, numeric, numeric, numeric, numeric, string, unknown]","Context Blockchain technology first implemented by Satoshi Nakamoto in 2009 as a core component of Bitcoin is a distributed public ledger recording transactions. Its usage allows secure peer-to-peer communication by linking blocks containing hash pointers to a previous block a timestamp and transaction data. Bitcoin is a decentralized digital currency (cryptocurrency) which leverages the Blockchain to store transactions in a distributed manner in order to mitigate against flaws in the financial industry. Nearly ten years after its inception Bitcoin and other cryptocurrencies experienced an explosion in popular awareness. The value of Bitcoin on the other hand has experienced more volatility. Meanwhile as use cases of Bitcoin and Blockchain grow mature and expand hype and controversy have swirled.  Content In this dataset you will have access to information about blockchain blocks and transactions. All historical data are in the bigquery-public-databitcoin_blockchain dataset. It’s updated it every 10 minutes. The data can be joined with historical prices in kernels. See available similar datasets here https//www.kaggle.com/datasets?search=bitcoin.  Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.bitcoin_blockchain.[TABLENAME]. Fork this kernel to get started. Method & Acknowledgements Allen Day (Twitter | Medium) Google Cloud Developer Advocate & Colin Bookman Google Cloud Customer Engineer retrieve data from the Bitcoin network using a custom client available on GitHub that they built with the bitcoinj Java library.  Historical data from the origin block to 2018-01-31 were loaded in bulk to two BigQuery tables blocks_raw and transactions.  These tables contain fresh data as they are now appended when new blocks are broadcast to the Bitcoin network.  For additional information visit the Google Cloud Big Data and Machine Learning Blog post ""Bitcoin in BigQuery Blockchain analytics on public data"".  Photo by Andre Francois on Unsplash.  Inspiration  How many bitcoins are sent each day? How many addresses receive bitcoin each day? Compare transaction volume to historical prices by joining with other available data sources ",BigQuery,,"[finance, money, internet, bigquery]",CC0,,,0,22599,840704,Complete live historical Bitcoin blockchain data,Bitcoin Blockchain,https://www.kaggle.com/bigquery/bitcoin-blockchain,Wed Jan 31 2018
,Mitchell J,[],[],Context YouTube (the world-famous video sharing website) maintains a list of the top trending videos on the platform. According to Variety magazine “To determine the year’s top-trending videos YouTube uses a combination of factors including measuring users interactions (number of views shares comments and likes). Note that they’re not the most-viewed videos overall for the calendar year”. Top performers on the YouTube trending list are music videos (such as the famously virile “Gangam Style”) celebrity and/or reality TV performances and the random dude-with-a-camera viral videos that YouTube is well-known for. This dataset is a daily record of the top trending YouTube videos. Note that this dataset is a structurally improved version of this dataset. Content This dataset includes several months (and counting) of data on daily trending YouTube videos. Data is included for the US GB DE CA and FR regions (USA Great Britain Germany Canada and France respectively) with up to 200 listed trending videos per day. Each region’s data is in a separate file. Data includes the video title channel title publish time tags views likes and dislikes description and comment count. The data also includes a category_id field which varies between regions. To retrieve the categories for a specific video find it in the associated JSON. One such file is included for each of the five regions in the dataset. For more information on specific columns in the dataset refer to the column metadata. Acknowledgements This dataset was collected using the YouTube API. Inspiration Possible uses for this dataset could include  Sentiment analysis in a variety of forms Categorising YouTube videos based on their comments and statistics. Training ML algorithms like RNNs to generate their own YouTube comments. Analysing what factors affect how popular a YouTube video will be. Statistical analysis over time.  For further inspiration see the kernels on this dataset!,CSV,,"[languages, popular culture, statistics]",CC0,,,6145,34782,54,Daily statistics for trending YouTube videos,Trending YouTube Video Statistics,https://www.kaggle.com/datasnaek/youtube-new,Thu Feb 22 2018
,Github,"[commit, tree, parent, author, committer, subject, message, trailer, difference, difference_truncated, repo_name, encoding]","[string, string, string, unknown, unknown, string, string, unknown, unknown, boolean, string, string]",GitHub is how people build software and is home to the largest community of open source developers in the world with over 12 million people contributing to 31 million projects on GitHub since 2008. This 3TB+ dataset comprises the largest released source of GitHub activity to date. It contains a full snapshot of the content of more than 2.8 million open source GitHub repositories including more than 145 million unique commits over 2 billion different file paths and the contents of the latest revision for 163 million files all of which are searchable with regular expressions.  Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.github_repos.[TABLENAME]. Fork this kernel to get started to learn how to safely manage analyzing large BigQuery datasets. Acknowledgements This dataset was made available per GitHub's terms of service. Inspiration  This is the perfect dataset for fighting language wars. Can you identify any signals that predict which packages or languages will become popular in advance of their mass adoption? ,BigQuery,,"[programming languages, programming, bigquery]",Other,,,0,12804,3145728,Code and comments from 2.8 million repos,GitHub Repos,https://www.kaggle.com/github/github-repos,Tue Dec 05 2017
,Hacker News,"[id, by, author, time, time_ts, text, parent, deleted, dead, ranking]","[numeric, string, string, numeric, dateTime, string, numeric, boolean, boolean, numeric]","Context This dataset contains all stories and comments from Hacker News from its launch in 2006.  Each story contains a story id the author that made the post when it was written and the number of points the story received. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator Y Combinator. In general content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"". Content Each story contains a story ID the author that made the post when it was written and the number of points the story received. Please note that the text field includes profanity. All texts are the author’s own do not necessarily reflect the positions of Kaggle or Hacker News and are presented without endorsement. Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.hacker_news.[TABLENAME]. Fork this kernel to get started. Acknowledgements This dataset was kindly made publicly available by Hacker News under the MIT license. Inspiration  Recent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News? Hacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?  Is the amount of coverage by Hacker News predictive of a startup’s success? ",BigQuery,,"[journalism, information technology, internet, bigquery]",CC0,,,0,14959,14336,All posts from Y Combinator's social news website from 2006 to late 2017,Hacker News,https://www.kaggle.com/hacker-news/hacker-news,Mon Dec 04 2017
,SRK,"[SNo, Date, StartupName, IndustryVertical, SubVertical, CityLocation, InvestorsName, InvestmentType, AmountInUSD, Remarks]","[numeric, dateTime, string, string, string, string, string, string, string, string]",Context Interested in the Indian startup ecosystem just like me? Wanted to know what type of startups are getting funded in the last few years? Wanted to know who are the important investors? Wanted to know the hot fields that get a lot of funding these days?   This dataset is a chance to explore the Indian start up scene. Deep dive into funding data and derive insights into the future! Content This dataset has funding information of the Indian startups from January 2015 to August 2017. It includes columns with the date funded the city the startup is based out of the names of the funders and the amount invested (in USD).   For more information on the values of individual fields check out the Column Metadata. Acknowledgements Thanks to trak.in who are generous enough to share the data publicly for free.  Inspiration Possible questions which could be answered are  How does the funding ecosystem change with time? Do cities play a major role in funding? Which industries are favored by investors for funding? Who are the important investors in the Indian Ecosystem? How much funds does startups generally get in India? ,CSV,,"[india, finance, lending]",CC0,,,1969,11471,0.2978515625,Funding details of the startups in India,Indian Startup Funding,https://www.kaggle.com/sudalairajkumar/indian-startup-funding,Fri Aug 11 2017
,US Environmental Protection Agency,"[state_code, county_code, site_num, parameter_code, poc, latitude, longitude, datum, parameter_name, sample_duration, pollutant_standard, metric_used, method_name, year, units_of_measure, event_type, observation_count, observation_percent, completeness_indicator, valid_day_count, required_day_count, exceptional_data_count, null_data_count, primary_exceedance_count, secondary_exceedance_count, certification_indicator, num_obs_below_mdl, arithmetic_mean, arithmetic_standard_dev, first_max_value, first_max_datetime, second_max_value, second_max_datetime, third_max_value, third_max_datetime, fourth_max_value, fourth_max_datetime, first_max_non_overlapping_value, first_no_max_datetime, second_max_non_overlapping_value, second_no_max_datetime, ninety_nine_percentile, ninety_eight_percentile, ninety_five_percentile, ninety_percentile, seventy_five_percentile, fifty_percentile, ten_percentile, local_site_name, address, state_name, county_name, city_name, cbsa_name, date_of_last_change]","[string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, numeric, string, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, dateTime, numeric, dateTime, numeric, dateTime, numeric, dateTime, numeric, dateTime, numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, dateTime]",The AQS Data Mart is a database containing all of the information from AQS. It has every measured value the EPA has collected via the national ambient air monitoring program. It also includes the associated aggregate values calculated by EPA (8-hour daily annual etc.). The AQS Data Mart is a copy of AQS made once per week and made accessible to the public through web-based applications. The intended users of the Data Mart are air quality data analysts in the regulatory academic and health research communities. It is intended for those who need to download large volumes of detailed technical data stored at EPA and does not provide any interactive analytical tools. It serves as the back-end database for several Agency interactive tools that could not fully function without it AirData AirCompare The Remote Sensing Information Gateway the Map Monitoring Sites KML page etc. AQS must maintain constant readiness to accept data and meet high data integrity requirements thus is limited in the number of users and queries to which it can respond. The Data Mart as a read only copy can allow wider access. The most commonly requested aggregation levels of data (and key metrics in each) are Sample Values (2.4 billion values back as far as 1957 national consistency begins in 1980 data for 500 substances routinely collected) The sample value converted to standard units of measure (generally 1-hour averages as reported to EPA sometimes 24-hour averages) Local Standard Time (LST) and GMT timestamps Measurement method Measurement uncertainty where known Any exceptional events affecting the data NAAQS Averages NAAQS average values (8-hour averages for ozone and CO 24-hour averages for PM2.5) Daily Summary Values (each monitor has the following calculated each day) Observation count Observation per cent (of expected observations) Arithmetic mean of observations Max observation and time of max AQI (air quality index) where applicable Number of observations > Standard where applicable Annual Summary Values (each monitor has the following calculated each year) Observation count and per cent Valid days Required observation count Null observation count Exceptional values count Arithmetic Mean and Standard Deviation 1st - 4th maximum (highest) observations Percentiles (99 98 95 90 75 50) Number of observations > Standard Site and Monitor Information FIPS State Code (the first 5 items on this list make up the AQS Monitor Identifier) FIPS County Code Site Number (unique within the county) Parameter Code (what is measured) POC (Parameter Occurrence Code) to distinguish from different samplers at the same site Latitude Longitude Measurement method information Owner / operator / data-submitter information Monitoring Network to which the monitor belongs Exemptions from regulatory requirements Operational dates City and CBSA where the monitor is located Quality Assurance Information  Various data fields related to the 19 different QA assessments possible Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.epa_historical_air_quality.[TABLENAME]. Fork this kernel to get started. Acknowledgements Data provided by the US Environmental Protection Agency Air Quality System Data Mart.,BigQuery,,"[pollution, bigquery]",CC0,,,0,13925,330752,Air Quality Data Collected at Outdoor Monitors Across the US,Historical Air Quality,https://www.kaggle.com/epa/epa-historical-air-quality,Fri Dec 01 2017
,"Yelp, Inc.",[],[],Context This dataset is a subset of Yelp's businesses reviews and user data. It was originally put together for the Yelp Dataset Challenge which is a chance for students to conduct research or analysis on Yelp's data and share their discoveries.  In the dataset you'll find information about businesses across 11 metropolitan areas in four countries.  Content This dataset contains seven CSV files. The original JSON files can be found in yelp_academic_dataset.zip.  You may find this documentation helpful https//www.yelp.com/dataset/documentation/json In total there are   5200000 user reviews Information on 174000 businesses The data spans 11 metropolitan areas  Acknowledgements The dataset was converted from JSON to CSV format and we thank the team of the Yelp dataset challenge for creating this dataset. By downloading this dataset you agree to the Yelp Dataset Terms of Use. Inspiration Natural Language Processing & Sentiment Analysis What's in a review? Is it positive or negative? Yelp's reviews contain a lot of metadata that can be mined and used to infer meaning business attributes and sentiment. Graph Mining We recently launched our Local Graph but can you take the graph further? How do user's relationships define their usage patterns? Where are the trend setters eating before it becomes popular?,CSV,,[food and drink],Other,,,3524,22664,3072,"A trove of reviews, businesses, users, tips, and check-in data!",Yelp Dataset,https://www.kaggle.com/yelp-dataset/yelp-dataset,Wed Feb 07 2018
,CITES,"[Year, App., Taxon, Class, Order, Family, Genus, Importer, Exporter, Origin, Importer reported quantity, Exporter reported quantity, Term, Unit, Purpose, Source]","[numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context The Convention on International Trade in Endangered Species of Wild Fauna and Flora or CITES for short is an international treaty organization tasked with monitoring reporting and providing recommendations on the international species trade. CITES is a division of the IUCN which is one of the principal international organization focused on wildlife conversation at large. It is not a part of the UN (though its reports are read closely by the UN). CITES is one of the oldest conservation organizations in existence. Participation in CITES is voluntary but almost every member nation in the UN (and therefore almost every country worldwide) participates. Countries participating in CITES are obligated to report on roughly 5000 animal species and 29000 plant species brought into or exported out of their countries and to honor limitations placed on the international trade of these species. Protected species are organized into three appendixes. Appendix I species are those whose trade threatens them with extinction. Two particularly famous examples of Class I species are the black rhinoceros and the African elephant whose extremely valuable tusks are an alluring target for poachers exporting ivory abroad. There are 1200 such species. Appendix II species are those not threatened with extinction but whose trade is nevertheless detrimental. Most species in cites around 21000 of them are in Appendix II. Finally Appendix III animals are those submitted to CITES by member states as a control mechanism. There are about 170 such species and their export or import requires permits from the submitting member state(s). This dataset records all legal species imports and exports carried out in 2016 (and a few records from 2017) and reported to CITES. Species not on the CITES lists are not included; nor is the significant and highly illegal ongoing black market trading activity. Content This dataset contains records on every international import or export conducted with species from the CITES lists in 2016. It contains columns identifying the species the import and export countries and the amount and characteristics of the goods being traded (which range from live animals to skins and cadavers). For further details on individual rows and columns refer to the metadata on the /data tab. A much more detailed description of each of the fields is available in the original CITES documentation. Acknowledgements This dataset was originally aggregated by CITES and made available online through this downloader tool. The CITES downloader goes back to 1975 however it is only possible to download fully international data two years at a time (or so) due to limitations in the number of rows allowed by the data exporter. If you would like data going further back check out the downloader. Be warned though this data takes a long time to generate! This data is prepared for CITES by UNEP a division of the UN and hence likely covered by the UN Data License. Inspiration  What is the geospatial distribution of the international plant/animal trade? How much export/import activity is there for well-known species like rhinos elephants etcetera? What percent of the trade is live as opposed to animal products (ivory skins cadavers etcetera)? ,CSV,,"[animals, environment, plants, international relations]",Other,,,276,1848,0.48828125,A year in the international wildlife trade,CITES Wildlife Trade Database,https://www.kaggle.com/cites/cites-wildlife-trade-database,Mon Jan 29 2018
,nic,[],[],"Context Invariance to translation and rotation is an important attribute we would like image classifiers to have in many applications. For many problems even if there doesn't seem to be a lot of translation in the data augmenting it with these transformations is often beneficial. There are not many datasets where these transformations are clearly relevant though. The ""Snake Eyes"" dataset seeks to provide a problem where rotation and translation are clearly a fundamental aspect of the problem and not just something intuitively believed to be involved.  Image classifiers are frequently utilized in a pipeline where a bounding box is first extracted from the complete image and this process might provide centered data to the classifier. Some translation might still be present in the data the classifier sees though making the phenomenon relevant to classification nevertheless. A Snake Eyes classifier can clearly benefit from such a pre-processing. But the point here is trying to learn how much a classifier can learn to do by itself. In special we would like to demonstrate the ""built-in"" invariance to translations from CNNs. Content Snake Eyes contains artificial images simulating the a roll of one or two dice. The face patterns were modified to contain at most 3 black spots making it impossible to solve the problem by merely counting them. The data was synthesized using a Python program each image produced from a set of floating-point parameters modeling the position and angle of each dice.  The data format is binary with records of 401 bytes. The first byte contains the class (1 to 12 notice it does not start at 0) and the other 400 bytes are the image rows. We offer 1 million images split in 10 files with 100k records each and an extra test set with 10000 images. Inspiration We were inspired by the popular ""tiny image"" datasets often studied in ML research MNIST CIFAR-10 and Fashion-MNIST. Our dataset has smaller images though only 20x20 and 12 classes. The reduced proportions should help approximate the actual 3D and 6D manifolds of each class with the available number of data points (1 million images). The data is artificial with limited and very well-defined patterns noise-free and properly anti-aliased. This is not about improving from 95% to 97% accuracy and wondering if 99% is possible with a deeper network. We don't expect less than 100% precision to be achieved with any method eventually. What we are interested to see is how do different methods compare in efficiency how hard is it to train different models and how the translation and rotation invariance is enforced or achieved. We are also interested in studying the concept of manifold learning. The data has some intra-class variability due to different possible face combinations with two dice. But most of the variation comes from translation and rotation. We hope to have sampled enough data to really allow for the extraction of these manifolds in 400 dimensions and to investigate topics such as the role of pre-training and the relation between modeling the manifold of the whole data and of the separate classes. Translations alone already create quite non-convex manifolds but our classes also have the property that some linear combinations are actually a different class (e.g. two images from the ""2"" face make an image from the ""4"" class). We are curious to see how this property can make the problem more challenging to different techniques. We are also secretly hoping to have created the image-detection version of the infamous ""spiral"" problem for neural networks. We are offering the prize of one ham sandwich collected at my local café to the first person who manages to train a neural network to solve this problem convolutional or not and using just traditional techniques such as logistic or ReLU activation functions and SGD training. 99% accuracy is enough. The resulting network may be susceptible to adversarial instances this is fine but we'll be constantly complaining about it in your ear while you eat the sandwich.",Other,,"[machine learning, image data, multiclass classification]",CC0,,,219,3285,126,Tiny dice images with translation and rotation for image classification,Snake Eyes,https://www.kaggle.com/nicw102168/snake-eyes,Sun Nov 26 2017
,Dave Fisher-Hickey,[],[],"Context The UK government amassed traffic data from 2000 and 2016 recording over 1.6 million accidents in the process and making this one of the most comprehensive traffic data sets out there. It's a huge picture of a country undergoing change. Note that all the contained accident data comes from police reports so this data does not include minor incidents. Content ukTrafficAADF.csv tracks how much traffic there was on all major roads in the given time period (2000 through 2016). AADT the core statistic included in this file stands for ""Average Annual Daily Flow"" and is a measure of how activity a road segment based on how many vehicle trips traverse it. The AADT page on Wikipedia is a good reference on the subject. Accidents data is split across three CSV files accidents_2005_to_2007.csv accidents_2009_to_2011.csv and accidents_2012_to_2014.csv. These three files together constitute 1.6 million traffic accidents. The total time period is 2005 through 2014 but 2008 is missing. A data dictionary for the raw dataset at large is available from the UK Department of Transport website here. For descriptions of individual columns see the column metadata. Acknowledgements The license for this dataset is the Open Givernment Licence used by all data on data.gov.uk (here). The raw datasets are available from the UK Department of Transport website here. Inspiration  How has changing traffic flow impacted accidents? Can we predict accident rates over time? What might improve accident rates? Plot interactive maps of changing trends e.g. How has London has changed for cyclists? Busiest roads in the nation? Which areas never change and why? Identify infrastructure needs failings and successes. How have Rural and Urban areas differed (see RoadCategory)? How about the differences between England Scotland and Wales? The UK government also like to look at miles driven. You can do this by multiplying the AADF by the corresponding length of road (link length) and by the number of days in the years. What does this tell you about UK roads? ",CSV,,"[climate, automobiles, road transport, taxi services]",ODbL,,,2567,23940,621,Visualise and analyse traffic demographics,1.6 million UK traffic accidents,https://www.kaggle.com/daveianhickey/2000-16-traffic-flow-england-scotland-wales,Sun Sep 17 2017
,freeCodeCamp,[],[],"Context freeCodeCamp is a web-based non-profit organization and learning platform which teaches programming newcomers how to code. It was founded by Quincy Larson in 2014 who in a 2015 interview stated that ""freeCodeCamp is my effort to correct the extremely inefficient and circuitous way I learned to code...all those things that made learning to code a nightmare to me are things that we are trying to fix with freeCodeCamp."" The original curriculum took approximately 800 hours to complete; today after several refreshes and additions there is over 2000 hours of learner's content on the site. freeCodeCamp also provides several helpful secondary resources for learners. One of them is a freeCodeCamp Gitter chatroom. Gitter is an open-source instant messaging service that lets users share thoughts and ideas with one another. This dataset is a record of activity from this /freeCodeCamp Gitter chatroom containing posts from students bots moderators and contributors between December 31 2014 and December 9 2017. Content The data includes the usernames screen names timestamps post content and metadata of every post made to /freeCodeCamp in the aforementioned three year period. This comes out to nearly 5 million records overall. The data comes in the form of a set of three JSON files each named freecodecamp_casual_chatroom_[01/02/03].json. The three files make a continuous dataset containing all posts sent during the aforementioned period but note that they overlap in a few days. The three files were extracted on 01-06-2016 09-03-2017 and 12-12-2017 respectively. The included convert.py file was used to concatenate these files into a unified CSV file freecodecamp_casual_chatroom.csv. Some preliminary analyses using this dataset can be found at the Github repository for the freeCodeCamp's Open Data Initiative. For more details on specific elements of the dataset refer to the column metadata tab or to the detailed documentation provided in the Gitter API Documentation. Acknowledgements The datasets are a contribution from freeCodeCamp as part of the freeCodeCamp's Open Data Initiative. More information about the rationale of this initiative can be found on this Medium article. This dataset was extracted using Python code over the Gitter API. All the files were prepared by Evaristo Caraballo (GitHub @evaristoc). Records are not anonymised or modified and are presented ""as they are"". Thanks to freeCodeCamp team specially to Quincy Larson for supporting the initiative. Thanks to all freeCodeCamp students who kindly allowed to share their personal progress and to Gitter for making these data available. If you have questions about this dataset please contact quincy@freecodecamp.com or get in touch with us through https//gitter.im/FreeCodeCamp/DataScience (Gitter registration might be required). Inspiration  This dataset presents three years of developer chat about web technologies. Can you use it to trace the rise and fall over certain technologies like Angular and React over time? What do programming newcomers tend to get stuck on the most? Are there any canned responses that are particularly common in the freeCodeCamp chatroom? What other interesting insights into programmer culture can you glean from examining this dataset? ",{}JSON,,"[social groups, education, internet]",Other,,,51,1172,719,Three years and 5 million posts in freeCodeCamp chat,"freeCodeCamp Gitter Chat, 2015-2017",https://www.kaggle.com/free-code-camp/all-posts-public-main-chatroom,Wed Jan 31 2018
,UCI Machine Learning,"[v1, v2, , , ]","[string, string, string, string, string]",Context The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5574 messages tagged acording being ham (legitimate) or spam.  Content The files contain one message per line. Each line is composed by two columns v1 contains the label (ham or spam) and v2 contains the raw text.  Acknowledgements The original dataset can be found here. The creators would like to note that in case you find the dataset useful please make a reference to previous paper and the web page http//www.dt.fee.unicamp.br/~tiago/smsspamcollection/ in your papers research etc. Inspiration  Can you use this dataset to build a prediction model that will accurately classify which texts are spam? ,CSV,,"[languages, linguistics, human-computer interaction]",Other,,,7362,51113,0.48046875,Collection of SMS messages tagged as spam or legitimate,SMS Spam Collection Dataset,https://www.kaggle.com/uciml/sms-spam-collection-dataset,Sat Dec 03 2016
,KP,[],[], Introduction Video games are a rich area for data extraction due to their digital nature.  Notable examples such as the complex EVE Online economy World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think.  Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.  In this Kaggle Dataset I provide just over 1400 competitive matchmaking matches from Valve's game Counter-strike Global Offensive (CSGO).  The data was extracted from competitive matchmaking replays submitted to csgo-stats.  I intend for this data-set to be purely exploratory however users are free to create their own predictive models they see fit.   About Counter-Strike Global Offensive Counter-Strike Global Offensive is a first-person shooter game pitting two teams of 5 players against each other.  Within a maximum of 30 rounds the two teams find themselves on either side as a Counter Terrorist or Terrorist.  Both sides are tasked with eliminating the opposition or as the terrorist team planting the C4 bomb at a bomb site and allowing it to explode.  Rounds are played out until either of those two objectives or if the maximum time is reached (in which the counter terrorists then win by default).  At the end of the 15th round the two teams switch sides and continue until one team reaches 16 round wins first.  CSGO is widely known for its competitive aspect of technical skill teamwork and in-game strategies.  Players are constantly rewarded with the efforts they put it in training and learning through advancing in rank. Click here to read more about the competitive mechanics of CSGO. Content This dataset within the 1400 matches provides every successful entry of duels (or battle) that took place for a player.  That is each row documents an event when a player is hurt by another player (or World e.g fall damage).  There are over 900000 entries within more than 31500 rounds. mm_master_demos.csv contains information on rounds fired while mm_grenades_demos.csv contains information on grenades thrown. The fields in the two datasets are similar highlights include shooters and victims event coordinates and timestamps. The datasets also includes static information on the match winner player ranks before and after the match and other miscellaneous match-level metadata. For further information on individual fields in the dataset refer to the Column Metadata. Interpreting Positional Data This dataset also includes a selection of the game's official radar maps as well as a table map_data.csv to aid in mapping data over them. The XY coordinates included in the dataset are all in in-game coordinates and need to be linearly scaled to be plotted on any official radar maps.  See converting to map coordinates for more information. Acknowledgements  Definitely the guys from csgo-stats without them this wouldn't have been possible! ) /r/globaloffensive for many years of lulz Akiver of CSGO Demo Manager for spending so much time perfecting his demo parser. ,Other,,"[video games, internet]",CC4,,,649,8192,366,"Damage/Grenade entries on over 35,000 rounds played in competitive matchmaking",CS:GO Competitive Matchmaking Data,https://www.kaggle.com/skihikingkevin/csgo-matchmaking-damage,Thu Oct 12 2017
,BioSENSE @ UC Berkeley School of Information,"[, id, indra_time, browser_latency, reading_time, attention_esense, meditation_esense, eeg_power, raw_values, signal_quality, createdAt, updatedAt, label]","[numeric, numeric, dateTime, numeric, dateTime, numeric, numeric, string, string, numeric, dateTime, dateTime, string]","Context EEG devices are becoming cheaper and more inconspicuous but few applications leverage EEG data effectively in part because there are few large repositories of EEG data. The MIDS class at the UC Berkeley School of Information is sharing a dataset collected using consumer-grade brainwave-sensing headsets along with the software code and visual stimulus used to collect the data. The dataset includes all subjects' readings during the stimulus presentation as well as readings from before the start and after the end of the stimulus. Content We presented two slightly different stimuli to two different groups. Stimuli 1 is available here and stimuli 2 is available here.  For both stimuli a group of about 15 people saw the stimuli at the same time while EEG data was being collected. The stimuli each person saw is available in the session field of subject-metadata.csv. (Subjects who saw stimulus 2 left the room during stimulus 1 and vice versa). Find the synchronized times for both stimuli in stimulus-timing.csv. For each participant we also anonymously collected some other metadata (1) whether or not they had previously seen the video displayed during the stimulus (a superbowl ad) (2) gender (3) whether or not they saw hidden icons displayed during the color counting exercise and (4) their chosen color during the color counting exercise. All of these can be found in subject-metadata.csv. We also collected the timing (in indra_time) of all stimulus events for both session 1 and session 2. These times are included in stimulus-times.csv. The server receives one data packet every second from each Mindwave Mobile device and stores the data in one row entry. Acknowledgements Please use the following citation if you publish your research results using this dataset or software code or stimulus file John Chuang Nick Merrill Thomas Maillart and Students of the UC Berkeley Spring 2015 MIDS Immersion Class. ""Synchronized Brainwave Recordings from a Group Presented with a Common Audio-Visual Stimulus (May 9 2015)."" May 2015.",CSV,,"[healthcare, human-computer interaction]",CC4,,,1208,13488,101,Brainwave recordings from a group presented with a shared audio-visual stimulus,Synchronized Brainwave Dataset,https://www.kaggle.com/berkeley-biosense/synchronized-brainwave-dataset,Thu Oct 27 2016
,Stack Overflow,"[Id, OwnerUserId, CreationDate, ParentId, Score, Body]","[numeric, numeric, dateTime, numeric, numeric, string]",Context Full text of all questions and answers from Stack Overflow that are tagged with the python tag. Useful for natural language processing and community analysis. See also the dataset of R questions. Content This dataset is organized as three tables  Questions contains the title body creation date score and owner ID for each Python question. Answers contains the body creation date score and owner ID for each of the answers to these questions. The ParentId column links back to the Questions table. Tags contains the tags on each question besides the Python tag.  Questions may be deleted by the user who posted them. They can also be closed by community vote if the question is deemed off-topic for instance. Such questions are not included in this dataset.  The dataset contains questions all questions asked between August 2 2008 and Ocotober 19 2016. License All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.,CSV,,"[internet, programming languages]",Other,,,1551,16243,2048,Full text of Stack Overflow Q&A about the Python programming language,Python Questions from Stack Overflow,https://www.kaggle.com/stackoverflow/pythonquestions,Fri Oct 21 2016
,ColinMorris,"[url, width, height, format, depth, tld, fname, file_size, color_mode, colorspace, compression, nimgs, split_index]","[string, numeric, numeric, string, numeric, string, string, numeric, string, string, string, numeric, numeric]","Context Favicons are the (usually tiny) image files that browsers may use to represent websites in tabs in the URL bar or for bookmarks. Kaggle for example uses an image of a blue lowercase ""k"" as its favicon. This dataset contains about 360000 favicons from popular websites. Content and Acknowledgements These favicons were scraped in July 2016. I wrote a crawler that went through Alexa's top 1 million sites and made a request for 'favicon.ico' at the site root. If I got a 200 response code I saved the result as ${site_url}.ico. For domains that were identical but for the TLD (e.g. google.com google.ca google.jp...) I scraped only one favicon. My scraping/cleaning code is on GitHub here. Of 1m sites crawled 540k responded with a 200 code. The dataset has 360k images which were the remains after filtering out  empty files (-140k) non-image files according to the file command (-40k). These mostly had type HTML ASCII or UTF-*. corrupt/malformed image files - i.e. those that were sufficiently messed up that ImageMagick failed to parse them. (-1k)  The remaining files are exactly as I received them from the site. They are mostly ICO files with the most common sizes being 16x16 32x32 and 48x48. But there's a long tail of more exotic formats and sizes (there is at least one person living among us who thought that 88x31 was a fine size for a favicon). The favicon files are divided among 6 zip files full-0.zip full-1.zip... full-5.zip. (If you wish to download the full dataset as a single tarball you can do so from the Internet Archive) favicon_metadata.csv is a csv file with one row per favicon in the dataset. The split_index says which of the zip files the image landed in. For an example of loading and interacting with particular favicons in a kernel context check out the Favicon helper functions kernel. As mentioned above the full dataset is a dog's breakfast of different file formats and dimensions. I've created 'standardized' subsets of the data that may be easier to work with (particularly for machine learning applications where it's necessary to have fixed dimensions). 16_16.tar.gz is a tarball containing all 16x16 favicons in the dataset converted to PNG. It has 290k images. ICO is a container format and many of the ico files in the raw dataset contain several versions of the same favicon at different resolutions. 16x16 favicons that were stuffed together in an ICO file with images of other sizes are included in this set. But I did no resizing - if a favicon has no 'native' 16x16 version it isn't in this set. 16_16_distinct.tar.gz is identical to the above but with 70k duplicate or near-duplicate images removed. There are a small number of commonly repeated favicons like the Blogger ""B"" that occur thousands of times which could be an annoyance depending on the use case - e.g. a generative model might get stuck in a local maximum of spitting out Blogger Bs. Alexa's top 1-million list includes 'adult' sites so some URLs and favicons may be NSFW or offensive. (It's pretty hard to make a credible depiction of nudity in 256 pixels but there are some occasional attempts.) Inspiration I hope this dataset might be especially useful for small-scale deep learning experiments. Scaling photographs down to 16x16 would render many of them unintelligible but these favicons were born tiny. The 16_16 fold has more instances than MNIST and the images are even smaller! (Though unlike MNIST most of the images in this dataset are not grayscale.) If you liked this you should also check out the recently released Large Logo Dataset. They've currently made available 550k favicons resized to 32x32. Their data was collected more recently and their scraping process was more robust so their dataset should probably be preferred (though you might still want to use this one if you need the raw favicon files or if you prefer to use 16x16 non-resized images).",Other,,"[internet, image data]",ODbL,,,455,6052,837,"Image data and metadata for 360,000 favicons scraped from popular websites",Favicons,https://www.kaggle.com/colinmorris/favicons,Wed Nov 15 2017
,Itamar Mushkin,"[settlement_name_hebrew, settlement_code, settlement_name_english, booth_number, Registered_voters, votes, bad_votes, proper_votes, Ale Yarok, Am Shalem, Balad, Brit Olam, Daam Workers Party, Eretz Hadasha, Green Party, Hadash, Haim Bekavod, Hatnua, Hope for Change, HaYisraelim, Kadima, Kalkala, Koah LeHashpi'a, The Jewish Home, Labour Party, Leader, Likud Beitenu, Meretz, Moreshet Avot, Na Nach, Or, Otzma LeYisrael, Pirate Party, Raam-Taal, Senior Citizens Party, Shas, Social Justice, United Torah Judaism, We're Brothers, Yesh Atid]","[string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset contains results from the 2015 and 2013 elections in Israel. Results are given by voting booths (of comparable sizes of 0-800) and not by settlements (which are very varied - think Tel Aviv compared to a small kibbutz). Content The first seven columns are information about each settlement and voting booth and from the eighth to the end is the number of votes each party has received in each booth. Acknowledgements This data is freely available at http//votes20.gov.il/ and http//www.votes-19.gov.il/nationalresults I just translated the column headers into English. Settlement names are translated according to this key from the Central Bureau of Statistics which uses the same settlement_code as the election results. Inspiration Personally I've viewed this dataset in order to map out the relationships between different parties (i.e which are 'closer' which are more 'central'). This question is significant in Israel where the composition of the parliament is determined almost directly by the popular vote (e.g a party with 25% of the total proper votes will recieve 25% of seats in parliament) but the government is formed by a coalition of parties (so the head of the largest party in parliament will not necessarily be the Prime Minister).,CSV,,[politics],CC0,,,244,2018,1,Vote statistics per party and booth for two Israeli election cycles,2013 and 2015 Israeli Elections,https://www.kaggle.com/itamarmushkin/israeli-elections-2015-2013,Sun Dec 17 2017
,DataSF,"[Patron Type Code, Patron Type Definition, Total Checkouts, Total Renewals, Age Range, Home Library Code, Home Library Definition, Circulation Active Month, Circulation Active Year, Notice Preference Code, Notice Preference Definition, Provided Email Address, Year Patron Registered, Outside of County, Supervisor District]","[numeric, string, numeric, numeric, string, string, string, string, numeric, string, string, boolean, numeric, boolean, string]",Context San Francisco's Integrated Library System (ILS) is composed of bibliographic records including inventoried items patron records and circulation data. The data is used in the daily operation of the library including circulation online public catalog cataloging acquisitions collection development processing and serials control. This dataset represents the usage of inventoried items by patrons (~420K records). Content The dataset includes approximately 420000 records with each record representing an anonymized library patron. Individual columns include statistics on the type code and age of the patron the year the patron registered (only since 2003) and how heavily the patron has been utilizing the library system (in terms of number of checkouts) since first registering. For more information on specific columns refer to the official data dictionary and the information in the Column Metadata on the /Data tab. Acknowledgements The data is provided by SF Public Library via the San Francisco Open Data Portal under the PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL). Photo via Flickr Kolya Miller (CC BY-NC-SA 2.0). Inspiration  What attributes are most associated with library activity (# of checkouts # of renewals)? Can you group the data into type of patrons?  What classifiers would you use to predict patron type? ,CSV,,[libraries],Other,,,804,7356,33,"Anonymized library usage data by over 420,000 patrons",San Francisco Library Usage,https://www.kaggle.com/datasf/sf-library-usage-data,Sat Jan 07 2017
,Massachusetts Institute of Technology,"[article_id, full_name, sex, birth_year, city, state, country, continent, latitude, longitude, occupation, industry, domain, article_languages, page_views, average_views, historical_popularity_index]","[numeric, string, string, numeric, string, string, string, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric]",Context Pantheon is a project celebrating the cultural information that endows our species with these fantastic capacities. To celebrate our global cultural heritage we are compiling analyzing and visualizing datasets that can help us understand the process of global cultural development. Dive in visualize and enjoy. Content The Pantheon 1.0 data measures the global popularity of historical characters using two measures. The simpler of the two measures which we denote as L is the number of different Wikipedia language editions that have an article about a historical character. The more sophisticated measure which we name the Historical Popularity Index (HPI) corrects L by adding information on the age of the historical character the concentration of page views among different languages the coefficient of variation in page views and the number of page views in languages other than English. For annotations of specific values visit the column metadata in the /Data tab. A more comprehensive breakdown is available on the Parthenon website. Acknowledgements Pantheon is a project developed by the Macro Connections group at the Massachusetts Institute of Technology Media Lab. For more on the dataset and to see visualizations using it visit its landing page on the MIT website. Inspiration Which historical figures have a biography in the most languages? Who received the most Wikipedia page views? Which occupations or industries are the most popular? What country has the most individuals with a historical popularity index over twenty?,CSV,,"[history, internet]",CC4,,,232,2909,1,Record of every historical figure with Wikipedia biography in 25+ languages,Pantheon Project: Historical Popularity Index,https://www.kaggle.com/mit/pantheon-project,Thu Mar 02 2017
,Stanford Network Analysis Project ,[],[],Context This dataset consists of reviews of fine foods from amazon. The data span a period of more than 10 years including all ~500000 reviews up to October 2012. Reviews include product and user information ratings and a plain text review. It also includes reviews from all other Amazon categories. Contents  Reviews.csv Pulled from the corresponding SQLite table named Reviews in database.sqlite database.sqlite Contains the table 'Reviews'  Data includes - Reviews from Oct 1999 - Oct 2012 - 568454 reviews - 256059 users - 74258 products - 260 users with > 50 reviews  Acknowledgements See this SQLite query for a quick sample of the dataset. If you publish articles based on this dataset please cite the following paper  J. McAuley and J. Leskovec. From amateurs to connoisseurs modeling the evolution of user expertise through online reviews. WWW 2013. ,SQLite,,"[food and drink, linguistics, internet]",CC0,,,16167,119981,642,"Analyze ~500,000 food reviews from Amazon",Amazon Fine Food Reviews,https://www.kaggle.com/snap/amazon-fine-food-reviews,Tue May 02 2017
,Facebook,[],[],English Word Vectors from Common Crawl  About fastText fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words even made-up ones. Indeed fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words. About the vectors These pre-trained vectors contain 2 million word vectors trained on Common Crawl (600B tokens). The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency. Acknowledgements These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0. P. Bojanowski* E. Grave* A. Joulin T. Mikolov Enriching Word Vectors with Subword Information A. Joulin E. Grave P. Bojanowski T. Mikolov Bag of Tricks for Efficient Text Classification A. Joulin E. Grave P. Bojanowski M. Douze H. Jégou T. Mikolov FastText.zip Compressing text classification models (* These authors contributed equally.),Other,,"[nlp, pre-trained model]",CC0,,,132,952,1024, 2 million word vectors trained on Common Crawl,fatstText Common Crawl,https://www.kaggle.com/facebook/fatsttext-common-crawl,Tue Feb 13 2018
,UCI Machine Learning,[],[],"Context The dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck  such as braking and gear changes. The datasets' positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data selected by experts.  Content The training set contains 60000 examples in total in which 59000 belong to the negative class and 1000 positive class. The test set contains 16000 examples. There are 171 attributes per record. The attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. Typically the histograms have open-ended conditions at each end. For example if we measuring the ambient temperature ""T"" then the histogram could be defined with 4 bins where  The attributes are as follows class then anonymized operational data. The operational data have an identifier and a bin id like ""Identifier_Bin"". In total there are 171 attributes of which 7 are histogram variables. Missing values are denoted by ""na"". Acknowledgements This file is part of APS Failure and Operational Data for Scania Trucks. It was imported from the UCI ML Repository. Inspiration The total cost of a prediction model the sum of Cost_1  multiplied by the number of Instances with type 1 failure and Cost_2 with the number of instances with type 2 failure resulting in a Total_cost. In this case Cost_1 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop while Cost_2 refer to the cost of missing a faulty truck which may cause a breakdown.  Cost_1 = 10 and Cost_2 = 500 and Total_cost = Cost_1*No_Instances + Cost_2*No_Instances. Can you create a model which accurately predicts and minimizes [the cost of] failures?",CSV,,[],GPL,,,47,474,36,Predict failures and minimize costs based on sensor readings, Air pressure system failures in Scania trucks,https://www.kaggle.com/uciml/aps-failure-at-scania-trucks-data-set,Mon Feb 19 2018
,Department of Transportation,"[state_number, state_name, consecutive_number, number_of_vehicle_forms_submitted_all, number_of_motor_vehicles_in_transport_mvit, number_of_parked_working_vehicles, number_of_forms_submitted_for_persons_not_in_motor_vehicles, number_of_persons_not_in_motor_vehicles_in_transport_mvit, number_of_persons_in_motor_vehicles_in_transport_mvit, number_of_forms_submitted_for_persons_in_motor_vehicles, county, city, day_of_crash, month_of_crash, year_of_crash, day_of_week, hour_of_crash, minute_of_crash, national_highway_system, land_use, land_use_name, functional_system, functional_system_name, ownership, ownership_name, route_signing, route_signing_name, trafficway_identifier, trafficway_identifier_2, milepoint, latitude, longitude, special_jurisdiction, special_jurisdiction_name, first_harmful_event, first_harmful_event_name, manner_of_collision, manner_of_collision_name, relation_to_junction_within_interchange_area, relation_to_junction_specific_location, relation_to_junction_specific_location_name, type_of_intersection, work_zone, relation_to_trafficway, relation_to_trafficway_name, light_condition, light_condition_name, atmospheric_conditions_1, atmospheric_conditions_1_name, atmospheric_conditions_2, atmospheric_conditions_2_name, atmospheric_conditions, atmospheric_conditions_name, school_bus_related, rail_grade_crossing_identifier, hour_of_notification, minute_of_notification, hour_of_arrival_at_scene, minute_of_arrival_at_scene, hour_of_ems_arrival_at_hospital, minute_of_ems_arrival_at_hospital, related_factors_crash_level_1, related_factors_crash_level_1_name, related_factors_crash_level_2, related_factors_crash_level_2_name, related_factors_crash_level_3, related_factors_crash_level_3_name, number_of_fatalities, number_of_drunk_drivers, timestamp_of_crash]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, numeric, string, string, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, numeric, dateTime]",Fatality Analysis Reporting System (FARS) was created in the United States by the National Highway Traffic Safety Administration (NHTSA) to provide an overall measure of highway safety to help suggest solutions and to help provide an objective basis to evaluate the effectiveness of motor vehicle safety standards and highway safety programs. FARS contains data on a census of fatal traffic crashes within the 50 States the District of Columbia and Puerto Rico. To be included in FARS a crash must involve a motor vehicle traveling on a trafficway customarily open to the public and result in the death of a person (occupant of a vehicle or a non-occupant) within 30 days of the crash. FARS has been operational since 1975 and has collected information on over 989451 motor vehicle fatalities and collects information on over 100 different coded data elements that characterizes the crash the vehicle and the people involved. FARS is vital to the mission of NHTSA to reduce the number of motor vehicle crashes and deaths on our nation's highways and subsequently reduce the associated economic loss to society resulting from those motor vehicle crashes and fatalities. FARS data is critical to understanding the characteristics of the environment trafficway vehicles and persons involved in the crash. NHTSA has a cooperative agreement with an agency in each state government to provide information in a standard format on fatal crashes in the state. Data is collected coded and submitted into a micro-computer data system and transmitted to Washington D.C. Quarterly files are produced for analytical purposes to study trends and evaluate the effectiveness highway safety programs. Content There are 40 separate data tables. You can find the manual which is too large to reprint in this space here. Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.nhtsa_traffic_fatalities.[TABLENAME]. Fork this kernel to get started. Acknowledgements This dataset was provided by the National Highway Traffic Safety Administration.,BigQuery,,"[automobiles, bigquery]",CC0,,,0,5003,580,Fatal car crashes for 2015-2016,US Traffic Fatality Records,https://www.kaggle.com/usdot/nhtsa-traffic-fatalities,Fri Dec 01 2017
,Cam Nugent,"[date, open, high, low, close, volume, Name]","[dateTime, numeric, numeric, numeric, numeric, numeric, string]",Context Stock market data can be interesting to analyze and as a further incentive strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here I provide a dataset with historical stock prices (last 5 years) for all companies currently found on the S&P 500 index.  The script I used to acquire all of these .csv files can be found in this GitHub repository  In the future if you wish for a more up to date dataset this can be used to acquire new versions of the .csv files. Feb 2018 note I have just updated the dataset to include data up to Feb 2018. I have also accounted for changes in the stocks on the S&P 500 index (RIP whole foods etc. etc.). Content The data is presented in a couple of formats to suit different individual's needs or computational limitations. I have included files containing 5 years of stock data (in the all_stocks_5yr.csv and corresponding folder). The folder individual_stocks_5yr contains files of data for individual stocks labelled by their stock ticker name. The all_stocks_5yr.csv contains the same data presented in a merged .csv file. Depending on the intended use (graphing modelling etc.) the user may prefer one of these given formats. All the files have the following columns Date - in format yy-mm-dd  Open - price of the stock at market open (this is NYSE data so all in USD) High - Highest price reached in the day Low Close - Lowest price reached in the day Volume - Number of shares traded Name - the stock's ticker name Acknowledgements Due to volatility in google finance for the newest version I have switched over to acquiring the data from The Investor's Exchange api the simple script I use to do this is found here. Special thanks to Kaggle Github pandas_datareader and The Market. Inspiration This dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time graph an compare multiple stocks at once or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is can you develop a model that can beat the market and allow you to make statistically informed trades!,Other,,[finance],CC0,,,3708,28653,19,Historical stock data for all current S&P 500 companies,S&P 500 stock data,https://www.kaggle.com/camnugent/sandp500,Sat Feb 10 2018
,GunnvantSaini,[],[],Context This dataset contains every line from every season of the HBO TV show Game of Thrones. Content Each season has one JSON file. In eachJSON file there is a key for each episode and each episode is further mapped at a dialogue level. Inspiration The idea is to use this data set to see if one can create a summary of what transpired in each episode or season.,{}JSON,,"[writing, popular culture, film]",CC0,,,54,553,2,Subtitles for each episode across 7 seasons,Game of Thrones Subtitles,https://www.kaggle.com/gunnvant/game-of-thrones-srt,Tue Feb 20 2018
,Oxford Poverty & Human Development Initiative,"[ISO, Country, MPI Urban, Headcount Ratio Urban, Intensity of Deprivation Urban, MPI Rural, Headcount Ratio Rural, Intensity of Deprivation Rural]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context Most countries of the world define poverty as a lack of money. Yet poor people themselves consider their experience of poverty much more broadly. A person who is poor can suffer from multiple disadvantages at the same time – for example they may have poor health or malnutrition a lack of clean water or electricity poor quality of work or little schooling. Focusing on one factor alone such as income is not enough to capture the true reality of poverty. Multidimensional poverty measures can be used to create a more comprehensive picture. They reveal who is poor and how they are poor – the range of different disadvantages they experience. As well as providing a headline measure of poverty multidimensional measures can be broken down to reveal the poverty level in different areas of a country and among different sub-groups of people. Content OPHI researchers apply the AF method and related multidimensional measures to a range of different countries and contexts. Their analyses span a number of different topics such as changes in multidimensional poverty over time comparisons in rural and urban poverty and inequality among the poor. For more information on OPHI’s research see our working paper series and research briefings. OPHI also calculates the Global Multidimensional Poverty Index MPI which has been published since 2010 in the United Nations Development Programme’s Human Development Report. The Global MPI is an internationally-comparable measure of acute poverty covering more than 100 developing countries. It is updated by OPHI twice a year and constructed using the AF method. The Alkire Foster (AF) method is a way of measuring multidimensional poverty developed by OPHI’s Sabina Alkire and James Foster. Building on the Foster-Greer-Thorbecke poverty measures it involves counting the different types of deprivation that individuals experience at the same time such as a lack of education or employment or poor health or living standards. These deprivation profiles are analysed to identify who is poor and then used to construct a multidimensional index of poverty (MPI). For free online video guides on how to use the AF method see OPHI’s online training portal. To identify the poor the AF method counts the overlapping or simultaneous deprivations that a person or household experiences in different indicators of poverty. The indicators may be equally weighted or take different weights. People are identified as multidimensionally poor if the weighted sum of their deprivations is greater than or equal to a poverty cut off – such as 20% 30% or 50% of all deprivations. It is a flexible approach which can be tailored to a variety of situations by selecting different dimensions (e.g. education) indicators of poverty within each dimension (e.g. how many years schooling a person has) and poverty cut offs (e.g. a person with fewer than five years of education is considered deprived). The most common way of measuring poverty is to calculate the percentage of the population who are poor known as the headcount ratio (H). Having identified who is poor the AF method generates a unique class of poverty measures (Mα) that goes beyond the simple headcount ratio. Three measures in this class are of high importance Adjusted headcount ratio (M0) otherwise known as the MPI This measure reflects both the incidence of poverty (the percentage of the population who are poor) and the intensity of poverty (the percentage of deprivations suffered by each person or household on average). M0 is calculated by multiplying the incidence (H) by the intensity (A). M0 = H x A. Find out about other ways the AF method is used in research and policy. Additional data here. Acknowledgements Alkire S. and Robles G. (2017). “Multidimensional Poverty Index Summer 2017 Brief methodological note and results.” OPHI Methodological Note 44 University of Oxford. Alkire S. and Santos M. E. (2010). “Acute multidimensional poverty A new index for developing countries.” OPHI Working Papers 38 University of Oxford. Alkire S. Jindra C. Robles G. and Vaz A. (2017). ‘Multidimensional Poverty Index – Summer 2017 brief methodological note and results’. OPHI MPI Methodological Notes No. 44 Oxford Poverty and Human Development Initiative University of Oxford. Inspiration  Which countries exhibit the largest subnational disparities in MPI? Which countries have high per-capita incomes yet still rank highly in MPI? ,CSV,,"[government agencies, public health, finance]",CC0,,,141,742,0.0751953125,Indexing different types of simultaneous deprivation ,Multidimensional Poverty Measures,https://www.kaggle.com/ophi/mpi,Fri Feb 16 2018
,Open AQ,"[location, city, country, pollutant, value, timestamp, unit, source_name, latitude, longitude, averaged_over_in_hours]","[string, string, string, string, numeric, dateTime, string, string, numeric, numeric, numeric]","OpenAQ is an open-source project to surface live real-time air quality data from around the world. Their “mission is to enable previously impossible science impact policy and empower the public to fight air pollution.” The data includes air quality measurements from 5490 locations in 47 countries. Scientists researchers developers and citizens can use this data to understand the quality of air near them currently. The dataset only includes the most current measurement available for the location (no historical data).  Update Frequency Weekly Querying BigQuery tables You can use the BigQuery Python client library to query tables in this dataset in Kernels. Note that methods available in Kernels are limited to querying data. Tables are at bigquery-public-data.openaq.[TABLENAME]. Fork this kernel to get started. Acknowledgements Dataset Source openaq.org Use This dataset is publicly available for anyone to use under the following terms provided by the Dataset Source and is provided ""AS IS"" without any warranty express or implied.",BigQuery,,"[pollution, bigquery]",CC4,,,0,8329,2,Global Air Pollution Measurements,OpenAQ,https://www.kaggle.com/open-aq/openaq,Fri Dec 01 2017
,Peter Romov,"[match, time, slot, text]","[numeric, numeric, numeric, string]","Dataset This dataset contains chat messages from Dota 2 — video game by Valve one of the most popular eSport discipline. The dataset was used to train Roflan bot. It contains chats of almost 1M matches from public matchmaking (when players are selected by the game server at random with about the same skill level). Caution and Disclaimer Important please read. This dataset is completely Not Safe For Work. In Dota 2 the players communicate with each other in a very specific way. For instance you may found a lot of abbreviations and game-specific terms. For Dota 2 player it is typical to blame teammates and opponents for failure in the game. Unfortunately many messages may contain coarse insults humiliation of another player's family expressions of racism and other awful things. We provide the messages ""as is"" without any filters and censorship and we are not responsible for offensive content inside the data.  Our goal is to give researchers an opportunity to explore players community by diving into a real dialogs. We want to draw an attention to the problem of outstanding toxicity of the most Dota 2 players we consider this behaviour of players unhealthy. Usage of the dataset  See rough explanations on how do we learn our Roflan bot intended to mirror typical player's chat behaviour. You can apply your own language models on this dataset and make alternative chat bot or just compare performance of learning. Look at this arXiv paper with analysis of esport spectators' chats. You can apply similar analysis to game participants chats. ",CSV,,"[video games, text data]",Other,,,18,521,313,Anonymized chats from Dota 2 match replays,GOSU.AI Dota 2 Game Chats,https://www.kaggle.com/romovpa/gosuai-dota-2-game-chats,Mon Feb 19 2018
,Eclipse MegaMovie,"[upload_session_id, model, lon, equatorial_mount, make, id, detected_circle, uploaded_date, camera_datetime, storage_uri, exposure_time, state, image_bucket, image_datetime, lat, vision_labels, user, is_mobile, image_type, width, totality, datetime_repaired, height, aperture_value]","[string, string, numeric, boolean, string, string, unknown, dateTime, dateTime, string, string, string, string, dateTime, numeric, string, string, boolean, string, numeric, boolean, boolean, numeric, numeric]",This first-of-its-kind citizen science project is a collection of photos submitted by a group of dedicated volunteers from locations across the United States during the August 21 2017 total solar eclipse.  The bigquery tables include metadata for the photos links to the photos and astronomical measurements extracted from the photos. Acknowledgements This dataset was kindly prepared by Google UC Berkeley and thousands of volunteers. Please see https//eclipsemega.movie/ for more information. Inspiration  Can you map out the locations of the contributors to the project? How many of them were outside the path of totality? ,BigQuery,,"[astronomy, bigquery]",CC0,,,0,866,77,Citizen science covering the 2017 eclipse,Eclipse Megamovie,https://www.kaggle.com/eclipse-megamovie/eclipse-megamovie,Thu Dec 07 2017
,Abhishek,[],[],Context The Dataset here is the CSV (Comma Separated Value) formatted data of 1000+ Indian companies' historical stock data which are listed on NSE web scrapped using python. This data helps the community to dive into algorithmic trading using the ML techniques and can be used for any task. Hope this will be of great use for everyone.  Content This dataset(.zip) is a collection of numerous CSV formatted files that are in format of ['Date''Open''high''low''close''adj close''volume']. I've acquired this data using the yahoo finance v7 server using the python requests and a bit of pre-processing.  Maruti_data.csv is the sample data of Maruti stock data from 2003-07 to till data (updated on 18-Feb-2018) . Companies_dict.d is the python pickle dictionary variable to get company name from the SYMBOL or name if the csv file. You can load this using the pickle library and get the actual company SYMBOL to Legal Name.  e.g.Python Code  Symbol2Name =  pickle.load(open('company_symbol_name_dict.d''rb')) print(Symbol2Name['MARUTI']) #Will give you Maruti_Suzuki_India_Ltd Acknowledgements I would like to thank this githubrepo for making the python file this script of mine is based on. Inspiration I would love to see many people like me to get their hands dirty with this data and use it effectively to correlate the inter relationships among the companies.,Other,,"[business, finance, money]",CC0,,,62,467,63,Indian stock data from the NSE,NSE Listed 1000+ Companies' Historical Data,https://www.kaggle.com/abhishekyana/nse-listed-1384-companies-data,Mon Feb 19 2018
,Obrocka,"[Disposed [ton], Inventory [ton], Place Name, TERYT, Latitude, Longitude]","[numeric, numeric, string, numeric, numeric, numeric]",Context First what is asbestos? Well it is a mineral that can be pulled into fine fibres with high resistance to heat electricity and chemical corrosion. In the past it was a common ingredient in construction materials (caveat this is at least true for the European Union). Why in the past? Asbestos is a threat to health due to its very fibre structure. Those microscopic fibers can become trapped in the respiratory system causing cancer and other disease decades after exposure. Second where is Poland?! The answer depends on how grumpy the internet is on that day. My home country Poland is located in the Eastern or Central Europe. Poland joined the European Union in 2004 and suddenly stuff was required of her. Strangely enough Poland is the only European country that plans to be free of asbestos by 2032. The National Asbestos Cleaning Program program was initiated in 2009 with one of the aims to create a complete database of asbestos contamination by 2012. In this blog post I’m hoping to shed some light on the progress of this ambitious plan. Content The database is run by the Ministry of Development and should be updated yearly. It was originally uploaded on March 21st 2016 and then updated 8 months later. As far as I can see they don’t keep older versions. The spreadsheet contains columns with the total number of asbestos in the given location (in kilograms) how much of that has been utilised (also in kilograms) and how much still needs to be utilised (not kidding). There is also name of the place and its code TERYT. TERYT translates as the National Official Register of the Territorial Division of the Country. It is a very useful thing in identifying cities and regions especially for languages that include certain letters with diacritics the overdot the tail and the stroke. As a side note TERYT code for asbestos dataset was incomplete i.e. missing the last digit (!). In addition there was no metadata that describes the data collection process or time when it was taken. Acknowledgements This dataset was downloaded from the Polish Public Data and  is considered public data and can be used under following restrictions - One should inform about the source of this data and the creation time of reused information as well Inspiration Is Poland on track to be free of asbestos by 20132?,CSV,,[pollution],CC0,,,27,259,0.119140625,Is Poland on track to be free of asbestos by 2032?,Asbestos Clean-up in Poland,https://www.kaggle.com/pinsleepe/asbestos-cleanup-in-poland,Wed Feb 14 2018
,Frédéric Kosmowski,"[Conc_ID_F, Conc_ID_HH, Dist, HH_GPS.Altitude, Sex, Head_rel, Nb_fields_planted, Nb_field_eligible, random, Note_round, HH_size, Mobile, Age, Educ, Nb_cattle, Nb_camel, Nb_goat, Nb_horse, Nb_chicken, Herd_size, knowledge, training, start, end, Enum_HH, Filter_Paper, Land_size, Field_Form, M5, M5_testb, M5_time, M5_time_r, M5_Clarity, M3, M1, EA, HH_ID, Parcel_ID, Field_ID, Filter_pad, Soil_type, Soil_quality, Land_tenure, Decision_maker, legume, intercropping, irrig, fert, Seedling, Harvest_date, Yield, Residues_use, grazzing_past, grazzing_futur, M4, M2, Standing, Rocks, Slope, Start_LT, Corner_1.Altitu, LT_a, Corner_2.Altitu, LT_b, Corner_3.Altitu, LT_c, Corner_4.Altitu, LT_d, Field_Size, LT, Field_start, Field_end, Enum_Field, D, NDTI_MEAN, NDI1_MEAN, NDI2_MEAN, M6]","[string, dateTime, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, numeric, string, numeric, string, dateTime, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, numeric, string, string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric]","Context In commercial agriculture it is common practice to retain so-called crop residue (agricultural product left on the field after a harvest) on planting fields. This is a commonly recommended practice in conservation agriculture.   This dataset is a measurement of the adoption and impact of the methodology for a selection of agricultural fields. Six different crop residue coverage measurement methods are included i) interviewee (respondent) estimation; ii) enumerator estimation visiting the field; iii) interviewee with visual-aid without visiting the field; iv) enumerator with visual-aid visiting the field; v) field picture collected with a drone and analyzed with image-processing methods and vi) satellite picture of the field analyzed with remote sensing methods.  Content This dataset is a CSV file with a selection of characteristics about fields included in the sample. Acknowledgements This dataset was created as part of the following study ""On the Ground or in the Air? A Methodological Experiment on Crop Residue Cover Measurement in Ethiopia"". Inspiration What are the characteristics of the Ethiopian farmers sampled in this dataset? How well do they follow conservation practices?",Other,,"[africa, agriculture, survey analysis]",CC4,,,41,414,0.224609375,On the ground or in the air? ,Crop Residue Cover Measurement,https://www.kaggle.com/fkosmowski/crop-residue-cover-measurement,Wed Feb 14 2018
,Stephen Bailey,"[, group1, group2, weight]","[numeric, numeric, numeric, numeric]","Context meetup.com is a website for people organizing and attending regular or semi-regular events (""meet-ups""). The relationships amongst users—who goes to what meetups—are a social network ideal for graph-based analysis. This dataset was generated for a talk titled Principles of Network Analysis with NetworkX embedded online here (or with notebooks etc. on Github). It forms the basis for a series of tutorials I presented on at PyNash and PyTennessee. In them we work through the basics of graph theory and how to use NetworkX a popular open-source Python package. We then apply this knowledge to extract insights about the social fabric of Tennessee MeetUp groups.  Content  Graph data   member-to-group-edges.csv Edge list for constructing a member-to-group bipartite graph. Weights represent number of events attended in each group. group-edges.csv Edge list for constructing a group-to-group graph. Weights represent shared members between groups. member-edges.csv Edge list for constructing a member-to-member graph. Weights represent shared group membership. rsvps.csv Raw member-to-event attendance data which was aggregated to form member-to-group-edges.csv.   Metadata   meta-groups.csv Information for each group including name and category. group_id can serve as index. meta-members.csv Information for each member including name and location. member_id can serve as index. meta-events.csv Information for each event including name and time. event_id can serve as index.  Acknowledgements I'd like to acknowledge the folks at MeetUp.com who have made their database publicly available via a convenient REST API. Even newbies like myself can access and enjoy!",CSV,,"[social groups, networks, tutorial, network analysis]",CC0,,,40,557,12,Teaching Dataset for NashNetX Presentation (PyTN),Nashville Meetup Network,https://www.kaggle.com/stkbailey/nashville-meetup,Fri Feb 09 2018
,jwjohnson314,[],[],Context The MNIST dataset is one of the best known image classification problems out there and a veritable classic of the field of machine learning. This dataset is more challenging version of the same root problem classifying letters from images. This is a multiclass classification dataset of glyphs of English letters A - J. This dataset is used extensively in the Udacity Deep Learning course and is available in the Tensorflow Github repo (under Examples). I'm not aware of any license governing the use of this data so I'm posting it here so that the community can use it with Kaggle kernels. Content notMNIST _large.zip is a large but dirty version of the dataset with 529119 images and notMNIST_small.zip is a small hand-cleaned version of the dataset with 18726 images. The dataset was assembled by Yaroslav Bulatov and can be obtained on his blog. According to this blog entry there is about a 6.5% label error rate on the large uncleaned dataset and a 0.5% label error rate on the small hand-cleaned dataset. The two files each containing 28x28 grayscale images of letters A - J organized into directories by letter. notMNIST_large.zip contains 529119 images and notMNIST_small.zip contains 18726 images. Acknowledgements Thanks to Yaroslav Bulatov for putting together the dataset.,Other,,"[image processing, deep learning, image data, multiclass classification]",CC0,,,68,726,359,A dataset for a more challenging MNIST-like classification task,notMNIST,https://www.kaggle.com/jwjohnson314/notmnist,Thu Feb 15 2018
,NathanGeorge,[],[],Context I wanted an easy way to share all the lending club data with others.  Unfortunately the data on their site is fragmented into many smaller files.  There is another lending club dataset on Kaggle but it hasn't been updated in years.  It also doesn't include the rejected loans which I put in here. I created a git repo for the code to create this data https//github.com/nateGeorge/preprocess_lending_club_data Content The definitions for the fields are here at the bottom of the page. Unfortunately there is a limit of 500MB for dataset files (so lame!) so I had to compress the files with gzip in the Python pandas package.   I cleaned the data a tiny bit  I removed %s from int_rate and revol_util and deleted the url column. To load the data in Python import pandas as pd  accept_df = pd.read_csv('../input/accepted_2007_to_2016.csv.gz' compression='gzip') reject_df = pd.read_csv('../input/rejected_2007_to_2016.csv.gz' compression='gzip')  # too many columns to print the info summary out so we need to force it print(accept_df.info(verbose=True null_counts=True))  In R library(data.table)  accepted_def &lt;- read.csv(gzfile('rejected_2007_to_2016.csv.gz') na.strings='') acc_dt &lt;- as.data.table(accepted_def) rejected_def &lt;- read.csv(gzfile('accepted_2007_to_2016.csv.gz') na.strings='') rej_dt &lt;- as.data.table(accepted_def)  There are also separate csvs in the main input folder but the only advantage over the lending club site is that the 2016 year is joined into one file instead of 4. Inspiration I wanted to make this dataset easily available for others to use.,CSV,,"[business, finance, lending]",CC0,,,890,5698,409,2007 through current Lending Club accepted and rejected loan data,All Lending Club loan data,https://www.kaggle.com/wordsforthewise/lending-club,Thu Feb 01 2018
,vikas,"[user_id, user_key, created_at, created_str, retweet_count, retweeted, favorite_count, text, tweet_id, source, hashtags, expanded_urls, posted, mentions, retweeted_status_id, in_reply_to_status_id]","[numeric, string, numeric, dateTime, string, string, string, string, numeric, string, string, string, string, string, string, string]","Context As part of the House Intelligence Committee investigation into how Russia may have influenced the 2016 US Election Twitter released the screen names of almost 3000 Twitter accounts believed to be connected to Russia’s Internet Research Agency a company known for operating social media troll accounts. Twitter immediately suspended these accounts deleting their data from Twitter.com and the Twitter API. A team at NBC News including Ben Popken and EJ Fox was able to reconstruct a dataset consisting of a subset of the deleted data for their investigation and were able to show how these troll accounts went on attack during key election moments. This dataset is the body of this open-sourced reconstruction. For more background read the NBC news article publicizing the release ""Twitter deleted 200000 Russian troll tweets. Read them here."" Content This dataset contains two CSV files. tweets.csv includes details on individual tweets while users.csv includes details on individual accounts. To recreate a link to an individual tweet found in the dataset replace user_key in https//twitter.com/user_key/status/tweet_id with the screen-name from the user_key field and tweet_id with the number in the tweet_id field. Following the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared including images can be found by entering the links on web caches like archive.org and archive.is. Acknowledgements If you publish using the data please credit NBC News and include a link to this page. Send questions to ben.popken@nbcuni.com. Inspiration What are the characteristics of the fake tweets? Are they distinguishable from real ones? ",CSV,,"[russia, politics, international relations]",CC0,,,193,1744,21,"200,000 malicious-account tweets captured by NBC",Russian Troll Tweets,https://www.kaggle.com/vikasg/russian-troll-tweets,Thu Feb 15 2018
,Rounak Banik,"[comments, description, duration, event, film_date, languages, main_speaker, name, num_speaker, published_date, ratings, related_talks, speaker_occupation, tags, title, url, views]","[numeric, string, numeric, string, numeric, numeric, string, string, numeric, numeric, string, string, string, string, string, string, numeric]",Context These datasets contain information about all audio-video recordings of TED Talks uploaded to the official TED.com website until September 21st 2017. The TED main dataset contains information about all talks including number of views number of comments descriptions speakers and titles. The TED transcripts dataset contains the transcripts for all talks available on TED.com. Content (for the CSV files) TED Main Dataset  name The official name of the TED Talk. Includes the title and the speaker. title The title of the talk description A blurb of what the talk is about. main_speaker The first named speaker of the talk. speaker_occupation The occupation of the main speaker. num_speaker The number of speakers in the talk. duration The duration of the talk in seconds. event The TED/TEDx event where the talk took place. film_date The Unix timestamp of the filming. published_date The Unix timestamp for the publication of the talk on TED.com comments The number of first level comments made on the talk. tags The themes associated with the talk. languages The number of languages in which the talk is available. ratings A stringified dictionary of the various ratings given to the talk (inspiring fascinating jaw dropping etc.) related_talks A list of dictionaries of recommended talks to watch next. url The URL of the talk. views The number of views on the talk.  TED Transcripts Dataset  url The URL of the talk transcript The official English transcript of the talk.  Acknowledgements The data has been scraped from the official TED Website and is available under the Creative Commons License. Inspiration I've always been fascinated by TED Talks and the immense diversity of content that it provides for free. I was also thoroughly inspired by a TED Talk that visually explored TED Talks stats and I was motivated to do the same thing albeit on a much less grander scale. Some of the questions that can be answered with this dataset 1. How is each TED Talk related to every other TED Talk? 2. Which are the most viewed and most favorited Talks of all time? Are they mostly the same? What does this tell us? 3. What kind of topics attract the maximum discussion and debate (in the form of comments)? 4. Which months are most popular among TED and TEDx chapters? 5. Which themes are most popular amongst TEDsters?,CSV,,[linguistics],CC4,,,6174,55114,34,"Data about TED Talks on the TED.com website until September 21st, 2017",TED Talks,https://www.kaggle.com/rounakbanik/ted-talks,Tue Sep 26 2017
,Mehdi Mohammadi,[],[],Content The dataset was created using the RSSI readings of an array of 13 ibeacons in the first floor of Waldo Library Western Michigan University. Data was collected using iPhone 6S. The dataset contains two sub-datasets a labeled dataset (1420 instances) and an unlabeled dataset (5191 instances). The recording was performed during the operational hours of the library. For the labeled dataset the input data contains the location (label column) a timestamp followed by RSSI readings of 13 iBeacons. RSSI measurements are negative values. Bigger RSSI values indicate closer proximity to a given iBeacon (e.g. RSSI of -65 represent a closer distance to a given iBeacon compared to RSSI of -85). For out-of-range iBeacons the RSSI is indicated by -200. The locations related to RSSI readings are combined in one column consisting a letter for the column and a number for the row of the position. The following figure depicts the layout of the iBeacons as well as the arrange of locations.   Attribute Information  location The location of receiving RSSIs from ibeacons b3001 to b3013; symbolic values showing the column and row of the location on the map (e.g. A01 stands for column A row 1).  date Datetime in the format of ‘d-m-yyyy hhmmss’ b3001 - b3013 RSSI readings corresponding to the iBeacons; numeric integers only.  Acknowledgements Provider Mehdi Mohammadi and Ala Al-Fuqaha {mehdi.mohammadi ala-alfuqaha}@wmich.edu Department of Computer Science Western Michigan University Citation Request M. Mohammadi A. Al-Fuqaha M. Guizani J. Oh “Semi-supervised Deep Reinforcement Learning in Support of IoT and Smart City Services” IEEE Internet of Things Journal Vol. PP No. 99 2017. Inspiration  How unlabeled data can help for an improved learning system. How a GAN model can synthesizes viable paths based on the little labeled data and larger set of unlabeled data.,CSV,,"[universities and colleges, networks, multiclass classification]",CC4,,,478,806,0.646484375,Bluetooth Low Energy iBeacon RSSI Dataset for Indoor localization and Navigation,BLE RSSI Dataset for Indoor localization,https://www.kaggle.com/mehdimka/ble-rssi-dataset,Fri Jan 26 2018
,zackthoutt,"[, country, description, designation, points, price, province, region_1, region_2, variety, winery]","[numeric, string, string, string, numeric, numeric, string, string, string, string, string]",Context After watching Somm (a documentary on master sommeliers) I wondered how I could create a predictive model to identify wines through blind tasting like a master sommelier would. The first step in this journey was gathering some data to train a model. I plan to use deep learning to predict the wine variety using words in the description/review. The model still won't be able to taste the wine but theoretically it could identify the wine based on a description that a sommelier could give. If anyone has any ideas on how to accomplish this please post them! Content This dataset contains three files  winemag-data-130k-v2.csv contains 10 columns and 130k rows of wine reviews.  winemag-data_first150k.csv contains 10 columns and 150k rows of wine reviews.   winemag-data-130k-v2.json contains 6919 nodes of wine reviews.   Click on the data tab to see individual file descriptions column-level metadata and summary statistics. Acknowledgements The data was scraped from WineEnthusiast during the week of June 15th 2017. The code for the scraper can be found here if you have any more specific questions about data collection that I didn't address. UPDATE 11/24/2017 After feedback from users of the dataset I scraped the reviews again on November 22nd 2017. This time around I collected the title of each review which you can parse the year out of the tasters name and the taster's Twitter handle. This should also fix the duplicate entry issue. Inspiration I think that this dataset offers some great opportunities for sentiment analysis and other text related predictive models. My overall goal is to create a model that can identify the variety winery and location of a wine based on a description. If anyone has any ideas breakthroughs or other interesting insights/models please post them.,CSV,,"[critical theory, food and drink]",CC4,,,10165,65574,51,"130k wine reviews with variety, location, winery, price, and description",Wine Reviews,https://www.kaggle.com/zynicide/wine-reviews,Mon Nov 27 2017
,Chris G,"[circuitId, circuitRef, name, location, country, lat, lng, alt, url]","[numeric, string, string, string, string, numeric, numeric, string, string]",Context Formula One (also Formula 1 or F1 and officially the FIA Formula One World Championship) is the highest class of single-seat auto racing that is sanctioned by the Fédération Internationale de l'Automobile (FIA). The FIA Formula One World Championship has been one of the premier forms of racing around the world since its inaugural season in 1950. Content This dataset contains data from 1950 all the way through the 2017 season and consists of tables describing constructors race drivers lap times pit stops and more. Acknowledgements The data was downloaded from http//ergast.com/mrd/ at the conclusion of the 2017 season. The data was originally gathered and published to the public domain by Chris Newell. Inspiration I think this dataset offers an exciting insight into a $ billion industry enjoyed by hundreds of millions of viewers all over the world. So please explore and enjoy!,CSV,,[sports],CC4,,,1656,7931,6,Race data from 1950 to 2017,Formula 1 Race Data,https://www.kaggle.com/cjgdev/formula-1-race-data-19502017,Wed Nov 29 2017
,Luís Gustavo Modelli,"[Id, job, sector, Month_salary, 13_salary, eventual_salary, indemnity, extra_salary, discount_salary, total_salary]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The monthly salary of the public workers of the State of São Paulo in Brazil is a Public data available in the transparency portal of the state government at http//www.transparencia.sp.gov.br/buscaRemunera.html Content The data is about the salary for all worker in the State for the month of October 2017.  There are just over one million records. The names of the employee are anonymous represented by the variable id. Inspiration This database may reveal  Higher salaries The contribution of extra remuneration to higher salaries  By the rules of the government no employee could receive more than the state governor salary R$ 21631.05,CSV,,"[government agencies, brazil, finance]",CC0,,,308,2033,18,Salary of Public Worker in Brazil,Monthly Salary of Public Worker in Brazil,https://www.kaggle.com/gustavomodelli/monthly-salary-of-public-worker-in-brazil,Sat Jan 13 2018
,Pablo Lebed,"[ID Provincia;Provincia;""ID Departamento"";Departamento;""Id Cultivo"";Cultivo;""ID Campa�a"";Campana;""Sup. Sembrada (Ha)"";""Sup. Cosechada (Ha)"";""Producci�n (Tn)"";""Rendimiento (Kg/Ha)"", Provincia, ID Departamento, Departamento, Id Cultivo, Cultivo, ID Campa�a, Campana, Sup. Sembrada (Ha), Sup. Cosechada (Ha), Producci�n (Tn), Rendimiento (Kg/Ha)]","[string, string, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric]",Context This dataset contains crops production and yield over the years in Argentina. Data is provided by province and district for each seed or harvest campaign from 1969 to 2017. Content  Sup. Sembrada (Ha) --- Hectares sown Sup. Cosechada (Ha) --- Hectares harvested Produccion (Tn) --- Tonnes produced Rendimiento (Kg/Ha) --- Harvest performance  Acknowledgements This dataset was kindly made publicly available by Datos Argentina under the ODbL license.  Photo by Benjamin Davies on Unsplash. Inspiration My Data Science adventure start here. My objective is to use this dataset together with Kaggle Kernels as a starting point in my learning process.,CSV,,"[agriculture, data cleaning]",ODbL,,,124,989,1,Main crops production and yield in ARG,Agricultural Estimates,https://www.kaggle.com/pablolebed/agricultural-estimates-arg,Wed Feb 14 2018
,Eliezer Bourchardt,"[DATA_GERACAO;""HORA_GERACAO"";""ANO_ELEICAO"";""DESCRICAO_ELEICAO"";""SIGLA_UF"";""SQ_CANDIDATO"";""CD_TIPO_BEM_CANDIDATO"";""DS_TIPO_BEM_CANDIDATO"";""DETALHE_BEM"";""VALOR_BEM"";""DATA_ULTIMA_ATUALIZACAO"";""HORA_ULTIMA_ATUALIZACAO"", HORA_GERACAO, ANO_ELEICAO, DESCRICAO_ELEICAO, SIGLA_UF, SQ_CANDIDATO, CD_TIPO_BEM_CANDIDATO, DS_TIPO_BEM_CANDIDATO, DETALHE_BEM, VALOR_BEM, DATA_ULTIMA_ATUALIZACAO, HORA_ULTIMA_ATUALIZACAO]","[string, string, numeric, string, string, numeric, numeric, string, string, numeric, dateTime, string]",Context Dataset criado para realizar o projeto de conclusão do curso de Engenheiro de Machine Learning pela Udacity. Content Dados de candidatos a deputados estadual e federal eleições 2014. Características dos candidatos (sexo idade raça/cor) valores dos bens declarados e doações recebidas.  Acknowledgements Agradeço a Felipe Antunes por disponibilizar o dataset de doações aos candidatos. Também agradeço Heitor Gomes revisor de meu projeto pelas ótimas sugestões de melhorias. Inspiration Como os dados dos candidatos podem ser utilizados para realizar uma previsão se o candidato será ou não eleito melhorando o resultado obtido neste projeto?,CSV,,"[brazil, government, politics]",CC0,,,21,256,6,"Dados pessoais, bens declarados e doações recebidas",Candidatos Deputado Federal e Estadual 2014,https://www.kaggle.com/eliezerfb/candidatos-deputado-federal-e-estadual-2014,Wed Jan 24 2018
,PromptCloud,"[area, city, country, crawl_date, highlight_value, hotel_overview, hotel_star_rating, image_urls, in_your_room, is_value_plus, latitude, longitude, mmt_holidayiq_review_count, mmt_location_rating, mmt_review_count, mmt_review_rating, mmt_review_score, mmt_traveller_type_review_count, mmt_tripadvisor_count, pageurl, property_address, property_id, property_name, property_type, qts, query_time_stamp, room_types, site_review_count, site_review_rating, sitename, state, traveller_rating, uniq_id]","[string, string, string, dateTime, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, numeric, string, numeric, string, string, numeric, string, string, string, dateTime, string, string, numeric, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 615000 hotels) that was created by extracting data from MakeMyTrip.com a travel portal in India. The complete dataset is available on DataStock a web data repository with historical records from several industries.  Content This dataset has following fields  area city country crawl_date highlight_value hotel_overview hotel_star_rating image_urls in_your_room is_value_plus latitude longitude mmt_holidayiq_review_count mmt_location_rating mmt_review_count mmt_review_rating mmt_review_score mmt_traveller_type_review_count mmt_tripadvisor_count pageurl property_address property_id property_name property_type qts query_time_stamp room_types site_review_count site_review_rating sitename state traveller_rating  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the reviews ratings and property description can be performed.,CSV,,"[hotels, internet]",CC4,,,841,5147,36,"Details of 20,000 hotels on MakeMyTrip.com", Hotels on Makemytrip,https://www.kaggle.com/PromptCloudHQ/hotels-on-makemytrip,Sat Sep 16 2017
,UCI Machine Learning,"[pelvic_incidence, pelvic_tilt numeric, lumbar_lordosis_angle, sacral_slope, pelvic_radius, degree_spondylolisthesis, class]","[numeric, numeric, numeric, numeric, numeric, numeric, string]",Context The data have been organized in two different but related classification tasks.   column_3C_weka.csv (file with three class labels) The first task consists in classifying patients as belonging to one out of three categories Normal (100 patients) Disk Hernia (60 patients) or Spondylolisthesis (150 patients).  column_2C_weka.csv (file with two class labels) For the second task the categories Disk Hernia and Spondylolisthesis were merged into a single category labelled as 'abnormal'. Thus the second task consists in classifying patients as belonging to one out of two categories Normal (100 patients) or Abnormal (210 patients).  Content Field Descriptions Each patient is represented in the data set by six biomechanical attributes derived from the shape and orientation of the pelvis and lumbar spine (each one is a column)   pelvic incidence pelvic tilt lumbar lordosis angle sacral slope pelvic radius grade of spondylolisthesis  Acknowledgements The original dataset was downloaded from UCI ML repository Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science Files were converted to CSV Inspiration Use these biomechanical features to classify patients according to their labels ,CSV,,[],CC0,,,1601,5143,0.048828125,Classifying patients based on six features,Biomechanical features of orthopedic patients,https://www.kaggle.com/uciml/biomechanical-features-of-orthopedic-patients,Thu Sep 07 2017
,Charmi,[],[],Project Description 1) Data Background In the Data Mining class we had the opportunity to analyze data by performing data mining algorithms to a dataset. Our dataset is from Office of Foreign Labor Certification (OFLC). OFLC is a division of the U.S. Department of Labor. The main duty of OFLC is to assist the Secretary of Labor to enforce part of the Immigration and Nationality Act (INA) which requires certain labor conditions exist before employers can hire foreign workers.  H-1B is a visa category in the United States of America under the INA section 101(a)(15)(H) which allows U.S. employers to employ foreign workers. The first step employer must take to hire a foreign worker is to file the Labor Condition Application. In this project we will analyze the data from the Labor Condition Application.  1.1) Introduction to H1B Dataset  The H-1B Dataset selected for this project contains data from employer’s Labor Condition Application and the case certification determinations processed by the Office of Foreign Labor Certification (OFLC) where the date of the determination was issues on or after October 1 2016 and on or before June 30 2017. The Labor Condition Application (LCA) is a document that a perspective H-1B employer files with U.S. Department of Labor Employment and Training Administration (DOLETA) when it seeks to employ non-immigrant workers at a specific job occupation in an area of intended employment for not more than three years.     1.2) Goal of the Project  Our goal for this project is to predict the case status of an application submitted by the employer to hire non-immigrant workers under the H-1B visa program. Employer can hire non-immigrant workers only after their LCA petition is approved. The approved LCA petition is then submitted as part of the Petition for a Non-immigrant Worker application for work authorizations for H-1B visa status.    We want to uncover insights that can help employers understand the process of getting their LCA approved. We will use WEKA software to run data mining algorithms to understand the relationship between attributes and the target variable. 2)Dataset Information a) Source Office of Foreign Labor Certification U.S. Department of Labor Employment and Training Administration  b) List Link https//www.foreignlaborcert.doleta.gov/performancedata.cfm  c) Dataset Type Record – Transaction Data  d) Number of Attributes 40  e) Number of Instances 528147  f) Date Created July 2017   3) Attribute List The detailed description of each attribute below is given in the Record Layout file available in the zip folder H1B Disclosure Dataset Files.  The H-1B dataset from OFLC contained 40 attributes and 528147 instances. The attributes are in the table below. The attributes highlighted bold were removed during the data cleaning process. 1) CASE_NUMBER  2)CASE_SUBMITTED  3)DECISION_DATE  4)VISA_CLASS  5)EMPLOYMENT_START_DATE  6)EMPLOYMENT_END_DATE  7)EMPLOYER_NAME  8)EMPLOYER_ADDRESS  9)EMPLOYER_CITY  10)EMPLOYER_STATE  11)EMPLOYER_POSTAL_CODE  12)EMPLOYER_COUNTRY  13)EMPLOYER_PROVINCE  14)EMPLOYER_PHONE  15)EMPLOYER_PHONE_EXT  16)AGENT_ATTORNEY_NAME  17)AGENT_ATTORNEY_CITY  18)AGENT_ATTORNEY_STATE  19)JOB_TITLE  20)SOC_CODE  21)SOC_NAME  22)NAICS_CODE  23)TOTAL_WORKERS  24)FULL_TIME_POSITION  25)PREVAILING_WAGE  26)PW_UNIT_OF_PAY  27)PW_SOURCE  28)PW_SOURCE_YEAR  29)PW_SOURCE_OTHER  30)WAGE_RATE_OF_PAY_FROM  31)WAGE_RATE_OF_PAY_TO  32)WAGE_UNIT_OF_PAY  33)H-1B_DEPENDENT  34) WILLFUL_VIOLATOR  35) WORKSITE_CITY  36)WORKSITE_COUNTY  37)WORKSITE_STATE  38)WORKSITE_POSTAL_CODE  39)ORIGINAL_CERT_DATE  40)CASE_STATUS* - __Class Attribute - To be predicted    3.1) Class Attribute For the H-1B Dataset our class attribute is ‘CASE_STATUS’. There are 4 categories of Case Status. The values of Case_Status attributes are 1) Certified  2) Certified_Withdrawn  3) Withdrawn  4) Denied   Certified means the LCA of an employer was approved. Certified Withdrawn means the case was withdrawn after it was certified by OFLC. Withdrawn means the case was withdrawn by the employer. Denied means the case was denied OFLC. ,Other,,"[united states, artificial intelligence, machine learning]",CC4,,,423,2719,42,H1B Disclosure Dataset - Predicting the Case Status,H1B Disclosure Dataset ,https://www.kaggle.com/trivedicharmi/h1b-disclosure-dataset,Mon Jan 01 2018
,FelipeLeiteAntunes,[],[],Context Brazil has elections every two years but alternating between two different types of elections each type occurring every four years. There are the municipal elections where mayors and city council members are elected (the last one occurred in 2016) and general elections where president governors senators and congressmen (regional and national) are elected (the last one occurred in 2014). Brazil has 26 federal units plus the federal district. Each one of these units (regions) elects its senators congressmen and governors. For each federal unit Brazil's TSE provides information on the donations declared by the three entities candidates parties and committees. The data comprises information describing every donation received. The donations can be divided in two categories with respect to the donor they can come from legal persons (private citizens identified by the CPF (CPF is an identification number used by the Brazilian tax revenue office. It is roughly the Brazilian analogue to a social security number. With the same purpose companies are identified with a similar number called CNPJ number) or from legal entities (i.e. companies identified by the CNPJ number). Also some entities can make donations among them (the party can give part of the money from a given donation to a candidate). In this type of transaction the information on the original donor is also specified in the declarations. From now on these type of donations will be referred to as non-original donations. Apart from information concerning each Brazilian federal unit separately one can also obtain the information declared by the parties and committees at national level and for the presidential campaign (which has national and not regional scope). Related paper https//arxiv.org/pdf/1707.08826.pdf,CSV,,"[crime, finance, politics]",ODbL,,,197,2733,69,Statistical analysis of Brazil in 2014,Electoral Donations in Brazil,https://www.kaggle.com/felipeleiteantunes/electoral-donations-brazil2014,Sun Nov 19 2017
,MuonNeutrino,"[CensusTract, State, County, TotalPop, Men, Women, Hispanic, White, Black, Native, Asian, Pacific, Citizen, Income, IncomeErr, IncomePerCap, IncomePerCapErr, Poverty, ChildPoverty, Professional, Service, Office, Construction, Production, Drive, Carpool, Transit, Walk, OtherTransp, WorkAtHome, MeanCommute, Employed, PrivateWork, PublicWork, SelfEmployed, FamilyWork, Unemployment]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset expands on my earlier New York City Census Data dataset. It includes data from the entire country instead of just New York City. The expanded data will allow for much more interesting analyses and will also be much more useful at supporting other data sets. Content The data here are taken from the DP03 and DP05 tables of the 2015 American Community Survey 5-year estimates. The full datasets and much more can be found at the American Factfinder website. Currently I include two data files  acs2015_census_tract_data.csv Data for each census tract in the US including DC and Puerto Rico. acs2015_county_data.csv Data for each county or county equivalent in the US including DC and Puerto Rico.  The two files have the same structure with just a small difference in the name of the id column. Counties are political subdivisions and the boundaries of some have been set for centuries. Census tracts however are defined by the census bureau and will have a much more consistent size. A typical census tract has around 5000 or so residents. The Census Bureau updates the estimates approximately every year. At least some of the 2016 data is already available so I will likely update this in the near future. Acknowledgements The data here were collected by the US Census Bureau. As a product of the US federal government this is not subject to copyright within the US. Inspiration There are many questions that we could try to answer with the data here. Can we predict things such as the state (classification) or household income (regression)? What kinds of clusters can we find in the data? What other datasets can be improved by the addition of census data? ,CSV,,"[united states, demographics]",CC0,,,1578,7434,6,Demographic and Economic Data for Tracts and Counties,US Census Demographic Data,https://www.kaggle.com/muonneutrino/us-census-demographic-data,Thu Dec 14 2017
,pmohun,"[Currency, Date, Open, High, Low, Close, Volume, Market Cap]","[string, string, string, string, string, string, string, string]","Context Recent growing interest in cryptocurrencies specifically as a speculative investment vehicle has sparked global conversation over the past 12 months. Although this data is available across various sites there is a lack of understanding as to what is driving the exponential rise of many individual currencies. This data set is intended to be a starting point for a detailed analysis into what is driving price action and what can be done to predict future movement. Content Consolidated financial information for the top 200 cryptocurrencies by marketcap. Pulled from CoinMarketCap.com. Attributes include  Currency name (e.g. bitcoin) Date   Open High Low Close Volume Marketcap  Inspiration For the past few months I have been searching for a reliable source for historical price information related to cryptocurrencies. I wasn't able to find anything that I could use to my liking so I built my own data set. I've written a small script that scrapes historical price information for the top 200 coins by market cap as listed on CoinMarketCap.com. I plan to run some basic analysis on it to answer questions that I have a ""gut"" feeling about but no quantitative evidence (yet!). Questions such as   What is the correlation between bitcoin and alt coin prices? What is the average age of the top 10 coins by market cap? What day of the week is best to buy/sell? Which coins in the top two hundred are less than 6 months old? Which currencies are the most volatile?  What the hell happens when we go to bed and Asia starts trading?  Feel free to use this for your own purposes! I just ask that you share your results with the group when complete. Happy hunting!",CSV,,"[business, finance, money, internet]",CC0,,,857,5177,2,Top 200 Cryptocurrencies by Marketcap,Complete Historical Cryptocurrency Financial Data,https://www.kaggle.com/philmohun/cryptocurrency-financial-data,Mon Feb 12 2018
,City of New York,"[, Date, Day, High Temp (°F), Low Temp (°F), Precipitation, Brooklyn Bridge, Manhattan Bridge, Williamsburg Bridge, Queensboro Bridge, Total]","[numeric, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The New York City Department of Transportation collects daily data about the number of bicycles going over bridges in New York City. This data is used to measure bike utilization as a part of transportation planning. This dataset is a daily record of the number of bicycles crossing into or out of Manhattan via one of the East River bridges (that is excluding Bronx thruways and the non-bikeable Hudson River tunnels) for a stretch of 9 months. Content A count of the number of bicycles on each of the bridges in question is provided on a day-by-day basis along with information on maximum and minimum temperature and precipitation. Acknowledgements This data is published in an Excel format by the City of New York (here). It has been processed into a CSV file for use on Kaggle. Inspiration  In this dataset how many bicycles cross into and out of Manhattan per day? How strongly do weather conditions affect bike volumes? What is the top bridge in terms of bike load? ,CSV,,"[cities, road transport]",CC0,,,655,2468,0.017578125,Daily bicycle counts for major bridges in NYC,New York City - East River Bicycle Crossings,https://www.kaggle.com/new-york-city/nyc-east-river-bicycle-crossings,Wed Sep 06 2017
,Joao Pedro Evangelista,[],[],Context The WTA (Women's Tennis Association) is the principal organizing body of women's professional tennis  it governs its own tour worldwide. On its website it provides a lot of data about the players as individuals as well the tour matches with results and the current rank during it. Luckily for us Jeff Sackmann scraped the website and collected everything from there and put in a nice way into easily consumable datasets. On Jeff's GitHub account you can find a lot more data about tennis! Content The dataset present here is directly downloaded from the source no alteration on the data was made the files were only placed in subdirectories so one can easily locate them. It covers statistics of players registered on the WTA the matches that happened on each tour by year with results as well some qualifying matches for the tours. As a reminder you may not find all data of the matches prior to 2006 so be warned when working with those sets. Acknowledgements Thanks to Jeff Sackmann for maintaining such collection and making it public! Also a thank you for WTA for collecting those stats and making them accessible to anyone on their site. Inspiration Here are some things to start  Which player did the most rapidly climb the ranks through the years? Does the rank correlates with the money earn by the player? What can we find about the age? There is some deterministic factor to own the match? ,Other,,"[tennis, sports]",CC4,,,365,2394,20,All Women's Tennis Association data you ever wanted,WTA Matches and Rankings,https://www.kaggle.com/joaoevangelista/wta-matches-and-rankings,Wed Nov 15 2017
,Panos Kostakos,"[User_ID, Gender, Age, Location, Verification, Sexual_orientation, Sexual_polarity, Looking_for, Points_Rank, Last_login, Member_since, Number_of_Comments_in_public_forum, Time_spent_chating_H:M, Number_of_advertisments_posted, Number_of_offline_meetings_attended, Profile_pictures, Friends_ID_list, Risk]","[numeric, string, numeric, string, string, string, string, string, numeric, string, string, numeric, string, numeric, numeric, numeric, string, string]","Context This database was used in the paper ""Covert online ethnography and machine learning for detecting individuals at risk of being drawn into online sex work"". https//www.flinders.edu.au/centre-crime-policy-research/illicit-networks-workshop Content The database includes data scraped from a European online adult forum. Using covert online ethnography we interviewed a small number of participants and determined their risk to either supply or demand sex services through that forum. This is a great dataset for semi-supervised learning.  Inspiration How can we identify individuals at risk of being drawn into online sex work? The spread of online social media enables a greater number of people to be involved into online sex trade; however detecting deviant behaviors online is limited by the low available of data. To overcome this challenge we combine covert online ethnography with semi-supervised learning using data from a popular European adult forum.",CSV,,[],CC0,,,336,2796,0.4638671875,Detecting individuals at risk using semi-supervised learning,Risk of being drawn into online sex work,https://www.kaggle.com/panoskostakos/online-sex-work,Tue Nov 28 2017
,Nick Brooks,"[, Clothing ID, Age, Title, Review Text, Rating, Recommended IND, Positive Feedback Count, Division Name, Department Name, Class Name]","[numeric, numeric, numeric, string, string, numeric, numeric, numeric, string, string, string]",Context Welcome. This is a Women’s Clothing E-Commerce dataset revolving around the reviews written by customers. Its nine supportive features offer a great environment to parse out the text through its multiple dimensions. Because this is real commercial data it has been anonymized and references to the company in the review text and body have been replaced with “retailer”. Content This dataset includes 23486 rows and 10 feature variables. Each row corresponds to a customer review and includes the variables  Clothing ID Integer Categorical variable that refers to the specific piece being reviewed.  Age Positive Integer variable of the reviewers age. Title String variable for the title of the review. Review Text String variable for the review body.  Rating Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst to 5 Best.  Recommended IND Binary variable stating where the customer recommends the    product where 1 is recommended 0 is not recommended.     Positive Feedback Count Positive Integer documenting the number of other customers who found this review positive. Division Name Categorical name of the product high level division. Department Name Categorical name of the product department name. Class Name Categorical name of the product class name.  Acknowledgements Anonymous Inspiration I look forward to come quality NLP! There is also some great opportunities for feature engineering and multivariate analysis.,CSV,,"[business, internet, nlp, text mining]",CC0,,,325,2139,3,"23,000 Customer Reviews and Ratings ",Women's E-Commerce Clothing Reviews,https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews,Sun Feb 04 2018
,Centers for Disease Control and Prevention,"[YearStart, YearEnd, LocationAbbr, LocationDesc, DataSource, Topic, Question, Response, DataValueUnit, DataValueType, DataValue, DataValueAlt, DataValueFootnoteSymbol, DatavalueFootnote, LowConfidenceLimit, HighConfidenceLimit, StratificationCategory1, Stratification1, StratificationCategory2, Stratification2, StratificationCategory3, Stratification3, GeoLocation, ResponseID, LocationID, TopicID, QuestionID, DataValueTypeID, StratificationCategoryID1, StratificationID1, StratificationCategoryID2, StratificationID2, StratificationCategoryID3, StratificationID3]","[numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, string, string, numeric, numeric, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string]",Context CDC's Division of Population Health provides cross-cutting set of 124 indicators that were developed by consensus and that allows states and territories and large metropolitan areas to uniformly define collect and report chronic disease data that are important to public health practice and available for states territories and large metropolitan areas. In addition to providing access to state-specific indicator data the CDI web site serves as a gateway to additional information and data resources. Content A variety of health-related questions were assessed at various times and places across the US over the past 15 years. Data is provided with confidence intervals and demographic stratification. Acknowledgements Data was compiled by the CDC. Inspiration  Any interesting trends in certain groups? Any correlation between disease indicators and locality hospital spending? ,CSV,,[healthcare],CC0,,,871,7294,117,"Disease Data Across the US, 2001-2016",Chronic Disease Indicators,https://www.kaggle.com/cdc/chronic-disease,Thu Aug 17 2017
,Sohier Dane,"[CrimeDate, CrimeTime, CrimeCode, Location, Description, Inside/Outside, Weapon, Post, District, Neighborhood, Longitude, Latitude, Location 1, Premise, Total Incidents]","[dateTime, dateTime, string, string, string, string, string, numeric, string, string, numeric, numeric, string, string, numeric]",All BPD data on Open Baltimore is preliminary data and subject to change. The information presented through Open Baltimore represents Part I victim based crime data. The data do not represent statistics submitted to the FBI's Uniform Crime Report (UCR); therefore any comparisons are strictly prohibited. For further clarification of UCR data please visit http//www.fbi.gov/about-us/cjis/ucr/ucr. Please note that this data is preliminary and subject to change. Prior month data is likely to show changes when it is refreshed on a monthly basis. All data is geocoded to the approximate latitude/longitude location of the incident and excludes those records for which an address could not be geocoded. Any attempt to match the approximate location of the incident to an exact address is strictly prohibited. Acknowledgements This dataset was kindly made available by the City of Baltimore. You can find the original dataset which is updated regularly here.,CSV,,[crime],CC0,,,887,5303,39,Crime data for 2012-2017,Crime in Baltimore,https://www.kaggle.com/sohier/crime-in-baltimore,Wed Sep 13 2017
,Rounak Banik,[],[],Context These files contain metadata for all 45000 movies listed in the Full MovieLens Dataset. The dataset consists of movies released on or before July 2017. Data points include cast crew plot keywords budget revenue posters release dates languages production companies countries TMDB vote counts and vote averages. This dataset also has files containing 26 million ratings from 270000 users for all 45000 movies. Ratings are on a scale of 1-5 and have been obtained from the official GroupLens website. Content This dataset consists of the following files movies_metadata.csv The main Movies Metadata file. Contains information on 45000 movies featured in the Full MovieLens dataset. Features include posters backdrops budget revenue release dates languages production countries and companies. keywords.csv Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object. credits.csv Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object. links.csv The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset. links_small.csv Contains the TMDB and IMDB IDs of a small subset of 9000 movies of the Full Dataset. ratings_small.csv The subset of 100000 ratings from 700 users on 9000 movies. The Full MovieLens Dataset consisting of 26 million ratings and 750000 tag applications from 270000 users on all the 45000 movies in this dataset can be accessed here  Acknowledgements This dataset is an ensemble of data collected from TMDB and GroupLens. The Movie Details Credits and Keywords have been collected from the TMDB Open API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies actors and actresses crew members and TV shows. You can try it for yourself here. The Movie Links and Ratings have been obtained from the Official GroupLens website. The files are a part of the dataset available here  Inspiration This dataset was assembled as part of my second Capstone Project for Springboard's Data Science Career Track. I wanted to perform an extensive EDA on Movie Data to narrate the history and the story of Cinema and use this metadata in combination with MovieLens ratings to build various types of Recommender Systems. Both my notebooks are available as kernels with this dataset The Story of Film and  Movie Recommender Systems Some of the things you can do with this dataset Predicting movie revenue and/or movie success based on a certain metric. What movies tend to get higher vote counts and vote averages on TMDB? Building Content Based and Collaborative Filtering Based Recommendation Engines.,CSV,,"[popular culture, film]",CC0,,,7921,51578,900,"Metadata on over 45,000 movies. 26 million ratings from over 270,000 users.",The Movies Dataset,https://www.kaggle.com/rounakbanik/the-movies-dataset,Fri Nov 10 2017
,paultimothymooney,[],[],Context The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples.  Automated methods to detect and classify blood cell subtypes have important medical applications. Content This dataset contains 12500 augmented images of blood cells (JPEG) with accompanying cell type labels (CSV).  There are approximately 3000 images for each of 4 different cell types grouped into 4 different folders (according to cell type).    The cell types are Eosinophil Lymphocyte Monocyte and Neutrophil.  This dataset is accompanied by an additional dataset containing the original 410 images (pre-augmentation) as well as two additional subtype labels (WBC vs WBC) and also bounding boxes for each cell in each of these 410 images (JPEG + XML metadata).  More specifically the folder 'dataset-master' contains 410 images of blood cells with subtype labels and bounding boxes (JPEG + XML) while the folder 'dataset2-master' contains 2500 augmented images as well as 4 additional subtype labels (JPEG + CSV).   There are approximately 3000 augmented images for each class of the 4 classes as compared to 88 33 21 and 207 images of each in folder 'dataset-master'. Acknowledgements https//github.com/Shenggan/BCCD_Dataset MIT License Inspiration The diagnosis of blood-based diseases often involves identifying and characterizing patient blood samples. Automated methods to detect and classify blood cell subtypes have important medical applications.,Other,,[medicine],CC0,,,355,2571,293,"12,500 images: 4 different cell types",Blood Cell Images,https://www.kaggle.com/paultimothymooney/blood-cells,Sat Jan 20 2018
,Facebook,[],[],English Word Vectors with sub-word information  About fastText fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words even made-up ones. Indeed fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words. About the vectors These pre-trained vectors contain 1 million word vectors learned with subword information on Wikipedia 2017 the UMBC webbase corpus and the statmt.org news dataset. In total it contains 16B tokens. The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency. Acknowledgements These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0. P. Bojanowski* E. Grave* A. Joulin T. Mikolov Enriching Word Vectors with Subword Information A. Joulin E. Grave P. Bojanowski T. Mikolov Bag of Tricks for Efficient Text Classification A. Joulin E. Grave P. Bojanowski M. Douze H. Jégou T. Mikolov FastText.zip Compressing text classification models (* These authors contributed equally.),Other,,"[machine learning, pre-trained model]",CC3,,,33,509,988,"Word vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org",fastText English Word Vectors Including Sub-words,https://www.kaggle.com/facebook/fasttext-english-word-vectors-including-subwords,Thu Jan 11 2018
,Rachael Tatman,"[company, year, race, gender, job_category, count]","[string, numeric, string, string, string, numeric]","Context There has been a lot of discussion of the ways in which the workforce for Silicon Valley tech companies differs from that of the United States as a whole. In particular a lot of evidence suggests that tech workers (who tend to be more highly paid than workers in many other professions) are more likely to be white and male. This dataset will allow you to investigate the demographics for 23 Silicon Valley tech companies for yourself. Contents This database contains EEO-1 reports filed by Silicon Valley tech companies. It was compiled by Reveal from The Center for Investigative Reporting. There are six columns in this dataset  company Company name year For now 2016 only race Possible values ""American_Indian_Alaskan_Native"" ""Asian"" ""Black_or_African_American"" ""Latino"" ""Native_Hawaiian_or_Pacific_Islander"" ""Two_or_more_races"" ""White"" ""Overall_totals"" gender  Possible values ""male"" ""female"". Non-binary gender is not counted in EEO-1 reports. job_category Possible values ""Administrative support"" ""Craft workers"" ""Executive/Senior officials & Mgrs"" ""First/Mid officials & Mgrs"" ""laborers and helpers"" ""operatives"" ""Professionals"" ""Sales workers"" ""Service workers"" ""Technicians"" ""Previous_totals"" ""Totals"" count Mostly integer values but contains ""na"" for a no-data variable.  Acknowledgements The EEO-1 database is licensed under the Open Database License (ODbL) by Reveal from The Center for Investigative Reporting. You are free to copy distribute transmit and adapt the spreadsheet so long as you  credit Reveal (including this link if it’s distributed online); inform Reveal that you are using the data in your work by emailing Sinduja Rangarajan at srangarajan@revealnews.org; and offer any new work under the same license.  Inspiration  How does each company’s workforce compare to the United States population as a whole? You can find county level diversity information here. Which company is the most diverse? Least diverse? ",CSV,,"[united states, employment, demographics]",ODbL,,,561,4698,0.2119140625,What’s diversity like for 23 top tech companies?,Silicon Valley Diversity Data,https://www.kaggle.com/rtatman/silicon-valley-diversity-data,Wed Nov 08 2017
,Mart Jürisoo,"[date, home_team, away_team, home_score, away_score, tournament, city, country]","[dateTime, string, string, numeric, numeric, string, string, string]",Context Well basically what happened was I was looking for a semi-definite easy to read list of international football matches and couldn't find anything decent. So I took it upon myself to collect it for my own use. I might as well share it. Content This dataset includes 38759 results of international football matches starting from the very first official match in 1972 up to 2018.  The matches range from World Cup to Baltic Cup to regular friendly matches. The matches are strictly men's full internationals and the data does not include Olympic Games or matches where at least one of the teams was the nation's B-team U-23 or a league select team. results.csv includes the following columns  date home_team away_team home_score away_score tournament city country  Acknowledgements The data is gathered from several sources including but not limited to wikipedia fifa.com rsssf.com and individual football associations' websites. Inspiration Some directions to take when exploring the data  Which teams dominated different eras of football Who is the best team of all time What trends have there been in international football throughout the ages - home advantage total goals scored distribution of teams' strength etc Can we say anything about geopolitics from football fixtures - how has the number of countries changed which teams like to play each other Which countries host the most matches where they themselves are not participating in How much if at all does hosting a major tournament help a country's chances in said tournament Which teams are the most active in playing friendlies and friendly tournaments - does it help or harm them Do you dare to make any predictions for 2018 World Cup based on this data? and so on...  The world's your oyster my friend.,CSVCSVx 13 MB,,"[association football, countries, sports]",CC0,,,1418,9404,0.4638671875,"An up-to-date dataset of nearly 40,000 international football results",International football results from 1872 to 2018,https://www.kaggle.com/martj42/international-football-results-from-1872-to-2017,Sat Jan 27 2018
,Patrick DeKelly,[],[],"Introduction This is the keystroke dataset for the study titled 'High-accuracy detection of early Parkinson's Disease using multiple characteristics of finger movement while typing'. This research report is currently under review for publication by PLOS ONE. The dataset contains keystroke logs collected from over 200 subjects with and without Parkinson's Disease (PD) as they typed normally on their own computer (without any supervision) over a period of weeks or months (having initially installed a custom keystroke recording app Tappy). This dataset has been collected and analyzed in order to indicate that the routine interaction with computer keyboards can be used to detect changes in the characteristics of finger movement in the early stages of PD. Data The participants from the U.S. Canada UK and Australia had visited the project website and agreed to participate in the study. The research was approved by the Human Research Ethics Committee at Charles Sturt University Australia protocol number H17013. Each data file collected includes the timing information from typing activity as the participants used their various Windows applications (such as email word processing web searches and the like). The keystroke acquisition software ('Tappy') provided timing accuracy of key press and release timestamps to within several milliseconds. The data files comprise two Zip archives one with the participant detail files and the other with the keystroke data files for each user. Acknowledgements This dataset is from the research article ""High-accuracy detection of early Parkinson's Disease using multiple characteristics of finger movement while typing"" by Warwick R. Adams. Read the article here http//journals.plos.org/plosone/article?id=10.1371/journal.pone.0188226#sec008 Inspiration While this is a difficult dataset to work with there is a rich trove of information. It is a great set to practice preprocessing attempt to replicate the results of the article or do your own analysis of keystroke data.",Other,,"[research, diseases]",CC0,,,75,786,85,Raw data used to predict the onset of Parkinson from typing tendencies,Tappy Keystroke Data with Parkinson's Patients,https://www.kaggle.com/valkling/tappy-keystroke-data-with-parkinsons-patients,Sun Feb 04 2018
,Zalando Research,"[label, pixel1, pixel2, pixel3, pixel4, pixel5, pixel6, pixel7, pixel8, pixel9, pixel10, pixel11, pixel12, pixel13, pixel14, pixel15, pixel16, pixel17, pixel18, pixel19, pixel20, pixel21, pixel22, pixel23, pixel24, pixel25, pixel26, pixel27, pixel28, pixel29, pixel30, pixel31, pixel32, pixel33, pixel34, pixel35, pixel36, pixel37, pixel38, pixel39, pixel40, pixel41, pixel42, pixel43, pixel44, pixel45, pixel46, pixel47, pixel48, pixel49, pixel50, pixel51, pixel52, pixel53, pixel54, pixel55, pixel56, pixel57, pixel58, pixel59, pixel60, pixel61, pixel62, pixel63, pixel64, pixel65, pixel66, pixel67, pixel68, pixel69, pixel70, pixel71, pixel72, pixel73, pixel74, pixel75, pixel76, pixel77, pixel78, pixel79, pixel80, pixel81, pixel82, pixel83, pixel84, pixel85, pixel86, pixel87, pixel88, pixel89, pixel90, pixel91, pixel92, pixel93, pixel94, pixel95, pixel96, pixel97, pixel98, pixel99]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60000 examples and a test set of 10000 examples. Each example is a 28x28 grayscale image associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits. The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact MNIST is often the first dataset researchers try. ""If it doesn't work on MNIST it won't work at all"" they said. ""Well if it does work on MNIST it may still fail on others."" Zalando seeks to replace the original MNIST dataset Content Each image is 28 pixels in height and 28 pixels in width for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it indicating the lightness or darkness of that pixel with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above) and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.  To locate a pixel on the image suppose that we have decomposed x as x = i * 28 + j where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix.  For example pixel31 indicates the pixel that is in the fourth column from the left and the second row from the top as in the ascii-diagram below.   Labels Each training and test example is assigned to one of the following labels  0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot   TL;DR  Each row is a separate image   Column 1 is the class label.  Remaining columns are pixel numbers (784 total).  Each value is the darkness of the pixel (1 to 255)  Acknowledgements  Original dataset was downloaded from https//github.com/zalandoresearch/fashion-mnist Dataset was converted to CSV with this script https//pjreddie.com/projects/mnist-in-csv/  License The MIT License (MIT) Copyright © [2017] Zalando SE https//tech.zalando.com Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files (the “Software”) to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and/or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED “AS IS” WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",Other,,"[clothing, image data, multiclass classification, object identification]",Other,,,6109,54143,69,"An MNIST-like dataset of 70,000 28x28 labeled fashion images",Fashion MNIST,https://www.kaggle.com/zalando-research/fashionmnist,Thu Dec 07 2017
,Zeeshan-ul-hassan Usmani,"[S#, Title, Location, Date, Summary, Fatalities, Injured, Total victims, Mental Health Issues, Race, Gender, Latitude, Longitude]","[numeric, string, string, dateTime, string, numeric, numeric, numeric, string, string, string, string, string]",Context Mass Shootings in the United States of America (1966-2017) The US has witnessed 398 mass shootings in last 50 years that resulted in 1996 deaths and 2488 injured. The latest and the worst mass shooting of October 2 2017 killed 58 and injured 515 so far. The number of people injured in this attack is more than the number of people injured in all mass shootings of 2015 and 2016 combined.  The average number of mass shootings per year is 7 for the last 50 years that would claim 39 lives and 48 injured per year.  Content Geography United States of America Time period 1966-2017 Unit of analysis Mass Shooting Attack Dataset The dataset contains detailed information of 398 mass shootings in the United States of America that killed 1996 and injured 2488 people.   Variables The dataset contains Serial No Title Location Date Summary Fatalities Injured Total Victims Mental Health Issue Race Gender and Lat-Long information. Acknowledgements I’ve consulted several public datasets and web pages to compile this data.  Some of the major data sources include Wikipedia Mother Jones Stanford USA Today and other web sources.  Inspiration With a broken heart I like to call the attention of my fellow Kagglers to use Machine Learning and Data Sciences to help me explore these ideas •   How many people got killed and injured per year? •   Visualize mass shootings on the U.S map •   Is there any correlation between shooter and his/her race gender •   Any correlation with calendar dates? Do we have more deadly days weeks or months on average •   What cities and states are more prone to such attacks •   Can you find and combine any other external datasets to enrich the analysis for example gun ownership by state •   Any other pattern you see that can help in prediction crowd safety or in-depth analysis of the event •   How many shooters have some kind of mental health problem? Can we compare that shooter with general population with same condition Mass Shootings Dataset Ver 3 This is the new Version of Mass Shootings Dataset. I've added eight new variables  Incident Area (where the incident took place)  Open/Close Location (Inside a building or open space)  Target (possible target audience or company)  Cause (Terrorism Hate Crime Fun (for no obvious reason etc.) Policeman Killed (how many on duty officers got killed) Age (age of the shooter) Employed (Y/N)  Employed at  (Employer Name)  Age Employed and Employed at (3 variables) contain shooter details Mass Shootings Dataset Ver 4 Quite a few missing values have been added Mass Shootings Dataset Ver 5 Three more recent mass shootings have been added including the Texas Church shooting of November 5 2017 I hope it will help create more visualization and extract patterns.  Keep Coding!,CSV,,"[united states, crime, violence, terrorism]",ODbL,,,4950,34623,0.6630859375,Last 50 Years (1966-2017),US Mass Shootings ,https://www.kaggle.com/zusmani/us-mass-shootings-last-50-years,Mon Nov 06 2017
,Boris Marjanovic,[],[],Context High-quality financial data is expensive to acquire and is therefore rarely shared for free.  Here I provide the full historical daily price and volume data for all U.S.-based stocks and ETFs trading on the NYSE NASDAQ and NYSE MKT. It's one of the best datasets of its kind you can obtain.  Content The data (last updated 11/10/2017) is presented in CSV format as follows Date Open High Low Close Volume OpenInt. Note that prices have been adjusted for dividends and splits.  Acknowledgements This dataset belongs to me. I’m sharing it here for free. You may do with it as you wish.  Inspiration Many have tried but most have failed to predict the stock market's ups and downs. Can you do any better? ,Other,,"[business, finance, economics, artificial intelligence]",CC0,,,3639,22728,245,Historical daily prices and volumes of all U.S. stocks and ETFs,Huge Stock Market Dataset,https://www.kaggle.com/borismarjanovic/price-volume-data-for-all-us-stocks-etfs,Thu Nov 16 2017
,Ebrahimi,"[Company, Time, Financial Distress, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x22, x23, x24, x25, x26, x27, x28, x29, x30, x31, x32, x33, x34, x35, x36, x37, x38, x39, x40, x41, x42, x43, x44, x45, x46, x47, x48, x49, x50, x51, x52, x53, x54, x55, x56, x57, x58, x59, x60, x61, x62, x63, x64, x65, x66, x67, x68, x69, x70, x71, x72, x73, x74, x75, x76, x77, x78, x79, x80, x81, x82, x83]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This data set deals with the financial distress prediction for a sample of companies.   Content First column Company represents sample companies. Second column Time shows different time periods that data belongs to. Time series length varies between 1 to 14 for each company. Third column The target variable is denoted by ""Financial Distress"" if it is greater than -0.50 the company should be considered as healthy (0). Otherwise it would be regarded as financially distressed (1).  Fourth column to the last column The features denoted by x1 to x83 are some financial and non-financial characteristics of the sampled companies. These features belong to the previous time period which should be used to predict whether the company will be financially distressed or not (classification). Feature  x80  is categorical variable. For example company 1 is financially distressed at time 4 but company 2 is still healthy at time 14.  This data set is imbalanced (there are 136 financially distressed companies against 286 healthy ones i.e. 136 firm-year observations are financially distressed while 3546 firm-year observations are healthy) and skewed so f-score should be employed as the performance evaluation criterion.  It should be noted that 30% of this data set should be randomly assigned as hold-out test set so the remaining 70% is used for feature selection and model selection i.e. train set. Note  1- This data could be viewed as a classification problem. 2- This data could also be considered as a regression problem and then the result will be converted into a classification. 3- This data could be regarded as a multivariate time series classification. Inspiration Which features are most indicative of financial distress? What types of machine learning models perform best on this dataset?",CSV,,"[finance, machine learning]",Other,,,547,5093,0.7958984375,Bankruptcy Prediction,Financial Distress Prediction,https://www.kaggle.com/shebrahimi/financial-distress,Fri Dec 15 2017
,Rachael Tatman,[],[],"Context “A blog (a truncation of the expression ""weblog"") is a discussion or informational website published on the World Wide Web consisting of discrete often informal diary-style text entries (""posts""). Posts are typically displayed in reverse chronological order so that the most recent post appears first at the top of the web page. Until 2009 blogs were usually the work of a single individual occasionally of a small group and often covered a single subject or topic.” -- Wikipedia article “Blog” This dataset contains text from blogs written on or before 2004 with each blog being the work of a single user. Content The Blog Authorship Corpus consists of the collected posts of 19320 bloggers gathered from blogger.com in August 2004. The corpus incorporates a total of 681288 posts and over 140 million words - or approximately 35 posts and 7250 words per person.   Each blog is presented as a separate file the name of which indicates a blogger id# and the blogger’s self-provided gender age industry and astrological sign. (All are labeled for gender and age but for many industry and/or sign is marked as unknown.) All bloggers included in the corpus fall into one of three age groups * 8240 ""10s"" blogs (ages 13-17) * 8086 ""20s"" blogs(ages 23-27) * 2994 ""30s"" blogs (ages 33-47). For each age group there are an equal number of male and female bloggers.    Each blog in the corpus includes at least 200 occurrences of common English words. All formatting has been stripped with two exceptions. Individual posts within a single blogger are separated by the date of the following post and links within a post are denoted by the label urllink. Acknowledgements The corpus may be freely used for non-commercial research purposes. Any resulting publications should cite the following J. Schler M. Koppel S. Argamon and J. Pennebaker (2006). Effects of Age and Gender on Blogging in Proceedings of 2006 AAAI Spring Symposium on Computational Approaches for Analyzing Weblogs. URL http//www.cs.biu.ac.il/~schlerj/schler_springsymp06.pdf Inspiration  This dataset contains information on writers demographics including their age gender and zodiac sign. Can you build a classifier to guess someone’s zodiac sign from blog posts they’ve written? Which are bigger differences between demographic groups or differences between blogs on different topics?  You may also like  News and Blog Data Crawl Content section from over 160000 news and blog articles 20 Newsgroups A collection of ~18000 newsgroup documents from 20 different newsgroups ",CSV,,"[languages, linguistics, internet]",Other,,,140,1696,763,"Over 600,000 posts from more than 19 thousand bloggers",Blog Authorship Corpus,https://www.kaggle.com/rtatman/blog-authorship-corpus,Wed Aug 16 2017
,Datafiniti,"[id, brand, categories, dateAdded, dateUpdated, ean, keys, manufacturer, manufacturerNumber, name, reviews.date, reviews.dateAdded, reviews.dateSeen, reviews.didPurchase, reviews.doRecommend, reviews.id, reviews.numHelpful, reviews.rating, reviews.sourceURLs, reviews.text, reviews.title, reviews.userCity, reviews.userProvince, reviews.username, upc]","[string, string, string, dateTime, dateTime, numeric, string, string, numeric, string, dateTime, dateTime, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric]",About This Data This is a list of over 71045 reviews from 1000 different products provided by Datafiniti's Product Database. The dataset includes the text and title of the review the name and manufacturer of the product reviewer metadata and more. What You Can Do With This Data You can use this data to assess how writing quality impacts positive and negative online product reviews. E.g.  Do reviewers use punctuation correctly? Does the number of spelling errors differ by rating? What is the distribution of star ratings across products? How does review length differ by rating? How long is the typical review? What is the frequency of words with spelling errors by rating?  What is the number of reviews that don’t end sentences with punctuation? What is the proportion of reviews with spelling errors?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, product]",CC4,,,50,589,9,"A list of 71,045 online reviews from 1,000 different products.",Grammar and Online Product Reviews,https://www.kaggle.com/datafiniti/grammar-and-online-product-reviews,Thu Feb 15 2018
,City of New York,"[Summons Number, Plate ID, Registration State, Plate Type, Issue Date, Violation Code, Vehicle Body Type, Vehicle Make, Issuing Agency, Street Code1, Street Code2, Street Code3, Vehicle Expiration Date, Violation Location, Violation Precinct, Issuer Precinct, Issuer Code, Issuer Command, Issuer Squad, Violation Time, Time First Observed, Violation County, Violation In Front Of Or Opposite, House Number, Street Name, Intersecting Street, Date First Observed, Law Section, Sub Division, Violation Legal Code, Days Parking In Effect    , From Hours In Effect, To Hours In Effect, Vehicle Color, Unregistered Vehicle?, Vehicle Year, Meter Number, Feet From Curb, Violation Post Code, Violation Description, No Standing or Stopping Violation, Hydrant Violation, Double Parking Violation, Latitude, Longitude, Community Board, Community Council , Census Tract, BIN, BBL, NTA]","[numeric, string, string, string, dateTime, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, string, string, numeric, numeric, string, string, string, string, string, string, numeric, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context The NYC Department of Finance collects data on every parking ticket issued in NYC (~10M per year!). This data is made publicly available to aid in ticket resolution and to guide policymakers. Content There are four files covering Aug 2013-June 2017. The files are roughly organized by fiscal year (July 1 - June 30) with the exception of the initial dataset. The initial dataset also lacks 8 columns that are included in the other three datasets (although be warned that these additional data columns are used sparingly). See the dataset descriptions for exact details. Columns include information about the vehicle ticketed the ticket issued location and time. Acknowledgements Data was produced by NYC Department of Finance. FY2018 data is found here with updates every third week of the month. Inspiration  When are tickets most likely to be issued? Any seasonality? Where are tickets most commonly issued? What are the most common years and types of cars to be ticketed? ,CSV,,"[cities, government, law, automobiles]",CC0,,,1226,6997,8192,"42.3M Rows of Parking Ticket Data, Aug 2013-June 2017",NYC Parking Tickets,https://www.kaggle.com/new-york-city/nyc-parking-tickets,Fri Oct 27 2017
,T7 - Pokemon Challenge,"[First_pokemon, Second_pokemon, Winner]","[numeric, numeric, numeric]",Welcome to Weedle's cave. Will you be able to predict the outcome of future matches? To do it you will have the pokemon characteristics and the results of previous combats. Three files are available. The first one contains the pokemon characteristics (the first column being the id of the pokemon). The second one contains information about previous combats. The first two columns contain the ids of the combatants and the third one the id of the winner.  Important The pokemon in the first columns attacks first. The goal is to develop a Machine Learning model able to predict the result of future pokemon combats. If you have any questions please email  t7pokemonchallenge@intelygenz.com DISCLAIMER In Intelygenz we are against animal abuse. No animal real or imaginary should be forced to fight against other.  Freedom for the pokemons !,CSV,,"[popular culture, video games]",CC0,,,6689,15377,0.666015625,Welcome to Weedle's cave,Pokemon- Weedle's Cave,https://www.kaggle.com/terminus7/pokemon-challenge,Thu Sep 21 2017
,United Nations,"[country_or_area, commodity_transaction, year, unit, quantity, quantity_footnotes, category]","[string, string, numeric, string, numeric, string, string]",Curious about the growth of wind energy? The extent to which the decline of coal is an American or international trend? Interested in using energy consumption as an alternate method of comparing national economies? This dataset has you covered. The Energy Statistics Database contains comprehensive energy statistics on the production trade conversion and final consumption of primary and secondary; conventional and non-conventional; and new and renewable sources of energy.  Acknowledgements This dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,Other,,"[economics, energy]",Other,,,911,6068,7,Global energy trade & production 1990-2014 ,International Energy Statistics,https://www.kaggle.com/unitednations/international-energy-statistics,Thu Nov 16 2017
,National Institutes of Health Chest X-Ray Dataset,[],[],"NIH Chest X-ray Dataset  National Institutes of Health Chest X-Ray Dataset Chest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult if not impossible to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset Openi was the largest publicly available source of chest X-ray images with 4143 images available. This NIH Chest X-ray Dataset is comprised of 112120 X-ray images with disease labels from 30805 unique patients. To create these labels the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper ""ChestX-ray8 Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases."" (Wang et al.) Link to paper  Data limitations  The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.  Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv) Chest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later maybe through manual annotation   File contents  Image format 112120 total images with size 1024 x 1024 images_001.zip Contains 4999 images images_002.zip Contains 10000 images images_003.zip Contains 10000 images images_004.zip Contains 10000 images images_005.zip Contains 10000 images images_006.zip Contains 10000 images images_007.zip Contains 10000 images images_008.zip Contains 10000 images images_009.zip Contains 10000 images images_010.zip Contains 10000 images images_011.zip Contains 10000 images images_012.zip Contains 7121 images README_ChestXray.pdf Original README file BBox_list_2017.csv Bounding box coordinates. Note Start at xy extend horizontally w pixels and vertically h pixels Image Index File name Finding Label Disease type (Class label) Bbox x  Bbox y Bbox w Bbox h Data_entry_2017.csv Class labels and patient data for the entire dataset Image Index File name Finding Labels Disease type (Class label) Follow-up #  Patient ID Patient Age Patient Gender View Position X-ray orientation OriginalImageWidth OriginalImageHeight OriginalImagePixelSpacing_x OriginalImagePixelSpacing_y   Class descriptions There are 15 classes (14 diseases and one for ""No findings""). Images can be classified as ""No findings"" or one or more disease classes  Atelectasis Consolidation Infiltration Pneumothorax Edema Emphysema Fibrosis Effusion Pneumonia Pleural_thickening Cardiomegaly Nodule Mass Hernia   Full Dataset Content There are 12 zip files in total and range from ~2 gb to 4 gb in size.  Additionally we randomly sampled 5% of these images and created a smaller dataset for use in Kernels. The random sample contains 5606 X-ray images and class labels.   Sample sample.zip   Modifications to original data  Original TAR archives were converted to ZIP archives to be compatible with the Kaggle platform CSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory   Citations  Wang X Peng Y Lu L Lu Z Bagheri M Summers RM. ChestX-ray8 Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017 ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf NIH News release NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community Original source files and documents https//nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345   Acknowledgements This work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov). ",Other,,"[medicine, machine learning]",CC0,,,2088,8826,43008,"Over 112,000 Chest X-ray images from more than 30,000 unique patients",NIH Chest X-rays,https://www.kaggle.com/nih-chest-xrays/data,Thu Feb 22 2018
,David Shinn,"[issue_url, issue_title, body]","[string, string, string]","Description Over 8 million GitHub issue titles and descriptions from 2017.  Prepared from instructions at How To Create Data Products That Are Magical Using Sequence-to-Sequence Models. Original Source The data was adapted from GitHub data accessible from GitHub Archive.  The constructocat image is from https//octodex.github.com/constructocat-v2. License MIT License Copyright (c) 2018 David Shinn Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files (the ""Software"") to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and/or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"" WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",CSV,,"[linguistics, computers]",Other,,,194,1780,980,GitHub issue titles and descriptions for NLP analysis.,GitHub Issues,https://www.kaggle.com/davidshinn/github-issues,Thu Jan 18 2018
,Selfish Gene,"[City, Country, Latitude, Longitude]","[string, string, numeric, numeric]",Historical Hourly Weather Data Who amongst us doesn't small talk about the weather every once in a while?  The goal of this dataset is to elevate this small talk to medium talk.   Just kidding I actually originally decided to collect this dataset in order to demonstrate basic signal processing concepts such as filtering Fourier transform auto-correlation cross-correlation etc... (for a data analysis course I'm currently preparing).  I wanted to demonstrate these concepts on signals that we all have intimate familiarity with and hope that this way these concepts will be better understood than with just made up signals.   The weather is excellent for demonstrating these kinds of concepts as it contains periodic temporal structure with two very different periods (daily and yearly).  Content The dataset contains ~5 years of high temporal resolution (hourly measurements) data of various weather attributes such as temperature humidity air pressure etc.  This data is available for 30 US and Canadian Cities as well as 6 Israeli cities.  I've organized the data according to a common time axis for easy use.  Each attribute has it's own file and is organized such that the rows are the time axis (it's the same time axis for all files) and the columns are the different cities (it's the same city ordering for all files as well).  Additionally for each city we also have the country latitude and longitude information in a separate file. Acknowledgements The dataset was aquired using Weather API on the OpenWeatherMap website and is available under the ODbL License.   Inspiration Weather data is both intrinsically interesting and also potentially useful when correlated with other types of data.  For example Wildfire spread is potentially related to weather conditions demand for cabs is famously known to be correlated with weather conditions (here here and here you can find NYC cab ride data) and use of city bikes is probably also correlated with weather in interesting ways (check out this Austin dataset this SF dataset this Montreal dataset and this NYC dataset).  Traffic is also probably related to weather.  Another potentially interesting source of correlation is between weather and crime. Here are a few crime datasets on kaggle of cities present in this weather dataset Chicago Philadelphia Los Angeles Vancouver Austin NYC  There are many other potentially interesting connections between everyday life and the weather that we can explore together with the help of this dataset. Have fun!,CSV,,"[united states, time series, geography]",ODbL,,,600,3662,12,Hourly weather data for 30 US & Canadian Cities + 6 Israeli Cities ,Historical Hourly Weather Data 2012-2017,https://www.kaggle.com/selfishgene/historical-hourly-weather-data,Thu Dec 28 2017
,Alexander Mamaev,[],[],Context This dataset contains 4242 images of flowers.  The data collection is based on the data flicr google images yandex images. You can use this datastet to recognize plants from the photo. Content The pictures are divided into five classes chamomile tulip rose sunflower dandelion.  For each class there are about 800 photos. Photos are not high resolution about 320x240 pixels. Photos are not reduced to a single size they have different proportions!,Other,,"[photography, plants, machine learning]",Other,,,539,3078,225,This dataset contains labled 4242 images of flowers. ,Flowers Recognition,https://www.kaggle.com/alxmamaev/flowers-recognition,Sat Jan 06 2018
,pavansubhash,"[Age, Attrition, BusinessTravel, DailyRate, Department, DistanceFromHome, Education, EducationField, EmployeeCount, EmployeeNumber, EnvironmentSatisfaction, Gender, HourlyRate, JobInvolvement, JobLevel, JobRole, JobSatisfaction, MaritalStatus, MonthlyIncome, MonthlyRate, NumCompaniesWorked, Over18, OverTime, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StandardHours, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager]","[numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Uncover the factors that lead to employee attrition and explore important questions such as ‘show me a breakdown of distance from home by job role and attrition’ or ‘compare average monthly income by education and attrition’. This is a fictional data set created by IBM data scientists. Education     1 'Below College'     2 'College'     3 'Bachelor'     4 'Master'     5 'Doctor' EnvironmentSatisfaction     1 'Low'     2 'Medium'     3 'High'     4 'Very High' JobInvolvement          1 'Low'     2 'Medium'     3 'High'     4 'Very High' JobSatisfaction         1 'Low'     2 'Medium'     3 'High'     4 'Very High' PerformanceRating          1 'Low'     2 'Good'     3 'Excellent'     4 'Outstanding' RelationshipSatisfaction          1 'Low'     2 'Medium'     3 'High'     4 'Very High' WorkLifeBalance          1 'Bad'     2 'Good'     3 'Better'     4 'Best',CSV,,"[employment, business]",ODbL,,,9947,68301,0.2177734375,Predict attrition of your valuable employees,IBM HR Analytics Employee Attrition & Performance,https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset,Fri Mar 31 2017
,OpenPowerlifting,"[MeetID, MeetPath, Federation, Date, MeetCountry, MeetState, MeetTown, MeetName]","[numeric, string, string, dateTime, string, string, string, string]",Context This dataset is a snapshot of the OpenPowerlifting database as of February 2018. OpenPowerlifting is an organization which tracks meets and competitor results in the sport of powerlifting in which competitors complete to lift the most weight for their class in three separate weightlifting categories. Content This dataset includes two files. meets.csv is a record of all meets (competitions) included in the OpenPowerlifting database. competitors.csv is a record of all competitors who attended those meets and the stats and lifts that they recorded at them. For more on how this dataset was collected see the OpenPowerlifting FAQ. Acknowledgements This dataset is republished as-is from the OpenPowerlifting source. Inspiration  How much influence does overall weight have on lifting capacity? How big of a difference does gender make? What is demographic of lifters more generally? ,CSV,,[weight training],CC0,,,281,1904,9,"Over 3,000 meets and 300,000 lifts from competitions worldwide",Powerlifting Database,https://www.kaggle.com/open-powerlifting/powerlifting-database,Fri Feb 02 2018
,Aleksey Bilogur,"[Invoice/Item Number, Date, Store Number, Store Name, Address, City, Zip Code, Store Location, County Number, County, Category, Category Name, Vendor Number, Vendor Name, Item Number, Item Description, Pack, Bottle Volume (ml), State Bottle Cost, State Bottle Retail, Bottles Sold, Sale (Dollars), Volume Sold (Liters), Volume Sold (Gallons)]","[string, dateTime, numeric, string, string, string, numeric, string, numeric, string, string, string, numeric, string, numeric, string, numeric, numeric, string, string, numeric, string, numeric, numeric]","Context The Iowa Department of Commerce requires that every store that sells alcohol in bottled form for off-the-premises consumption must hold a class ""E"" liquor license (an arrangement typical of most of the state alcohol regulatory bodies). All alcoholic sales made by stores registered thusly with the Iowa Department of Commerce are logged in the Commerce department system which is in turn published as open data by the State of Iowa. Content This dataset contains information on the name kind price quantity and location of sale of sales of individual containers or packages of containers of alcoholic beverages. This dataset is relatively straightforward but one source of further information on the contents of the data is this Gist. Acknowledgements This data was originally published by the State of Iowa here and has been republished as-is on Kaggle. Inspiration This data is probably a representative sample of sale activity for alcohol in the United States and can be used to answer many questions thereof like how much alcohol is sold and consumed in the United States? What kind? What are the most popular brands and labels? What are the most popular mixers? What is the distribution of prices paid in-store? Etcetera.",CSV,,"[food and drink, alcohol]",CC0CC0: Public Domain,,,697,4174,731,12 million alcoholic beverage sales in the Midwest,Iowa Liquor Sales,https://www.kaggle.com/residentmario/iowa-liquor-sales,Wed Nov 15 2017
,Mitchell J,"[type, posts]","[string, string]","Context The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis  Introversion (I) – Extroversion (E) Intuition (N) – Sensing (S) Thinking (T) – Feeling (F) Judging (J) – Perceiving (P)  (More can be learned about what these mean here) So for example someone who prefers introversion intuition thinking and perceiving would be labelled an INTP in the MBTI system and there are lots of personality based components that would model or describe this person’s preferences or behaviour based on the label. It is one of if not the the most popular personality test in the world. It is used in businesses online for fun for research and lots more. A simple google search reveals all of the different ways the test has been used over time. It’s safe to say that this test is still very relevant in the world in terms of its use. From scientific or psychological perspective it is based on the work done on cognitive functions by Carl Jung i.e. Jungian Typology. This was a model of 8 distinct functions thought processes or ways of thinking that were suggested to be present in the mind. Later this work was transformed into several different personality systems to make it more accessible the most popular of which is of course the MBTI.  Recently its use/validity has come into question because of unreliability in experiments surrounding it among other reasons. But it is still clung to as being a very useful tool in a lot of areas and the purpose of this dataset is to help see if any patterns can be detected in specific types and their style of writing which overall explores the validity of the test in analysing predicting or categorising behaviour. Content This dataset contains over 8600 rows of data on each row is a person’s  Type (This persons 4 letter MBTI code/type) A section of each of the last 50 things they have posted (Each entry separated by ""|||"" (3 pipe characters))  Acknowledgements This data was collected through the PersonalityCafe forum as it provides a large selection of people and their MBTI personality type as well as what they have written.  Inspiration Some basic uses could include  Use machine learning to evaluate the MBTIs validity and ability to predict language styles and behaviour online. Production of a machine learning algorithm that can attempt to determine a person’s personality type based on some text they have written. ",CSV,,"[demographics, linguistics]",CC0,,,4287,42688,60,Includes a large number of people's MBTI type and content written by them,(MBTI) Myers-Briggs Personality Type Dataset,https://www.kaggle.com/datasnaek/mbti-type,Fri Sep 22 2017
,Jacob Boysen,"[orcid_id, phd_year, country_2016, earliest_year, earliest_country, has_phd, phd_country, has_migrated]","[string, numeric, string, numeric, string, boolean, string, boolean]",Context ORCID provides a persistent digital identifier that distinguishes you from every other researcher and through integration in key research workflows such as manuscript and grant submission supports automated linkages between you and your professional activities ensuring that your work is recognized. Find out more. Content This data is a subset of the entire ORCID collection. The subset here was produced by John Bohannon. You can see his excellent Ipython notebook and the entire (300GB!) ORCID archives here. The data covers ~742k unique researchers and includes  orcid_id phd_year country_2016 earliest_year earliest_country has_phd phd_country has_migrated  Acknowledgements Bohannon J Doran K (2017) Introducing ORCID. Science 356(6339) 691-692. http//dx.doi.org/10.1126/science.356.6339.691 Additionally please cite the Dryad data package Bohannon J Doran K (2017) Data from Introducing ORCID. Dryad Digital Repository. http//dx.doi.org/10.5061/dryad.48s16 Inspiration  Where do most researchers move to? What countries experience the largest ‘brain drain’? As a % of population? Can you predict researcher migration? ,CSV,,[demographics],CC0,,,248,3138,34,Movement of ~742k Scientists,Scientific Researcher Migrations,https://www.kaggle.com/jboysen/scientist-migrations,Thu Aug 31 2017
,KevinH,"[ID, name, full_name, club, club_logo, special, age, league, birth_date, height_cm, weight_kg, body_type, real_face, flag, nationality, photo, eur_value, eur_wage, eur_release_clause, overall, potential, pac, sho, pas, dri, def, phy, international_reputation, skill_moves, weak_foot, work_rate_att, work_rate_def, preferred_foot, crossing, finishing, heading_accuracy, short_passing, volleys, dribbling, curve, free_kick_accuracy, long_passing, ball_control, acceleration, sprint_speed, agility, reactions, balance, shot_power, jumping, stamina, strength, long_shots, aggression, interceptions, positioning, vision, penalties, composure, marking, standing_tackle, sliding_tackle, gk_diving, gk_handling, gk_kicking, gk_positioning, gk_reflexes, rs, rw, rf, ram, rcm, rm, rdm, rcb, rb, rwb, st, lw, cf, cam, cm, lm, cdm, cb, lb, lwb, ls, lf, lam, lcm, ldm, lcb, gk, 1_on_1_rush_trait, acrobatic_clearance_trait, argues_with_officials_trait, avoids_using_weaker_foot_trait, backs_into_player_trait, bicycle_kicks_trait]","[numeric, string, string, string, string, numeric, numeric, string, dateTime, numeric, numeric, string, boolean, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, boolean, boolean, boolean, boolean, boolean, boolean]",Context This dataset is an extension of that found here. It contains several extra fields and is pre-cleaned to a much greater extent. After talking with the creator of the original dataset he and I agreed that merging our work would require making breaking changes to the original and that this should be published as a new dataset. Content  185 fields for every player in FIFA 18. Player info such as age club league nationality salary and physical attributes All playing attributes such as finishing and dribbling Special attributes like skill moves and international reputation Traits and specialities Overall potential and ratings for each position  Differences Here are the columns in this dataset that aren't in the original  birth_date eur_release_clause height_cm weight_kg body_type real_face league Headline attributes pac sho pas dri def and phy. These are what appear on Ultimate Team cards international_reputation skill_moves weak_foot work_rate_att work_rate_def preferred_foot all traits and specialities as dummy variables all position preferences as dummy variables  Acknowledgements Credit goes to Aman Shrivastava for building the original dataset. And thanks of course to  https//sofifa.com for not banning my IP when I scraped over 18000 pages to get this data. Inspiration What insights can this data give us not only into FIFA 18 but into real-world football? The kernels on last year's dataset are a good place to find ideas. Contributing Contributions to the GitHub project are more than welcome. Do let me know if you think of ways to improve either the code or the dataset!,CSV,,"[popular culture, video games, association football, sports]",CC0,,,2042,11529,5,FIFA 18 Player Data++. ,Fifa 18 More Complete Player Dataset,https://www.kaggle.com/kevinmh/fifa-18-more-complete-player-dataset,Tue Dec 26 2017
,jvent,"[slug, symbol, name, date, ranknow, open, high, low, close, volume, market, close_ratio, spread]","[string, string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Cryptocurrency Market Data Historical Cryptocurrency Prices For ALL Tokens! Summary > Observations 649051 > Variables 13   > Crypto Tokens 1382   > Start Date 28/04/2017   > End Date 03/01/2018    Description All historic open high low close trading volume and market cap info for all cryptocurrencies.   I've had to go over the code with a fine tooth comb to get it compatible with CRAN so there have been significant enhancements to how some of the field conversions have been undertaken and the data being cleaned. This should eliminate a few issues around number formatting or unexpected handling of scientific notations.   Data Structure Observations 649051     Variables 13     $ slug        <chr> ""bitcoin"" ""bitcoin"" ""bitcoin"" ""bitcoin""...         $ symbol      <chr> ""BTC"" ""BTC"" ""BTC"" ""BTC"" ""BTC"" ""BTC"" ...     $ name        <chr> ""Bitcoin"" ""Bitcoin"" ""Bitcoin"" ""Bitcoin""...     $ date        <date> 2013-04-28 2013-04-29 2013-04-30 2013-...     $ ranknow     <dbl> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ...     $ open        <dbl> 135.30 134.44 144.00 139.00 116.38 10...     $ high        <dbl> 135.98 147.49 146.93 139.89 125.60 10...     $ low         <dbl> 132.10 134.00 134.05 107.72 92.28 79...     $ close       <dbl> 134.21 144.54 139.00 116.99 105.21 97...     $ volume      <dbl> 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...     $ market      <dbl> 1500520000 1491160000 1597780000 154282...     $ close_ratio <dbl> 0.5438 0.7813 0.3843 0.2882 0.3881 0...     $ spread      <dbl> 3.88 13.49 12.88 32.17 33.32 29.03 2...      Built With heart R I've ported my original kernel to generate this data across into a R package which is awaiting being published on CRAN. Run the below to go scrape all the historical tables of all the different cryptocurrencies listed on CoinMarketCap and turn it into a data frame.   You can install it via the github link below or   devtoolsinstall_github(""jessevent/crypto"")   library(crypto)   will_i_get_rich <- getCoins()    Authors Jesse Vent - Package Author - jessevent  Acknowledgments  Github - View my github repository for the full package.   CoinSpot - Invest $AUD into Crypto today!   CoinMarketCap - Providing amazing data @CoinMarketCap    If this helps you become rich please consider making a donation!     ERC-20 0x375923Bf82F0b728d23A5704261a6e16341fd860     XRP rK59semLsuJZEWftxBFhWuNE6uhznjz2bK     LTC LWpiZMd2cEyqCdrZrs9TjsouTLWbFFxwCj ",CSV,,"[business, finance, internet]",Other,,,3993,32246,16,"Daily crypto markets open, close, low, high data for every token ever",Every Cryptocurrency Daily Market Price,https://www.kaggle.com/jessevent/all-crypto-currencies,Thu Feb 22 2018
,ValerioVaccaro,"[, Date, Open, High, Low, Close, Volume, Market.Cap, coin, Delta]","[numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric]",Introduction This file contains the values of the price for more than 1000 different cryptocurrencies (including scams) recorded on daily base I decide to include all coins in order to analyze exotic coins and compare with well knows cryptocurrencies. All this dataset come from  coinmarketcap  historical pages grabbed using just an R script.  Thanks coinmarketcap to making this data available for free (and for every kind of usage). The dataset Available columns in the dataset  Date - the day of recorded values  Open - the opening price (in USD) High - the highest price (in USD) Low - the lowest price (in USD) Close - the closing price (in USD) Volume - total exchanged volume (in USD) Market.Cap - the total market capitalization for the coin (in USD) coin - the name of the coin Delta - calculated as (Close - Open) / Open ,CSV,,[],CC0,,,864,6277,20,A collection of prices for cryptocurrencies ,Cryptocoins Historical Prices,https://www.kaggle.com/valeriovaccaro/cryptocoinshistoricalprices,Fri Jan 12 2018
,United Nations,"[country_or_area, year, comm_code, commodity, flow, trade_usd, weight_kg, quantity_name, quantity, category]","[string, numeric, numeric, string, string, numeric, numeric, string, numeric, string]",Are you curious about fertilizer use in developing economies? The growth of Chinese steel exports? American chocolate consumption? Which parts of the world still use typewriters? You'll find all of that and more here. This dataset covers import and export volumes for 5000 commodities across most countries on Earth over the last 30 years. Acknowledgements This dataset was kindly published by the United Nations Statistics Division on the UNData site. You can find the original dataset here. Inspiration  Some of these numbers are more trustworthy than others. I'd expect that British tea imports are fairly accurate but doubt that Afghanistan exported exactly 51 sheep in 2016. Can you identify which nations appear to have the most trustworthy data? Which industries?  License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,"[supply chain, economics, shipping]",Other,,,597,3815,121,Three decades of global trade flows,Global Commodity Trade Statistics,https://www.kaggle.com/unitednations/global-commodity-trade-statistics,Wed Nov 15 2017
,beluga,[],[],Context Kaggle has more and more computer vision challenges. Although Kernel resources were increased recently we still can not train useful CNNs without GPU. The other main problem is that Kernels can't use network connection to download pretrained keras model weights. This dataset helps you to apply your favorite pretrained model in the Kaggle Kernel environment.  Happy data exploration and transfer learning! Content Model (Top-1 Accuracy | Top -5 Accuracy)  Xception (0.790 | 0.945) VGG16 (0.715 | 0.901) VGG19 (0.727 | 0.910) ResNet50 (0.759 | 0.929) InceptionV3 (0.788 | 0.944) InceptionResNetV2 (0.804 | 0.953) (could not upload due to 500 MB limit)  For more information see https//keras.io/applications/ Acknowledgements Thanks to François Chollet for collecting these models and for the awesome keras.,Other,,"[artificial intelligence, pre-trained model]",CC4,,,1405,7933,943,This dataset helps to use pretrained keras models in Kernels.,Keras Pretrained models,https://www.kaggle.com/gaborfodor/keras-pretrained-models,Fri Nov 17 2017
,PromptCloud,"[uniq_id, crawl_timestamp, product_url, product_name, product_category_tree, pid, retail_price, discounted_price, image, is_FK_Advantage_product, description, product_rating, overall_rating, brand, product_specifications]","[string, dateTime, string, string, string, string, numeric, numeric, string, boolean, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 5.8 million products) that was created by extracting data from Flipkart.com a leading Indian eCommerce store. Content This dataset has following fields  product_url product_name product_category_tree pid retail_price discounted_price image is_FK_Advantage_product description product_rating overall_rating brand product_specifications  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the pricing product specification and brand can be performed.,CSV,,[internet],CC4,,,982,6089,36,"20,000 products on Flipkart",Flipkart Products,https://www.kaggle.com/PromptCloudHQ/flipkart-products,Fri Sep 15 2017
,Stephanerappeneau,"[director_name, ceremony, year, category, outcome, original_language]","[string, string, numeric, string, string, string]","Context I love movies.  I tend to avoid marvel-transformers-standardized products and prefer a mix of classic hollywood-golden-age and obscure polish artsy movies. Throw in an occasional japanese-zombie-slasher-giallo as an alibi. Good movies don't exist without bad movies.  On average I watch 200+ movies each year with peaks at more than 500 movies. Nine years ago I started to log my movies to avoid watching the same movie twice and also assign scores. Over the years it gave me a couple insights on my viewing habits but nothing more than what a tenth-grader would learn at school.  I've recently suscribed to Netflix and it pains me to see the global inefficiency of recommendation systems for people like me who mostly swear by ""La politique des auteurs"". It's a term coined by famous new-wave french movie critic André Bazin meaning that the quality of a movie is essentially linked to the director and it's capacity to execute his vision with his crew. We could debate it depends on movie production pipeline but let's not for now. Practically what it means is that I essentially watch movies from directors who made films I've liked.  I suspect Neflix calibrate their recommandation models taking into account the way the ""average-joe"" chooses a movie. A few months ago I had read a study based on a survey showing that people chose a movie mostly based on genre (55%) then by leading actors (45%). Director or Release Date were far behind around 10% each. It is not surprising since most people I know don't care who the director is. Lots of US blockbusters don't even mention it on the movie poster. I am aware that collaborative filtering is based on user proximity  which I believe decreases (or even eliminates) the need to characterize a movie. So here I'm more interested in content based filtering which is based on product proximity for several reasons   Users tastes are not easily accessible. It is after all Netflix treasure chest Movie offer on Netflix is so bad for someone who likes author's movies that it wouldn't help Modeling a movie intrinsic qualities is a nice challenge  Enough. ""The secret of getting ahead is getting started"" (Mark Twain)  Content The primary source is www.themoviedb.org. If you watch obscure artsy romanian homemade movies you may find only 95% of your movies referenced...but for anyone else it should be in the 98%+ range.  movies details are from www.themoviedb.org API  movies/details movies crew & casting are from www.themoviedb.org API  movies/credits both can be joined by id they contain all 350k movies up from end of 19th century to august 2017. If you remove short movies from imdb you get similar amounts of movies. I uploaded the program to retrieve incremental movie details on github  https//github.com/stephanerappeneau/scienceofmovies/tree/master/PycharmProjects/GetAllMovies (need a dev API key from themoviedb.org though) I have tried various supervised (decision tree) / unsupervised (clustering NLP) approaches described in the discussions source code is on github  https//github.com/stephanerappeneau/scienceofmovies As a bonus I've uploaded the bio summary from top 500 critically-acclaimed directors from wikipedia for some interesting NLTK analysis  Here is overview of the available sources that I've tried  • Imdb.com free csv dumps (ftp//ftp.funet.fi/pub/mirrors/ftp.imdb.com/pub/temporaryaccess/) are badly documented incomplete loosely structured and impossible to join/merge. There's an API hosted by Amazon Web Service  1€ every 100 000 requests. With around 1 million movies it could become expensive also features are bare. So I've searched for other sources.  • www.themoviedb.org is based on crowdsourcing and has an excellent API limited to 40 requests every 10 seconds. It is quite generous well documented and enough to sweep the 450 000 movies in a few days. For my purpose data quality is not significantly worse than imdb and as imdb key is also included there's always the possibility to complete my dataset later (I actually did it) • www.Boxofficemojo.com has some interesting budget/revenue figures (which are sorely lacking in both imdb & tmdb) but it actually tracks only a few thousand movies mainly blockbusters. There are other professional sources that are used by film industry to get better predictive / marketing insights but that's beyond my reach for this experiment.   • www.wikipedia.com is an interesting source with no real cap on API calls however it requires a bit of webscraping and for movies or directors the layout and quality varies a lot so I suspected it'd get a lot of work to get insights so I put this source in lower priority. • www.google.com will ban you after a few minutes of web scraping because their job is to scrap data from others than sell it duh.   • It's worth mentionning that there are a few dumps of Netflix anonymized user tastes on kaggle because they've organised a few competitions to improve their recommendation models. https//www.kaggle.com/netflix-inc/netflix-prize-data • Online databases are largely white anglo-saxon centric meaning bollywood (India is the 2nd bigger producer of movies) offer is mostly absent from datasets. I'm fine with that as it's not my cup of tea plus I lack domain knowledge. The sheer amount of indian movies would probably skew my results anyway (I don't want to have too many martial-arts-musicals in my recommendations ;-)). I have however tremendous respect for indian movie industry so I'd love to collaborate with an indian cinephile !  Inspiration Starting from there I had multiple problem statements for both supervised / unsupervised machine learning  Can I program a tailored-recommendation system based on my own criteria ? What are the characteristics of movies/directors I like the most ? What is the probability that I will like my next movie ? Can I find the data ?  One of the objectives of sharing my work here is to find cinephile data-scientists who might be interested and hopefully contribute or share insights ) Other interesting leads  use tagline for NLP/Clustering/Genre guessing leverage on budget/revenue link with other data sources using the imdb normalized title etc.  Motivation Disclaimer and Acknowledgements  I've graduated from an french engineering school majoring in artificial intelligence but that was 17 years ago right in the middle of A.I-winter. Like a lot of white male rocket scientists I've ended up in one of the leading european investment bank quickly abandonning IT development to specialize in trading/risk project management and internal politics. My recent appointment in the Data Office made me aware of recent breakthroughts in datascience and I thought that developing a side project would be an excellent occasion to learn something new. Plus it'd give me a well-needed credibility which too often lack decision makers when it comes to datascience. I've worked on some of the features with Cédric Paternotte a fellow friend of mine who is a professor of philosophy of sciences in La Sorbonne. Working with someone with a different background seem a good idea for motivation creativity and rigor. Kudos to www.themoviedb.org or www.wikipedia.com sites who really have a great attitude towards open data. This is typically NOT the case of modern-bigdata companies who mostly keep data to themselves to try to monetize it. Such a huge contrast with imdb or instagram API which generously let you grab your last 3 comments at a miserable rate. Even if 15 years ago this seemed a mandatory path to get services for free I predict one day governments will need to break this data monopoly.  [Disclaimer  I apologize in advance for my engrish (I'm french ^-^) any bad-code I've written (there are probably hundreds of way to do it better and faster) any pseudo-scientific assumption I've made I'm slowly getting back in statistics and lack senior guidance one day I regress a non-stationary time series and the day after I'll discover I shouldn't have and any incorrect use of machine-learning models] ",CSV,,[film],Other,,,952,6953,192,More than 350k movies and main casting/crew up to Aug17,350 000+ movies from themoviedb.org,https://www.kaggle.com/stephanerappeneau/350-000-movies-from-themoviedborg,Fri Oct 13 2017
,Mitchell J,[],[], UPDATED https//www.kaggle.com/datasnaek/youtube-new  Plan Data collected from the (up to) 200 listed trending YouTube videos every day in the US and the UK. Description The dataset includes data gathered from videos on YouTube that are contained within the trending category each day. There are two kinds of data files one includes comments and one includes video statistics. They are linked by the unique video_id field. The headers in the video file are  video_id (Common id field to both comment and video csv files) title channel_title category_id (Can be looked up using the included JSON files but varies per region so use the appropriate JSON file for the CSV file's country) tags (Separated by | character [none] is displayed if there are no tags) views likes dislikes thumbnail_link date (Formatted like so [day].[month])  The headers in the comments file are  video_id (Common id field to both comment and video csv files) comment_text likes replies  Extra info The YouTube API is not effective at formatting comments by relevance although it claims to do so. As a result the most relevant comments do not align with the top comments at all they aren't even sorted by likes or replies. Inspiration Possible uses for this dataset could include  Sentiment analysis in a variety of forms Categorising YouTube videos based on their comments and statistics. Training ML algorithms to generate their own YouTube comments. Analysing what factors affect how popular a YouTube video will be.  Although there are likely many more possibilities including analysis of changes over time etc.,CSV,,"[languages, popular culture, statistics]",CC0,,,3140,27130,149,"Daily statistics (views, likes, category, comments+) for trending YouTube videos",Trending YouTube Video Statistics and Comments,https://www.kaggle.com/datasnaek/youtube,Thu Oct 26 2017
,SHAHIR,"[structureId, classification, experimentalTechnique, macromoleculeType, residueCount, resolution, structureMolecularWeight, crystallizationMethod, crystallizationTempK, densityMatthews, densityPercentSol, pdbxDetails, phValue, publicationYear]","[string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, numeric]","Context This is a protein data set retrieved from RCS PDB.  The PDB archive is a repository of atomic coordinates and other information describing proteins and other important biological macromolecules. Structural biologists use methods such as X-ray crystallography NMR spectroscopy and cryo-electron microscopy to determine the location of each atom relative to each other in the molecule. They then deposit this information which is then annotated and publicly released into the archive by the wwPDB. The constantly-growing PDB is a reflection of the research that is happening in laboratories across the world. This can make it both exciting and challenging to use the database in research and education. Structures are available for many of the proteins and nucleic acids involved in the central processes of life so you can go to the PDB archive to find structures for ribosomes oncogenes drug targets and even whole viruses. However it can be a challenge to find the information that you need since the PDB archives so many different structures. You will often find multiple structures for a given molecule or partial structures or structures that have been modified or inactivated from their native form. Content There are two data sets. Both are arranged on ""structureId"" of the protein  pdb_data_no_dups.csv - Protein data set deatils of classification extraction methods etc. Containing 141401 instances and       14 attributes. data_seq - Protein sequence information. Containing 467304 instances and 5 attributes.  ​ Acknowledgements Original data set down loaded from http//www.rcsb.org/pdb/ Inspiration Protein data base helped the life science community to study about different diseases and come with new drugs and solution that help the human survival.",CSV,,"[healthcare, biology, multiclass classification]",ODbL,,,167,1678,27,Protein Sequance Data set ,Protein Data Set,https://www.kaggle.com/shahir/protein-data-set,Sat Feb 03 2018
,Minat Verma,"[SYMBOL, SERIES, OPEN, HIGH, LOW, CLOSE, LAST, PREVCLOSE, TOTTRDQTY, TOTTRDVAL, TIMESTAMP, TOTALTRADES, ISIN]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, string]",Context The data is of National Stock Exchange of India. The data is compiled to felicitate Machine Learning without bothering much about Stock APIs. Content The data is of National Stock Exchange of India's stock listings for each trading day of 2016 and 2017. A brief description of columns. SYMBOL Symbol of the listed company.  SERIES Series of the equity. Values are [EQ BE BL BT GC and IL]  OPEN The opening market price of the equity symbol on the date.  HIGH The highest market price of the equity symbol on the date.  LOW The lowest recorded market price of the equity symbol on the date.  CLOSE The closing recorded price of the equity symbol on the date.  LAST The last traded price of the equity symbol on the date.  PREVCLOSE The previous day closing price of the equity symbol on the date.  TOTTRDQTY Total traded quantity of the equity symbol on the date.  TOTTRDVAL Total traded volume of the equity symbol on the date.  TIMESTAMP Date of record.  TOTALTRADES Total trades executed on the day.  ISIN International Securities Identification Number.  Acknowledgements All data is fetched from NSE official site.  https//www.nseindia.com/ Inspiration This dataset is compiled to felicitate Machine learning on Stocks.,CSV,,[strategy],CC0,,,565,3224,30,The data is of National Stock Exchange of India for 2016 and 2017,NSE Stocks Data,https://www.kaggle.com/minatverma/nse-stocks-data,Mon Jan 01 2018
,Jeremy Seibert,[],[],"Context This dataset includes the 2010-2014 ""Facility-Level"" emissions data combined with geographical & industry-related data.  It is based on the EPA's Toxic Release Inventory (TRI) & Greenhouse Gas Reporting Inventory (GHG) the national system of nomenclature that is used to describe the industry-related emissions. Although the EPA publishes and maintains the TRI & GHG report in various forms the combination of the two is not readily available. Hence this dataset.  Content The CSV has 28 columnar variables defined as   UniqueID Facility name Rank TRI '14 Rank GHG '14 Latitude Longitude Location address City State ZIP County FIPS code Primary NAICS Second primary NAICS Third primary NAICS Industry type Parent companies 2014 (GHG) Parent companies 2014 (TRI) TRI air emissions 14 (in pounds) TRI air emissions 13 [and previous years] GHG direct emissions 14 (in metric tons) GHG direct emissions 13 [and previous years] GHG Facility Id Second GHG Facility Id [and Third Fourth etc.] TRI Id Second TRI Id [and Third Fourth etc.] FRS Id Second FRS Id [and Third Fourth etc.]  Acknowledgements This dataset was made available by the Center for Public Integrity. ",CSV,,"[united states, pollution, demographics]",CC0,,,229,1772,6,EPA Toxic & Greenhouse Gas Emissions Data,US Facility-Level Air Pollution (2010-2014),https://www.kaggle.com/jaseibert/us-facilitylevel-air-pollution-20102014,Thu Nov 23 2017
,szrlee,"[Date, Open, High, Low, Close, Volume, Name]","[dateTime, numeric, numeric, numeric, numeric, numeric, string]",Context The script used to acquire all of the following data can be found in this GitHub repository. This repository also contains the modeling codes and will be updated continually so welcome starring or watching! Stock market data can be interesting to analyze and as a further incentive strong predictive models can have large financial payoff. The amount of financial data on the web is seemingly endless. A large and well structured dataset on a wide array of companies can be hard to come by. Here provided a dataset with historical stock prices (last 12 years) for 29 of 30 DJIA companies (excluding 'V' because it does not have the whole 12 years data).       ['MMM' 'AXP' 'AAPL' 'BA' 'CAT' 'CVX' 'CSCO' 'KO' 'DIS' 'XOM' 'GE'        'GS' 'HD' 'IBM' 'INTC' 'JNJ' 'JPM' 'MCD' 'MRK' 'MSFT' 'NKE' 'PFE'        'PG' 'TRV' 'UTX' 'UNH' 'VZ' 'WMT' 'GOOGL' 'AMZN' 'AABA']  In the future if you wish for a more up to date dataset this can be used to acquire new versions of the .csv files. Content The data is presented in a couple of formats to suit different individual's needs or computational limitations.  I have included files containing 13 years of stock data (in the all_stocks_2006-01-01_to_2018-01-01.csv and corresponding folder) and a smaller version of the dataset (all_stocks_2017-01-01_to_2018-01-01.csv) with only the past year's stock data for those wishing to use something more manageable in size. The folder individual_stocks_2006-01-01_to_2018-01-01 contains files of data for individual stocks labelled by their stock ticker name.  The all_stocks_2006-01-01_to_2018-01-01.csv and all_stocks_2017-01-01_to_2018-01-01.csv contain this same data presented in merged .csv files.  Depending on the intended use (graphing modelling etc.) the user may prefer one of these given formats. All the files have the following columns Date - in format yy-mm-dd  Open - price of the stock at market open (this is NYSE data so all in USD) High - Highest price reached in the day Low Close - Lowest price reached in the day Volume - Number of shares traded Name - the stock's ticker name Inspiration This dataset lends itself to a some very interesting visualizations. One can look at simple things like how prices change over time graph an compare multiple stocks at once or generate and graph new metrics from the data provided. From these data informative stock stats such as volatility and moving averages can be easily calculated. The million dollar question is can you develop a model that can beat the market and allow you to make statistically informed trades! Acknowledgement This Data description is adapted from the dataset named 'S&P 500 Stock data'. This data is scrapped from Google finance using the python library 'pandas_datareader'. Special thanks to Kaggle Github and the Market.,CSV,,"[stochastic processes, time series, finance]",CC0,,,424,2167,6,Historical stock data for DIJA 30 companies (2006-01-01 to 2018-01-01),DJIA 30 Stock Time Series,https://www.kaggle.com/szrlee/stock-time-series-20050101-to-20171231,Wed Jan 03 2018
,City of Chicago,"[Inspection ID, DBA Name, AKA Name, License #, Facility Type, Risk, Address, City, State, Zip, Inspection Date, Inspection Type, Results, Violations, Latitude, Longitude, Location]","[numeric, string, string, numeric, string, string, string, string, string, numeric, dateTime, string, string, string, numeric, numeric, string]",Context Restaurant inspections ensure that food served to the public at licensed food establishments follows food safety guidelines. The Food Protection Division of the Chicago Department of Public Health (CDPH) is committed to maintaining the safety of food bought sold or prepared for public consumption in Chicago by carrying out science-based inspections of all retail food establishments. These inspections promote public health in areas of food safety and sanitation and prevent the occurrence of food-borne illness. CDPH's licensed accredited sanitarians inspect retail food establishments such as restaurants grocery stores bakeries convenience stores hospitals nursing homes day care facilities shelters schools and temporary food service events. Inspections focus on food handling practices product temperatures personal hygiene facility maintenance and pest control. All restaurants are subject to certain recurring inspections. Each year a restaurant is subject to annual inspections to ensure continued compliance with City ordinances and regulations. In addition to recurring inspections restaurants may also be inspected in response to a complaint. Some of these recurring inspections such as the inspection by the Buildings Department will be scheduled while others will not. Generally inspections are conducted by the Health Department for sanitation and safe food handling practices the Buildings Department to ensure the safety of the structure and the Fire Department to ensure safe fire exits.The City's Dumpster Task Force a collaborative effort between the Health Department and Streets and Sanitation Department also inspects restaurants to ensure compliance with sanitation regulations. Content Data includes inspection date results violations noted business name and lat/lon license# and risk. Data covers 01/02/2013-08/28/17. Acknowledgements Data was collected by City of Chicago Department of Health. Inspiration  Can you predict restaurant closings?  Do restaurants in certain neighborhoods gather more/less violations? Any seasonal or time anomalies in the data? ,CSV,,[food and drink],CC0,,,315,3158,176,~154k Rows of Inspections Data,Chicago Restaurant Inspections,https://www.kaggle.com/chicago/chi-restaurant-inspections,Wed Aug 30 2017
,philipjames11,"[Vendor,  Category,  Item,  Item Description,  Price,  Origin,  Destination,  Rating,  Remarks]","[string, string, string, string, string, string, string, string, string]","Context This data set was made from an html rip made by reddit user ""usheep"" who threatened to expose all the vendors on Agora to the police if they did not meet his demands (sending him a small monetary amount ~few hundred dollars in exchange for him not leaking their info). Most information about what happened to ""usheep"" and his threats is nonexistent. He posted the html rip and was never heard from again. Agora shut down a few months after. It is unknown if this was related to ""usheep"" or not but the raw html data remained.  Content This is a data parse of marketplace data ripped from Agora (a dark/deep web) marketplace from the years 2014 to 2015. It contains drugs weapons books services and more. Duplicate listings have been removed and prices have been averaged of any duplicates. All of the data is in a csv file and has over 100000 unique listings.  It is organized by Vendor The seller Category Where in the marketplace the item falls under Item The title of the listing Description The description of the listing Price Cost of the item (averaged across any duplicate listings between 2014 and 2015) Origin Where the item is listed to have shipped from Destination Where the item is listed to be shipped to (blank means no information was provided but mostly likely worldwide. I did not enter worldwide for any blanks however as to not make assumptions) Rating The rating of the seller (a rating of [0 deals] or anything else with ""deals"" in it means there is not concrete rating as the amount of deals is too small for a rating to be displayed) Remarks Only remark options are blank or ""Average price may be skewed outliar > .5 BTC found"" which is pretty self explanatory. Acknowledgements Though I got this data from a 3rd party it seems as though it originally came from here https//www.gwern.net/DNM-archives Gwern Branwen seems to have complied all of his dark net marketplace leaks and html rips and has a multitude of possible uses for the data at the link above. It is free for anyone to use as long as proper credit is given to the creator. I would be happy to parse more data if anyone would like to request a specific website and/or format. Inspiration This data could be used to track drug dealers across different platforms. Potentially find correlations between different drugs and from where/to they ship in the world to show correlations between types of drugs and where drug dealers that supply them are located. Prices can estimate drug economies in certain regions of the world. Similar listings from 2 different vendors can perhaps point to competition to corner a market or even show that some vendors may work together to corner a market. There are quite a few opportunities to do some really great stuff to find correlations between illegal drugs weapons and more in order to curb the flow of dark net drug trade by identifying high risk regions or vendors. I can potentially do a new parse of other websites so you can find correlations across websites rather than just within Agora.",CSV,,"[crime, illegal drugs, internet]",CC0,,,383,3747,8,"Includes over 100,000 unique listings of drugs, weapons and more",Dark Net Marketplace Data (Agora 2014-2015),https://www.kaggle.com/philipjames11/dark-net-marketplace-drug-data-agora-20142015,Wed Dec 06 2017
,United Nations,[],[],The Economic Statistics Branch of the United Nations Statistics Division (UNSD) maintains and annually updates the National Accounts Official Country Data database. This work is carried out in accordance with the recommendation of the Statistical Commission at its first session that the Statistics Division of the United Nations should publish regularly the most recent available data on national accounts for as many countries and areas as possible. The database contains detailed official national accounts statistics in national currencies as provided by the National Statistical Offices.  Data are available for most of the countries or areas of the world and form a valuable source of information on their economies. The database contains data as far back as 1946 up to the year t-1 with data for most countries available from the 1970s. The database covers not only national accounts main aggregates such as gross domestic product national income saving value added by industry and household and government consumption expenditure and its relationships; but also detailed statistics for institutional sectors (including the rest of the world) comprising the production account the generation of income account the allocation of primary income account the secondary distribution of income account the use of disposable income account the capital account and the financial account if they are compiled by countries.  The statistics for each country or area are presented according to the uniform table headings and classifications as recommended in the United Nations System of National Accounts 1993 (1993 SNA). A summary of the 1993 SNA conceptual framework classifications and definitions are included in the yearly publication “National Accounts Statistics Main Aggregates and Detailed Tables”. Acknowledgements This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,[economics],Other,,,104,918,34,Global GDP & Government Expenditures since 1946,National Accounts,https://www.kaggle.com/unitednations/national-accounts,Fri Nov 17 2017
,Olga Belitskaya,"[letter, label, file, background]","[string, numeric, string, numeric]","History I made the database from my own photos of Russian lowercase letters written by hand.  Content The GitHub repository with examples Handwritten Letters on GitHub The main dataset (letters3.zip)  6600 (200x33) color images (32x32x3) with 33 letters and the file with labels letters3.txt.  Photo files are in the .png format and the labels are integers and values. Additional letters3.csv file. The file LetterColorImages3.h5 consists of preprocessing images of this set image tensors and targets (labels)  The data can be combined with the database ""Classification of Handwritten Letters""  Letter Symbols => Letter Labels  а=>1 б=>2 в=>3 г=>4 д=>5 е=>6 ё=>7 ж=>8 з=>9 и=>10 й=>11 к=>12 л=>13 м=>14 н=>15 о=>16 п=>17 р=>18 с=>19 т=>20 у=>21 ф=>22 х=>23 ц=>24 ч=>25 ш=>26 щ=>27 ъ=>28 ы=>29 ь=>30 э=>31 ю=>32 я=>33  Background Images => Background Labels  striped=>0 gridded=>1 background=>2 graph paper=>3 Acknowledgements As an owner of this database I have published it for absolutely free using by any site visitor. Usage Classification image generation etc. in a case of handwritten letters with a small number of images are useful exercises. Improvement There are lots of ways for increasing this set and the machine learning algorithms applying to it. For example add the same images but written by other person or add capital letters. ",Other,,"[languages, photography, classification, deep learning]",Other,,,87,1056,58,Images of Russian Letters,Handwritten Letters 2,https://www.kaggle.com/olgabelitskaya/handwritten-letters-2,Sun Dec 31 2017
,Mario,[],[],Content The dataset consists of two files training and validation. Each folder contains 10 subforders labeled as n0~n9 each corresponding a species form Wikipedia's monkey cladogram. Images are 400x300 px or larger and JPEG format (almost 1400 images).  Images were downloaded with help of the googliser open source code. Label mapping > Label Latin Nama  > n0 alouatta_palliata  > n1 erythrocebus_patas  > n2 cacajao_calvus  > n3 macaca_fuscata  > n4 cebuella_pygmea  > n5 cebus_capucinus  > n6 mico_argentatus  > n7 saimiri_sciureus  > n8 aotus_nigriceps  > n9 trachypithecus_johnii         For more information on the monkey species and number of images per class make sure to check monkey_labels.txt file.   Aim This dataset is intended as a test case for fine-grain classification tasks perhaps best  used in combination with transfer learning. Hopefully someone can help us expand the number of classes or number of images.  Acknowledgements Thanks to Romain Renard for his help with the code implementation. Also thanks to Gustavo Montoya Jacky Zhang and Sofia Loaiciga  for their help with the dataset curation. Notes Some demo code for usage of the dataset in combination with Keras can be found in this repo.,Other,,"[animals, image data]",CC0,,,271,1687,547,Image dataset for fine-grain classification ,10 Monkey Species,https://www.kaggle.com/slothkong/10-monkey-species,Sun Jan 21 2018
,City of New York,"[Unique Key, Created Date, Closed Date, Agency, Agency Name, Complaint Type, Descriptor, Location Type, Incident Zip, Incident Address, Street Name, Cross Street 1, Cross Street 2, Intersection Street 1, Intersection Street 2, Address Type, City, Landmark, Facility Type, Status, Due Date, Resolution Action Updated Date, Community Board, Borough, X Coordinate (State Plane), Y Coordinate (State Plane), Park Facility Name, Park Borough, School Name, School Number, School Region, School Code, School Phone Number, School Address, School City, School State, School Zip, School Not Found, School or Citywide Complaint, Vehicle Type, Taxi Company Borough, Taxi Pick Up Location, Bridge Highway Name, Bridge Highway Direction, Road Ramp, Bridge Highway Segment, Garage Lot Name, Ferry Direction, Ferry Terminal Name, Latitude, Longitude, Location]","[numeric, dateTime, dateTime, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, dateTime, dateTime, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string]",Context Rats in New York City are prevalent as in many densely populated areas. For a long time the exact number of rats in New York City was unknown and a common urban legend was that there were up to four times as many rats as people. In 2014 however scientists more accurately measured the entire city's rat population to be approximately only 25% of the number of humans; i.e. there were approximately 2 million rats to New York's 8.4 million people at the time of the study.[1][2] Content New York City rodent complaints can be made online or by dialing 3-1-1 and the New York City guide Preventing Rats on Your Property discusses how the New York City Health Department inspects private and public properties for rats. Property owners that fail inspections receive a Commissioner's Order and have five days to correct the problem. If after five days the property fails a second inspection the owner receives a Notice of Violation and can be fined. The property owner is billed for any clean-up or extermination carried out by the Health Department. Data is from 2010-Sept 16th 2017 and includes date location (lat/lon) type of structure borough and community board. Acknowledgements Data was produced by the City of New York via their 311 portal. Inspiration  Where and when are rats most seen? Can you predict rat sightings from previous data? Are there any trends in rat sightings? ,CSV,,"[government agencies, animals, government]",CC0,,,269,2187,52,~102k Observations Around New York,NYC Rat Sightings,https://www.kaggle.com/new-york-city/nyc-rat-sightings,Mon Sep 18 2017
,Mihai Oltean,[],[],"Context A high-quality dataset of images containing fruits. The following fruits are included  Apples - (different varieties Golden Golden-Red Granny Smith Red Red Delicious) Apricot Avocado Avocado ripe Banana (Yellow Red) Cactus fruit Carambula Cherry Clementine Cocos Dates Granadilla Grape (Pink White White2) Grapefruit (Pink White) Guava Huckleberry Kiwi Kaki Kumsquats Lemon (normal Meyer) Lime Litchi Mandarine Mango Maracuja Nectarine Orange Papaya Passion fruit Peach Pepino Pear (different varieties Abate Monster Williams) Pineapple Pitahaya Red Plum Pomegranate Quince Raspberry Salak Strawberry Tamarillo Tangelo. Dataset properties Training set size 28736 images. Validation set size 9673 images. Number of classes 60 (fruits). Image size 100x100 pixels. Filename format image_index_100.jpg (e.g. 32_100.jpg) or r_image_index_100.jpg (e.g. r_32_100.jpg). ""r"" stands for rotated fruit. ""100"" comes from image size (100x100 pixels). Different varieties of the same fruit (apple for instance) are shown having different labels. Content Fruits were planted in the shaft of a low speed motor (3 rpm) and a short movie of 20 seconds was recorded.  A Logitech C920 camera was used for filming the fruits. This is one of the best webcams available. Behind the fruits we placed a white sheet of paper as background.  However due to the variations in the lighting conditions the background was not uniform and we wrote a dedicated algorithm which extract the fruit from the background. This algorithm is of flood fill type  we start from each edge of the image and we mark all pixels there then we mark all pixels found in the neighborhood of the already marked pixels for which the distance between colors is less than a prescribed value. We repeat the previous step until no more pixels can be marked. All marked pixels are considered as being background (which is then filled with white) and the rest of pixels are considered as belonging to the object. The maximum value for the distance between 2 neighbor pixels is a parameter of the algorithm and is set (by trial and error) for each movie. How to cite Horea Muresan Mihai Oltean Fruit recognition from images using deep learning Technical Report Babes-Bolyai University 2017 Alternate download This dataset is also available for download from GitHub Fruits-360 dataset History Fruits were filmed at the dates given below 2017.02.25 - Apple (golden). 2017.02.28 - Apple (red-yellow red golden2) Kiwi Pear Grapefruit Lemon Orange Strawberry Banana. 2017.03.05 - Apple (golden3 Braeburn Granny Smith red2). 2017.03.07 - Apple (red3). 2017.05.10 - Plum Peach Peach flat Apricot Nectarine Pomegranate. 2017.05.27 - Avocado Papaya Grape Cherrie. 2017.12.25 - Carambula Cactus fruit Granadilla Kaki Kumsquats Passion fruit Avocado ripe Quince. 2017.12.28 - Clementine Cocos Mango Lime Litchi. 2017.12.31 - Apple Red Delicious Pear Monster Grape White. 2018.01.14 - Ananas Grapefruit Pink Mandarine Pineapple Tangelo. 2018.01.19 - Huckleberry Raspberry. 2018.01.26 - Dates Maracuja Salak Tamarillo. 2018.02.05 - Guava Grape White 2 Lemon Meyer 2018.02.07 - Banana Red Pepino Pitahaya Red. 2018.02.08 - Pear Abate Pear Williams.",Other,,"[food and drink, image data, multiclass classification]",CC4,,,1087,7341,180,A dataset with 60 fruits and 38409 images,Fruits 360 dataset,https://www.kaggle.com/moltean/fruits,Thu Feb 08 2018
,START Consortium,"[eventid, iyear, imonth, iday, approxdate, extended, resolution, country, country_txt, region, region_txt, provstate, city, latitude, longitude, specificity, vicinity, location, summary, crit1, crit2, crit3, doubtterr, alternative, alternative_txt, multiple, success, suicide, attacktype1, attacktype1_txt, attacktype2, attacktype2_txt, attacktype3, attacktype3_txt, targtype1, targtype1_txt, targsubtype1, targsubtype1_txt, corp1, target1, natlty1, natlty1_txt, targtype2, targtype2_txt, targsubtype2, targsubtype2_txt, corp2, target2, natlty2, natlty2_txt, targtype3, targtype3_txt, targsubtype3, targsubtype3_txt, corp3, target3, natlty3, natlty3_txt, gname, gsubname, gname2, gsubname2, gname3, gsubname3, motive, guncertain1, guncertain2, guncertain3, individual, nperps, nperpcap, claimed, claimmode, claimmode_txt, claim2, claimmode2, claimmode2_txt, claim3, claimmode3, claimmode3_txt, compclaim, weaptype1, weaptype1_txt, weapsubtype1, weapsubtype1_txt, weaptype2, weaptype2_txt, weapsubtype2, weapsubtype2_txt, weaptype3, weaptype3_txt, weapsubtype3, weapsubtype3_txt, weaptype4, weaptype4_txt, weapsubtype4, weapsubtype4_txt, weapdetail, nkill, nkillus]","[numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric]","Context Information on more than 170000 Terrorist Attacks The Global Terrorism Database (GTD) is an open-source database including information on terrorist attacks around the world from 1970 through 2016 (with annual updates planned for the future). The GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 170000 cases. The database is maintained by researchers at the National Consortium for the Study of Terrorism and Responses to Terrorism (START) headquartered at the University of Maryland. More Information Content Geography Worldwide Time period 1970-2016 except 1993 (2017 in progress publication expected June 2018) Unit of analysis Attack Variables >100 variables on location tactics perpetrators targets and outcomes Sources Unclassified media articles (Note Please interpret changes over time with caution. Global patterns are driven by diverse trends in particular regions and data collection is influenced by fluctuations in access to media coverage over both time and place.) Definition of terrorism ""The threatened or actual use of illegal force and violence by a non-state actor to attain a political economic religious or social goal through fear coercion or intimidation."" See the GTD Codebook for important details on data collection methodology definitions and coding schema. Acknowledgements The Global Terrorism Database is funded through START by the US Department of State (Contract Number SAQMMA12M1292) and the US Department of Homeland Security Science and Technology Directorate’s Office of University Programs (Award Number 2012-ST-061-CS0001 CSTAB 3.1). The coding decisions and classifications contained in the database are determined independently by START researchers and should not be interpreted as necessarily representing the official views or policies of the United States Government. GTD Team Publications The GTD has been leveraged extensively in scholarly publications reports and media articles. Putting Terrorism in Context Lessons from the Global Terrorism Database by GTD principal investigators LaFree Dugan and Miller investigates patterns of terrorism and provides perspective on the challenges of data collection and analysis. The GTD's data collection manager Michael Jensen discusses important Benefits and Drawbacks of Methodological Advancements in Data Collection and Coding. Terms of Use Use of the data signifies your agreement to the following terms and conditions. Definitions Within this section ""GTD"" will refer to the Global Terrorism Database produced by the National Consortium for the Study of Terrorism and Responses to Terrorism. This includes the data and codebook any auxiliary materials present and the World Wide Web interface by which the data are presented. ""START"" will refer to the National Consortium for the Study of Terrorism and Responses to Terrorism a United States Department of Homeland Security Center of Excellence based at the University of Maryland. ""USER"" denotes the individual or set of individuals who access the GTD i.e. the data codebook any auxiliary materials and the World Wide Web interface by which the data are presented. ""GTD representatives"" denotes any senior management staff of START and any employee or representative of said organization whom senior management staff designate to represent START in dealings with the USER. Usage Rights Pursuant to this agreement START grants the USER the non-exclusive non-guaranteed right to search browse and view all contents of the GTD World Wide Web interface. Authorship All contents of the GTD were assembled by representatives of START and do not purport to reflect the official position or data collections of the Department of Homeland Security or any other agency of the United States government. Acknowledgement All information sourced from the GTD should be acknowledged by the USER and cited as follows ""National Consortium for the Study of Terrorism and Responses to Terrorism (START). (2017). Global Terrorism Database [Data file]. Retrieved from https//www.kaggle.com/START-UMD/gtd"" Unauthorized Publication of the Data No part of the GTD may be republished on any website or accessible for public download in any format without the express permission of a GTD staff member. In addition no part of the GTD may be distributed for any commercial purpose nor with the intent that the data be used in any commercial enterprise without the express permission of a GTD staff member. START reserves the right to withhold this permission. Penalties Penalties for failure to comply with the terms of this agreement may result in loss of access to the GTD and the forfeiture of user privileges in addition to any other appropriate legal remedies. Limitation of Liability Although every reasonable effort has been made to check sources and verify facts START cannot guarantee that accounts reported in the open literature are complete and accurate. START shall not be held liable for any loss or damage caused by errors or omissions or resulting from any use misuse or alteration of GTD data by the USER. The USER should not infer any additional actions or results beyond what is presented in a GTD entry and specifically the USER should not infer an individual associated with a particular incident was tried and convicted of terrorism or any other criminal offense. If new documentation about an event becomes available an entry may be modified as necessary and appropriate. Termination of Rights The GTD developers reserve the right to remove access to the GTD website from any particular IP address or set of IP addresses or to remove the database entirely from public access at their discretion. In such an event all USER rights granted in this document are terminated. Training START has released the first in a series of training modules designed to equip GTD users with the knowledge and tools to best leverage the database. This training module provides a general overview of the GTD including the data collection process uses of the GTD and patterns of global terrorism. Participants will learn basic data handling and how to generate summary statistics from the GTD using PivotTables in Microsoft Excel. Questions? Find answers to Frequently Asked Questions. Contact the GTD staff at gtd@start.umd.edu.",CSV,,"[crime, terrorism, international relations]",Other,,,26370,188226,144,"More than 170,000 terrorist attacks worldwide, 1970-2016",Global Terrorism Database,https://www.kaggle.com/START-UMD/gtd,Tue Jul 18 2017
,Kemical,"[ID , name , category , main_category , currency , deadline , goal , launched , pledged , state , backers , country , usd pledged , , , , ]","[numeric, string, string, string, string, dateTime, numeric, dateTime, numeric, string, numeric, string, numeric, string, string, string, numeric]",Context I'm a crowdfunding enthusiast and i'm watching kickstarter since its early days. Right now I just collect data and the only app i've made is this twitter bot which tweet any project reaching some milestone @bloomwatcher . I have a lot of other ideas but sadly not enough time to develop them... But I hope you can! Content You'll find most useful data for project analysis. Columns are self explanatory except  usd_pledged conversion in US dollars of the pledged column  (conversion done by kickstarter). usd pledge real conversion in US dollars of the pledged column (conversion from Fixer.io API). usd goal real conversion in US dollars of the goal column (conversion from Fixer.io API).  Acknowledgements Data are collected from Kickstarter Platform usd conversion (usd_pledged_real and usd_goal_real columns) were generated from convert ks pledges to usd script done by tonyplaysguitar  Inspiration I hope to see great projects and why not a model to predict if a project will be successful before it is released? ),CSV,,"[finance, crowdfunding]",CC4,,,2626,13960,37,"More than 300,000 kickstarter projects",Kickstarter projects,https://www.kaggle.com/kemical/kickstarter-projects,Thu Feb 08 2018
,UCI Machine Learning,"[fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, density, pH, sulphates, alcohol, quality]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context The two datasets are related to red and white variants of the Portuguese ""Vinho Verde"" wine. For more details consult the reference [Cortez et al. 2009]. Due to privacy and logistic issues only physicochemical (inputs) and sensory (the output) variables are available (e.g. there is no data about grape types wine brand wine selling price etc.).  These datasets can be viewed as classification or regression tasks. The classes are ordered and not balanced (e.g. there are much more normal wines than excellent or poor ones).   This dataset is also available from the UCI machine learning repository https//archive.ics.uci.edu/ml/datasets/wine+quality  I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so I will take this down if requested.) Content For more information read [Cortez et al. 2009]. Input variables (based on physicochemical tests) 1 - fixed acidity  2 - volatile acidity  3 - citric acid  4 - residual sugar  5 - chlorides  6 - free sulfur dioxide   7 - total sulfur dioxide  8 - density  9 - pH  10 - sulphates  11 - alcohol  Output variable (based on sensory data)  12 - quality (score between 0 and 10)  Tips What might be an interesting thing to do is aside from using regression modelling is to set an arbitrary cutoff for your dependent variable (wine quality) at e.g. 7 or higher getting classified as 'good/1' and the remainder as 'not good/0'. This allows you to practice with hyper parameter tuning on e.g. decision tree algorithms looking at the ROC curve and the AUC value. Without doing any kind of feature engineering or overfitting you should be able to get an AUC of .88 (without even using random forest algorithm) KNIME is a great tool (GUI) that can be used for this. 1 - File Reader (for csv) to linear correlation node and to interactive histogram for basic EDA. 2- File Reader to 'Rule Engine Node' to turn the 10 point scale to dichtome variable (good wine and rest) the code to put in the rule engine is something like this  -  $quality$ > 6.5 => ""good""  -  TRUE => ""bad""  3- Rule Engine Node output to input of Column Filter node to filter out your original 10point feature (this prevent leaking) 4- Column Filter Node output to input of Partitioning Node (your standard train/tes split e.g. 75%/25% choose 'random' or 'stratified') 5- Partitioning Node train data split output to input of Train data split to input Decision Tree Learner node and  6- Partitioning Node test data split output to input Decision Tree predictor Node 7- Decision Tree learner Node output to input Decision Tree Node input 8- Decision Tree output to input ROC Node.. (here you can evaluate your model base on AUC value) Inspiration Use machine learning to determine which physiochemical properties make a wine 'good'! Acknowledgements This dataset is also available from the UCI machine learning repository https//archive.ics.uci.edu/ml/datasets/wine+quality  I just shared it to kaggle for convenience. (I am mistaken and the public license type disallowed me from doing so I will take this down at first request. I am not the owner of this dataset. Please include this citation if you plan to use this database  P. Cortez A. Cerdeira F. Almeida T. Matos and J. Reis.  Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems Elsevier 47(4)547-553 2009. Relevant publication P. Cortez A. Cerdeira F. Almeida T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties.  In Decision Support Systems Elsevier 47(4)547-553 2009. ",CSV,,"[food and drink, beginner, regression analysis]",ODbL,,,3413,16029,0.0966796875,Simple and clean practice dataset for regression or classification modelling,Red Wine Quality,https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009,Tue Nov 28 2017
,Rachael Tatman,[],[],Context This data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to support the national Fire Program Analysis (FPA) system. The wildfire records were acquired from the reporting systems of federal state and local fire organizations. The following core data elements were required for records to be included in this data publication discovery date final fire size and a point location at least as precise as Public Land Survey System (PLSS) section (1-square mile grid). The data were transformed to conform when possible to the data standards of the National Wildfire Coordinating Group (NWCG). Basic error-checking was performed and redundant records were identified and removed to the degree possible. The resulting product referred to as the Fire Program Analysis fire-occurrence database (FPA FOD) includes 1.88 million geo-referenced wildfire records representing a total of 140 million acres burned during the 24-year period. Content This dataset is an SQLite database that contains the following information  Fires Table including wildfire data for the period of 1992-2015 compiled from US federal state and local reporting systems. FOD_ID = Global unique identifier. FPA_ID = Unique identifier that contains information necessary to track back to the original record in the source dataset. SOURCE_SYSTEM_TYPE = Type of source database or system that the record was drawn from (federal nonfederal or interagency). SOURCE_SYSTEM = Name of or other identifier for source database or system that the record was drawn from. See Table 1 in Short (2014) or \Supplements\FPA_FOD_source_list.pdf for a list of sources and their identifier. NWCG_REPORTING_AGENCY = Active National Wildlife Coordinating Group (NWCG) Unit Identifier for the agency preparing the fire report (BIA = Bureau of Indian Affairs BLM = Bureau of Land Management BOR = Bureau of Reclamation DOD = Department of Defense DOE = Department of Energy FS = Forest Service FWS = Fish and Wildlife Service IA = Interagency Organization NPS = National Park Service ST/C&L = State County or Local Organization and TRIBE = Tribal Organization). NWCG_REPORTING_UNIT_ID = Active NWCG Unit Identifier for the unit preparing the fire report. NWCG_REPORTING_UNIT_NAME = Active NWCG Unit Name for the unit preparing the fire report. SOURCE_REPORTING_UNIT = Code for the agency unit preparing the fire report based on code/name in the source dataset. SOURCE_REPORTING_UNIT_NAME = Name of reporting agency unit preparing the fire report based on code/name in the source dataset. LOCAL_FIRE_REPORT_ID = Number or code that uniquely identifies an incident report for a particular reporting unit and a particular calendar year. LOCAL_INCIDENT_ID = Number or code that uniquely identifies an incident for a particular local fire management organization within a particular calendar year. FIRE_CODE = Code used within the interagency wildland fire community to track and compile cost information for emergency fire suppression (https//www.firecode.gov/). FIRE_NAME = Name of the incident from the fire report (primary) or ICS-209 report (secondary). ICS_209_INCIDENT_NUMBER = Incident (event) identifier from the ICS-209 report. ICS_209_NAME = Name of the incident from the ICS-209 report. MTBS_ID = Incident identifier from the MTBS perimeter dataset. MTBS_FIRE_NAME = Name of the incident from the MTBS perimeter dataset. COMPLEX_NAME = Name of the complex under which the fire was ultimately managed when discernible. FIRE_YEAR = Calendar year in which the fire was discovered or confirmed to exist. DISCOVERY_DATE = Date on which the fire was discovered or confirmed to exist. DISCOVERY_DOY = Day of year on which the fire was discovered or confirmed to exist. DISCOVERY_TIME = Time of day that the fire was discovered or confirmed to exist. STAT_CAUSE_CODE = Code for the (statistical) cause of the fire. STAT_CAUSE_DESCR = Description of the (statistical) cause of the fire. CONT_DATE = Date on which the fire was declared contained or otherwise controlled (mm/dd/yyyy where mm=month dd=day and yyyy=year). CONT_DOY = Day of year on which the fire was declared contained or otherwise controlled. CONT_TIME = Time of day that the fire was declared contained or otherwise controlled (hhmm where hh=hour mm=minutes). FIRE_SIZE = Estimate of acres within the final perimeter of the fire. FIRE_SIZE_CLASS = Code for fire size based on the number of acres within the final fire perimeter expenditures (A=greater than 0 but less than or equal to 0.25 acres B=0.26-9.9 acres C=10.0-99.9 acres D=100-299 acres E=300 to 999 acres F=1000 to 4999 acres and G=5000+ acres). LATITUDE = Latitude (NAD83) for point location of the fire (decimal degrees). LONGITUDE = Longitude (NAD83) for point location of the fire (decimal degrees). OWNER_CODE = Code for primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident. OWNER_DESCR = Name of primary owner or entity responsible for managing the land at the point of origin of the fire at the time of the incident. STATE = Two-letter alphabetic code for the state in which the fire burned (or originated) based on the nominal designation in the fire report. COUNTY = County or equivalent in which the fire burned (or originated) based on nominal designation in the fire report. FIPS_CODE = Three-digit code from the Federal Information Process Standards (FIPS) publication 6-4 for representation of counties and equivalent entities. FIPS_NAME = County name from the FIPS publication 6-4 for representation of counties and equivalent entities. NWCG_UnitIDActive_20170109 Look-up table containing all NWCG identifiers for agency units that were active (i.e. valid) as of 9 January 2017 when the list was downloaded from https//www.nifc.blm.gov/unit_id/Publish.html and used as the source of values available to populate the following fields in the Fires table NWCG_REPORTING_AGENCY NWCG_REPORTING_UNIT_ID and NWCG_REPORTING_UNIT_NAME. UnitId = NWCG Unit ID. GeographicArea = Two-letter code for the geographic area in which the unit is located (NA=National IN=International AK=Alaska CA=California EA=Eastern Area GB=Great Basin NR=Northern Rockies NW=Northwest RM=Rocky Mountain SA=Southern Area and SW=Southwest). Gacc = Seven or eight-letter code for the Geographic Area Coordination Center in which the unit is located or primarily affiliated with (CAMBCIFC=Canadian Interagency Forest Fire Centre USAKCC=Alaska Interagency Coordination Center USCAONCC=Northern California Area Coordination Center USCAOSCC=Southern California Coordination Center USCORMCC=Rocky Mountain Area Coordination Center USGASAC=Southern Area Coordination Center USIDNIC=National Interagency Coordination Center USMTNRC=Northern Rockies Coordination Center USNMSWC=Southwest Area Coordination Center USORNWC=Northwest Area Coordination Center USUTGBC=Western Great Basin Coordination Center USWIEACC=Eastern Area Coordination Center). WildlandRole = Role of the unit within the wildland fire community. UnitType = Type of unit (e.g. federal state local). Department = Department (or state/territory) to which the unit belongs (AK=Alaska AL=Alabama AR=Arkansas AZ=Arizona CA=California CO=Colorado CT=Connecticut DE=Delaware DHS=Department of Homeland Security DOC= Department of Commerce DOD=Department of Defense DOE=Department of Energy DOI= Department of Interior DOL=Department of Labor FL=Florida GA=Georgia IA=Iowa IA/GC=Non-Departmental Agencies ID=Idaho IL=Illinois IN=Indiana KS=Kansas KY=Kentucky LA=Louisiana MA=Massachusetts MD=Maryland ME=Maine MI=Michigan MN=Minnesota MO=Missouri MS=Mississippi MT=Montana NC=North Carolina NE=Nebraska NG=Non-Government NH=New Hampshire NJ=New Jersey NM=New Mexico NV=Nevada NY=New York OH=Ohio OK=Oklahoma OR=Oregon PA=Pennsylvania PR=Puerto Rico RI=Rhode Island SC=South Carolina SD=South Dakota ST/L=State or Local Government TN=Tennessee Tribe=Tribe TX=Texas USDA=Department of Agriculture UT=Utah VA=Virginia VI=U. S. Virgin Islands VT=Vermont WA=Washington WI=Wisconsin WV=West Virginia WY=Wyoming). Agency = Agency or bureau to which the unit belongs (AG=Air Guard ANC=Alaska Native Corporation BIA=Bureau of Indian Affairs BLM=Bureau of Land Management BOEM=Bureau of Ocean Energy Management BOR=Bureau of Reclamation BSEE=Bureau of Safety and Environmental Enforcement C&L=County & Local CDF=California Department of Forestry & Fire Protection DC=Department of Corrections DFE=Division of Forest Environment DFF=Division of Forestry Fire & State Lands DFL=Division of Forests and Land DFR=Division of Forest Resources DL=Department of Lands DNR=Department of Natural Resources DNRC=Department of Natural Resources and Conservation DNRF=Department of Natural Resources Forest Service DOA=Department of Agriculture DOC=Department of Conservation DOE=Department of Energy DOF=Department of Forestry DVF=Division of Forestry DWF=Division of Wildland Fire EPA=Environmental Protection Agency FC=Forestry Commission FEMA=Federal Emergency Management Agency FFC=Bureau of Forest Fire Control FFP=Forest Fire Protection FFS=Forest Fire Service FR=Forest Rangers FS=Forest Service FWS=Fish & Wildlife Service HQ=Headquarters JC=Job Corps NBC=National Business Center NG=National Guard NNSA=National Nuclear Security Administration NPS=National Park Service NWS=National Weather Service OES=Office of Emergency Services PRI=Private SF=State Forestry SFS=State Forest Service SP=State Parks TNC=The Nature Conservancy USA=United States Army USACE=United States Army Corps of Engineers USAF=United States Air Force USGS=United States Geological Survey USN=United States Navy). Parent = Agency subgroup to which the unit belongs (A concatenation of State and Unit from this report - https//www.nifc.blm.gov/unit_id/publish/UnitIdReport.rtf). Country = Country in which the unit is located (e.g. US = United States). State = Two-letter code for the state in which the unit is located (or primarily affiliated). Code = Unit code (follows state code to create UnitId). Name = Unit name.  Acknowledgements These data were collected using funding from the U.S. Government and can be used without additional permissions or fees. If you use these data in a publication presentation or other research product please use the following citation Short Karen C. 2017. Spatial wildfire occurrence data for the United States 1992-2015 [FPA_FOD_20170508]. 4th Edition. Fort Collins CO Forest Service Research Data Archive. https//doi.org/10.2737/RDS-2013-0009.4 Inspiration  Have wildfires become more or less frequent over time? What counties are the most and least fire-prone? Given the size location and date can you predict the cause of a fire wildfire? ,SQLite,,"[climate, firefighting]",CC0,,,1597,13853,759,24 years of geo-referenced wildfire records,1.88 Million US Wildfires,https://www.kaggle.com/rtatman/188-million-us-wildfires,Thu Sep 14 2017
,United Nations,"[oid, country_or_area, year, description, magnitude, value, category]","[string, string, numeric, string, string, numeric, string]",International Financial Statistics (IFS) is a standard source of international statistics on all aspects of international and domestic finance. It reports for most countries of the world current data needed in the analysis of problems of international payments and of inflation and deflation i.e. data on exchange rates international liquidity international banking money and banking interest rates prices production international transactions government accounts and national accounts. Last update in UNdata 14 May 2010 If you need more current data the IMF has made their current database available for bulk download for personal use. Acknowledgements This dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,[economics],Other,,,505,3870,7,Global indicators from 1960-2010,International Financial Statistics,https://www.kaggle.com/unitednations/international-financial-statistics,Wed Nov 15 2017
,Alan Du,[],[],"Context In Daily Fantasy Sports (DFS) contests contestants construct a virtual lineup of players that score points based on their real-world performances. Unlike in season-long Fantasy Sports contestsin DFS contestants submit a new lineup for each set of games. DFS contests are held for several professional sports leagues including the National Football League (NFL) National Basketball League (NBA) and National Hockey League (NHL). The leading DFS sites today are DraftKings and Fanduel which control approximately 90% of the $3B DFS market. There are three primary types of DFS games Head-to-Heads (H2Hs) Double-Ups and Guaranteed Prize Pools (GPPs). In H2H games two contestants play for a single cash prize. In Double-Up games a pool of contestants compete to place in the top 50% of lineups which are awarded twice the entry fee. In GPPs a pool of contestants compete for a fixed prize structure that tends to be very top heavy; some contests payout hundreds of thousands of dollars to the top finisher. Over the last year I have developed a winning system for daily fantasy football and baseball contests. Building this system from scratch was a fantastic compliment to the things I learned as a student from machine learning and optimization to optimal learning and game theory. I hope others can join me in researching daily fantasy basketball and perhaps get involved with the burgeoning world of daily fantasy sports. Content This dataset contains 20 days of DraftKings NBA contest data scraped between 2017-11-27 and 2017-12-28. For DraftKings NBA daily fantasy basketball contest rules see https//www.draftkings.com/help/rules/nba. Format  One folder per day One folder per contest for a given day Salary file (“DKSalaries.csv”) payout structure file (“payout_structure.csv”) and contest results file (“contest-standings.csv”) for a given contest. Column headers in each files are pretty self-explanatory. Some additional files (e.g. “players.csv” “covariance_mat_unfiltered.csv” “hist_fpts_mat.csv”) for a given contest. These files were for my personal research feel free to use or ignore. “projections” folder contains projections data for each player from rotogrinders and daily fantasy nerd labeled by date. “contests.csv” contains information about each contest e.g. entry fee slate and contest size.  Acknowledgements Thank you to my friend from college Michael Chiang for contributions to this project. Inspiration A few ideas to get started  What kind of position ""stacks"" tend to maximize correlation within a lineup? How can you minimize correlation between lineups such that you maximize your chances of winning a GPP? What are the tendencies of some of the top DFS pros? Can you improve rotogrinders and daily fantasy nerd player projections? Can you predict which players are undervalued (i.e. high fantasy points / salary ratio)? Can you predict the ownership percentage for each player in a given contest? ",Other,,"[basketball, covariance and correlation, decision theory]",CC0,,,134,1215,124,Data for DraftKings NBA Daily Fantasy Basketball Contests,Daily Fantasy Basketball - DraftKings NBA,https://www.kaggle.com/alandu20/daily-fantasy-basketball-draftkings,Sat Dec 30 2017
,Mohammad Kachuee,[],[],Abstract This dataset provides a collection of vital signals and reference blood pressure values acquired from 26 subjects that can be used for the purpose of non-invasive cuff-less blood pressure estimation. Source Creators Amirhossein Esmaili Mohammad Kachuee Mahdi Shabany Department of Electrical Engineering Sharif University of Technology Tehran Iran Date October 2017 Relevant Information This dataset is to be used for research methods trying to estimate blood pressure in a cuff-less non-invasive manner. For each subject in this dataset phonocardiogram (PCG) electrocardiogram (ECG) and photoplethysmogram (PPG) signals are acquired. Alongside the acquisition of the signals per subject a number of reference BPs are measured. Here a signal from a force-sensing resistor (FSR) placed under the cuff of the BP reference device is used to distinguish exact moments of reference BP measurements which are corresponding to the inflation and deflation of the cuff. The signal from FSR is also included in our dataset. For each subject age weight and height are also recorded. Attribute Information In the dataset corresponding to each subject there is a “.json” file. In each file we have the following attributes “UID” Subject number “age” Age of the subject “weight” Weight of the subject (Kg) “height” Height of the subject (cm) “data_PCG” Acquired PCG signal from chest (Fs = 1 KHz) “data_ECG” Acquired ECG signal (Fs = 1 KHz) “data_PPG” Acquired PPG signal from fingertip (Fs = 1 KHz) “data_FSR” Acquired FSR signal (Fs = 1 KHz) “data_BP” reference systolic blood pressure (SBP) and diastolic blood pressure (DBP) values acquired from the subjects. To distinguish the time instances in the signals in which SBP and DBP values are measured FSR signal can be used. For more details please refer to our paper. Please note that the FSR signal should be inverted prior to any analysis to reflect the instantaneous cuff pressure. Also the moment of each reference device measurement is approximately the time at which the cuff pressure drops with a considerable negative slope. Relevant Papers A. Esmaili M. Kachuee M. Shabany Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time IEEE Transactions on Instrumentation and Measurement 2017. A. Esmaili M. Kachuee M. Shabany Non-invasive Blood Pressure Estimation Using Phonocardiogram IEEE International Symposium on Circuits and Systems (ISCAS'17) 2017. Citation Request A. Esmaili M. Kachuee M. Shabany Nonlinear Cuffless Blood Pressure Estimation of Healthy Subjects Using Pulse Transit Time and Arrival Time IEEE Transactions on Instrumentation and Measurement 2017.,Other,,"[healthcare, health sciences]",Other,,,313,3679,186,Vital signals and reference blood pressure values acquired from 26 subjects.,Non-invasive Blood Pressure Estimation,https://www.kaggle.com/mkachuee/noninvasivebp,Mon Oct 16 2017
,Zeeshan-ul-hassan Usmani,[# This data file generated by 23andMe at: Wed Jan 25 11:45:20 2017],[string],Context Zeeshan-ul-hassan Usmani’s Genome Phenotype SNPs Raw Data Genomics is a branch of molecular biology that involves structure function variation evolution and mapping of genomes. There are several companies offering next generation sequencing of human genomes from complete 3 billion base-pairs to a few thousand Phenotype SNPs. I’ve used 23andMe (using Illumina HumanOmniExpress-24) for my DNA’s Phenotype SNPs. I am sharing the entire raw dataset here for the international research community for following reasons  I am a firm believer in open dataset transparency and the right to learn research explores and educate. I do not want to restrict the knowledge flow for mere privacy concerns. Hence I am offering my entire DNA raw data for the world to use for research without worrying about privacy. I call it copyleft dataset.  Most of available test datasets for research come from western world and we don’t see much from under-developing countries. I thought to share my data to bridge the gap and I expect others to follow the trend.  I would be the happiest man on earth if a life can be saved knowledge can be learned an idea can be explore or a fact can be found using my DNA data. Please use it the way you will  Content Name Zeeshan-ul-hassan Usmani Age 38 Years Country of Birth Pakistan Country of Ancestors  India (Utter Pradesh - UP) File GenomeZeeshanUsmani.csv Size 15 MB Sources 23andMe Personalized Genome Report The research community is still progressively working in this domain and it is agreed upon by professionals that genomics is still in its infancy. You now have the chance to explore this novel domain via the dataset and become one of the few genomics early adopters. The data-set is a complete genome extracted from www.23andme.com and is represented as a sequence of SNPs represented by the following symbols A (adenine) C (cytosine) G (guanine) T (thymine) D (base deletions) I (base insertions) and '_' or '-' if the SNP for particular location is not accessible. It contains Chromosomes 1-22 X Y and mitochondrial DNA. A complete list of the exact SNPs (base pairs) available and their data-set index can be found at  https//api.23andme.com/res/txt/snps.b4e00fe1db50.data For more information about how the data-set was extracted follow https//api.23andme.com/docs/reference/#genomes  Moreover for a more detailed understanding of the data-set content please acquaint yourself with the description of https//api.23andme.com/docs/reference/#genotypes Acknowledgements Users are allowed to use copy distribute and cite the dataset as follows “Zeeshan-ul-hassan Usmani Genome Phenotype SNPS Raw Data File by 23andMe Kaggle Dataset Repository Jan 25 2017.” Useful Links You may use the following human genome database sites for help  GenBank - https//www.ncbi.nlm.nih.gov/genbank/ The Human Genome Project - https//www.genome.gov/hgp/ Genomes OnLine Database (GOLD) - https//gold.jgi.doe.gov Complete Genomics - http//www.completegenomics.com/public-data/  Inspiration Some ideas worth exploring  Is the individual in question more susceptible to cancer? Does he tend to gain weight?  Where is his place of origin?  Which gene determines certain biological feature (cancer susceptibility fat generation rate hair color etc. How does this phenotype SNPs compare with other similar datasets from the western-world? What would be the likely cause of death for this person? What are the most likely diseases/illnesses this person is going to face in lifetime? What is unique about this dataset? What else you can extract from this dataset when it comes to personal trait intelligence level ancestry and body makeup?  Sample Reports Please check out following reports to understand what can be done with this data Ancestry –  https//www.23andme.com/published-report/eeb4f9bbd6b5474f/?share_id=f6c5562848e84586 Weight Report -  https//you.23andme.com/published/reports/65c9af9f8223456d/?share_id=0126f129e4f3458b,CSV,,[human genetics],CC0,,,750,12191,15,"6,000 Base-Pairs of Phenotype SNPs - Complete Raw Data",My Complete Genome,https://www.kaggle.com/zusmani/mygenome,Mon Jan 30 2017
,Etienne LQ,"[CODGEO, LIBGEO, REG, DEP, E14TST, E14TS0ND, E14TS1, E14TS6, E14TS10, E14TS20, E14TS50, E14TS100, E14TS200, E14TS500]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context INSEE is the official french institute gathering data of many types around France. It can be demographic (Births Deaths Population Density...) Economic (Salary Firms by activity / size...) and more.  It can be a great help to observe and measure inequality in the french population. Content Four files are in the dataset   base_etablissement_par_tranche_effectif  give information on the number of firms in every french town categorized by size  come from INSEE. CODGEO  geographique code for the town (can be joined with *code_insee* column from ""name_geographic_information.csv') LIBGEO  name of the town (in french) REG  region number DEP  depatment number E14TST  total number of firms in the town E14TS0ND  number of unknown or null size firms in the town E14TS1  number of firms with 1 to 5 employees in the town E14TS6  number of firms with 6 to 9 employees in the town E14TS10  number of firms with 10 to 19 employees in the town E14TS20  number of firms with 20 to 49 employees in the town E14TS50  number of firms with 50 to 99 employees in the town E14TS100  number of firms with 100 to 199 employees in the town E14TS200  number of firms with 200 to 499 employees in the town E14TS500  number of firms with more than 500 employees in the town name_geographic_information  give geographic data on french town (mainly latitude and longitude but also region / department codes and names ) EU_circo  name of the European Union Circonscription code_région  code of the region attached to the town nom_région  name of the region attached to the town chef.lieu_région  name the administrative center around the town numéro_département  code of the department attached to the town nom_département  name of the department attached to the town préfecture  name of the local administrative division around the town numéro_circonscription  number of the circumpscription nom_commune  name of the town codes_postaux  post-codes relative to the town code_insee  unique code for the town latitude  GPS latitude longitude  GPS longitude éloignement  i couldn't manage to figure out what was the meaning of this number net_salary_per_town_per_category  salaries around french town per job categories age and sex CODGEO  unique code of the town LIBGEO  name of the town SNHM14  mean net salary SNHMC14  mean net salary per hour for executive SNHMP14  mean net salary per hour for middle manager SNHME14  mean net salary per hour for employee SNHMO14  mean net salary per hour for worker SNHMF14  mean net salary for women SNHMFC14  mean net salary per hour for feminin executive SNHMFP14  mean net salary per hour for feminin middle manager SNHMFE14  mean net salary per hour for feminin employee SNHMFO14  mean net salary per hour for feminin worker SNHMH14  mean net salary for man SNHMHC14  mean net salary per hour for masculin executive SNHMHP14  mean net salary per hour for masculin middle manager SNHMHE14  mean net salary per hour for masculin employee SNHMHO14  mean net salary per hour for masculin worker SNHM1814  mean net salary per hour for 18-25 years old SNHM2614  mean net salary per hour for 26-50 years old SNHM5014  mean net salary per hour for >50 years old SNHMF1814  mean net salary per hour for women between 18-25 years old SNHMF2614  mean net salary per hour for women between 26-50 years old SNHMF5014  mean net salary per hour for women >50 years old SNHMH1814  mean net salary per hour for men between 18-25 years old SNHMH2614  mean net salary per hour for men between 26-50 years old SNHMH5014  mean net salary per hour for men >50 years old  population  demographic information in France per town age sex and living mode NIVGEO  geographic level (arrondissement communes...) CODGEO  unique code for the town LIBGEO  name of the town (might contain some utf-8 errors this information has better quality name_geographic_information) MOCO  cohabitation mode  [list and meaning available in Data description] AGE80_17  age category (slice of 5 years) | ex  0 -> people between 0 and 4 years old SEXE  sex 1 for men | 2 for women NB  Number of people in the category departments.geojson  contains the borders of french departments. From Gregoire David (github)  These datasets can be merged by  CODGEO = code_insee Acknowledgements The entire dataset has been created (and actualized) by INSEE I just uploaded it on Kaggle after doing some jobs and checks on it. I haven't seen INSEE on Kaggle yet but I think it would be great to bring the organization in as a Kaggle actor. Inspiration First aim I had creating that dataset was to provide a map of french towns with the number of firm that are settled in by size.  Now my goal is to explore inequality between men and women youngsters and elders working / social classes.  Population can also be a great filter to explain some phenomenons on the maps.",CSV,,"[employment, demographics]",CC0,,,1077,8200,344,Some data to show equality and inequalities in France,"French employment, salaries, population per town",https://www.kaggle.com/etiennelq/french-employment-by-town,Fri Oct 27 2017
,World Bank,"[Country Code, Short Name, Table Name, Long Name, 2-alpha code, Currency Unit, Special Notes, Region, Income Group, WB-2 code, National accounts base year, National accounts reference year, SNA price valuation, Lending category, Other groups, System of National Accounts, Alternative conversion factor, PPP survey year, Balance of Payments Manual in use, External debt Reporting status, System of trade, Government Accounting concept, IMF data dissemination standard, Latest population census, Latest household survey, Source of most recent Income and expenditure data, Vital registration complete, Latest agricultural census, Latest industrial data, Latest trade data, Latest water withdrawal data, ]","[string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string]",World Development Indicators provides a compilation of relevant high-quality and internationally comparable statistics about global development and the fight against poverty. It is intended to help policymakers students analysts professors program managers and citizens find and use data related to all aspects of development including those that help monitor progress toward the World Bank Group’s two goals of ending poverty and promoting shared prosperity.  Content This dataset includes indicators at both national and regional levels for  -Agriculture & Rural Development -Aid Effectiveness -Climate Change -Economy & Growth -Education -Energy & Mining -Environment -External Debt -Financial Sector -Gender -Health -Infrastructure -Labor & Social Protection -Poverty -Private Sector -Public Sector -Science & Technology -Social Development -Trade Urban Development Acknowledgements This dataset was kindly made available by the World Bank. Please check their instance at http//data.worldbank.org/data-catalog/world-development-indicators for updates and related information.,CSV,,"[countries, economics]",Other,,,509,3766,234,The most accurate global development data available,World Development Indicators,https://www.kaggle.com/theworldbank/world-development-indicators,Mon Aug 14 2017
,Aleksey Bilogur,[],[],Context Where's Waldo is a popular children's book series where the reader is presented with a sequence of scenes. Each scene contains potentially hundreds of individuals doing different things. Exactly one of these figures is Waldo a tall man in a striped red shirt red beanie and glasses and the objective of the game is to find Waldo is the least time possible. This dataset is raw data from the books for these challenges. Content This dataset contains a number of cuts of Where's Waldo scenes including scenes. See the complimentary kernel to learn more about the dataset contents! Acknowledgements This dataset was collected and published as-is by Valentino Constantinou (vc1492a) on GitHub (here). Inspiration Can you come up with a strategy better than randomly scanning the page for this task? Can you identify Waldos and not-Waldos?,Other,,"[popular culture, visual arts, image data, object detection]",ODbL,,,291,2761,125,Find Waldo with image recognition,Where's Waldo,https://www.kaggle.com/residentmario/wheres-waldo,Wed Oct 25 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes 2gb of stop data from Washington state. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[crime, law]",Other,,,91,810,2048,Data on Traffic and Pedestrian Stops by Police in Washington,Stanford Open Policing Project - Washington State,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-washington-state,Tue Jul 11 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[question, answer_date, ministry, question_type, question_no, question_by, question_title, question_description, answer]","[numeric, string, string, string, numeric, string, string, string, string]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context The Rajya Sabha has published the questions and answers that were discussed in each session on their webiste. But one has to search by question/session/ministry wise to get the detail. There were no direct way to get all the quesions and answers in structured way(i.e csv) to do an analysis and find insights. So I've written a web scrapper in R to scrape the data from the Rajya Sabha website and created csv files for each year. Content This dataset helps one to understand what was being discussed in Parliament (Rajya Sabha) of India. There are over 88000+ questions and answers that were discussed in Rajya sabha from 2009 till date (Sep'2017). Variables detail  id - Unique identifier answer_date - Answer date ministry - Ministry name question_type - Type of question (Starred or Unstarred)* question_no - Question number. (This question no. is unique per session)    question_by - Minister who has raised the question. question_title - Discussion title   question_description - Detailed question. answer - Detailed answer to the above question.   *Starred Questions    These are Questions to which answers are desired to be given orally on the floor of the House during the Question Hour. These are distinguished in the printed lists by asterisks. 15 such questions are listed each day. Unstarred Questions   These are Questions to which written answers are given by Ministers which are deemed to have been laid on the Table of the House at the end of the Question Hour. Upto 160 such questions are listed each day in a separate list.  source Acknowledgements Thanks to Rajya Sabha for making the question and answers searchable. Inspiration The below are some of the questions can be answered from this dataset.  Which state or district names mentioned most in question/answer? Sentiment analysis No. of questions ministry wise No. of questions/answers per day. Typically it should be 175 per day. How many days were less productive? Who has raised more question? etc. ,CSV,,"[india, politicians, government]",CC4,,,137,1437,159,88000+ Questions & answers discussed in Rajya Sabha from 2009 till date.,Q & A Discussed in Parliament of India,https://www.kaggle.com/rajanand/rajyasabha,Mon Oct 02 2017
,Kaggle,"[, originCountry, exchangeRate]","[numeric, string, numeric]","Context For the first time Kaggle conducted an industry-wide survey to establish a comprehensive view of the state of data science and machine learning. The survey received over 16000 responses and we learned a ton about who is working with data what’s happening at the cutting edge of machine learning across industries and how new data scientists can best break into the field. To share some of the initial insights from the survey we’ve worked with the folks from The Pudding to put together this interactive report. They’ve shared all of the kernels used in the report here. Content The data includes 5 files   schema.csv a CSV file with survey schema. This schema includes the questions that correspond to each column name in both the multipleChoiceResponses.csv and freeformResponses.csv. multipleChoiceResponses.csv Respondents' answers to multiple choice and ranking questions. These are non-randomized and thus a single row does correspond to all of a single user's answers. -freeformResponses.csv Respondents' freeform answers to Kaggle's survey questions. These responses are randomized within a column so that reading across a single row does not give a single user's answers. conversionRates.csv Currency conversion rates (to USD) as accessed from the R package ""quantmod"" on September 14 2017 RespondentTypeREADME.txt This is a schema for decoding the responses in the ""Asked"" column of the schema.csv file.  Kernel Awards in November In the month of November we’re awarding $1000 a week for code and analyses shared on this dataset via Kaggle Kernels. Read more about this month’s Kaggle Kernels Awards and help us advance the state of machine learning and data science by exploring this one of a kind dataset. Methodology  This survey received 16716 usable respondents from 171 countries and territories. If a country or territory received less than 50 respondents we grouped them into a group named “Other” for anonymity. We excluded respondents who were flagged by our survey system as “Spam” or who did not answer the question regarding their employment status (this question was the first required question so not answering it indicates that the respondent did not proceed past the 5th question in our survey). Most of our respondents were found primarily through Kaggle channels like our email list discussion forums and social media channels. The survey was live from August 7th to August 25th. The median response time for those who participated in the survey was 16.4 minutes. We allowed respondents to complete the survey at any time during that window. We received salary data by first asking respondents for their day-to-day currency and then asking them to write in either their total compensation. We’ve provided a csv with an exchange rate to USD for you to calculate the salary in US dollars on your own. The question was optional Not every question was shown to every respondent. In an attempt to ask relevant questions to each respondent we generally asked work related questions to employed data scientists and learning related questions to students. There is a column in the schema.csv file called ""Asked"" that describes who saw each question. You can learn more about the different segments we used in the schema.csv file and RespondentTypeREADME.txt in the data tab. To protect the respondents’ identity the answers to multiple choice questions have been separated into a separate data file from the open-ended responses. We do not provide a key to match up the multiple choice and free form responses. Further the free form responses have been randomized column-wise such that the responses that appear on the same row did not necessarily come from the same survey-taker.  ",CSV,,"[employment, sociology, artificial intelligence]",ODbL,,,9448,95836,28,A big picture view of the state of data science and machine learning.,"Kaggle ML and Data Science Survey, 2017",https://www.kaggle.com/kaggle/kaggle-survey-2017,Sat Oct 28 2017
,GeoNames,[],[],Context The GeoNames geographical database contains over 10 million geographical names and consists of over 9 million unique features with 2.8 million populated places and 5.5 million alternate names. All features are categorized into one out of nine feature classes and further subcategorized into one out of 645 feature codes.  Content The main 'geoname' table has the following fields   geonameid          integer id of record in geonames database name               name of geographical point (utf8) varchar(200) asciiname          name of geographical point in plain ascii characters varchar(200) alternatenames     alternatenames comma separated ascii names automatically transliterated convenience attribute from alternatename table varchar(10000) latitude           latitude in decimal degrees (wgs84) longitude          longitude in decimal degrees (wgs84) feature class      see http//www.geonames.org/export/codes.html char(1) feature code       see http//www.geonames.org/export/codes.html varchar(10) country code       ISO-3166 2-letter country code 2 characters cc2                alternate country codes comma separated ISO-3166 2-letter country code 200 characters admin1 code        fipscode (subject to change to iso code) see exceptions below see file admin1Codes.txt for display names of this code; varchar(20) admin2 code        code for the second administrative division a county in the US see file admin2Codes.txt; varchar(80)  admin3 code        code for third level administrative division varchar(20) admin4 code        code for fourth level administrative division varchar(20) population         bigint (8 byte int)  elevation          in meters integer dem                digital elevation model srtm3 or gtopo30 average elevation of 3''x3'' (ca 90mx90m) or 30''x30'' (ca 900mx900m) area in meters integer. srtm processed by cgiar/ciat. timezone           the iana timezone id (see file timeZone.txt) varchar(40) modification date  date of last modification in yyyy-MM-dd format  AdminCodes Most adm1 are FIPS codes. ISO codes are used for US CH BE and ME. UK and Greece are using an additional level between country and fips code. The code '00' stands for general features where no specific adm1 code is defined. The corresponding admin feature is found with the same countrycode and adminX codes and the respective feature code ADMx. feature classes  A country state region... H stream lake ... L parksarea ... P city village... R road railroad  S spot building farm T mountainhillrock...  U undersea V forestheath...  Acknowledgements  Data Sources http//www.geonames.org/data-sources.html ,Other,,[geography],CC3,,,282,2338,1024,Geographical database covering all countries with over eleven million placenames,GeoNames database,https://www.kaggle.com/geonames/geonames-database,Wed Aug 30 2017
,PromptCloud,[],[],Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 9.4 million job listings) that was created by extracting data from Naukri.com a leading job board. Content This dataset has following fields  company education experience industry job description jobid joblocation_address job title number of positions pay rate postdate site_name skills  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the pay rate job title industry and experience can be performed to name a few starting points.,CSV,,[],CC4,,,482,3154,50,"22,000 job listings on Naukri.com",Jobs on Naukri.com,https://www.kaggle.com/PromptCloudHQ/jobs-on-naukricom,Sat Sep 16 2017
,FuzzyFrogHunter,[],[],"Context This is a collection of all the works of Charles Dickens that are available through Project Gutenberg. This dataset is subject to the Project Gutenberg license. It is very possible that I have missed some of his works. Please add them and update the ""Last updated"" date below if you get the chance. Otherwise if you leave a comment on this dataset I will try and do so myself. I did some very rough deduplication of his works. If I missed anything please call it out with a comment and I will rectify the situation. Content Last updated October 13 2017  To be Read at Dusk - 924-0.txt The Seven Poor Travellers - pg1392.txt The Pickwick Papers - 580-0.txt A Message from the Sea - pg1407.txt The Old Curiosity Shop - 700-0.txt Pictures from Italy - 650-0.txt The Magic Fishbone A Holiday Romance from the Pen of Miss Alice Rainbird Aged 7 - pg23344.txt A Tale of Two Cities A Story of the French Revolution - 98-0.txt The Life And Adventures Of Nicholas Nickleby - 967-0.txt Little Dorrit - 963-0.txt The Uncommercial Traveller - 914-0.txt Oliver Twist - pg730.txt Three Ghost Stories - 1289-0.txt The Chimes - 653-0.txt Mugby Junction - 27924-0.txt Great Expectations - 1400-0.txt The Battle of Life - pg676.txt David Copperfield - 766-0.txt Bleak House - pg1023.txt Sketches by Boz illustrative of everyday life and every-day people - 882-0.txt The Haunted Man and the Ghost's Bargain - 644-0.txt A Child's History of England - pg699.txt American Notes for General Circulation - 675-0.txt Hunted Down [1860] - 807-0.txt Hard Times - 786-0.txt The Mystery of Edwin Drood - 564-0.txt Dickens' Stories About Children Every Child Can Read - pg32241.txt The Cricket on the Hearth A Fairy Tale of Home - 678-0.txt Our Mutual Friend - 883-0.txt A Christmas Carol - pg19337.txt Barnaby Rudge - 917-0.txt Some Christmas Stories - 1467-0.txt  Acknowledgements Content taken from Project Gutenberg Image taken from Wikimedia Commons Inspiration Making this data available for any kind of textual analysis. I intend for this to be part of a series.",Other,,"[books, writing, literature]",Other,,,78,879,24,Collected from Project Gutenberg [text],The Works of Charles Dickens,https://www.kaggle.com/fuzzyfroghunter/dickens,Sat Oct 14 2017
,PromptCloud,"[uniq_id, product_name, manufacturer, price, number_available_in_stock, number_of_reviews, number_of_answered_questions, average_review_rating, amazon_category_and_sub_category, customers_who_bought_this_item_also_bought, description, product_information, product_description, items_customers_buy_after_viewing_this_item, customer_questions_and_answers, customer_reviews, sellers]","[string, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string]","Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 7 million products) that was created by extracting data from Amazon.com. Content This dataset has following fields  product_name manufacturer - The item manufacturer as reported on Amazon. Some common ""manufacturers"" like Disney actually outsource their assembly line. price number_available_in_stock number_of_reviews number_of_answered_questions - Amazon includes a Question and Answer service on all or most of its products. This field is a count of how many questions that were asked actually got answered. average_review_rating amazon_category_and_sub_category - A tree-based >>-delimited categorization for the item in question. customers_who_bought_this_item_also_bought - References to other items that similar users bought. This is a recommendation engine component that played a big role in making Amazon popular initially. description product_information product_description items_customers_buy_after_viewing_this_item customer_questions_and_answers - A string entry with all of the product's JSON question and answer pairs. customer_reviews - A string entry with all of the product's JSON reviews. sellers - A string entry with all of the product's JSON seller information (many products on Amazon are sold by third parties).  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration This detailed dataset can be used to answer questions like  What types of toys are most popular on Amazon? How dominant are brands in the Amazon toy market? Can you break down reviews to analyze their sentiment and contents? ",CSV,,[internet],CC4,,,1271,8886,34,"10,000 toy products on Amazon.com",Toy Products on Amazon,https://www.kaggle.com/PromptCloudHQ/toy-products-on-amazon,Sat Sep 16 2017
,Roy Garrard,[],[],"Context The United States Census Bureau conducts annual surveys to assess the finances of elementary and high schools. The attached CSV file contains a summary of revenue and expenditure for the years 1992-2015 organized by state. Content [elsec_main.csv] A comma-separated spreadsheet containing revenues and expenditures for all U.S. school districts 1993-2015.  STATEENROLLNAMEYRDATATOTALREVTFEDREVTSTREVTLOCREVTOTALEXPTCURINSTTCURSSVCTCURONONTCAPOUT Alabama7568AUTAUGA CO SCH DIST19953182728212138976172745715228712325752176 Alabama19961BALDWIN CO SCH DIST1995933796655551083161687973487502296169276795  [elsec_summary.csv] A comma-separated spreadsheet containing state summaries of revenues and expenditures organized by year. STATEYEARENROLLTOTAL_REVENUEFEDERAL_REVENUESTATE_REVENUELOCAL_REVENUETOTAL_EXPENDITUREINSTRUCTION_EXPENDITURESUPPORT_SERVICES_EXPENDITUREOTHER_EXPENDITURECAPITAL_OUTLAY_EXPENDITURE Alabama19922678885304177165902871568026537981481703735036174053 Alaska1992104959110678072071122210097248849836235090237451  Be warned some data will be NaN's (most notably the 1992 records contain no data for enrollment).  Data was created from the spreadsheets in [elsec.zip] (taken from the U.S. Census Bureau site) using [chew_data.py] and [state_summary.py]. Column names are documented in [school15doc.pdf]. Sources https//www.census.gov/programs-surveys/school-finances/data/tables.html  Changelog [v 0.2] Added data from 1993-2001. Data is now harvested from the main spreadsheets instead of the summary spreadsheets. Data by school district is now available. [v 0.3] Added 1992 data. Added enrollment data for all years except 1992 (unavailable). [v 0.4] Straightening a few things out as I play with the data in my own kernel. Changed ""program_other_expenditure"" to ""other_expenditure"" and fixed chew_data.py to properly pull that information. Removed ""non-elsec"" funding and ""program_current_expenditure"" columns.",Other,,"[united states, education, finance]",CC0,,,683,4781,81,"Revenues and expenditures for U.S. grade schools, by year and state",U.S. Educational Finances,https://www.kaggle.com/noriuk/us-educational-finances,Sun Feb 04 2018
,Datafiniti,"[address, categories, city, claimed, country, cuisines, dateOpened, dateUpdated, descriptions, facebookPageURL, features, hours, images, isClosed, key, lat, languages, long, menus, menuURL, name, paymentTypes, phones, postalCode, priceRange, province, sic, twitter, websites]","[string, string, string, numeric, string, numeric, numeric, dateTime, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",About This Data This is a list of over 18000 restaurants in the US that serve vegetarian or vegan food provided by Datafiniti's Business Database. The dataset includes address city state business name business categories menu data phone numbers and more. What You Can Do With This Data You can use this data to determine the most vegetarian and vegan-friendly cities in the US. E.g.  How many restaurants in each metro area offers vegetarian options? Which metros among the 25 most popular metro areas have the most and least vegetarian restaurants per 100000 residents? Which metros with at least 10 vegetarian restaurants have the most vegetarian restaurants per 100000 residents? How many restaurants in each metro area offers vegan options? Which metros among the 25 most popular metro areas have the most and least vegan restaurants per 100000 residents? Which metros with at least 10 vegan restaurants have the most vegan restaurants per 100000 residents? Which cuisines are served the most at vegetarian restaurants?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, food and drink, business]",CC4,,,831,6415,79,"A list of over 18,000 restaurants that serve vegetarian or vegan food in the US.",Vegetarian and Vegan Restaurants,https://www.kaggle.com/datafiniti/vegetarian-vegan-restaurants,Thu Nov 17 2016
,Sohier Dane,"[Crime ID, Month, Reported by, Falls within, Longitude, Latitude, Location, LSOA code, LSOA name, Outcome type]","[string, dateTime, string, string, numeric, numeric, string, string, string, string]",This dataset provides a complete snapshot of crime outcome and stop and search data as held by the Home Office from late 2014 through mid 2017 for London both the greater metro and the city. Content The core fields are as follows Reported by    The force that provided the data about the crime. Falls within   At present also the force that provided the data about the crime. This is currently being looked into and is likely to change in the near future. Longitude and Latitude The anonymised coordinates of the crime. LSOA code and LSOA name    References to the Lower Layer Super Output Area that the anonymised point falls into according to the LSOA boundaries provided by the Office for National Statistics. Crime type One of the crime types listed in the Police.UK FAQ. Last outcome category  A reference to whichever of the outcomes associated with the crime occurred most recently. For example this crime's 'Last outcome category' would be 'Offender fined'. Context    A field provided for forces to provide additional human-readable data about individual crimes. Currently for newly added CSVs this is always empty. For additional details including the steps taken to anonymize the data please see https//data.police.uk/about/#provenance. Acknowledgements This dataset was kindly released by the British Home Office under the Open Government License 3.0 at https//data.police.uk/data/. If you are looking for more data they cover much more than London! All major cities in England and Wales are available adding up to roughly 2gb of new data per month.,CSV,,"[crime, government]",Other,,,407,2550,1024,"A snapshot of crime, outcome, and stop and search data",London Police Records,https://www.kaggle.com/sohier/london-police-records,Tue Aug 15 2017
,PromptCloud,"[country, country_code, date_added, has_expired, job_board, job_description, job_title, job_type, location, organization, page_url, salary, sector, uniq_id]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 4.7 million job listings) that was created by extracting data from Monster.com a leading job board. Content This dataset has following fields  country country_code date_added has_expired - Always false. job_description - The primary field for this dataset containing the bulk of the information on what the job is about. job_title job_type - The type of tasks and skills involved in the job. For example ""management"". location organization page_url salary sector - The industry sector the job is in. For example ""Medical services"".  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration  What kinds of jobs titles correspond with what kinds of wages? What can you learn about the Moster.com-based US job market based on analyzing the contents of the job descriptions? How do job descriptions different between different industry sectors? ",CSV,,[internet],CC4,,,632,4560,65,"22,000 US-based Job Listings",US jobs on Monster.com,https://www.kaggle.com/PromptCloudHQ/us-jobs-on-monstercom,Fri Sep 15 2017
,HM Land Registry,"[Transaction unique identifier, Price, Date of Transfer, Property Type, Old/New, Duration, Town/City, District, County, PPDCategory Type, Record Status - monthly file only]","[string, dateTime, dateTime, string, string, string, string, string, string, string, string]",The Price Paid Data includes information on all registered property sales in England and Wales that are sold for full market value. Address details have been truncated to the town/city level.  You might also find the HM Land Registry transaction records to be a useful supplement to this dataset https//www.kaggle.com/hm-land-registry/uk-land-registry-transactions The available fields are as follows Transaction unique identifier   A reference number which is generated automatically recording each published sale. The number is unique and will change each time a sale is recorded. Price   Sale price stated on the transfer deed. Date of Transfer    Date when the sale was completed as stated on the transfer deed. Property Type   D = Detached S = Semi-Detached T = Terraced F = Flats/Maisonettes O = Other  Note that  - we only record the above categories to describe property type we do not separately identify bungalows.  - end-of-terrace properties are included in the Terraced category above.  - ‘Other’ is only valid where the transaction relates to a property type that is not covered by existing values. Old/New Indicates the age of the property and applies to all price paid transactions residential and non-residential. Y = a newly built property N = an established residential building Duration    Relates to the tenure F = Freehold L= Leasehold etc. Note that HM Land Registry does not record leases of 7 years or less in the Price Paid Dataset. Town/City     District      County    PPD Category Type   Indicates the type of Price Paid transaction. A = Standard Price Paid entry includes single residential property sold for full market value. B = Additional Price Paid entry including transfers under a power of sale/repossessions buy-to-lets (where they can be identified by a Mortgage) and transfers to non-private individuals. Note that category B does not separately identify the transaction types stated. HM Land Registry has been collecting information on Category A transactions from January 1995. Category B transactions were identified from October 2013.  Record Status - monthly file only   Indicates additions changes and deletions to the records.(see guide below). A = Addition C = Change D = Delete. Note that where a transaction changes category type due to misallocation (as above) it will be deleted from the original category type and added to the correct category with a new transaction unique identifier. This data was kindly released by HM Land Registry under the Open Government License 3.0. You can find their current release here. Data produced by HM Land Registry © Crown copyright 2017.,CSV,,"[housing, finance, government]",Other,,,709,5813,2048,Records of all individual transactions in England and Wales since 1995,UK Housing Prices Paid,https://www.kaggle.com/hm-land-registry/uk-housing-prices-paid,Wed Aug 16 2017
,Shane Smith,"[STA, Date, Precip, WindGustSpd, MaxTemp, MinTemp, MeanTemp, Snowfall, PoorWeather, YR, MO, DA, PRCP, DR, SPD, MAX, MIN, MEA, SNF, SND, FT, FB, FTI, ITH, PGT, TSHDSBRSGF, SD3, RHX, RHN, RVG, WTE]","[numeric, dateTime, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string]",Context While exploring the Aerial Bombing Operations of World War Two dataset (https//www.kaggle.com/usaf/world-war-ii) and recalling that the D-Day landings were nearly postponed due to poor weather I sought out weather reports from the period to compare with missions in the bombing operations dataset.  Content The dataset contains information on weather conditions recorded on each day at various weather stations around the world. Information includes precipitation snowfall temperatures wind speed and whether the day included thunder storms or other poor weather conditions.  Acknowledgements The data are taken from the United States National Oceanic and Atmospheric Administration (https//www.kaggle.com/noaa) National Centres for Environmental Information website https//www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/world-war-ii-era-data Inspiration This dataset is mostly to assist with the analysis of the Aerial Bombing Operations dataset also hosted on Kaggle. ,CSV,,"[weather, history, war]",Other,,,366,2637,11,Daily Weather Summaries from 1940-1945,Weather Conditions in World War Two,https://www.kaggle.com/smid80/weatherww2,Wed Nov 01 2017
,United Nations,"[country_or_area, subgroup, year, source, unit, value, value_footnotes, category]","[string, string, numeric, string, string, numeric, string, string]",Gender Info 2007 is a global database of gender statistics and indicators on a wide range of policy areas including population families health education work and political participation. It can be used by governments international organizations advocacy groups researchers and others in need of statistics for planning analysis advocacy and awareness-raising. Users will find in Gender Info an easy-to-use tool to shed light on gender issues through customizable tables graphs and maps. It is an initiative of the United Nations Statistics Division produced in collaboration with the United Nations Children’s Fund (UNICEF) and the United Nations Population Fund (UNFPA).  This dataset was last updated in 2008. If you need a more current version of the data please visit http//unstats.un.org/unsd/gender/data.html for other Gender Statistics. Acknowledgements This dataset was kindly published by the United Nations on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,[gender],Other,,,430,3324,2,Global gender statistics,Gender Info 2007,https://www.kaggle.com/unitednations/gender-info-2007,Wed Nov 15 2017
,Rohk,"[publish_date, headline_category, headline_text]","[dateTime, string, string]",Context This dataset is a compilation of 2.7 million news headlines published by Times of India from 2001 to 2017 17 years. A majority of the data is focusing on Indian local news including national city level and entertainment. Agency Website https//timesofindia.indiatimes.com Prepared by Rohit Kulkarni Content CSV Rows 2735347  publish_date Date of the article being published online in yyyyMMdd format headline_category Category of the headline ascii dot delimited lowercase values headline_text Text of the Headline in English very rare non-ascii characters  Start Date 2001-01-01 End Date 2017-12-31 See This Kernal for Overview of Trends and Categories  Inspiration This News Dataset is a persistent historical archive of noteable events in the Indian subcontinent from start-2001 to end-2017 recorded in real time by the journalists of India. Times Group as a news agency reaches out a very wide audience across Asia and drawfs every other agency in the quantity of English Articles published per day.  Due to the heavy daily volume (avg. 650 articles) over multiple years this data offers a deep insight into Indian society its priorities events issues and talking points and how they have unfolded over time.  It is possible to chop this dataset into a smaller piece for a more focused analysis based on one or more facets.   Time Range Records during 2014 election 2006 Mumbai Bombings One or more Categories like Mumbai Movie Releases ICC updates Magazine Middle East One or more Keywords like crime or ecology related words; names of political parties celebrities corporations.  Acknowledgements The headlines are extracted from several GB of raw HTML files using Jsoup Java and Bash. The entire process takes 3.5 minutes. This logic also  chooses the best worded headline for each article (longest one is usually picked) ; clusters about 17k categories to 200 large groups ; removes records where the date is ambiguous (9k cases) ; finally cleans the selected headline via a string 'domestication' function (which I use for any wild text from the internet). The final categories are as per the latest sitemap.  Around 1.5k rare categories remain and these records (~20k) can be filtered out easily during analysis. The category is unknown for ~200k records. Similar news datasets exploring other attributes countries and topics can be seen on my profile. Citation for usage Rohit Kulkarni (2017) News Headlines of India 2001-2017 [CSV data file] doi10.7910/DVN/J7BYRX Retrieved from [this url],CSV,,"[news agencies, cities, historiography]",CC4,,,244,2787,62,16 years of categorized headlines focusing on India,News Headlines Of India,https://www.kaggle.com/therohk/india-headlines-news-dataset,Wed Jan 10 2018
,Rachael Tatman,"[Company 
(Maker-if known), Specific Bean Origin
or Bar Name, REF, Review
Date, Cocoa
Percent, Company
Location, Rating, Bean
Type, Broad Bean
Origin]","[string, string, numeric, numeric, string, string, numeric, string, string]",Context Chocolate is one of the most popular candies in the world. Each year residents of the United States collectively eat more than 2.8 billions pounds. However not all chocolate bars are created equal! This dataset contains expert ratings of over 1700 individual chocolate bars along with information on their regional origin percentage of cocoa the variety of chocolate bean used and where the beans were grown. Flavors of Cacao Rating System  5= Elite (Transcending beyond the ordinary limits) 4= Premium (Superior flavor development character and style) 3= Satisfactory(3.0) to praiseworthy(3.75) (well made with special qualities) 2= Disappointing (Passable but contains at least one significant flaw) 1= Unpleasant (mostly unpalatable)  Each chocolate is evaluated from a combination of both objective qualities and subjective interpretation. A rating here only represents an experience with one bar from one batch. Batch numbers vintages and review dates are included in the database when known.  The database is narrowly focused on plain dark chocolate with an aim of appreciating the flavors of the cacao when made into chocolate. The ratings do not reflect health benefits social missions or organic status. Flavor is the most important component of the Flavors of Cacao ratings. Diversity balance intensity and purity of flavors are all considered. It is possible for a straight forward single note chocolate to rate as high as a complex flavor profile that changes throughout. Genetics terroir post harvest techniques processing and storage can all be discussed when considering the flavor component.  Texture has a great impact on the overall experience and it is also possible for texture related issues to impact flavor. It is a good way to evaluate the makers vision attention to detail and level of proficiency. Aftermelt is the experience after the chocolate has melted. Higher quality chocolate will linger and be long lasting and enjoyable. Since the aftermelt is the last impression you get from the chocolate it receives equal importance in the overall rating. Overall Opinion is really where the ratings reflect a subjective opinion. Ideally it is my evaluation of whether or not the components above worked together and an opinion on the flavor development character and style. It is also here where each chocolate can usually be summarized by the most prominent impressions that you would remember about each chocolate. Acknowledgements These ratings were compiled by Brady Brelinski Founding Member of the Manhattan Chocolate Society. For up-to-date information as well as additional content (including interviews with craft chocolate makers) please see his website Flavors of Cacao Inspiration  Where are the best cocoa beans grown? Which countries produce the highest-rated bars? What’s the relationship between cocoa solids percentage and rating? ,CSV,,"[critical theory, food and drink]",CC0,,,7178,36263,0.1220703125,"Expert ratings of over 1,700 chocolate bars",Chocolate Bar Ratings,https://www.kaggle.com/rtatman/chocolate-bar-ratings,Sat Aug 12 2017
,Jonas Almeida,"[Health Service Area, Hospital County, Operating Certificate Number, Facility Id, Facility Name, Age Group, Zip Code - 3 digits, Gender, Race, Ethnicity, Length of Stay, Type of Admission, Patient Disposition, Discharge Year, CCS Diagnosis Code, CCS Diagnosis Description, CCS Procedure Code, CCS Procedure Description, APR DRG Code, APR DRG Description, APR MDC Code, APR MDC Description, APR Severity of Illness Code, APR Severity of Illness Description, APR Risk of Mortality, APR Medical Surgical Description, Payment Typology 1, Payment Typology 2, Payment Typology 3, Attending Provider License Number, Operating Provider License Number, Other Provider License Number, Birth Weight, Abortion Edit Indicator, Emergency Department Indicator, Total Charges, Total Costs]","[string, string, numeric, numeric, string, string, numeric, string, string, string, numeric, string, string, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, string, string, string, string, string, numeric, string, string, numeric, string, string, string, string]",Public Health Data This is the public dataset made available at https//health.data.ny.gov/Health/Hospital-Inpatient-Discharges-SPARCS-De-Identified/82xm-y6g8 by the Dept of Health of New York state. The following description can be found at that page The Statewide Planning and Research Cooperative System (SPARCS) Inpatient De-identified File contains discharge level detail on patient characteristics diagnoses treatments services and charges. This data file contains basic record level detail for the discharge. The de-identified data file does not contain data that is protected health information (PHI) under HIPAA. The health information is not individually identifiable; all data elements considered identifiable have been redacted. For example the direct identifiers regarding a date have the day and month portion of the date removed. It would be nice to ... ... for example be able to predict length of stay in the hospital using the parameters likely to be available when teh patient is admitted.,CSV,,[],CC0,,,104,760,109,Patient characteristics and charges,2015 de-identified NY inpatient discharge (SPARCS),https://www.kaggle.com/jonasalmeida/2015-deidentified-ny-inpatient-discharge-sparcs,Thu Jan 25 2018
,Evgenii Vasilev,"[Location Type, Incident Zip, City, Borough, Latitude, Longitude, num_calls]","[string, numeric, string, string, numeric, numeric, numeric]","Context This dataset contains all noise complaints calls that were received by the  city police with complaint type ""Loud music/Party"" in 2016. The data contains the time of the call time of the police response coordinates and part of the city. This data should help match taxi rides from ""New York City Taxi Trip Duration"" competition to the night rides of partygoers.  Content The New York city hotline receives non-urgent community concerns which are made public by the city through NYC Open Data portal. The full dataset contains a variety of complaints ranging from illegal parking to customer complaints. This dataset focuses on Noise complaints that were collected in 2016  and indicate ongoing party in a given neighborhood.  parties_in_nyc.csv Columns Created Date - time of the call  Closed Date - time when ticket was closed by police  Location Type - type of the location  Incident Zip - zip code of the location  City - name of the city (almost the same as the Borough field)  Borough - administrative division of the city  Latitude - latitude of the location  Longitude - longitude of the location   test_parties and train_parties Columns id - id of the ride  num_complaints - number of noise complaints about ongoing parties within ~500 meters and within 2 hours of pickup place and time  Acknowledgements https//opendata.cityofnewyork.us/ - NYC Open Data portal contains many other interesting datasets Photo by Yvette de Wit on Unsplash Inspiration After a fun night out in the city majority of people are too exhausted to travel by public transport so they catch a cab to their home. I hope this data will help the community to find the patterns in the data that will lead to better solutions.",CSV,,[cities],CC0,,,1843,12135,53,225k noise complaints to the police about ongoing parties in the city,2016 Parties in New York,https://www.kaggle.com/somesnm/partynyc,Wed Aug 23 2017
,US Environmental Protection Agency,"[state_code, county_code, site_num, parameter_code, poc, latitude, longitude, datum, parameter_name, sample_duration, pollutant_standard, date_local, units_of_measure, event_type, observation_count, observation_percent, arithmetic_mean, first_max_value, first_max_hour, aqi, method_code, method_name, local_site_name, address, state_name, county_name, city_name, cbsa_name, date_of_last_change]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, dateTime]",Context Carbon Monoxide (CO) is a colorless odorless gas that can be harmful when inhaled in large amounts. CO is released when something is burned. The greatest sources of CO to outdoor air are cars trucks and other vehicles or machinery that burn fossil fuels. A variety of items in your home such as unvented kerosene and gas space heaters leaking chimneys and furnaces and gas stoves also release CO and can affect air quality indoors. Content The daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is  The aggregate of all sub-daily measurements taken at the monitor. The single sample value if the monitor takes a single daily sample (e.g. there is only one sample with a 24-hour duration). In this case the mean and max daily sample will have the same value.  Within the data file you will find these fields 1. State Code The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.  County Code The FIPS code of the county in which the monitor resides. Site Num A unique number within the county identifying the site. Parameter Code The AQS code corresponding to the parameter measured by the monitor. POC This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site. Latitude The monitoring site’s angular distance north of the equator measured in decimal degrees. Longitude The monitoring site’s angular distance east of the prime meridian measured in decimal degrees. Datum The Datum associated with the Latitude and Longitude measures. Parameter Name The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants. Sample Duration The length of time that air passes through the monitoring device before it is analyzed (measured). So it represents an averaging period in the atmosphere (for example a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors it can represent an averaging time of many samples (for example a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour). Pollutant Standard A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.) Date Local The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor. Units of Measure The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations. Event Type Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question the data will have multiple records for each monitor. Observation Count The number of observations (samples) taken during the day. Observation Percent The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g. only certain parameters). Arithmetic Mean The average (arithmetic mean) value for the day. 1st Max Value The highest value for the day. 1st Max Hour The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken. AQI The Air Quality Index for the day for the pollutant if applicable. Method Code  An internal system code indicating the method (processes equipment and protocols) used in gathering and measuring the sample. The method name is in the next column. Method Name A short description of the processes equipment and protocols used in gathering and measuring the sample. Local Site Name The name of the site (if any) given by the State local or tribal air pollution control agency that operates it. Address The approximate street address of the monitoring site. State Name The name of the state where the monitoring site is located. County Name The name of the county where the monitoring site is located. City Name The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas. CBSA Name The name of the core bases statistical area (metropolitan area) where the monitoring site is located. Date of Last Change The date the last time any numeric values in this record were updated in the AQS data system.  Acknowledgements These data come from the EPA and are current up to May 1 2017. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/epa. Inspiration Breathing air with a high concentration of CO reduces the amount of oxygen that can be transported in the bloodstream to critical organs like the heart and brain. At very high levels which are  possible indoors or in other enclosed environments CO can cause dizziness confusion unconsciousness and death. Very high levels of CO are not likely to occur outdoors. However when CO levels are elevated outdoors they can be of particular concern for people with some types of heart disease. These people already have a reduced ability for getting oxygenated blood to their hearts in situations where the heart needs more oxygen than usual. They are especially vulnerable to the effects of CO when exercising or under increased stress. In these situations short-term exposure to elevated CO may result in reduced oxygen to the heart accompanied by chest pain also known as angina.,CSV,,[environment],CC0,,,321,2579,2048,A summary of daily CO levels from 1990 to 2017,Carbon Monoxide Daily Summary,https://www.kaggle.com/epa/carbon-monoxide,Fri Jun 30 2017
,def love(x):,"[Column, Description]","[string, string]","Context With this dataset I hope to raise awareness on the trends in crime. Content For NYPD Complaint Data each row represents a crime. For information on the columns please see the attached csv ""Crime_Column_Description"". Reported crime go back 5 years but I only attached reported crime from 2014-2015 due to file size. The full report can be found at NYC Open Data (https//data.cityofnewyork.us/Public-Safety/NYPD-Complaint-Data-Historic/qgea-i56i) Acknowledgements I would like to thank NYC Open Data for the dataset. Inspiration Additional things I would like to better understand 1.  Differences in crime that exist between the 5 boroughs 2. A mapping of the crimes per borough 3. Where do the most dangerous crimes happen and what time?",CSV,,[crime],CC0,,,695,4083,253,2014-2015 Crimes reported in all 5 boroughs of New York City,New York City Crimes,https://www.kaggle.com/adamschroeder/crimes-new-york-city,Fri Aug 11 2017
,Olga Belitskaya,[],[],History I made the database from the fragments of my own photos of flowers. The images are selected to reflect the flowering features of these plant species. Content The content is very simple  210 images (128x128x3)  with 10 species of flowering plants and the file with labels flower-labels.csv. Photo files are in the .png format and the labels are the integers.  Label => Name  0 => phlox; 1 => rose; 2 => calendula; 3 => iris; 4 => leucanthemum maximum;  5 => bellflower; 6 => viola; 7 => rudbeckia laciniata (Goldquelle); 8 => peony; 9 => aquilegia. Acknowledgements As an owner of this database I have published it for absolutely free using by any site visitor. Usage Accurate classification of plant species with a small number of images isn't a trivial task. I hope this set can be interesting for training skills in this field. A wide spectrum of algorithms can be used for classification.,Other,,"[photography, plants]",Other,,,526,4558,49,Set for Classification,Flower Color Images,https://www.kaggle.com/olgabelitskaya/flower-color-images,Wed Nov 01 2017
,Jacob Boysen,[],[],Context Traffic management is a critical concern for policymakers and a fascinating data question. This ~2gb dataset contains daily volumes of traffic binned by hour. Information on flow direction and sensor placement is also included. Content Two datasets are included dot_traffic_2015.txt.gz  daily observation of traffic volume divided into 24 hourly bins station_id location information (geographical place) traffic flow direction and type of road  dot_traffic_stations_2015.txt.gz  deeper location and historical data on individual observation stations cross-referenced by station_id  Acknowledgements This dataset was compiled by the US Department of Transportation and available on Google BigQuery Inspiration  Where are the heaviest traffic volumes? By time of day? By type of road? Any interesting seasonal patterns to traffic volumes? ,Other,,[],CC0,,,571,3828,446,"7.1M Daily Traffic Volume Observations, By Hour and Direction","US Traffic, 2015",https://www.kaggle.com/jboysen/us-traffic-2015,Tue Aug 15 2017
,United Nations,[],[],FAOSTAT provides access to over 3 million time-series and cross sectional data relating to food and agriculture. The full FAO data can be found in the large zipfile while a (somewhat out of date) summary of FAOSTAT is in the top level csv files. FAOSTAT contains data for 200 countries and more than 200 primary products and inputs in its core data set. The national version of FAOSTAT CountrySTAT is being implemented in about 20 countries and three regions. It offers a two-way bridge amongst sub-national national regional and international statistics on food and agriculture. Acknowledgements This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,[agriculture],Other,,,441,3645,453,Land use and farming inputs,Global Food & Agriculture Statistics,https://www.kaggle.com/unitednations/global-food-agriculture-statistics,Fri Nov 17 2017
,Sumit Kumar,"[category_id, category_name, shortname, sort_name]","[numeric, string, string, string]",Summary “Meetup is a social networking website that aims to brings people together to do explore teach and learn the things that help them come alive.” Meetup allows members to find and join groups unified by a common interest. As of 2017 there are 32 million users with 280 thousand groups available across 182 countries. A member needs to be able to identify groups and activities which interest them the most to be able to use this platform to network effectively. The aim of our team was to use this dataset to build a recommender system which will identify and suggest groups and activities to a member based on their interest and additional interests of similar members. Furthermore a social network analysis was done to identify the relationship between groups and people. Database EER diagram  Data Collection Method  Data was collected using Meetup API.    Python script was used to ping meetup API and collect responses as JSON objects.   Logical chunks of data were exported and saved as csv files.    Data Cleaning  Data is filtered to include only 3 cities' information (New York San Francisco Chicago). Character encoding is normalized to ASCII characters across tables.  Example Visualizations     ,CSV,,"[social groups, internet]",CC0,,,407,3054,197,Organized data for various meetup.com entities along with relational schema,Meetups data from meetup.com,https://www.kaggle.com/sirpunch/meetups-data-from-meetupcom,Fri Dec 15 2017
,Derrick M,"[tweet_id, created_at, tweet , retweets, username]","[numeric, dateTime, string, numeric, string]",Context I collected this data from incubators and accelerators to find out what they have been talking about in 2017. Content The data contains the twitter usernames of various organizations and tweets for the year 2017 collected on 28th Dec 2017. Acknowledgements Much appreciation to @emmanuelkens for helping in thinking through this Inspiration I am very curious to find out what the various organizations have been talking about in 2017. I would also like to find out the most popular organization by tweets and engagement. I am also curious to find out if there is any relationship between the number of retweets a tweet gets and the time of day it was posted!,CSV,,[twitter],GPL,,,145,1386,4,2017 tweets from incubators and accelerators,24 thousand tweets later ,https://www.kaggle.com/derrickmwiti/24-thousand-tweets-later,Sun Jan 07 2018
,John Ruth,"[FILE NUMBER, OFFENSE DATE, OFFENSE TIME, CRIME, COMMITTED, OFFENSE, OFFENSE DESC, ADDRESS, ST NUMBER, ST DIR, ST NAME, ST TYPE, CITY, STATE, ZIP, DISTRICT, ZONE, SUBZONE, COMPLETE DISTRICT, GEOLOCATION]","[numeric, dateTime, string, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, string]",Context Crimes reported in Baton Rouge and handled by the Baton Rouge Police Department. Crimes include Burglaries (Vehicle Residential and Non-residential) Robberies (Individual and Business) Theft Narcotics Vice Crimes Assault Nuisance Battery Firearm Homicides Criminal Damage to Property Sexual Assaults and Juvenile. Content Dataset only includes records through September 21st 2017 Columns included FILE NUMBER OFFENSE DATE OFFENSE TIME CRIME COMMITTED OFFENSE OFFENSE DESC ADDRESS ST NUMBER ST DIR ST NAME ST TYPE CITY STATE ZIP DISTRICT ZONE SUBZONE COMPLETE DISTRICT GEOLOCATION Acknowledgements This public domain data is provided by Open Data BR through Socrata.  See this dataset's official page for more information.  Public domain licensed banner image provided by GoodFreePhotos.com.,CSV,,[crime],CC0,,,95,765,66,"Through September 21st, 2017",Baton Rouge Crime Incidents,https://www.kaggle.com/johnruth/baton-rouge-crime-incidents-through-09212017,Fri Sep 22 2017
,SFuh,"[Origin, Destination, Throughput, DateTime]","[string, string, numeric, dateTime]","Context BART short for ""Bay Area Rapid Transit"" is the transit system severing the San Francisco Bay Area in California. BART operates six routes 46 stations and and 112 miles of track. It serves an average weekday ridership of 423000 people making it the fifth-busiest rapid transit system in the United States. This dataset contains daily information on BART ridership for a period covering all of 2016 and part of 2017. Unlike some other rapid transit system datasets this data includes movements between specific stations (there are just over 2000 station-to-station combinations). Content This dataset is split in three files. station_info.csv includes generic information on individual stations. date-hour-soo-dest-2017.csv contains daily inter-station ridership for (part of) 2017. date-hour-soo-dest-2016.csv contains daily inter-station ridership for all of 2016.  Acknowledgements Would like to thank the BART organization for recording the data and providing it to the public. https//www.bart.gov/about/reports/ridership  Inspiration  Which BART station is the busiest?  What is the least popular BART route?   When is the best time to go to SF if you want to find a seat? Which day of the week is the busiest?  How many people take the BART late at night?  Does the BART ever stop in a station without anyone going off or on? ",CSV,,[],CC0,,,105,727,50,2016-2017 daily station-to-station BART ridership,Bay Area Rapid Transit Ridership,https://www.kaggle.com/saulfuh/bart-ridership,Sat Feb 10 2018
,Alberto Barradas,"[#, Name, Type 1, Type 2, Total, HP, Attack, Defense, Sp. Atk, Sp. Def, Speed, Generation, Legendary]","[numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, boolean]",This data set includes 721 Pokemon including their number name first and second type and basic stats HP Attack Defense Special Attack Special Defense and Speed. It has been of great use when teaching statistics to kids. With certain types you can also give a geeky introduction to machine learning. This are the raw attributes that are used for calculating how much damage an attack will do in the games. This dataset is about the pokemon games (NOT pokemon cards or Pokemon Go). The data as described by Myles O'Neill is  # ID for each pokemon Name Name of each pokemon Type 1 Each pokemon has a type this determines weakness/resistance to attacks Type 2 Some pokemon are dual type and have 2 Total sum of all stats that come after this a general guide to how strong a pokemon is HP hit points or health defines how much damage a pokemon can withstand before fainting Attack the base modifier for normal attacks (eg. Scratch Punch) Defense the base damage resistance against normal attacks SP Atk special attack the base modifier for special attacks (e.g. fire blast bubble beam) SP Def the base damage resistance against special attacks Speed determines which pokemon attacks first each round  The data for this table has been acquired from several different sites including   pokemon.com pokemondb bulbapeida  One question has been answered with this database The type of a pokemon cannot be inferred only by it's Attack and Deffence. It would be worthy to find which two variables can define the type of a pokemon if any. Two variables can be plotted in a 2D space and used as an example for machine learning. This could mean the creation of a visual example any geeky Machine Learning class would love.,CSV,,"[popular culture, games and toys, video games]",CC0,,,16642,133457,0.0419921875,721 Pokemon with stats and types,Pokemon with stats,https://www.kaggle.com/abcsds/pokemon,Mon Aug 29 2016
,Gustavo Bonesso,[],[],Context My objective sharing this data is to make studies about stock quotes using real data from the Brazilian stock market. Content This is the daily stock quotes from B3 for the 2017 year. B3 is the unique stock exchange in Brazil here we don't have competition in this sector as in USA. Acknowledgements This data is directly extracted from B3 site without changes. ,Other,,[brazil],CC0,,,47,635,221,Daily historical data,Brazillian Stock Quotes,https://www.kaggle.com/gbonesso/b3-stock-quotes,Wed Jan 24 2018
,Marcos Boaglio,[],[],"This is a simplified version of the ""Human Activity Recognition Using Smartphones Data Set "" that can be found in (https//archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) The sole purpose of this simplification is to use it as a teaching tool at the introductory Machine Learning Course of Universidad de Palermo  (Buenos Aires Argentina).  URL of the competition https//www.kaggle.com/t/5c27656d61ec4808bcbddd67ac1fdc5a There are no claimed rights of any kind please refer to original dataset (link above) in order to get the full dataset. Abstract Human Activity Recognition database built from the recordings of 30 subjects performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. Data Set Characteristics  Multivariate Time-Series Source Jorge L. Reyes-Ortiz(12) Davide Anguita(1) Alessandro Ghio(1) Luca Oneto(1) and Xavier Parra(2) 1 - Smartlab - Non-Linear Complex Systems Laboratory DITEN - Università degli Studi di Genova Genoa (I-16145) Italy.  2 - CETpD - Technical Research Centre for Dependency Care and Autonomous Living Universitat Politècnica de Catalunya (BarcelonaTech). Vilanova i la Geltrú (08800) Spain activityrecognition '@' smartlab.ws Data Set Information The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING WALKING_UPSTAIRS WALKING_DOWNSTAIRS SITTING STANDING LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets where 70% of the volunteers was selected for generating the training data and 30% the test data.  The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal which has gravitational and body motion components was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components therefore a filter with 0.3 Hz cutoff frequency was used. From each window a vector of features was obtained by calculating variables from the time and frequency domain. Check the README.txt file for further details about this dataset.  An updated version of this dataset can be found at (https//archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones) It includes labels of postural transitions between activities and also the full raw inertial signals instead of the ones pre-processed into windows.",CSV,,[],Other,,,122,1382,5,Recordings of subjects performing activities while carrying inertial sensors.,Simplified Human Activity Recognition w/Smartphone,https://www.kaggle.com/mboaglio/simplifiedhuarus,Thu Jan 04 2018
,PromptCloud,"[address, city, country, crawl_date, hotel_brand, hotel_description, hotel_facilities, hotel_star_rating, image_count, latitude, locality, longitude, pageurl, property_id, property_name, property_type, province, qts, room_count, room_type, similar_hotel, site_review_count, site_review_rating, site_stay_review_rating, sitename, special_tag, state, uniq_id, zone]","[string, string, string, dateTime, string, string, string, string, numeric, numeric, string, numeric, string, numeric, string, numeric, string, dateTime, numeric, string, string, numeric, numeric, string, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 94000 hotels) that was created by extracting data from Booking.com a leading travel portal. Content This dataset has following fields  address city country crawl_date hotel_brand hotel_description hotel_facilities hotel_star_rating image_count latitude locality longitude pageurl property_id property_name property_type province qts room_count room_type similar_hotel site_review_count site_review_rating site_stay_review_rating sitename special_tag state uniq_id zone  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the property ratings and property type can be performed.,CSV,,"[hotels, internet]",CC4,,,250,1739,12,"6,000 Indian hotels on Booking.com",Indian hotels on Booking.com,https://www.kaggle.com/PromptCloudHQ/indian-hotels-on-bookingcom,Sat Sep 16 2017
,Rachael Tatman,"[DATE, PRCP, TMAX, TMIN, RAIN]","[dateTime, numeric, numeric, numeric, boolean]",Context Besides coffee grunge and technology companies one of the things that Seattle is most famous for is how often it rains. This dataset contains complete records of daily rainfall patterns from January 1st 1948 to December 12 2017.  Content This data was collected at the Seattle-Tacoma International Airport. The dataset contains five columns   DATE = the date of the observation PRCP = the amount of precipitation in inches  TMAX = the maximum temperature for that day in degrees Fahrenheit TMIN = the minimum temperature for that day in degrees Fahrenheit RAIN = TRUE if rain was observed on that day FALSE if it was not  Acknowledgements This dataset was compiled by NOAA and is in the public domain. Inspiration  Can you use this dataset to build a model of whether it will rain on a specific day given information on the previous days? Is there a correlation between the minimum and maximum temperature? Can you predict one given the other? Can you model changes in the amount of precipitation over time? Is there seasonality?   ,CSV,,"[united states, north america, climate, weather]",CC0,,,934,4710,0.7265625,"More than 25,000 consecutive days of Seattle weather data",Did it rain in Seattle? (1948-2017),https://www.kaggle.com/rtatman/did-it-rain-in-seattle-19482017,Thu Dec 21 2017
,City of New York,"[Fiscal Year, Agency Name, Last Name, First Name, Mid Init, Agency Start Date, Work Location Borough, Title Description, Leave Status as of June 30, Base Salary, Pay Basis, Regular Hours, Regular Gross Paid, OT Hours, Total OT Paid, Total Other Pay]","[numeric, string, string, string, string, dateTime, string, string, string, string, string, numeric, string, numeric, string, string]","Context This dataset contains the salary pay rate and total compensation of every New York City employee. In this dataset this information is provided for the 2014 2015 2016 and 2017 fiscal years and provides a transparent lens into who gets paid how much and for what. Note that fiscal years in the New York City budget cycle start on July 1st and end on June 30th (see here). That means that this dataset contains in its sum compensation information for all City of New York employees for the period July 1 2014 to June 30 2017. Content This dataset provides columns for fiscal year employee name the city department they work for their job title and various fields describing their compensation. The most important of these fields is ""Regular Gross Pay"" which provides that employee's total compensation. Acknowledgements This information was published as-is by the City of New York. Inspiration  How many people do the various city agencies employ and how much does each department spend on salary in total? What are the most numerous job titles in civic government employment? Where does overtime pay seem to be especially common? How much of it is there? How do New York City employee salaries compare against salaries of city employees in Chicago? Is the difference more or less than the difference in cost of living between the two cities? ",CSV,,"[cities, money]",CC0,,,593,3512,395,Salaries paid to New York City employees over four years,New York City - Citywide Payroll Data,https://www.kaggle.com/new-york-city/nyc-citywide-payroll-data,Wed Sep 06 2017
,United States Air Force,[],[],Context THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data.  Each theater of warfare has a separate data file in addition to a THOR Overview. Content 4.8 million rows with 47 columns describing each run. See the data dictionary here. Acknowledgements THOR is a dataset project initiated by  Lt Col Jenns Robertson and continued in partnership with Data.mil  an experimental project created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.  Inspiration  Which campaigns saw the heaviest bombings? Which months saw the most runs? What were the most used aircraft? ,CSV,,[military],CC0,,,156,1514,2048,Details on 4.8 Million Runs,Vietnam War Bombing Operations,https://www.kaggle.com/usaf/vietnam-war-bombing-operations,Thu Sep 14 2017
,Yun Solutions,"[, timeStamp, tripID, accData, gps_speed, battery, cTemp, dtc, eLoad, iat, imap, kpl, maf, rpm, speed, tAdv, tPos, deviceID]","[numeric, dateTime, numeric, string, numeric, numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric]",We are updating this dataset everymonth. Access updated data for every month here Context The dataset is proprietary  data of Yun Solutions collected from Beta Testing phase. The dataset contains sensor and OBD data for over 4 months and around 30 vehicles. Content The Dataset contains Vehicle telematics and Driving data. Metadata is explained in the file description on the data tab. Acknowledgements We look forward to analysis on the data. Inspiration We want to make this data accessible for learning and analysis. ,CSV,,"[india, business, internet]",CC4,,,130,1354,587,Vehicle and Driving Data,levin vehicle telematics,https://www.kaggle.com/yunlevin/levin-vehicle-telematics,Sat Feb 10 2018
,Cornell University,"[u0, BIANCA, m0, 10 things i hate about you, f, 4]","[string, string, string, string, string, numeric]","Context This corpus contains a metadata-rich collection of fictional conversations extracted from raw movie scripts  220579 conversational exchanges between 10292 pairs of movie characters involves 9035 characters from 617 movies in total 304713 utterances movie metadata included genres release year IMDB rating number of IMDB votes IMDB rating character metadata included gender (for 3774 characters) position on movie credits (3321 characters)  Content In all files the original field separator was "" +++$+++ "" and have been converted to tabs (\t). Additionally the original file encoding was ISO-8859-2. It's possible that the field separator conversion and decoding may have left some artifacts.   movie_titles_metadata.txt contains information about each movie title fields  movieID  movie title movie year  IMDB rating no. IMDB votes genres in the format ['genre1''genre2'É'genreN'] movie_characters_metadata.txt contains information about each movie character fields characterID character name movieID movie title gender (""?"" for unlabeled cases) position in credits (""?"" for unlabeled cases)  movie_lines.txt contains the actual text of each utterance fields lineID characterID (who uttered this phrase) movieID character name text of the utterance movie_conversations.txt the structure of the conversations fields characterID of the first character involved in the conversation characterID of the second character involved in the conversation movieID of the movie in which the conversation occurred list of the utterances that make the conversation in chronological  order ['lineID1''lineID2'É'lineIDN'] has to be matched with movie_lines.txt to reconstruct the actual content raw_script_urls.txt the urls from which the raw sources were retrieved  Acknowledgements This corpus comes from the paper ""Chameleons in imagined conversations A new approach to understanding coordination of linguistic style in dialogs"" by Cristian Danescu-Niculescu-Mizil and Lillian Lee.   The paper and up-to-date data can be found here http//www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html Please see the README for more information on the authors' collection procedures. The file formats were converted to TSV and may contain a few errors Inspiration  What are all of these imaginary people talking about? Are they representative of how real people communicate? Can you identify themes in movies from certain writers or directors?  How does the dialog change between characters? ",Other,,"[film, linguistics]",Other,,,734,4894,29,A metadata-rich collection of fictional conversations from raw movie scripts,Movie Dialog Corpus,https://www.kaggle.com/Cornell-University/movie-dialog-corpus,Wed Jul 12 2017
,Sohier Dane,[],[],The National Oceanic and Atmospheric Administration (NOAA) National Status and Trends (NS&T) Mussel Watch Program is a contaminant monitoring program that started in 1986. It is the longest running continuous contaminant monitoring program of its kind in the United States. Mussel Watch monitors the concentration of contaminants in bivalves (mussels and oysters) and sediments in the coastal waters of the U.S. including the Great Lakes to monitor bivalve health and by extension the health of their local and regional environment. Mussel Watch consults with experts to determine appropriate contaminants to monitor; these include dichlorodiphenyltrichloroethane (DDT) polycyclic aromatic hydrocarbons (PAHs) and polychlorinated biphenyls (PCBs). As of 2008 Mussel Watch monitors approximately 140 analytes. In addition to the effects of contaminants Mussel Watch is able to assess the effects of natural disasters such as the 2005 Hurricane Katrina and environmental disasters such as the 2010 Deepwater Horizon oil spill. Data collected by Mussel Watch can also be used to monitor the effectiveness of coastal remediation. The Mussel Watch Program utilized its 20 years of monitoring data to effectively analyze the impacts of Hurricane Katrina and has affected regulatory decisions based on the data it has collected on bivalve parasites. You can find additional details about the history of the program here. Data Notes  This version has been consolidated and lightly cleaned from its original format.  It was not possible to acquire data for all sites in mussel watch while preparing the dataset. The pdf manuals are technically for specific sites and may not map perfectly to the data here. You can find manuals specific to each site here if need be.   Acknowledgements This dataset is the result of the work of generations of scientists working for NOAA. You can find the original data here. ,CSV,,"[ecology, pollution]",CC0,,,229,1275,156,The longest running contaminant monitoring program in U.S. coastal waters,Mussel Watch,https://www.kaggle.com/sohier/mussel-watch,Mon Sep 18 2017
,Wilian Osaku,"[TYPE, YEAR, MONTH, DAY, HOUR, MINUTE, HUNDRED_BLOCK, NEIGHBOURHOOD, X, Y, Latitude, Longitude]","[string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric]","Content The data comes from the Vancouver Open Data Catalogue. It was extracted on 2017-07-18 and it contains 530652 records from 2003-01-01 to 2017-07-13. The original data set contains coordinates in UTM Zone 10 (columns X and Y). I also included Latitude and Longitude which I converted using this spreadsheet that can be found here. There's also a Google Trends data that shows how often a search-term is entered relative to the total search-volume. From Google Trends  ""Numbers represent search interest relative to the highest point on the chart for the given region and time. A value of 100 is the peak popularity for the term. A value of 50 means that the term is half as popular. Likewise a score of 0 means the term was less than 1% as popular as the peak.""  Original data for search term ""crime"" location British Columbia https//trends.google.com/trends/explore?date=2004-01-01%202017-06-30&geo=CA-BC&q=crime Acknowledgements Photo By Charles de Jesus [CC BY 3.0 (http//creativecommons.org/licenses/by/3.0)] via Wikimedia Commons",CSV,,[crime],ODbL,,,600,4069,56,Data of crimes in Vancouver (Canada) from 2003 to 2017,Crime in Vancouver,https://www.kaggle.com/wosaku/crime-in-vancouver,Tue Nov 07 2017
,Centers for Disease Control and Prevention,"[_STATE, _GEOSTR, _DENSTR2, PRECALL, REPNUM, REPDEPTH, FMONTH, IDATE, IMONTH, IDAY, IYEAR, INTVID, DISPCODE, SEQNO, _PSU, NATTMPTS, NRECSEL, NRECSTR, CTELENUM, CELLFON, PVTRESID, NUMADULT, NUMMEN, NUMWOMEN, GENHLTH, PHYSHLTH, MENTHLTH, POORHLTH, HLTHPLN1, PERSDOC2, MEDCOST, CHECKUP1, BPHIGH4, BPMEDS, BLOODCHO, CHOLCHK, TOLDHI2, CVDINFR4, CVDCRHD4, CVDSTRK3, ASTHMA3, ASTHNOW, CHCSCNCR, CHCOCNCR, CHCCOPD, HAVARTH3, ADDEPEV2, CHCKIDNY, CHCVISON, DIABETE3, SMOKE100, SMOKDAY2, STOPSMK2, LASTSMK2, USENOW3, AGE, HISPANC2, MRACE, ORACE2, VETERAN3, MARITAL, CHILDREN, EDUCA, EMPLOY, INCOME2, WEIGHT2, HEIGHT3, CTYCODE1, NUMHHOL2, NUMPHON2, CPDEMO1, CPDEMO2, CPDEMO3, CPDEMO4, RENTHOM1, SEX, PREGNANT, FRUITJU1, FRUIT1, FVBEANS, FVGREEN, FVORANG, VEGETAB1, EXERANY2, EXRACT01, EXEROFT1, EXERHMM1, EXRACT02, EXEROFT2, EXERHMM2, STRENGTH, QLACTLM2, USEEQUIP, LMTJOIN3, ARTHDIS2, ARTHSOCL, JOINPAIN, SEATBELT, FLUSHOT5, FLSHTMY2]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The objective of the BRFSS is to collect uniform state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases injuries and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use health care coverage HIV/AIDS knowledge or prevention physical activity and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey. The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors chronic health conditions and use of preventive services. Established in 1984 with 15 states BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400000 adult interviews each year making it the largest continuously conducted health survey system in the world. Content  Each year contains a few hundred columns. Please see one of the annual code books for complete details. These CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result. If you like this dataset you might also like the data for 2001-2010.  Acknowledgements This dataset was released by the CDC. You can find the original dataset and additional years of data here.,CSV,,"[mental health, public health]",CC0,,,538,4091,3072,Public health surveys of 400k people from 2011-2015,Behavioral Risk Factor Surveillance System,https://www.kaggle.com/cdc/behavioral-risk-factor-surveillance-system,Thu Aug 24 2017
,GrishaSizov,"[, Department code, Department, Constituency code, Constituency, Commune code, Commune, Polling station, Registered, Abstentions, % Abs/Reg, Voters, % Vot/Reg, None of the above(NOTA), % NOTA/Reg, % NOTA/Vot, Nulls, % Nulls/Reg, % Nulls/Vot, Expressed, % Exp/Reg, % Exp/Vot, Signboard, Sex, Surname, First name, Voted, % Votes/Reg, % Votes/Exp, INSEE code, Coordinates, Polling station name, Address, Postal code, City, Poll.St.-unique]","[numeric, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, string, string]",Presidential elections have just finished in France with two rounds on April 23rd and May 7th 2017. The results of the elections are available online for each ~67000 polling stations around the country. This data can be used to gain insights about both the electorate and the candidates. Examples of basic questions worth looking at are   How are candidates' scores correlated with each other? Can you infer 'political affinity' of the candidates just looking at the data without any other a priori knowledge?  How are scores of various candidates correlated with the turnout? Supporters of which candidate are the most 'participative'?  How did the votes get redistributed from the first to the second round? Can you build an a posteriori predictive model of the second round results taking as an input the results of the first round ? The data provides coordinates of each polling station. Can you gain any insight from the geography?  Some preliminary analysis is described in two blog posts here and here. This dataset is inspired by an analogous one for the 2016 US elections by Ben Hamner.,CSV,,[politics],Other,,,286,2307,228,Results of both rounds at polling station level,"French Presidential Election, 2017",https://www.kaggle.com/grishasizov/frenchpresidentialelection2017,Fri May 26 2017
,Sohier Dane,[],[],This dataset contains the entire contents of each major API data set published by the US Energy Information Administration. That's everything from the hourly electricity consumption in the United States to natural gas futures contracts.  This data has been lightly reprocessed from the EIA's bulk download facility by converting each file from a zip of jsons into a single json with the series name as the keys to the specific time series.  Please note that there are thousands of time series in here and many of them may still require additional cleaning to deal with odd date formats and so on. The file preview is unable to show a complete listing. You can usually find full details of a given time series in the 'description' field.,{}JSON,,"[government agencies, energy]",Other,,,390,3889,911,All data published by the US Energy Information Administration,US Energy Statistics,https://www.kaggle.com/sohier/us-energy-statistics,Tue Aug 22 2017
,Netflix,[],[],"Context Netflix held the Netflix Prize open competition for the best algorithm to predict user ratings for films. The grand prize was $1000000 and was won by BellKor's Pragmatic Chaos team. This is the dataset that was used in that competition. Content This comes directly from the README TRAINING DATASET FILE DESCRIPTION The file ""training_set.tar"" is a tar of a directory containing 17770 files one per movie.  The first line of each file contains the movie id followed by a colon.  Each subsequent line in the file corresponds to a rating from a customer and its date in the following format CustomerIDRatingDate  MovieIDs range from 1 to 17770 sequentially. CustomerIDs range from 1 to 2649429 with gaps. There are 480189 users. Ratings are on a five star (integral) scale from 1 to 5. Dates have the format YYYY-MM-DD.  MOVIES FILE DESCRIPTION Movie information in ""movie_titles.txt"" is in the following format MovieIDYearOfReleaseTitle  MovieID do not correspond to actual Netflix movie ids or IMDB movie ids. YearOfRelease can range from 1890 to 2005 and may correspond to the release of corresponding DVD not necessarily its theaterical release. Title is the Netflix movie title and may not correspond to  titles used on other sites.  Titles are in English.  QUALIFYING AND PREDICTION DATASET FILE DESCRIPTION The qualifying dataset for the Netflix Prize is contained in the text file ""qualifying.txt"".  It consists of lines indicating a movie id followed by a colon and then customer ids and rating dates one per line for that movie id. The movie and customer ids are contained in the training set.  Of course the ratings are withheld. There are no empty lines in the file. MovieID1 CustomerID11Date11 CustomerID12Date12 ... MovieID2 CustomerID21Date21 CustomerID22Date22 For the Netflix Prize your program must predict the all ratings the customers gave the movies in the qualifying dataset based on the information in the training dataset. The format of your submitted prediction file follows the movie and customer id date order of the qualifying dataset.  However your predicted rating takes the place of the corresponding customer id (and date) one per line. For example if the qualifying dataset looked like 111 32452005-12-19 56662005-12-23 67892005-03-14 225 12342005-05-26 34562005-11-07 then a prediction file should look something like 111 3.0 3.4 4.0 225 1.0 2.0 which predicts that customer 3245 would have rated movie 111 3.0 stars on the 19th of Decemeber 2005 that customer 5666 would have rated it slightly higher at 3.4 stars on the 23rd of Decemeber 2005 etc. You must make predictions for all customers for all movies in the qualifying dataset. THE PROBE DATASET FILE DESCRIPTION To allow you to test your system before you submit a prediction set based on the qualifying dataset we have provided a probe dataset in the file ""probe.txt"". This text file contains lines indicating a movie id followed by a colon and then customer ids one per line for that movie id. MovieID1 CustomerID11 CustomerID12 ... MovieID2 CustomerID21 CustomerID22 Like the qualifying dataset the movie and customer id pairs are contained in the training set.  However unlike the qualifying dataset the ratings (and dates) for each pair are contained in the training dataset. If you wish you may calculate the RMSE of your predictions against those ratings and compare your RMSE against the Cinematch RMSE on the same data.  See http//www.netflixprize.com/faq#probe for that value. Acknowledgements The training data came in 17000+ files. In the interest of keeping files together and file sizes as low as possible I combined them into four text files combined_data_(1234).txt  The contest was originally hosted at http//netflixprize.com/index.html The dataset was downloaded from https//archive.org/download/nf_prize_dataset.tar Inspiration This is a fun dataset to work with. You can read about the winning algorithm by BellKor's Pragmatic Chaos here",Other,,"[film, artificial intelligence]",Other,,,3982,40870,2048,Dataset from Netflix's competition to improve their reccommendation algorithm,Netflix Prize data,https://www.kaggle.com/netflix-inc/netflix-prize-data,Thu Jul 20 2017
,Sharan Naribole,"[, CASE_STATUS, EMPLOYER_NAME, SOC_NAME, JOB_TITLE, FULL_TIME_POSITION, PREVAILING_WAGE, YEAR, WORKSITE, lon, lat]","[numeric, string, string, string, string, string, numeric, numeric, string, numeric, numeric]",Context H-1B visas are a category of employment-based non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position. The following articles contain more information about the H-1B visa process  What is H1B LCA ? Why file it ? Salary Processing times – DOL H1B Application Process Step by Step Guide  Content This dataset contains five year's worth of H-1B petition data with approximately 3 million records overall. The columns in the dataset include case status employer name worksite coordinates job title prevailing wage occupation code and year filed. For more information on individual columns refer to the column metadata. A detailed description of the underlying raw dataset is available in an official data dictionary. Acknowledgements The Office of Foreign Labor Certification (OFLC) generates program data including data about H1-B visas. The disclosure data updated annually and is available online. The raw data available is messy and not immediately suitable analysis. A set of data transformations were performed making the data more accessible for quick exploration. To learn more refer to this blog post and to the complimentary R Notebook. Inspiration  Is the number of petitions with Data Engineer job title increasing over time? Which part of the US has the most Hardware Engineer jobs? Which industry has the most number of Data Scientist positions? Which employers file the most petitions each year? ,CSV,,"[law, international relations]",CC4,,,8608,71400,469,3 million petitions for H-1B visas,H-1B Visa Petitions 2011-2016,https://www.kaggle.com/nsharan/h-1b-visa,Tue Feb 28 2017
,Murder Accountability Project,"[Record ID, Agency Code, Agency Name, Agency Type, City, State, Year, Month, Incident, Crime Type, Crime Solved, Victim Sex, Victim Age, Victim Race, Victim Ethnicity, Perpetrator Sex, Perpetrator Age, Perpetrator Race, Perpetrator Ethnicity, Relationship, Weapon, Victim Count, Perpetrator Count, Record Source]","[numeric, string, string, string, string, string, numeric, string, numeric, string, string, string, numeric, string, string, string, numeric, string, string, string, string, numeric, numeric, string]",Content The Murder Accountability Project is the most complete database of homicides in the United States currently available. This dataset includes murders from the FBI's Supplementary Homicide Report from 1976 to the present and Freedom of Information Act data on more than 22000 homicides that were not reported to the Justice Department. This dataset includes the age race sex ethnicity of victims and perpetrators in addition to the relationship between the victim and perpetrator and weapon used. Acknowledgements The data was compiled and made available by the Murder Accountability Project founded by Thomas Hargrove.,CSV,,[crime],CC4,,,9413,56071,107,Can you develop an algorithm to detect serial killer activity?,"Homicide Reports, 1980-2014",https://www.kaggle.com/murderaccountability/homicide-reports,Fri Feb 10 2017
,City of Los Angeles,"[DR Number, Date Reported, Date Occurred, Time Occurred, Area ID, Area Name, Reporting District, Crime Code, Crime Code Description, MO Codes, Victim Age, Victim Sex, Victim Descent, Premise Code, Premise Description, Weapon Used Code, Weapon Description, Status Code, Status Description, Crime Code 1, Crime Code 2, Crime Code 3, Crime Code 4, Address, Cross Street, Location ]","[numeric, dateTime, dateTime, numeric, numeric, string, numeric, numeric, string, numeric, numeric, string, string, numeric, string, numeric, string, string, string, numeric, string, string, string, string, string, string]",This dataset reflects incidents of crime in the City of Los Angeles dating back to 2010. This data is transcribed from original crime reports that are typed on paper and therefore there may be some inaccuracies within the data. Some location fields with missing data are noted as (0° 0°). Address fields are only provided to the nearest hundred block in order to maintain privacy. Reporting District Shapefile Attributes REPDIST Number min 101   max 2199   avg 1162   count 1135 PREC Number min 1   max 21   avg 11   count 1135 APREC Text PACIFIC (74) DEVONSHIRE (70) WEST LOS ANGELES (69) NORTHEAST (64) HOLLENBECK (63) MISSION (62)... (15 more) BUREAU Text VALLEY BUREAU (399) WEST BUREAU (288) CENTRAL BUREAU (267) SOUTH BUREAU (181) BASICCAR Text 8A29 (17) 17A35 (17) 1A1 (15) 17A49 (14) 16A35 (14) 14A73 (14) 19A43 (13) 8A95 (12) 19A7 (12)... (160 more) TOOLTIP Text Bureau SOUTH BUREAU\nDistrict 562\nDivision HARBOR (1)... (1134 more) OBJECTID Unique ID Acknowledgements This dataset was kindly released by the City of Los Angeles. You can find the original dataset updated weekly here. Inspiration  Some of the MO codes seem unlikely or unrelated to crime. Can you find out what would lead to the use of code 0107 God or 1021 Repair? ,Other,,[crime],CC0,,,1359,10106,360,Crime data from 2010 through September 2017,Crime in Los Angeles,https://www.kaggle.com/cityofLA/crime-in-los-angeles,Mon Sep 18 2017
,romainvincent,"[craft_cost, date, deck_archetype, deck_class, deck_format, deck_id, deck_set, deck_type, rating, title, user, card_0, card_1, card_2, card_3, card_4, card_5, card_6, card_7, card_8, card_9, card_10, card_11, card_12, card_13, card_14, card_15, card_16, card_17, card_18, card_19, card_20, card_21, card_22, card_23, card_24, card_25, card_26, card_27, card_28, card_29]","[numeric, dateTime, string, string, string, numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Hearthstone is a very popular collectible card game published by Blizzard Entertainment in 2014. The goal of the game consists in building a 30 cards deck in order to beat your opponent. A few weeks ago I decided to download all the decks posted by players at Hearthpwn. The code to download the data is available here. Content This upload is composed of two files  data.json / data.csv Contains the actual Hearthstone deck records. Each record features   date (str)  the date of publication (or last update) of the deck. user (str)  the user who uploaded the deck. deck_class (str)  one of the nine character class in Hearthstone (Druid Priest ...). deck_archetype (str)  the theme of deck labelled by the user (Aggro Druid Dragon Priest ...). deck_format (str)  the game format of the deck on the day data was recorded (W for ""Wild"" or S for ""Standard""). deck_set (str)  the latest expansion published prior the deck publication (Naxxramas TGT Launch ...). deck_id (int)  the ID of the deck. deck_type (str)  the type of the deck labelled by the user  Ranked Deck  a deck played on ladder. Theorycraft  a deck built with unreleased cards to get a gist of the future metagame. PvE Adventure  a deck built to beat the bosses in adventure mode. Arena  a deck built in arena mode. Tavern Brawl  a deck built for the weekly tavern brawl mode. Tournament  a deck brought at tournament by a pro-player. None  the game type was not mentioned. rating (int)  the number of upvotes received by that deck. title (str)  the name of the deck. craft_cost (int)  the amount of dust (in-game craft material) required to craft the deck. cards (list)  a list of 30 card ids. Each ID can be mapped to the card description using the reference file.  refs.json Contains the reference to the cards played in Hearthstone. This file was originally proposed on HearthstoneJSON. Each record features a lot of informations about the cards I'll list the most important   dbfId (int)  the id of the card (the one used in data.json). rarity (str)  the rarity of the card (EPIC RARE ...). cardClass (str)  the character class (WARLOCK PRIEST ...). artist (str)  the artist behind the card's art. collectible (bool)  whether or not the card can be collected. cost (int)  the card play cost. health (int)  the card health (if it's a minion). attack (int)  the card attack (if it's a minion). name (str)  the card name. flavor (str)  the card's flavor text. set (str)  the set / expansion which featured this card. text (int)  the card's text. type (str)  the card's type (MINION SPELL ...). race (str)  the card's race (if it's a minion). set (str)  the set / expansion which featured this card. ...  If you need help cleaning the data take a look at my start over kernel! What you could do   Try to predict the deck archetype based on the cards features in the deck. Seek relationships between the cost of the deck and it's popularity. Describe the evolution of the meta-game over-time. Find out unbalanced (overplayed) cards ",CSV,,[video games],Other,,,481,8015,78,"346,242 decks representing more than 3 years of gameplay!",History of Hearthstone,https://www.kaggle.com/romainvincent/history-of-hearthstone,Thu Jul 06 2017
,Machine Learning Group - ULB,"[Time, V1, V2, V3, V4, V5, V6, V7, V8, V9, V10, V11, V12, V13, V14, V15, V16, V17, V18, V19, V20, V21, V22, V23, V24, V25, V26, V27, V28, Amount, Class]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The datasets contains transactions made by credit cards in September 2013 by european cardholders.  This dataset presents transactions that occurred in two days where we have 492 frauds out of 284807 transactions. The dataset is highly unbalanced the positive class (frauds) account for 0.172% of all transactions. It contains only numerical input variables which are the result of a PCA transformation. Unfortunately due to confidentiality issues we cannot provide the original features and more background information about the data. Features V1 V2 ... V28 are the principal components obtained with PCA the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.  Given the class imbalance ratio we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification. The dataset has been collected and analysed during a  research collaboration of Worldline and the Machine Learning Group (http//mlg.ulb.ac.be) of ULB (Université Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http//mlg.ulb.ac.be/BruFence and http//mlg.ulb.ac.be/ARTML Please cite Andrea Dal Pozzolo Olivier Caelen Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM) IEEE 2015,CSV,,"[crime, finance]",ODbL,,,53238,442912,144,Anonymized credit card transactions labeled as fraudulent or genuine,Credit Card Fraud Detection,https://www.kaggle.com/mlg-ulb/creditcardfraud,Sat Nov 05 2016
,rhammell,[],[],"Context Satellite imagery provides unique insights into various markets including agriculture defense and intelligence energy and finance. New commercial imagery providers such as Planet and BlackSky are using constellations of small satellites to exponentially increase the amount of images of the earth captured every day.  This flood of new imagery is outgrowing the ability for organizations to manually look at each image that gets captured and there is a need for machine learning and computer vision algorithms to help automate the analysis process.  The aim of this dataset is to help address the difficult task of detecting the location of large ships in satellite images. Automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis. Continusouly updates will be made to this dataset as new Planet imagery released. Current images were collected as late as September 2017.  Content The dataset consists of image chips extracted from Planet satellite imagery collected over the San Franciso Bay area.  It includes 2800 80x80 RGB images labeled with either a ""ship"" or ""no-ship"" classification. Image chips were derived from PlanetScope full-frame visual scene products which are orthorectified to a 3 meter pixel size.  Provided is a zipped directory shipsnet.7z that contains the entire dataset as .png image chips. Each individual image filename follows a specific format {label} __ {scene id} __ {longitude} _ {latitude}.png  label Valued 1 or 0 representing the ""ship"" class and ""no-ship"" class respectively.  scene id The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene. longitude_latitude The longitude and latitude coordinates of the image center point with values separated by a single underscore.   The dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data label scene_ids and location lists.  The pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values the next 6400 the green and the final 6400 the blue. The image is stored in row-major order so that the first 80 entries of the array are the red channel values of the first row of the image. The list values at index i in labels scene_ids and locations each correspond to the i-th image in the data list. Class Labels The ""ship"" class includes 700 images. Images in this class are near-centered on the body of a single ship. Ships of different ship sizes orientations and atmospheric collection conditions are included. Example images from this class are shown below.   The ""no-ship"" class includes 2100 images. A third of these are a random sampling of different landcover features - water vegetion bare earth buildings etc. - that do not include any portion of an ship. The next third are ""partial ships"" that contain only a portion of an ship but not enough to meet the full definition of the ""ship"" class. The last third are images that have previously been mislabeled by machine learning models typically caused by bright pixels or strong linear features. Example images from this class are shown below.  Acknowledgements Satellite imagery used to build this dataset is made available through Planet's Open California dataset which is openly licensed. As such this dataset is also available under the same CC-BY-SA license. Users can sign up for a free Planet account to search view and download thier imagery and gain access to their API. ",Other,,[business],CC4,,,964,8296,148,Classify ships in San Franciso Bay using Planet satellite imagery,Ships in Satellite Imagery,https://www.kaggle.com/rhammell/ships-in-satellite-imagery,Tue Nov 14 2017
,Paul Rossotti,"[gmDate, gmTime, seasTyp, offLNm, offFNm, teamAbbr, teamConf, teamDiv, teamLoc, teamRslt, teamMin, teamDayOff, teamPTS, teamAST, teamTO, teamSTL, teamBLK, teamPF, teamFGA, teamFGM, teamFG%, team2PA, team2PM, team2P%, team3PA, team3PM, team3P%, teamFTA, teamFTM, teamFT%, teamORB, teamDRB, teamTRB, teamPTS1, teamPTS2, teamPTS3, teamPTS4, teamPTS5, teamPTS6, teamPTS7, teamPTS8, teamTREB%, teamASST%, teamTS%, teamEFG%, teamOREB%, teamDREB%, teamTO%, teamSTL%, teamBLK%, teamBLKR, teamPPS, teamFIC, teamFIC40, teamOrtg, teamDrtg, teamEDiff, teamPlay%, teamAR, teamAST/TO, teamSTL/TO, opptAbbr, opptConf, opptDiv, opptLoc, opptRslt, opptMin, opptDayOff, opptPTS, opptAST, opptTO, opptSTL, opptBLK, opptPF, opptFGA, opptFGM, opptFG%, oppt2PA, oppt2PM, oppt2P%, oppt3PA, oppt3PM, oppt3P%, opptFTA, opptFTM, opptFT%, opptORB, opptDRB, opptTRB, opptPTS1, opptPTS2, opptPTS3, opptPTS4, opptPTS5, opptPTS6, opptPTS7, opptPTS8, opptTREB%, opptASST%, opptTS%]","[dateTime, dateTime, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Dataset is based on box score and standing statistics from the NBA. Calculations such as number of possessions floor impact counter strength of schedule and simple rating system are performed.   Finally extracts are created based on a perspective  teamBoxScore.csv communicates game data from each teams perspective officialBoxScore.csv communicates game data from each officials perspective playerBoxScore.csv communicates game data from each players perspective standing.csv communicates standings data for each team every day during the season  Content Data Sources Box score and standing statistics were obtained by a Java application using RESTful APIs provided by xmlstats. Calculation Sources Another Java application performs advanced calculations on the box score and standing data.  Formulas for these calculations were primarily obtained from these sources  https//basketball.realgm.com/info/glossary https//www.nbastuffer.com/team-evaluation-metrics/ https//www.basketball-reference.com/about/glossary.html  Inspiration Favoritism Does a referee impact the number of fouls made against a player or the pace of a game? Forcasting Can the aggregated points scored by and against a team along with their strength of schedule be used to determine their projected winning percentage for the season? Predicting the Past For a given game can games played earlier in the season help determine how a team will perform? Lots of data elements and possibilities.  Let your imagination roam!,CSV,,"[basketball, sports]",CC4,,,577,3769,5,Box Scores and Standings with advanced calculations applied,NBA Enhanced Box Score and Standings Stats,https://www.kaggle.com/pablote/nba-enhanced-stats,Sat Feb 17 2018
,Gustavo Bonesso,"[NU_INSCRICAO, NU_ANO, CO_MUNICIPIO_RESIDENCIA, NO_MUNICIPIO_RESIDENCIA, CO_UF_RESIDENCIA, SG_UF_RESIDENCIA, NU_IDADE, TP_SEXO, TP_ESTADO_CIVIL, TP_COR_RACA, TP_NACIONALIDADE, CO_MUNICIPIO_NASCIMENTO, NO_MUNICIPIO_NASCIMENTO, CO_UF_NASCIMENTO, SG_UF_NASCIMENTO, TP_ST_CONCLUSAO, TP_ANO_CONCLUIU, TP_ESCOLA, TP_ENSINO, IN_TREINEIRO, CO_ESCOLA, CO_MUNICIPIO_ESC, NO_MUNICIPIO_ESC, CO_UF_ESC, SG_UF_ESC, TP_DEPENDENCIA_ADM_ESC, TP_LOCALIZACAO_ESC, TP_SIT_FUNC_ESC, IN_BAIXA_VISAO, IN_CEGUEIRA, IN_SURDEZ, IN_DEFICIENCIA_AUDITIVA, IN_SURDO_CEGUEIRA, IN_DEFICIENCIA_FISICA, IN_DEFICIENCIA_MENTAL, IN_DEFICIT_ATENCAO, IN_DISLEXIA, IN_DISCALCULIA, IN_AUTISMO, IN_VISAO_MONOCULAR, IN_OUTRA_DEF, IN_SABATISTA, IN_GESTANTE, IN_LACTANTE, IN_IDOSO, IN_ESTUDA_CLASSE_HOSPITALAR, IN_SEM_RECURSO, IN_BRAILLE, IN_AMPLIADA_24, IN_AMPLIADA_18, IN_LEDOR, IN_ACESSO, IN_TRANSCRICAO, IN_LIBRAS, IN_LEITURA_LABIAL, IN_MESA_CADEIRA_RODAS, IN_MESA_CADEIRA_SEPARADA, IN_APOIO_PERNA, IN_GUIA_INTERPRETE, IN_MACA, IN_COMPUTADOR, IN_CADEIRA_ESPECIAL, IN_CADEIRA_CANHOTO, IN_CADEIRA_ACOLCHOADA, IN_PROVA_DEITADO, IN_MOBILIARIO_OBESO, IN_LAMINA_OVERLAY, IN_PROTETOR_AURICULAR, IN_MEDIDOR_GLICOSE, IN_MAQUINA_BRAILE, IN_SOROBAN, IN_MARCA_PASSO, IN_SONDA, IN_MEDICAMENTOS, IN_SALA_INDIVIDUAL, IN_SALA_ESPECIAL, IN_SALA_ACOMPANHANTE, IN_MOBILIARIO_ESPECIFICO, IN_MATERIAL_ESPECIFICO, IN_NOME_SOCIAL, IN_CERTIFICADO, NO_ENTIDADE_CERTIFICACAO, CO_UF_ENTIDADE_CERTIFICACAO, SG_UF_ENTIDADE_CERTIFICACAO, CO_MUNICIPIO_PROVA, NO_MUNICIPIO_PROVA, CO_UF_PROVA, SG_UF_PROVA, TP_PRESENCA_CN, TP_PRESENCA_CH, TP_PRESENCA_LC, TP_PRESENCA_MT, CO_PROVA_CN, CO_PROVA_CH, CO_PROVA_LC, CO_PROVA_MT, NU_NOTA_CN, NU_NOTA_CH, NU_NOTA_LC, NU_NOTA_MT]","[numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric]",Context This dataset was downloaded from INEP a department from the Brazilian Education Ministry. It contains data from the applicants for the 2016 National High School Exam. Content Inside this dataset there are not only the exam results but the social and economic context of the applicants. Acknowledgements The original dataset is provided by INEP (http//portal.inep.gov.br/microdados). Inspiration The objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.,CSV,,[education],CC0,,,119,1041,1024,"Data from ENEM 2016, the Brazilian High School National Exam.",ENEM 2016,https://www.kaggle.com/gbonesso/enem-2016,Thu Nov 30 2017
,US Bureau of Labor Statistics,"[area_code, area_name, display_level, selectable, sort_sequence]","[numeric, string, numeric, string, numeric]","Context The Bureau of Labor Statistics defines the Consumer Price Index (CPI) as “a statistical measure of change over time of the prices of goods and services in major expenditure groups--such as food housing apparel transportation and medical care--typically purchased by urban consumers.  Essentially it compares the cost of a sample of goods and services in a specific month relative to the cost of the same ""market basket"" in an earlier reference period.  Make sure to read the cu.txt for more descriptive summaries on each data file and how to use the unique identifiers. Content This dataset was collected June 27th 2017 and may not be up-to-date. The revised CPI introduced by the BLS in 1998 includes indexes for two populations; urban wage earners and clerical workers (CW) and all urban consumers (CU).  This dataset covers all urban consumers (CU). The Consumer Price Index (CPI) is a statistical measure of change over time of the prices of goods and services in major expenditure groups--such as food housing apparel transportation and medical care--typically purchased by urban consumers.  Essentially it compares the cost of a sample ""market basket"" of goods and services in a specific month relative to the cost of the same ""market basket"" in an earlier reference period.  This reference period is designated as the base period. As a result of the 1998 revision both the CW and the CU utilize updated expenditure weights based upon data tabulated from three years (1982 1983 and 1984) of the Consumer Expenditure Survey and incorporate a number of technical improvements including an updated and revised item structure. To construct the two indexes prices for about 100000 items and data on about 8300 housing units are collected in a sample of 91 urban places.  Comparison of indexes for individual CMSA's or cities show only the relative change over time in prices between locations.  These indexes cannot be used to measure interarea differences in price levels or living costs. Summary Data Available U.S. average indexes for both populations are available for about 305 consumer items and groups of items.  In addition over 100 of the indexes have been adjusted for seasonality.  The indexes are monthly with some beginning in 1913. Semi-annual indexes have been calculated for about 100 items for comparison with semi-annual areas mentioned below.  Semi-annual indexes are available from 1984 forward. Area indexes for both populations are available for 26 urban places.  For each area indexes are published for about 42 items and groups.  The indexes are published monthly for three areas bimonthly for eleven areas and semi-annually for 12 urban areas. Regional indexes for both populations are available for four regions with  about 55 items and groups per region.  Beginning with January 1987 indexes are monthly with some beginning as early as 1966.  Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned above.  Semi-annual indexes have been calculated for about 42 items in the 27 urban places for comparison with semi-annual areas. City-size indexes for both populations are available for three size classes with about 55 items and groups per class.  Beginning with January 1987 indexes are monthly and most begin in 1977.  Semi-annual indexes have been calculated for about 42 items for comparison with semi-annual areas mentioned below. Region/city-size indexes for both populations are available cross classified by region and city-size class.  For each of 13 cross calculations about 42 items and groups are available.  Beginning with January 1987 indexes are monthly and most begin in 1977.  Semi-annual indexes have been calculated for about 42 items in the 26 urban places for comparison with semi-annual areas. Frequency of Observations  U.S. city average indexes some area indexes and regional indexes city-size indexes and region/city-size indexes for both populations are monthly.  Other area indexes for both populations are bimonthly or semi-annual. Annual Averages Annual averages are available for all unadjusted series in the CW and CU. Base Periods Most indexes have a base period of 1982-1984 = 100.  Other indexes mainly those which have been added to the CPI program with the 1998 revision are based more recently.  The base period value is 100.0 except for the ""Purchasing Power"" values (AAOR and SAOR) where the base period value is 1.000. Data Characteristics Indexes are stored to one decimal place except for the ""Purchasing Power"" values which are stored to three decimal places. References  BLS Handbook of Methods Chapter 17 ""Consumer Price Index""  BLS Bulletin 2285 April 1988. Acknowledgements This dataset was taken directly from the U.S. Bureau of Labor Statistics website at http//www.bls.gov/data/ and converted to CSV format. Inspiration The Bureau of Labor Statistics has done a great job of providing this source of information for the public to explore. You can use this information to compare the cost of living in urban areas around the United States. What are the top 10 most expensive places to live? Which cities have the most expensive snacks or college textbooks? Coffee? Beer?",CSV,,"[business, finance]",CC0,,,413,3688,63,Statistical measures of change in prices of consumer goods,Consumer Price Index,https://www.kaggle.com/bls/consumer-price-index,Wed Jun 28 2017
,Kevin Mader,"[Case ID, Sex]","[numeric, string]",Context At RSNA 2017 there was a contest to correctly identify the age of a child from an X-ray of their hand. This is the dataset on Kaggle making it easier to experiment with and do educational demos. Additionally maybe there are some new ideas for building smarter models for handling X-ray images. Content A number of folders full of images (digital and scanned) with a CSV containing the age (what is to be predicted) and the gender (useful additional information) Acknowledgements The dataset was originally published on CloudApp as an RSNA challenge. Original Dataset Acknowledgements The Radiological Society of North America (RSNA) Radiology Informatics Committee (RIC) Pediatric Bone Age Machine Learning Challenge Organizing Committee   Kathy Andriole Massachusetts General Hospital Brad Erickson Mayo Clinic Adam Flanders Thomas Jefferson University  Safwan Halabi Stanford University Jayashree Kalpathy-Cramer Massachusetts General Hospital Marc Kohli University of California - San Francisco  Luciano Prevedello The Ohio State University  Data sets used in the Pediatric Bone Age Challenge have been contributed by Stanford University the University of Colorado and the University of California - Los Angeles.  The MedICI platform (built CodaLab) used for the challenge is provided by Jayashree Kalpathy-Cramer supported through NIH grants (U24CA180927) and a contract from Leidos. Inspiration  Can you predict with better than 4.2 months accuracy?  Is identifying the joints an important step? What algorithms work best? What do the algorithms focus on? Is gender a necessary piece of information or can it be automatically derived from the image? ,Other,,"[healthcare, image data]",Other,,,447,3392,9216,Predict Age from X-Rays,RSNA Bone Age,https://www.kaggle.com/kmader/rsna-bone-age,Wed Jan 24 2018
,Foxtrot,"[goodreads_book_id, tag_id, count]","[numeric, numeric, numeric]","This version of the dataset is obsolete. It contains duplicate ratings (same user_idbook_id) as reported by Philipp Spachtholz in his illustrious notebook. The current version has duplicates removed and more ratings (six million) sorted by time. Book and user IDs are the same.  It is available at https//github.com/zygmuntz/goodbooks-10k.  There have been good datasets for movies (Netflix Movielens) and music (Million Songs) recommendation but not for books. That is until now.  This dataset contains ratings for ten thousand popular books. As to the source let's say that these ratings were found on the internet. Generally there are 100 reviews for each book although some have less - fewer - ratings. Ratings go from one to five. Both book IDs and user IDs are contiguous. For books they are 1-10000 for users 1-53424. All users have made at least two ratings. Median number of ratings per user is 8. There are also books marked to read by the users book metadata (author year etc.) and tags. Contents ratings.csv contains ratings and looks like that book_iduser_idrating 13145 14393 15885 111694 111854  to_read.csv provides IDs of the books marked ""to read"" by each user as user_idbook_id pairs. books.csv has metadata for each book (goodreads IDs authors title average rating etc.). The metadata have been extracted from goodreads XML files available in the third version of this dataset as books_xml.tar.gz. The archive contains 10000 XML files. One of them is available as sample_book.xml. To make the download smaller these files are absent from the current version. Download version 3 if you want them. book_tags.csv contains tags/shelves/genres assigned by users to books. Tags in this file are represented by their IDs. tags.csv translates tag IDs to names. See the notebook for some basic stats of the dataset. goodreads IDs Each book may have many editions.  goodreads_book_id and best_book_id generally point to the most popular edition of a given book while goodreads  work_id refers to the book in the abstract sense. You can use the goodreads book and work IDs to create URLs as follows https//www.goodreads.com/book/show/2767052  https//www.goodreads.com/work/editions/2792775",CSV,,[books],CC4,,,1807,16171,41,"Ten thousand books, one million ratings. Also books marked to read, and tags.",goodbooks-10k,https://www.kaggle.com/zygmunt/goodbooks-10k,Sat Sep 02 2017
,Sohier Dane,"[, callDateTime, priority, district, description, callNumber, incidentLocation, location]","[numeric, dateTime, string, string, string, string, string, string]",This dataset records the time location priority and reason for calls to 911 in the city of Baltimore. Acknowledgements This dataset was kindly made available by the City of Baltimore. They update the data daily; you can find the original version here. Inspiration  The study discussed in this Atlantic article reviewing 911 calls in Milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. Does this hold true for Baltimore as well? ,CSV,,[crime],CC3,,,212,2142,282,Records of 2.8 million calls from 2015 onwards,Baltimore 911 Calls,https://www.kaggle.com/sohier/baltimore-911-calls,Wed Aug 30 2017
,MichaelStone,"[RecordedAtTime, DirectionRef, JourneyPatternRef, PublishedLineName, OriginRef, DestinationRef, DestinationName, Bearing, ProgressRate, BlockRef, VehicleRef, OriginAimedDepartureTime, ProgressStatus, DatedVehicleJourneyRef, VehicleLocation.Longitude, VehicleLocation.Latitude, ExpectedArrivalTime, ArrivalProximityText, DistanceFromStop, NumberOfStopsAway, StopPointRef, VisitNumber, StopPointName, ScheduledArrivalTime]","[dateTime, numeric, string, string, string, string, string, numeric, string, string, string, dateTime, string, string, numeric, numeric, dateTime, string, numeric, numeric, numeric, numeric, string, dateTime]",Context I wanted to find a better way to provide live traffic updates. We dont all have access to the data from traffic monitoring sensors or whatever gets uploaded from people's smart phones to Apple Google etc plus I question how accurate the traffic congestion is on Google Maps or other apps. So I figured that since buses are also in the same traffic and many buses stream their GPS location and other data live that would be an ideal source for traffic data. I investigated the data streams available from many bus companies around the world and found MTA in NYC to be very reliable.  Content This dataset is from the NYC MTA buses data stream service. In roughly 10 minute increments the bus location route bus stop and more is included in each row. The scheduled arrival time from the bus schedule is also included to give an indication of where the bus should be (how much behind schedule or on time or even ahead of schedule). Data for the entire month of June 2017 is included. Due to space limitations on Kaggle for datasets only selected bus routes have been included. Acknowledgements Data is recorded from the MTA SIRI Real Time data feed and the MTA GTFS Schedule data. Inspiration I want to see what exploratory & discovery people come up with from this data. Feel free to download this dataset for your own use however I would appreciate as many Kernals included on Kaggle as we can get.  Based on the interest this generates I plan to collect more data for subsequent months down the track.,CSV,,"[transport, public transport]",Other,,,400,4098,326,"Periodic data recorded from NYC Buses - Location, Time, Schedule & more",New York City Transport Statistics,https://www.kaggle.com/stoney71/new-york-city-transport-statistics,Tue Jul 18 2017
,Rachael Tatman,"[file_id, genus, species, english_cname, who_provided_recording, country, latitude, longitute, type, license]","[numeric, string, string, string, string, string, numeric, numeric, string, string]","Context Birds use songs and calls of varying length and complexity to attract mates warn of nearby danger and mark their territory. This dataset contains a recordings of different birdsongs from bird species that can be found in Britain (although the recordings themselves are from many different locations). Content This is a dataset of bird sound recordings a specific subset gathered from the Xeno Canto collection to form a balanced dataset across 88 species commonly heard in the United Kingdom. The copyright in each audio file is owned by the user who donated the file to Xeno Canto. Please see ""birdsong_metadata.tsv"" for the full listing which gives the authors' names and the CC licences applicable for each file. The audio files are encoded as .flac files. Acknowledgements These recordings were collected by 68 separate birding enthusiasts and uploaded to and stored by xeno-canto www.xeno-canto.org. If you make use of these recordings in your work please cite the specific recording and include acknowledgement of and a link to the xeno-canto website. Inspiration  Can you build a classifier to identify birds based on their songs? Can you visualize the songs of specific birds? Can you generate new birdsongs based on this data? ",CSV,,"[animals, acoustics]",Other,,,77,804,633,264 recordings from 88 species,British Birdsong Dataset,https://www.kaggle.com/rtatman/british-birdsong-dataset,Fri Nov 17 2017
,Federal Communications Commission,"[Ticket ID, Ticket Created, Date of Issue, Time of Issue, Form, Method, Issue, Caller ID Number, Type of Call or Messge, Advertiser Business Number, City, State, Zip, Location (Center point of the Zip Code)]","[numeric, dateTime, dateTime, dateTime, string, string, string, string, string, string, string, string, numeric, string]",Individual informal consumer complaint data detailing complaints filed with the Consumer Help Center beginning October 31 2014. This data represents information selected by the consumer. The FCC does not verify the facts alleged in these complaints. This dataset contains everything you need to analyze the jerk companies who call during dinner time mode of communication logged phone number (of the hassler) and what they were trying to do.  Acknowledgements This dataset was kindly made available by the FCC. You can find the original dataset here.,CSV,,[telecommunications],CC0,,,57,847,152,Consumer complaints filed with the FCC ,Robocall Complaints,https://www.kaggle.com/fcc/robocall-complaints,Wed Sep 13 2017
,Bharadwaj Srigiriraju,[],[],Context A small subset of dataset of product reviews from Amazon Kindle Store category. Content 5-core dataset of product reviews from Amazon Kindle Store category from May 1996 - July 2014. Contains total of 982619 entries. Each reviewer has at least 5 reviews and each product has at least 5 reviews in this dataset.  Columns   asin - ID of the product like B000FA64PK helpful - helpfulness rating of the review - example 2/3. overall - rating of the product. reviewText - text of the review (heading). reviewTime - time of the review (raw). reviewerID - ID of the reviewer like A3SPTOKDG7WBLN reviewerName - name of the reviewer. summary - summary of the review (description). unixReviewTime - unix timestamp.  Acknowledgements This dataset is taken from Amazon product data Julian McAuley UCSD website. http//jmcauley.ucsd.edu/data/amazon/ License to the data files belong to them. Inspiration  Sentiment analysis on reviews. Understanding how people rate usefulness of a review/ What factors influence helpfulness of a review. Fake reviews/ outliers. best rated product IDs or similarity between products based on reviews alone (not the best idea ikr). Any other interesting analysis. ,{}JSON,,"[business, linguistics, internet]",Other,,,343,2800,265,Amazon reviews: Kindle Store category,Amazon reviews: Kindle Store Category,https://www.kaggle.com/bharadwaj6/kindle-reviews,Mon Dec 11 2017
,Brian J,"[CompanyNumber, IncorporationDate, RegAddressPostCode, Latitude, Longitude, SIC]","[string, string, string, numeric, numeric, string]",I work with UK company information on a daily basis and I thought it would be useful to publish a list of all active companies in a way that could be used for machine learning. There are 3801733 rows in the dataset one for each active company. The postcode which is included in the dataset has been geolocated and the resultant latitude and longitudes have been included along with the Standard Industrial Classification Code and date of incorporation. The company list is from the publicly available 1st November 2017 Companies House snapshot.   The postcode geolocations and SIC Codes are from the gov.uk website. In the file AllCompanies.csv each row is formatted as follows  CompanyNumber - in the format of 99999999 for England/Wales SC999999 for Scotland and NI999999 for Northern Ireland. IncorporationDate - in British date format dd/mm/yyyy RegisteredAddressPostCode - standard British format Postcode Latitude - to 6 decimal places Longitude  - to 6 decimal places SIC - 5 digits or if not known None - see separate file for description of each code.  Inspiration Possible uses for this data is to see where certain types of companies are located in the UK and how over time they multiply and spread throughout the UK. Training ML algorithms to predict where there are a high (or low) density of certain types of companies and where would be a good area for a company to be located if it wanted minimal competition or the inverse where there are clusters of high densities where it might be easier to recruit specialised staff. A useful addition would be to overlay population density which I am currently working on as an option for this dataset.  I am sure there are many more possible uses for this data in ways that I cannot imagine. This is my first go at publishing a dataset on any medium so any useful tips and hints would be extremely welcome. Links to the raw data sources are here  Companies House http//download.companieshouse.gov.uk/en_output.html Postcode to Geolocation  https//data.gov.uk/dataset/national-statistics-postcode-lookup-uk SIC Codes https//www.gov.uk/government/publications/standard-industrial-classification-of-economic-activities-sic ,CSV,,[business],CC0,,,84,929,50,"3,801,733 company details",All UK Active Companies By SIC And Geolocated,https://www.kaggle.com/dalreada/all-uk-active-companies-by-sic-and-geolocated,Thu Nov 16 2017
,Cards Against Humanity,"[Income, Gender, Age, Age Range, Political Affiliation, Do you approve or disapprove of how Donald Trump is handling his job as president?, What is your highest level of education? , Q5OTH1, What is your race?, Q6OTH1, What is your marital status?, Q7OTH1, q8x, What would you say is the likelihood that your current job will be entirely performed by robots or computers within the next decade?, Do you believe that climate change is real and caused by people, real but not caused by people, or not real at all?, How many Transformers movies have you seen? , q11x, Do you agree or disagree with the following statement: scientists are generally honest and are serving the public good. , Do you agree or disagree with the following statement: vaccines are safe and protect children from disease., How many books, if any, have you read in the past year?, q14x, Do you believe in ghosts?, What percentage of the federal budget would you estimate is spent on scientific research?, q16x, Is federal funding of scientific research too high, too low, or about right?, True or false: the earth is always farther away from the sun in the winter than in the summer., If you had to choose: would you rather be smart and sad, or dumb and happy?, Do you think it is acceptable or unacceptable to urinate in the shower?]","[numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, numeric, string, string, string, string, string, boolean, string, string]","THE POLL As part of Cards Against Humanity Saves America this poll is funded for one year of monthly public opinion polls. Cards Against Humanity is asking the American people about their social and political views what they think of the president and their pee-pee habits. To conduct their polls in a scientifically rigorous manner they partnered with Survey Sampling International — a professional research firm — to contact a nationally representative sample of the American public. For the first three polls they interrupted people’s dinners on both their cell phones and landlines and a total of about 3000 adults didn’t hang up immediately. They examined the data for statistically significant correlations which can be found here https//thepulseofthenation.com/ Content  Polls are released each month (they are still polling so this will be updated each month) Row one is the header and contains the questions Each row is one respondent's answers   Questions in the Sep 2017 poll  Income Gender Age Age Range Political Affiliation Do you approve or disapprove of how Donald Trump is handling his job as president? What is your highest level of education?  What is your race? What is your marital status? What would you say is the likelihood that your current job will be entirely performed by robots or computers within the next decade? Do you believe that climate change is real and caused by people real but not caused by people or not real at all?"" How many Transformers movies have you seen?  Do you agree or disagree with the following statement scientists are generally honest and are serving the public good.  Do you agree or disagree with the following statement vaccines are safe and protect children from disease. ""How many books  if any have you read in the past year?"" Do you believe in ghosts? What percentage of the federal budget would you estimate is spent on scientific research? ""Is federal funding of scientific research too high too low  or about right?"" True or false the earth is always farther away from the sun in the winter than in the summer. ""If you had to choose would you rather be smart and sad or dumb and happy?"" Do you think it is acceptable or unacceptable to urinate in the shower?   Questions from Oct 2017 poll  Income Gender Age Age Range Political Affiliation  Do you approve or disapprove of how Donald Trump is handling his job as president? What is your highest level of education? What is your race? From what you have heard or seen do you mostly agree or mostly disagree with the beliefs of White Nationalists? If you had to guess what percentage of Republicans would say that they mostly agree with the beliefs of White Nationalists? Would you say that you love America? If you had to guess  what percentage of Democrats would say that they love America? Do you think that government policies should help those who are poor and struggling in America? If you had to guess what percentage of Republicans would say yes to that question? Do you think that most white people in America are racist? If you had to guess what percentage of Democrats would say yes to that question? Have you lost any friendships or other relationships as a result of the 2016 presidential election? Do you think it is likely or unlikely that there will be a Civil War in the United States within the next decade? Have you ever gone hunting? Have you ever eaten a kale salad? If Dwayne ""The Rock"" Johnson ran for president as a candidate for your political party would you vote for him? Who would you prefer as president of the United States Darth Vader or Donald Trump?   Questions from Nov 2017 poll  Income Gender Age Age Range In politics today  do you consider yourself a Democrat  a Republican  or Independent?  Would you say you are liberal conservative or moderate? What is your highest level of education? (High school or less  Some college  College degree Graduate degree) What is your race? (white black latino asian other) Do you live in a city suburb or small town? Do you approve disapprove or neither approve nor disapprove of how Donald Trump is handling his job as president? Do you think federal funding for welfare programs in America should be increased decreased or kept the same? Do you think poor black people are more likely to benefit from welfare programs than poor white people? Do you think poor people in cities are more likely to benefit from welfare programs than poor people in small towns? If you had to choose would you rather live in a more equal society or a more unequal society?   Acknowledgements These polls are from Cards Against Humanity Saves America and the raw data can be found here https//thepulseofthenation.com/#future",CSV,,"[social sciences, demographics]",CC0,,,260,1237,0.466796875,Cards Against Humanity's Pulse of the Nation,Pulse of the Nation,https://www.kaggle.com/cardsagainsthumanity/pulse-of-the-nation,Thu Dec 21 2017
,US Environmental Protection Agency,"[state_code, county_code, site_num, parameter_code, poc, latitude, longitude, datum, parameter_name, sample_duration, pollutant_standard, date_local, units_of_measure, event_type, observation_count, observation_percent, arithmetic_mean, first_max_value, first_max_hour, aqi, method_code, method_name, local_site_name, address, state_name, county_name, city_name, cbsa_name, date_of_last_change]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, dateTime, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, string, dateTime]",Context Hazardous air pollutants also known as toxic air pollutants or air toxics are those pollutants that are known or suspected to cause cancer or other serious health effects such as reproductive effects or birth defects or adverse environmental effects. The Environmental Protection Agency (EPA) tracks 187 air pollutants. See https//www.epa.gov/haps/ for more information. Content The daily summary file contains data for every monitor (sampled parameter) in the Environmental Protection Agency (EPA) database for each day. This file will contain a daily summary record that is 1. The aggregate of all sub-daily measurements taken at the monitor. 2. The single sample value if the monitor takes a single daily sample (e.g. there is only one sample with a 24-hour duration). In this case the mean and max daily sample will have the same value. Fields Descriptions 1. State Code The Federal Information Processing Standards (FIPS) code of the state in which the monitor resides.  County Code The FIPS code of the county in which the monitor resides. Site Num A unique number within the county identifying the site. Parameter Code The AQS code corresponding to the parameter measured by the monitor. POC This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site. Latitude The monitoring site’s angular distance north of the equator measured in decimal degrees. Longitude The monitoring site’s angular distance east of the prime meridian measured in decimal degrees. Datum The Datum associated with the Latitude and Longitude measures. Parameter Name The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants. Sample Duration The length of time that air passes through the monitoring device before it is analyzed (measured). So it represents an averaging period in the atmosphere (for example a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors it can represent an averaging time of many samples (for example a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour). Pollutant Standard A description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.) Date Local The calendar date for the summary. All daily summaries are for the local standard day (midnight to midnight) at the monitor. Units of Measure The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations. Event Type Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question the data will have multiple records for each monitor. Observation Count The number of observations (samples) taken during the day. Observation Percent The percent representing the number of observations taken with respect to the number scheduled to be taken during the day. This is only calculated for monitors where measurements are required (e.g. only certain parameters). Arithmetic Mean The average (arithmetic mean) value for the day. 1st Max Value The highest value for the day. 1st Max Hour The hour (on a 24-hour clock) when the highest value for the day (the previous field) was taken. AQI The Air Quality Index for the day for the pollutant if applicable. Method Code  An internal system code indicating the method (processes equipment and protocols) used in gathering and measuring the sample. The method name is in the next column. Method Name A short description of the processes equipment and protocols used in gathering and measuring the sample. Local Site Name The name of the site (if any) given by the State local or tribal air pollution control agency that operates it. Address The approximate street address of the monitoring site. State Name The name of the state where the monitoring site is located. County Name The name of the county where the monitoring site is located. City Name The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas. CBSA Name The name of the core bases statistical area (metropolitan area) where the monitoring site is located. Date of Last Change The date the last time any numeric values in this record were updated in the AQS data system.  Acknowledgements These data came from the EPA and are current up to May 01 2017. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/epa. Inspiration People exposed to toxic air pollutants at sufficient concentrations and durations may have an increased chance of getting cancer or experiencing other serious health effects. These health effects can include damage to the immune system as well as neurological reproductive (e.g. reduced fertility) developmental respiratory and other health problems. In addition to exposure from breathing air toxics some toxic air pollutants such as mercury can deposit onto soils or surface waters where they are taken up by plants and ingested by animals and are eventually magnified up through the food chain. Like humans animals may experience health problems if exposed to sufficient quantities of air toxics over time. Use this dataset to find out where the highest concentrations of hazardous air pollutants are for each state. You could also use the GPS locations to find out where the EPA has the most monitoring stations and identify places that could use more.,CSV,,"[environment, pollution]",CC0,,,417,3589,2048,A summary of daily Hazardous Air Pollutants from 1990 to 2017,Hazardous Air Pollutants,https://www.kaggle.com/epa/hazardous-air-pollutants,Sat Jul 01 2017
,Eduardo,"[Position, Track Name, Artist, Streams, URL, Date, Region]","[numeric, string, string, numeric, string, dateTime, string]","Context Music streaming is ubiquitous. Currently Spotify plays an important part on that. This dataset enable us to explore how artists and songs' popularity varies in time. Content This dataset contains the daily ranking of the 200 most listened songs in 53 countries from 2017 and 2018 by Spotify users. It contains more than 2 million rows which comprises 6629 artists 18598 songs for a total count of one hundred five billion streams count. The data spans from 1st January 2017 to 9th January 2018 and will be kept up-to-date on following versions. It has been collected from Spotify's regional chart data. Inspiration  Can you predict what is the rank position or the number of streams a song will have in the future? How long does songs ""resist"" on the top 3 5 10 20 ranking? What are the signs of a song that gets into the top rank to stay? Do continents share same top ranking artists or songs? Are people listening to the very same top ranking songs on countries far away from each other? How long time does a top ranking song takes to get into the ranking of neighbor countries?  Example To start out you can take a look into a simple Kernel I have made in order to read the data filter data from a song plot is temporal tendency per country than make a simple forecast of the its streams count here. Crawler The crawler used to collect this data can be found here.",CSV,,[music],Other,,,1559,11412,43,The 200 daily most streamed songs in 53 countries,Spotify's Worldwide Daily Song Ranking,https://www.kaggle.com/edumucelli/spotifys-worldwide-daily-song-ranking,Sat Jan 13 2018
,Chris Crawford,[],[],EMNIST  The EMNIST dataset is a set of handwritten character digits derived from the NIST Special Database 19  and converted to a 28x28 pixel image format and dataset structure that directly matches the MNIST dataset. Further information on the dataset contents and conversion process can be found in the paper available at https//arxiv.org/abs/1702.05373v1. Format There are six different splits provided in this dataset and each are provided in two formats  Binary (see emnist_source_files.zip) CSV (combined labels and images) Each row is a separate image 785 columns First column = class_label (see mappings.txt for class label definitions) Each column after represents one pixel value (784 total for a 28 x 28 image)  ByClass and ByMerge datsets The full complement of the NIST Special Database 19 is available in the ByClass and ByMerge splits. These two datasets have the same image information but differ in the number of images in each class. Both datasets have an uneven number of images per class and there are more digits than letters. The number of letters roughly equate to the frequency of use in the English language.  train 697932 test 116323 total 814255 classes ByClass 62 (unbalanced)  / ByMerge 47 (unbalanced)   Balanced dataset The EMNIST Balanced dataset is meant to address the balance issues in the ByClass and ByMerge datasets. It is derived from the ByMerge dataset to reduce mis-classification errors due to capital and lower case letters and also has an equal number of samples per class. This dataset is meant to be the most applicable.  train 112800 test 18800 total 131600 classes 47 (balanced)   Letters datasets The EMNIST Letters dataset merges a balanced set of the uppercase and lowercase letters into a single 26-class task.   train 88800 test 14800 total 103600 classes 37 (balanced)   Digits and MNIST datsets The EMNIST Digits and EMNIST MNIST dataset provide balanced handwritten digit datasets directly compatible with the original MNIST dataset.   train Digits 240000 / MNIST 60000 test Digits 40000 / MNIST 10000 total Digits 280000 / MNIST 70000 classes 47 (balanced)   Visual breakdown of EMNIST datasets Please refer to the EMNIST paper for details on the structure of the dataset https//arxiv.org/abs/1702.05373v1.  Acknowldgements Cohen G. Afshar S. Tapson J. & van Schaik A. (2017). EMNIST an extension of MNIST to handwritten letters.  Dataset retrieved from https//www.nist.gov/itl/iad/image-group/emnist-dataset Gregory Cohen Saeed Afshar Jonathan Tapson and Andre van Schaik The MARCS Institute for Brain Behaviour and Development Western Sydney University Penrith Australia 2751,CSV,,[machine learning],CC0,,,524,3122,1024,An extended variant of the full NIST dataset,EMNIST (Extended MNIST),https://www.kaggle.com/crawford/emnist,Wed Dec 20 2017
,TESTIMON @ NTNU,"[step, customer, age, gender, zipcodeOri, merchant, zipMerchant, category, amount, fraud]","[numeric, string, string, string, string, string, string, string, numeric, numeric]",Context BankSim is an agent-based simulator of bank payments based on a sample of aggregated transactional data provided by a bank in Spain. The main purpose of BankSim is the generation of synthetic data that can be used for fraud detection research.  Statistical and a Social Network Analysis (SNA) of relations between merchants and customers were used to develop and calibrate the model.  Our ultimate goal is for BankSim to be usable to model relevant scenarios that combine normal payments and injected known fraud signatures.  The data sets generated by BankSim contain no personal information or disclosure of legal and private customer transactions. Therefore it can be shared by academia and others to develop and reason about fraud detection methods. Synthetic data has the added benefit of being easier to acquire faster and at less cost for experimentation even for those that have access to their own data. We argue that BankSim generates data that usefully approximates the relevant aspects of the real data.  Content We ran BankSim for 180 steps (approx. six months) several times and calibrated the parameters in order to obtain a distribution that get close enough to be reliable for testing. We collected several log files and selected the most accurate. We injected thieves that aim to steal an average of three cards per step and perform about two fraudulent transactions per day. We produced 594643 records in total. Where 587443 are normal payments and 7200 fraudulent transactions. Since this is a randomised simulation the values are of course not identical to original data. Acknowledgements This research was conducted during my PhD studies in Sweden at Blekinge Institute of Technology (BTH ww.bth.se). More about it http//edgarlopez.net Original paper Please refer to this dataset using the following citations Lopez-Rojas Edgar Alonso ; Axelsson Stefan Banksim A bank payments simulator for fraud detection research Inproceedings 26th European Modeling and Simulation Symposium EMSS 2014 Bordeaux France pp. 144–152 Dime University of Genoa 2014 ISBN 9788897999324. https//www.researchgate.net/publication/265736405_BankSim_A_Bank_Payment_Simulation_for_Fraud_Detection_Research,CSV,,[finance],CC4,,,636,5810,78,Synthetic datasets generated by the BankSim payments simulator,Synthetic data from a financial payment system,https://www.kaggle.com/ntnu-testimon/banksim1,Tue Jul 11 2017
,Francis Paul Flores,"[Total Household Income, Region, Total Food Expenditure, Main Source of Income, Agricultural Household indicator, Bread and Cereals Expenditure, Total Rice Expenditure, Meat Expenditure, Total Fish and  marine products Expenditure, Fruit Expenditure, Vegetables Expenditure, Restaurant and hotels Expenditure, Alcoholic Beverages Expenditure, Tobacco Expenditure, Clothing, Footwear and Other Wear Expenditure, Housing and water Expenditure, Imputed House Rental Value, Medical Care Expenditure, Transportation Expenditure, Communication Expenditure, Education Expenditure, Miscellaneous Goods and Services Expenditure, Special Occasions Expenditure, Crop Farming and Gardening expenses, Total Income from Entrepreneurial Acitivites, Household Head Sex, Household Head Age, Household Head Marital Status, Household Head Highest Grade Completed, Household Head Job or Business Indicator, Household Head Occupation, Household Head Class of Worker, Type of Household, Total Number of Family members, Members with age less than 5 year old, Members with age 5 - 17 years old, Total number of family members employed, Type of Building/House, Type of Roof, Type of Walls, House Floor Area, House Age, Number of bedrooms, Tenure Status, Toilet Facilities, Electricity, Main Source of Water Supply, Number of Television, Number of CD/VCD/DVD, Number of Component/Stereo set, Number of Refrigerator/Freezer, Number of Washing Machine, Number of Airconditioner, Number of Car, Jeep, Van, Number of Landline/wireless telephones, Number of Cellular phone, Number of Personal Computer, Number of Stove with Oven/Gas Range, Number of Motorized Banca, Number of Motorcycle/Tricycle]","[numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The Philippine Statistics Authority (PSA) spearheads the conduct of the Family Income and Expenditure Survey (FIES) nationwide. The survey which is undertaken every three (3) years is aimed at providing data on family income and expenditure including among others levels of consumption by item of expenditure sources of income in cash and related information affecting income and expenditure levels and patterns in the Philippines. Content Inside this data set is some selected variables from the latest Family Income and Expenditure Survey (FIES) in the Philippines. It contains more than 40k observations and 60 variables which is primarily comprised of the household income and expenditures of that specific household Acknowledgements The Philippine Statistics Authority for providing the publisher with their raw data Inspiration Socio-economic classification models in the Philippines has been very problematic. In fact not one SEC model has been widely accepted. Government bodies uses their own SEC models and private research entities uses their own. We all know that household income is the greatest indicator of one's socio-economic classification that's why the publisher would like to find out the following 1) Best model in predicting household income 2) Key drivers of household income we want to make the model as sparse as possible 3) Some exploratory analysis in the data would also be useful,CSV,,"[income, finance, demographics]",CC0,,,572,3719,22,Annual Household Income and Expenses in the Philippines,Filipino Family Income and Expenditure,https://www.kaggle.com/grosvenpaul/family-income-and-expenditure,Thu Oct 05 2017
,Jacob Boysen,[],[],Context A permanent labor certification issued by the Department of Labor (DOL) allows an employer to hire a foreign worker to work permanently in the United States. In most instances before the U.S. employer can submit an immigration petition to the Department of Homeland Security's U.S. Citizenship and Immigration Services (USCIS) the employer must obtain a certified labor certification application from the DOL's Employment and Training Administration (ETA). The DOL must certify to the USCIS that there are not sufficient U.S. workers able willing qualified and available to accept the job opportunity in the area of intended employment and that employment of the foreign worker will not adversely affect the wages and working conditions of similarly employed U.S. workers. Content Data covers 2012-2017 and includes information on employer position wage offered job posting history employee education and past visa history associated lawyers and final decision. Acknowledgements This data was collected and distributed by the US Department of Labor. Inspiration  Can you predict visa decisions based on employee/employer/wage? How does this data compare to H1B decisions in this dataset? ,CSV,,"[government agencies, immigration]",CC0,,,1301,8463,285,Detailed information on 374k visa decisions,US Permanent Visa Applications,https://www.kaggle.com/jboysen/us-perm-visas,Fri Aug 25 2017
,Olga Belitskaya,"[brand_name, brand_label, product_name, product_label, file]","[string, numeric, string, numeric, string]",History I have made the database of photos sorted by products and brands. Screenshots were performed only on official brand websites. Content The main dataset (style.zip) is 894 color images (150x150x3) with 7 brands and 10 products and the file with labels style.csv. Photo files are in the .png format and the labels are integers and values. The file StyleColorImages.h5 consists of preprocessing images of this set image tensors and targets (labels). Acknowledgements I have published the data for absolutely free using by any site visitor. But this database contains the names of famous brands so it can not be used for commercial purposes. Usage Classification image recognition and colorizing etc. in a case of a small number of images are useful exercises. The main question we can try to answer with the help of the data is whether the algorithms can recognize the unique design style well enough. To facilitate the task I chose the most easily recognizable brands with a bright style. The example of usage Improvement There are lots of ways for improving this set and the machine learning algorithms applying to it. At first it needs to increase the number of photos.,Other,,"[photography, clothing, deep learning]",Other,,,166,1730,49,Brand and Product Recognition,Style Color Images,https://www.kaggle.com/olgabelitskaya/style-color-images,Wed Dec 20 2017
,Chris Crawford,"[name, mfr, type, calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, shelf, weight, cups, rating]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context If you like to eat cereal do yourself a favor and avoid this dataset at all costs. After seeing these data it will never be the same for me to eat Fruity Pebbles again. Content Fields in the dataset  Name Name of cereal  mfr Manufacturer of cereal  A = American Home Food Products;  G = General Mills K = Kelloggs N = Nabisco P = Post Q = Quaker Oats R = Ralston Purina  type  cold  hot  calories calories per serving  protein grams of protein  fat grams of fat  sodium milligrams of sodium  fiber grams of dietary fiber  carbo grams of complex carbohydrates  sugars grams of sugars  potass milligrams of potassium  vitamins vitamins and minerals - 0 25 or 100 indicating the typical percentage of FDA recommended  shelf display shelf (1 2 or 3 counting from the floor)  weight weight in ounces of one serving  cups number of cups in one serving  rating a rating of the cereals (Possibly from Consumer Reports?)  Acknowledgements These datasets have been gathered and cleaned up by Petra Isenberg Pierre Dragicevic and Yvonne Jansen. The original source can be found here This dataset has been converted to CSV Inspiration Eat too much sugary cereal? Ruin your appetite with this dataset!,CSV,,[food and drink],CC3,,,2000,11610,0.0048828125,Nutrition data on 80 cereal products,80 Cereals,https://www.kaggle.com/crawford/80-cereals,Tue Oct 24 2017
,Joel Jacobsen,"[Country, Dwellings without basic facilities as pct, Housing expenditure as pct, Rooms per person as rat, Household net adjusted disposable income in usd, Household net financial wealth in usd, Labour market insecurity as pct, Employment rate as pct, Long-term unemployment rate as pct, Personal earnings in usd, Quality of support network as pct, Educational attainment as pct, Student skills as avg score, Years in education in yrs, Air pollution in ugm3, Water quality as pct, Stakeholder engagement for developing regulations as avg score, Voter turnout as pct, Life expectancy in yrs, Self-reported health as pct, Life satisfaction as avg score, Feeling safe walking alone at night as pct, Homicide rate as rat, Employees working very long hours as pct, Time devoted to leisure and personal care in hrs]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This is the Better Life Index for 2017 gathered from the OECD stats page. Grouping labels have been removed and the row for units of measurment for each column has been removed with the units added to the end of each column label as such (Percentage 'as pct'; Ratio 'as rat'; US Dollar 'in usd'; Average score 'as avg score'; Years 'in years'; Micrograms per cubic metre 'in ugm3'; Hours 'in hrs'). Also although included in the report Brazil Russia and South Africa are non-OECD economies at the time of reporting Acknowledgements OECD stats page For full index and others please visit http//stats.oecd.org/Index.aspx?DataSetCode=BLI,CSV,,"[politics, demographics, international relations]",Other,,,485,2406,0.0048828125,Cleaned dataset for the 2017 OECD Better Life Index,OECD Better Life Index 2017,https://www.kaggle.com/jej13b/oecd-better-life-index,Wed Dec 13 2017
,Rachael Tatman,[],[],Context Glottolog (http//glottolog.org) provides a comprehensive catalogue of the world's languages language families and dialects. It assigns a unique and stable identifier (the Glottocode) to (in principle) all languoids i.e. all families languages and dialects. Content This dataset contains information on 1) the geographic location of languages and dialects 2) their familial relationships and 3) a list of scholarly sources where information on languages was found. Acknowledgements This dataset was the current version of Glottolog as of July 20 2017. If you publish work using this dataset please use the following citation Hammarström Harald & Haspelmath Martin & Forkel Robert. 2017. Glottolog 3.0. Jena Max Planck Institute for the Science of Human History. (Available online at http//glottolog.org Accessed on 2017-03-23.) Inspiration  Can you plot the geographic location of each language family or langoid? Where are most extinct languages found? Can you find which language was documented in what decade? Which areas of the focus of more or less documentation?  You may also be interested in  Atlas of Pidgin and Creole Language Structures The Sign Language Analyses (SLAY) Database World Atlas of Language Structures Information on the linguistic structures in 2679 languages ,CSV,,"[languages, geography, linguistics]",CC4,,,253,2266,198,Where are the world’s language families used?,World Language Family Map,https://www.kaggle.com/rtatman/world-language-family-map,Fri Jul 21 2017
,New York Philharmonic,"[Date, Location, Time, Venue, eventType, season, programID, orchestra, id]","[dateTime, string, dateTime, string, string, string, numeric, string, string]",Context The New York Philharmonic played its first concert on December 7 1842. Since then it has merged with the New York Symphony the New/National Symphony and had a long-running summer season at New York's Lewisohn Stadium. The Performance History database documents all known concerts of all of these organizations amounting to more than 20000 performances. Content Dataset is a single csv with over 800k rows. Data contains information on season orchestra venue date time conductor work title composer movement and soloists. Acknowledgements This dataset was compiled by the New York Philharmonic. Original json files hosted here. Original json files were flattened and joined on guid to form a single csv file. Image courtesy of Larisa Birta. Inspiration Nearly 175 years of performance history covering over 11k unique works--which composers are most popular? Have there been any trends in popularity by conductor or by season?,CSV,,[music],CC0,,,236,1695,246,"All Performances, 1842-Present",NY Philharmonic Performance History,https://www.kaggle.com/nyphil/perf-history,Wed Aug 16 2017
,Stack Overflow,[],[],Every year Stack Overflow conducts a massive survey of people on the site covering all sorts of information like programming languages salary code style and various other information. This year they amassed more than 64000 responses fielded from 213 countries. Data The data is made up of two files  1. survey_results_public.csv - CSV file with main survey results one respondent per row and one column per answer  2. survey_results_schema.csv - CSV file with survey schema i.e. the questions that correspond to each column name m Acknowledgements Data is directly taken from StackOverflow and licensed under the ODbL license.,CSV,,"[information technology, internet]",ODbL,,,2611,23650,89,"A look into the lives of over 64,000 Stack Overflow developers","Stack Overflow Developer Survey, 2017",https://www.kaggle.com/stackoverflow/so-survey-2017,Thu Jun 15 2017
,GavinArmstrong,[],[],OpenSprayer.com Open Sprayer will hopefully be an open sourced autonomous land drone that will propel itself across the fields spraying weeds it can see with its mounted cameras. The project should involve a mix of mechanical engineering classical software design and machine learning to achieve its goal. The project is meant to be a DIY effort to compete with the big companies like John Deere currently developing similar tech. The benefit of an open design is cheaper capital and maintenance cost. The ability to fix update and repair your own sprayer would offer a great alternative to the usual high running costs of branded machines. The data set includes around 150 photos with annotations (Bounding box coordinates) locating the broad leaved docks. 70% were taken by me with the remaining being collected on google. I plan to update the images to better reflect the images that the sprayer drone will produce when operating. When the drone is running photos will most likely be taken from a height looking straight down at the ground therefore the google images may be useless or not? Give me feedback and I can take more pictures to improve the dataset.,Other,,"[agriculture, image data]",CC0,,,81,922,148,A collection of broad leaved dock images for weed sprayer,Open Sprayer images,https://www.kaggle.com/gavinarmstrong/open-sprayer-images,Fri Nov 10 2017
,Bank of England,[Sheet_name],[string],The dataset contains a broad set of macroeconomic and financial data for the UK stretching back in some cases to the C13th and with one or two benchmark estimates available for 1086 the year of the Domesday Book. The dataset was originally called the 'Three centuries of macroeconomic data' spreadsheet but has now been renamed given its broader coverage. Version 3 of the dataset has now been updated to 2016. Content The Excel file contains the original data. It contains hundreds of time series while the csv is an extract of several dozen headline time series. If you would like to see more of the data made available in CSV format; please let me know what you would like extracted and I'll be happy to add it. Please see excel_sheet_names.csv for details of what other data has yet to be unpacked. Acknowledgements This dataset was kindly made available by the Bank of England. You can find the original dataset here. Inspiration  Which metrics give similar answers about when the industrial revolution began? How clear is the cutoff point?  If you like If you enjoyed this dataset you might also like the Allen-Unger Global Commodity Prices dataset which provides historic commodity prices from locations around the world.,CSV,,"[history, finance, banking, economics]",CC0,,,394,3410,25,Economic Data for the UK from 1086-2016,A millennium of macroeconomic data,https://www.kaggle.com/bank-of-england/a-millennium-of-macroeconomic-data,Wed Sep 20 2017
,tiredgeek,"[sku, national_inv, lead_time, in_transit_qty, forecast_3_month, forecast_6_month, forecast_9_month, sales_1_month, sales_3_month, sales_6_month, sales_9_month, min_bank, potential_issue, pieces_past_due, perf_6_month_avg, perf_12_month_avg, local_bo_qty, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, went_on_backorder]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string]",Context Part backorders is a common supply chain problem.  Working to identify parts at risk of backorder before the event occurs so the business has time to react. Content Training data file contains the historical data for the 8 weeks prior to the week we are trying to predict.  The data was taken as weekly snapshots at the start of each week.  Columns are defined as follows sku - Random ID for the product national_inv - Current inventory level for the part lead_time - Transit time for product (if available) in_transit_qty - Amount of product in transit from source forecast_3_month - Forecast sales for the next 3 months forecast_6_month - Forecast sales for the next 6 months forecast_9_month - Forecast sales for the next 9 months sales_1_month - Sales quantity for the prior 1 month time period  sales_3_month - Sales quantity for the prior 3 month time period  sales_6_month - Sales quantity for the prior 6 month time period  sales_9_month - Sales quantity for the prior 9 month time period  min_bank - Minimum recommend amount to stock potential_issue - Source issue for part identified pieces_past_due - Parts overdue from source perf_6_month_avg - Source performance for prior 6 month period  perf_12_month_avg - Source performance for prior 12 month period  local_bo_qty - Amount of stock orders overdue deck_risk - Part risk flag oe_constraint - Part risk flag ppap_risk - Part risk flag stop_auto_buy - Part risk flag rev_stop - Part risk flag went_on_backorder - Product actually went on backorder.  This is the target value.,CSV,,[business],CC4,,,6063,45802,134,Based on historical data predict backorder risk for products,Can You Predict Product Backorders?,https://www.kaggle.com/tiredgeek/predict-bo-trial,Thu Apr 27 2017
,JohannesBuchner,[],[],"Context I want my computer to react to simple short predefined commands. I do not need it to understand any meaning or do complex things just to recognize and react. I created this dataset to explore possible audio classification algorithms that can ultimately be transferred to the real world. Content I have selected single-syllable English verbs obtained their pronounciations (phonemes) via to the British English Example Pronciation dictionary and let espeak pronounce it varying the pronounciations stress pitch speed and speaker.  You can play individual words samples with any audio player that understands ogg (e.g. VLC). The samples still sound a bit mechanic but it is a start. For simplicity consider the dog sub-sample which only contains four commands ""chase"" ""fetch"" ""sit"" ""walk"". They are fairly easy for the ear to distinguish even in poor quality audio. To create the classification data set (db.dog.hdf5) I added AURORA noise with varying volume and convertedd the audio data to 24x24 frequency-vs-time ""images"" (spectrograms).  The file comes with class labels (word ID). Example python script to load and train https//github.com/JohannesBuchner/spoken-command-recognition/blob/master/traincommanddetect_svm.py More details and generating scripts can be found at https//github.com/JohannesBuchner/spoken-command-recognition Acknowledgements Pronounciation dictionary BEEP http//svr-www.eng.cam.ac.uk/comp.speech/Section1/Lexical/beep.html Noise samples AURORA https//www.ee.columbia.edu/~dpwe/sounds/noise/ eSPEAK http//espeak.sourceforge.net/ and mbrola voices http//www.tcts.fpms.ac.be/synthesis/mbrola/mbrcopybin.html Inspiration The open question is if these computer-generated data sets robustified by pronounciation and noise variations can be transferred into real applications.  At the moment I see companies trying to solve a hard problem (map arbitrary open-ended speech to text and identify meaning) while the easier problem of detecting a predefined word and mapping it to a predefined action should be solvable with currently available tools. Machine learning audio training data is lacking and this aims to solve that.",Other,,"[languages, acoustics, communication, human-computer interaction]",CC4,,,18,449,349,Classify simple audio commands,Spoken Verbs,https://www.kaggle.com/jbuchner/spokenverbs,Thu Dec 21 2017
,NorbertBudincsevity,"[Formatted Date, Summary, Precip Type, Temperature (C), Apparent Temperature (C), Humidity, Wind Speed (km/h), Wind Bearing (degrees), Visibility (km), Loud Cover, Pressure (millibars), Daily Summary]","[dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context This is a dataset for a larger project I have been working on. My idea is to analyze and compare real historical weather with weather folklore. Content The CSV file includes a hourly/daily summary for Szeged Hungary area between 2006 and 2016. Data available in the hourly response  time summary precipType temperature apparentTemperature humidity windSpeed windBearing visibility loudCover pressure  Acknowledgements Many thanks to Darksky.net team for their awesome API.,CSV,,[climate],CC4,,,629,2953,16,"Hourly/daily summary with temperature, pressure, wind speed and more",Weather in Szeged 2006-2016,https://www.kaggle.com/budincsevity/szeged-weather,Sun Jan 08 2017
,NOAA,"[CatalogNumber, DataProvider, ScientificName, VernacularNameCategory, TaxonRank, Station, ObservationDate, latitude, longitude, DepthInMeters, DepthMethod, Locality, LocationAccuracy, SurveyID, Repository, IdentificationQualifier, EventID, SamplingEquipment, RecordType, SampleID]","[numeric, string, string, string, string, string, dateTime, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string]","Context This dataset contains information about deep sea corals and sponges collected by NOAA and NOAA’s partners. Amongst the data are geo locations of deep sea corals and sponges and the whole thing is tailored to the occurrences of azooxanthellates - a subset of all corals and all sponge species (i.e. they don't have symbiotic relationships with certain microbes).  Additionally these records only consists of observations deeper than 50 meters to truly focus on the deep sea corals and sponges. Content Column descriptions  CatalogNumber Unique record identifier assigned by the Deep-Sea Coral Research and Technology Program. DataProvider The institution publication or individual who ultimately deserves credit for acquiring or aggregating the data and making it available. ScientificName Taxonomic identification of the sample as a Latin binomial. VernacularNameCategory Common (vernacular) name category of the organism. TaxonRank Identifies the level in the taxonomic hierarchy of the ScientificName term. ObservationDate Time as hhmmss when the sample/observation occurred (UTC). Latitude (degrees North) Latitude in decimal degrees where the sample or observation was collected. Longitude (degrees East) Longitude in decimal degrees where the sample or observation was collected. DepthInMeters Best single depth value for sample as a positive value in meters. DepthMethod Method by which best singular depth in meters (DepthInMeters) was determined. ""Averaged"" when start and stop depths were averaged. ""Assigned"" when depth was derived from bathymetry at the location. ""Reported"" when depth was reported based on instrumentation or described in literature. Locality A specific named place or named feature of origin for the specimen or observation (e.g. Dixon Entrance Diaphus Bank or Sur Ridge). Multiple locality names can be separated by a semicolon arranged in a list from largest to smallest area (e.g. Gulf of Mexico; West Florida Shelf Pulley Ridge). IdentificationQualifier Taxonomic identification method and level of expertise. Examples “genetic ID”; “morphological ID from sample by taxonomic expert”; “ID by expert from image”; “ID by non-expert from video”; etc.  SamplingEquipment Method of data collection. Examples ROV submersible towed camera SCUBA etc. RecordType Denotes the origin and type of record. published literature (""literature""); a collected specimen (""specimen""); observation from a still image (""still image""); observation from video (""video observation""); notation without a specimen or image (""notation""); or observation from trawl surveys longline surveys and/or observer records (""catch record"").  Acknowledgements Big shout out to NOAA and it's partners. Thank you for being scientists!  The original and probably more up-to-date dataset can be found here https//deepseacoraldata.noaa.gov/website/AGSViewers/DeepSeaCorals/mapSites.htm This dataset hasn't been changed in anyway. NOAA (2015) National Database for Deep-Sea Corals and Sponges (version 20170324-0). https//deepseacoraldata.noaa.gov/; NOAA Deep Sea Coral Research & Technology Program. Inspiration Who doesn't love coral and sponges?!  I challenge you to find the best algorithm that successfully SAVES the world's corals 100% of the time! ",CSV,,"[science and culture, fishing, oceans]",CC0,,,159,1952,139,Coral Records from NOAA’s Deep-Sea Coral Research and Technology Program,Deep Sea Corals,https://www.kaggle.com/noaa/deep-sea-corals,Mon Aug 28 2017
,Aleksey Bilogur,"[id, title, inscription, latitude, longitude, country, area, address, erected, main_photo, colour, organisations, language, series, series_ref, geolocated?, photographed?, number_of_subjects, lead_subject_name, lead_subject_born_in, lead_subject_died_in, lead_subject_type, lead_subject_roles, lead_subject_wikipedia, lead_subject_dbpedia, lead_subject_image, subjects]","[numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, boolean, boolean, numeric, string, string, string, string, string, string, string, string, string]","Context Blue plaques are a well-known and very popular permanent historical marker scheme administered in the United Kingdom that has since spread to many other countries in Europe and the world. According to Wikipedia  A blue plaque is a permanent sign installed in a public place in the United Kingdom and elsewhere to commemorate a link between that location and a famous person or event serving as a historical marker. The brainchild of British politician William Ewart in 1863 it is the oldest such scheme in the world. The world's first blue plaques were erected in London in the 19th century to mark the homes and workplaces of famous people. This scheme continues to the present day...the term ""blue plaque"" may be used narrowly to refer to the official English Heritage scheme but is often used informally to encompass all similar schemes.""  Here's what a model blue plaque looks like  (image via Wikimedia Commons) This dataset contains data about most of the blue plaques installed in Europe as of June 2017 as reported by Open Plaques. Content This dataset contains information on the location of each plaque who the subject is and metadata about the person or organization being recognized. Acknowledgements This dataset is republished as-is from the original on Open Plauqes. Inspiration  Where are the blue plaques located? What kinds of people and places get awarded a plaque? What is the geospatial distribution of these plaques throughout the UK? Worldwide? ",CSV,,"[culture and humanities, europe, history]",CC0,,,69,513,26,Almost 40000 blue plaque historical markers worldwide,Blue Plaques,https://www.kaggle.com/residentmario/blue-plaques,Mon Nov 06 2017
,NLTK Data,[],[],Context The Word2Vec sample model redistributed by NLTK is used to demonstrate how word embeddings can be used together with Gensim.  The detailed demonstration can be found on https//github.com/nltk/nltk/blob/develop/nltk/test/gensim.doctest  Acknowledgements Credit goes to Word2Vec and Gensim developers. ,Other,,[],Other,,,118,1606,132,Sample Word2Vec Model,Word2Vec Sample,https://www.kaggle.com/nltkdata/word2vec-sample,Sun Aug 20 2017
,Sohier Dane,"[NCIC, AgencyName, ComplaintYear, TotalComplaintsReported, NonCriminalReported, MisdemeanorReported, FelonyReported, TotalComplaintsInDetentionReported, NonCriminalInDetentionReported, MisdemeanorInDetentionReported, FelonyInDetentionReported, TotalRacialProfilingComplaintsReported, RaceEthnicityReported, NationalityReported, GenderReported, AgeReported, ReligionReported, GenderIdentityExpressionReported, SexualOrientationReported, MentalDisabilityReported, PhysicalDisabilityReported, TotalComplaintsSustained, NonCriminalSustained, MisdemeanorSustained, FelonySustained, TotalComplaintsInDetentionSustained, NonCriminalInDetentionSustained, MisdemeanorInDetentionSustained, FelonyInDetentionSustained, TotalRacialProfilingComplaintsSustained, RaceEthnicitySustained, NationalitySustained, GenderSustained, AgeSustained, ReligionSustained, GenderIdentityExpressionSustained, SexualOrientationSustained, MentalDisabilitySustained, PhysicalDisabilitySustained, TotalComplaintsExonerated, NonCriminalExonerated, MisdemeanorExonerated, FelonyExonerated, TotalComplaintsInDetentionExonerated, NonCriminalInDetentionExonerated, MisdemeanorInDetentionExonerated, FelonyInDetentionExonerated, TotalRacialProfilingComplaintsExonerated, RaceEthnicityExonerated, NationalityExonerated, GenderExonerated, AgeExonerated, ReligionExonerated, GenderIdentityExpressionExonerated, SexualOrientationExonerated, MentalDisabilityExonerated, PhysicalDisabilityExonerated, TotalComplaintsNotSustained, NonCriminalNotSustained, MisdemeanorNotSustained, FelonyNotSustained, TotalComplaintsInDetentionNotSustained, NonCriminalInDetentionNotSustained, MisdemeanorInDetentionNotSustained, FelonyInDetentionNotSustained, TotalRacialProfilingComplaintsNotSustained, RaceEthnicityNotSustained, NationalityNotSustained, GenderNotSustained, AgeNotSustained, ReligionNotSustained, GenderIdentityExpressionNotSustained, SexualOrientationNotSustained, MentalDisabilityNotSustained, PhysicalDisabilityNotSustained, TotalComplaintsUnfounded, NonCriminalUnfounded, MisdemeanorUnfounded, FelonyUnfounded, TotalComplaintsInDetentionUnfounded, NonCriminalInDetentionUnfounded, MisdemeanorInDetentionUnfounded, FelonyInDetentionUnfounded, TotalRacialProfilingComplaintsUnfounded, RaceEthnicityUnfounded, NationalityUnfounded, GenderUnfounded, AgeUnfounded, ReligionUnfounded, GenderIdentityExpressionUnfounded, SexualOrientationUnfounded, MentalDisabilityUnfounded, PhysicalDisabilityUnfounded, TotalComplaintsPending, NonCriminalPending, MisdemeanorPending, FelonyPending, TotalComplaintsInDetentionPending, NonCriminalInDetentionPending, MisdemeanorInDetentionPending]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset provides information on violence in California's criminal justice system whether the victim was a civilian or an officer or if the incident occurred during the arrest or while a subject was in custody. It's composed of a few related datasets  Use of force incidents The use of force (URSUS) incidents that result in serious bodily injury or death or involved the discharge of a firearm are reported annually from LEAs and other entities throughout the state that employ peace officers. The URSUS data is narrowly defined and does not represent the totality of use of force incidents that occur in California. LEAs are only required to report use of force incidents that result in serious bodily injury or death of either the civilian or the officer and all incidents where there is a discharge of a firearm. As such caution must be used when using the data for comparisons or in calculating rates. Law enforcement officers killed Law Enforcement Officer's Killed or Assaulted (LEOKA) data are reported as part of the Federal Uniform Crime Reporting (UCR) Program by LEAs throughout the state. LEOKA data are summary data meaning it is a collection of information describing the totality of incidents not a collection at the detailed incident level. LEOKA is a federally mandated collection. From the 1960's until 1990 the CJSC did not retain any of the LEOKA data; the forms were passed along to the Federal Bureau of Investigation (FBI). In 1990 the DOJ began to collect and retain the data from the LEOKA form for statistical purposes but it wasn't until 2000 that full retention at the State level was defined and standardized. Death in Custody & Arrest-Related Deaths State and local law enforcement agencies and correctional facilities report information on deaths that occur in custody or during the process of arrest in compliance with Section 12525 of the California Government Code. Contributors include California law enforcement agencies county probation departments state hospitals and state correctional facilities. Data are subject to revision as reports are received by the California Department of Justice (DOJ); figures in previous and current releases may not match. Citizens' Complaints Against Peace Officers State and local law enforcement agencies that employ peace officers provide Citizens' Complaints Against Peace Officers (CCAPO) data via an annual summary. The information includes the number of criminal and non-criminal complaints reported by citizens and the number of complaints sustained. Assembly Bill 953 (2015) modified the reporting requirements to expand the types of findings and also include complaints based upon racial and identity profiling claims. 2016 was the first year of collection under the new reporting requirements.  Acknowledgements This dataset was made available by the state of California's open justice program. Photo by Meric Dagli.,CSV,,[crime],CC0,,,169,1198,20,"Records on use of force incidents, officer deaths, and deaths in prison",Arrest Related Violence in California,https://www.kaggle.com/sohier/arrest-related-violence-in-california,Fri Nov 03 2017
,alexattia,[],[],Feel free to check and recommend my Medium post Part 1 on a classification model  and Part 2 on a detection model  (Faster R-CNN) about this dataset and what I am doing with it.  You can also find the related GitHub repo here . Context As a big Simpsons fan I have watched a lot (and still watching) of The Simpson episodes -multiple times each- over the years. I wanted to build a neural network which can recognize characters Content I am still building this dataset (labeling pictures) I will upload new versions of this dataset. Please check the files there are descriptions and explanations. File simpson-set.tar.gz  This is an image dataset 20 folders (one for each character) with 400-2000 pictures in each folder. File simpson-test-set.zip.  Preview of the image dataset File weights.best.h5  Weights computed in order to predict in Kernels. File annotation.txt  Annotation file for bounding boxes for each character  Help me to build this dataset If someone wants to contribute and make this dataset bigger and more relevant any help will be appreciated. Acknowledgements Data is directly taken and labeled from TV show episodes.,Other,,"[popular culture, image data, object detection]",CC4,,,2712,16655,588,Image dataset of 20 characters from The Simpsons,The Simpsons Characters Data,https://www.kaggle.com/alexattia/the-simpsons-characters-dataset,Sat Jul 01 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[STATE_UT_NAME, DISTRICT, JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, DEC, ANNUAL, Jan-Feb, Mar-May, Jun-Sep, Oct-Dec]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context This data set contains monthly rainfall detail of 36 meteorological sub-divisions of India. Content Time Period 1901 - 2015 Granularity Monthly Location 36 meteorological sub-divisions in India  Rainfall unit mm  Acknowledgements India Meteorological Department(IMD) Govt. of India has shared this dataset  under Govt. Open Data License - India.,CSV,,"[india, weather]",CC4,,,1316,6106,0.5693359375,Sub-division wise monthly data for 115 years from 1901-2015.,Rainfall in India,https://www.kaggle.com/rajanand/rainfall-in-india,Sat Aug 05 2017
,PromptCloud,"[advertiserurl, company, employmenttype_jobstatus, jobdescription, jobid, joblocation_address, jobtitle, postdate, shift, site_name, skills, uniq_id]","[string, string, string, string, string, string, string, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 4.6 million job listings) that was created by extracting data from Dice.com a prominent US-based technology job board. Content This dataset has following fields  advertiserurl  company  employmenttype_jobstatus  jobdescription  joblocation_address  jobtitle  postdate  shift  skills  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the job description with respect to the job title and skills can be performed.,CSV,,[internet],CC4,,,396,2961,58,"22,000 US-based Technology Job Listings",U.S. Technology Jobs on Dice.com,https://www.kaggle.com/PromptCloudHQ/us-technology-jobs-on-dicecom,Fri Sep 15 2017
,Soumitra Agarwal,"[Artist, Urls]","[string, string]",Dataset for people who double on Music and Data Science How it began One fine day when someone with the idea of self sufficient AI capable of writing it's own poems wanted data he stumbled upon the idea of using songs as a source. The journey wasn't easy since each song has it's own page with the lyrics and scraping pages one at a time (when there are over a million of them) is a slow task. I worked up some optimisations here and there. For people interested in going through the process GITHUB PROJECT.  Content There is a lot to play with here though all of it vertically. There isn't a lot of variation in the types of fields until you look deep enough. The main dataset is split up into 2 files each containing ~250000 songs with their artists and lyrics. The files are titled Lyrics1 and Lyrics2. The fields include band name song name and lyrics.   Go through the exploration scripts for how to use the lyrics data set and other interesting observations. Another file containing urls of description pages of different artists is also provided titled ArtistUrls.  Acknowledgements After seeing the response for the FIFA PLAYER DATASET it was exciting as ever to get this one off.  Soon it was realised not a lot of places exist where extracting data is straightforward.  Then one stumbles upon the perfectly indexed page  https//www.lyrics.com/. They deserve major credit for the existence of this dataset.  Inspiration This started off as a source for some kind of intelligent poet which writes poems on it's own. It would be great to see what the artificially intelligent world has to express once it knows enough and beautifully if at all?,CSV,,"[languages, music, internet]",ODbL,,,626,5662,601,"Over 500,000 song lyrics, urls for over a million artists",Every song you have heard (almost)!,https://www.kaggle.com/artimous/every-song-you-have-heard-almost,Sun Aug 20 2017
,Safecast,"[Captured Time, Latitude, Longitude, Value, Unit, Location Name, Device ID, MD5Sum, Height, Surface, Radiation, Uploaded Time, Loader ID]","[dateTime, numeric, numeric, numeric, string, string, string, string, string, string, string, dateTime, numeric]","Context Safecast is a volunteer driven non-profit organization whose goal is to create useful accessible and granular environmental data for public information and research. Begun in response to the nuclear disaster in Japan in March 2011 Safecast collects radiation and other environmental data from all over the world. All Safecast data is published free of charge under a CC0 designation.  The official “SAFECAST data” published for others to use is collected by Safecast volunteers using professional quality devices. A combination of off the shelf commercial radiation monitors and devices are used in the collection process. Most devices are standardized on the same sensor the LND7317 which is commonly referred to as the 2″ pancake. This is a highly sensitive piece of equipment that is used by nuclear professionals all over the world. Content ""Presently we assume the radiation comes from cesium-137 which is the most prevalent isotope still around from nuclear weapons testing and from the accidents at Chernobyl and Fukushima.  Based on calibration tests of multiple device designs using the same detector we've settled on a conversion factor of 334. That is 334CPM from a bGeigie equates to 1uSv/h. It would be more accurate to have conversion factors tuned to each locale based on the spectrum of radiation present but we don't have much of that data and in practice the error is estimated to be relatively small. "" - Joe Moross from Safecast  Captured Time Time data was captured Latitude Longitude Value Actual data in whatever units are in the ""Unit"" field. About 130K out of over 80 million measurements are not in CPM (from a 2-inch pancake Geiger tube) Unit Describes the raw form of the data. The vast majority of the measurements in the database are from Safecast-designed bGeigies which record radiation levels in CPM (counts per minute) Location Name Device ID MD5Sum Height Height from the ground in meters Surface Denotes data recorded very close to a surface such as pavement. This is typically done at 1cm height so should be regarded as contamination density and displayed or analyzed in units such as Becquerels not as dose data. Radiation Uploaded Time Loader ID  Acknowledgements Thank you to Safecast for collecting and sharing this dataset. The source files were downloaded from Safecast.org and have not been modified. Inspiration Safecast shared this datset for the public to have an un-biased source of radiation measurements. Use this dataset to see where Safecast volunteers have recorded data. ",CSV,,[],CC0,,,70,763,3072,80 million radiation readings from volunteers around the world,Safecast Radiation Measurements,https://www.kaggle.com/safecast/safecast,Thu Dec 07 2017
,Rachael Tatman,[],[],Context The Spoken Wikipedia project unites volunteer readers of Wikipedia articles. Hundreds of spoken articles in multiple languages are available to users who are – for one reason or another – unable or unwilling to consume the written version of the article. This is time-aligned corpus of these spoken articles well suited to research and fostering new ways of interacting with the material. Content All spoken articles use a template with slots filename speaker date and revision spoken ... to insert the audio player and a display of the meta-data on the Wikipedia page. The template also adds spoken articles to a root category. Templates categories and meta-data vary between language communities! The Dutch language portion of this corpus contains 3073 articles read by 145 speakers. There are 224 hours of speech of which 79 hours is aligned at the word level. Each article is tokenized into sections sentences and tokens. Each token is normalized and the normalization is aligned to the audio. Complete information on the annotation schema can be found in the schema file. For each article the corpus contains   audio file(s) original WikiText  HTML generated by MediaWiki  cleaned and normalized text  alignment between text and audio  meta-information (who when what)  For additional information and updates please see the project website. Acknowledgements This dataset was collected by Arne Köhn Florian Stegen and Timo Baumann. If you use this dataset in your work please cite the following paper Köhn A. Stegen F. & Baumann T. (2016). Mining the Spoken Wikipedia for Speech Data and Beyond. In the Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC 2016). Inspiration Some possible uses for this corpus include  train or evaluate automatic speech recognition systems improve accessibility / spoken article navigation top contributors speak >30 hours which is enough audio to train synthesis voices analyze prosody of reading (large amounts of diversely read text)  analyze prosody of information structure (accessible through links research on semantic Wikipedia Dbpedia ...) ,Other,,"[languages, europe, acoustics, linguistics]",CC4,,,29,433,8192,224 hours of Dutch audio with transcriptions,Spoken Wikipedia Corpus (Dutch),https://www.kaggle.com/rtatman/spoken-wikipedia-corpus-dutch,Thu Jan 04 2018
,Centers for Disease Control and Prevention,"[_STATE, _GEOSTR, _DENSTR, LISTSTAT, PRECALL, REPNUM, REPDEPTH, _RECORD, FMONTH, IMONTH, IDAY, IYEAR, INTVID, DISPCODE, WINDDOWN, SEQNO, _PSU, NATTMPTS, NRECSEL, NRECSTR, NUMADULT, NUMMEN, NUMWOMEN, GENHLTH, PHYSHLTH, MENTHLTH, POORHLTH, HLTHPLAN, NOCOVR12, PERSDOC2, EXERANY2, BPHIGH2, BPMEDS, BLOODCHO, CHOLCHK, TOLDHI2, ASTHMA2, ASTHNOW, DIABETES, PAIN12MN, SYMTMMTH, LMTJOINT, JOINTRT, HAVARTH, TRTARTH, FLUSHOT, PNEUVAC2, SMOKE100, SMOKEDAY, STOPSMK2, ALCDAYS, AVEDRNK, DRNK2GE5, FIREARM3, AGE, HISPANC2, MRACE, ORACE2, MARITAL, CHILDREN, EDUCA, EMPLOY, INCOME2, WEIGHT, HEIGHT, CTYCODE, NUMHHOL2, NUMPHON2, CELLPHON, SEX, PREGNT2, QLACTLM2, USEEQUIP, JOBACTIV, MODPACT, MODPADAY, MODPATIM, VIGPACT, VIGPADAY, VIGPATIM, PSATEST, PSATIME, DIGRECEX, DRETIME, PROSTATE, PROSHIST, BLDSTOOL, LSTBLDS2, HADSIGM2, LASTSIG2, HIVTF1A, HIVTF1B, HIVOPT1A, HIVOPT1B, HIVTST3, HIVTSTDT, RSNTST3, WHRTST4, PCSAIDS2, DIABAGE2]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The objective of the BRFSS is to collect uniform state-specific data on preventive health practices and risk behaviors that are linked to chronic diseases injuries and preventable infectious diseases in the adult population. Factors assessed by the BRFSS include tobacco use health care coverage HIV/AIDS knowledge or prevention physical activity and fruit and vegetable consumption. Data are collected from a random sample of adults (one per household) through a telephone survey. The Behavioral Risk Factor Surveillance System (BRFSS) is the nation's premier system of health-related telephone surveys that collect state data about U.S. residents regarding their health-related risk behaviors chronic health conditions and use of preventive services. Established in 1984 with 15 states BRFSS now collects data in all 50 states as well as the District of Columbia and three U.S. territories. BRFSS completes more than 400000 adult interviews each year making it the largest continuously conducted health survey system in the world. Content  Each year contains a few hundred columns. Please see one of the annual code books for complete details. These CSV files were converted from a SAS data format using pandas; there may be some data artifacts as a result. If you like this data you might also enjoy the 2011-2015 batch. Please note that those years use a different format.  Acknowledgements This dataset was released by the CDC. You can find the original dataset manuals and additional years of data here.,CSV,,"[mental health, public health]",CC0,,,215,1307,4096,Behavioral Risk Factor Surveillance System for 2001-2010,BRFSS 2001-2010,https://www.kaggle.com/cdc/brfss-20012010,Thu Aug 24 2017
,Kris,"[Date Time, p (mbar), T (degC), Tpot (K), Tdew (degC), rh (%), VPmax (mbar), VPact (mbar), VPdef (mbar), sh (g/kg), H2OC (mmol/mol), rho (g/m**3), wv (m/s), max. wv (m/s), wd (deg)]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context The Dataset is used by ""A temperature-forecasting problem"" from the ""Deep Learning with Python"" book Content The data was downloaded from https//s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip It represents time period between 2009 and 2016 Acknowledgements The dataset recorded at the Weather Station at the Max Planck Institute for Biogeochemistry in Jena Germany. https//www.bgc-jena.mpg.de/wetter/ It was reassembled by François Chollet the author of the ""Deep Learning with Python"" book Inspiration The main purpose of this dataset is to perform RNN exercise (6.3.1 A temperature-forecasting problem) from the ""Deep Learning with Python"" book.",CSV,,"[climate, weather]",Other,,,157,1333,13,"Air temperature, atmospheric pressure, humidity, etc recorded over seven years",Weather archive Jena,https://www.kaggle.com/pankrzysiu/weather-archive-jena,Sun Jan 21 2018
,Documenting the American South (DocSouth),[],[],"The goal of the ""Library of Southern Literature"" is to make one hundred of the most important works of Southern literature published before 1920 available world-wide for teaching and research. Currently this collection includes over eighty titles that were digitized with special funding from the Chancellor and the University Library of the University of North Carolina at Chapel Hill. The Southern United States has been a distinct region since the colonial period and its literature has developed in connection with but also divergently from American literature as a whole. The South claims prominent and world-renowned authors including Edgar Allan Poe and Mark Twain but also lesser known authors who created works that reflected Southern attitudes and experiences. Teachers of literature as well as historians and other scholars of Southern culture need access to literary texts that illustrate these differences and many of these important Southern works are no longer in print and are not widely held in libraries. Noting this lack of available titles the late Dr. Robert Bain volunteered to help Documenting the American South by developing the bibliography for it. Dr. Bain was a faculty member at the University of North Carolina at Chapel Hill and taught American and Southern literature from 1964 until 1995. He also co-edited five scholarly works on Southern writers. To prepare this bibliography of Southern Literature Dr. Bain wrote to some fifty scholars throughout the United States who specialize in Southern and American literature requesting that they nominate what they considered to be the ten most important works of Southern literature published before 1920. From their responses Dr. Bain compiled the bibliography on which this collection is based. He completed the list three months before his death in July 1996. With additional funding from the University Library this collection continues to grow. Dr. Joe Flora professor of English at UNC-Chapel Hill guides the expansion of this collection beyond Dr. Bain's original bibliography. The original texts for the ""Library of Southern Literature"" come from the University Library of the University of North Carolina at Chapel Hill which includes the Southern Historical Collection one of the largest collections of Southern manuscripts in the country and the North Carolina Collection the most complete printed documentation of a single state anywhere. The DocSouth Editorial Board composed of faculty and librarians at UNC and staff from the UNC Press oversees this collection and all other collections on Documenting the American South. Context The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world. The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives. More information about the collection and access to individual page images can be be found here http//docsouth.unc.edu/neh The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives. The .csv file acts as a table of contents for the collection and includes Title Author Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant http//voyant-tools.org/).  Copyright Statement and Acknowledgements With the exception of ""Fields's Observation The Slave Narrative of a Nineteenth-Century Virginian"" which has no known rights the texts encoding and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0http//creativecommons.org/licenses/by/4.0/). Users are free to copy share adapt and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available. If you make use of this data considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu. About the DocSouth Data Project Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools. Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured corrected and machine readable text. Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http//voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling named-entity recognition or sentiment analysis.",CSV,,"[literature, united states, history, linguistics]",Other,,,69,671,46,The full text of 115 influential works of Southern literature,Library of Southern Literature,https://www.kaggle.com/docsouth-data/library-of-southern-literature,Tue Aug 15 2017
,Team PuppyGoGo,"[State, Region, Reference, Animal_Type, Animal_Name, Breed_Description, Status_Description, Suburb, Year, Gender_Desexed, Colour, Animal_Desexed, Animal Date of Birth, latitude, longitude]","[string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, numeric, numeric]",Context... Ever wondered the what and where of dog ownership? So have we! Content... Have a look at a sample set of South Australian and Victorian animal registration data. Data is publicly available from the data.gov.au website under a creative commons licence. Information includes breed location desexed and colour. Datasets are for the 2015 2016 & 2017 periods (depending on availability). SA information has been consolidated in ~82500 lines of data! Acknowledgements... A big thank you to the SA and Victorian shires for having such great datasets! Inspiration... We love dogs and really want to understand the distribution of pets across SA and Victoria. We will leave it up to you the insights you want to create!,CSV,,"[australia, animals, pets, demographics]",CC0,,,90,851,3,Love animals? Have a crack at providing geographic insights on animal ownership!,SA & Victorian pet ownership data,https://www.kaggle.com/puppygogo/sa-dog-ownership-sample,Fri Nov 24 2017
,Jacob Boysen,"[dropoff_site, load_id, load_time, load_type, load_weight, report_date, route_number, route_type]","[string, numeric, dateTime, string, numeric, dateTime, string, string]",Context This dataset is trash. Who in Austin makes it who takes it and where does it go? Content Data ranges 2008-2016 and includes dropoff site load id time of load type of load weight of load date route number and route type (recycling street cleaning garbage etc). Acknowledgements This dataset was created by Austin city government and hosted on Google Cloud Platform. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too Inspiration  How much trash is Austin generating? Which are the trashiest routes? Who recycles the best? Any seasonal changes? Try to predict trash route usage from historical trash data ,CSV,,[government agencies],CC0,,,89,1045,61,"Garbage In, Garbage Out",Austin Waste and Diversion,https://www.kaggle.com/jboysen/austin-waste,Sat Aug 26 2017
,Zillow,[],[],Context Zillow's Economic Research Team collects cleans and publishes housing and economic data from a variety of public and proprietary sources. Public property record data filed with local municipalities -- including deeds property facts parcel information and transactional histories -- forms the backbone of our data products and is fleshed out with proprietary data derived from property listings and user behavior on Zillow.  The large majority of Zillow's aggregated housing market and economic data is made available for free download at zillow.com/data. Content Variable Availability  Zillow Home Value Index (ZHVI) A smoothed seasonally adjusted measure of the median estimated home value across a given region and housing type. A dollar denominated alternative to repeat-sales indices. Find a more detailed methodology here http//www.zillow.com/research/zhvi-methodology-6032/  Zillow Rent Index (ZRI) A smoothed seasonally adjusted measure of the median estimated market rate rent across a given region and housing type. A dollar denominated alternative to repeat-rent indices. Find a more detailed methodology here http//www.zillow.com/research/zillow-rent-index-methodology-2393/ For-Sale Listing/Inventory Metrics Zillow provides many variables capturing current and historical for-sale listings availability generally from 2012 to current. These variables include median list prices and inventory counts both by various property types. Variables capturing for-sale market competitiveness including share of listings with a price cut median price cut size age of inventory and the days a listing spend on Zillow before the sale is final.  Home Sales Metrics Zillow provides data on sold homes including median sale price by various housing types sale counts (methodology here http//www.zillow.com/research/home-sales-methodology-7733/) and a normalized view of sale volume referred to as turnover. The prevalence of foreclosures is also provided as ratio of the housing stock and the share of all sales in which the home was previously foreclosed upon.  For-Rent Listing Metrics Zillow provides median rents prices and median rent price per square foot by property type and bedroom count. Housing type definitions All Homes Zillow defines all homes as single-family condominium and co-operative homes with a county record. Unless specified all series cover this segment of the housing stock. Condo/Co-op Condominium and co-operative homes. Multifamily 5+ units Units in buildings with 5 or more housing units that are not a condominiums or co-ops. Duplex/Triplex Housing units in buildings with 2 or 3 housing units. Tiers By metro we determine price tier cutoffs that divide the all homes housing stock into thirds using the full distribution of estimated home values. We then estimate real estate metrics within the property sets Bottom Middle and Top defined by these cutoffs. When reported at the national level all Bottom Tier homes defined at the metro level are pooled together to form the national bottom tier. The same holds for Middle and Top Tier homes.     Regional Availability  Zillow metrics are reported for common US geographies including Nation State Metro (2013 Census Defined CBSAs) County City ZIP code and Neighborhood.  We provide a crosswalk between colloquial Zillow region names and federally defined region names and linking variables such as County FIPS codes and CBSA codes. Cities and Neighborhoods do not match standard jurisdictional boundaries. Zillow city boundaries reflect mailing address conventions and so are often visually similar to collections of ZIP codes. Zillow neighborhood boundaries can be found here.  Suppression Rules To ensure reliability of reported values the Zillow Economic Research team applies suppression rules triggered by low sample sizes and excessive volatility. These rules are customized to the metric and region type and explain most missingness found in the provided datasets.  Additional Data Products  The following data products and more are available for free download exclusively at Zillow.com/Data  Zillow Home Value Forecast Zillow Rent Forecast Negative Equity (the share of mortgaged properties worth less than mortgage balance) Zillow Home Price Expectations Survey Zillow Housing Aspirations Report Zillow Rising Sea Levels Research Cash Buyers Time Series Buy vs. Rent Breakeven Horizon Mortgage Affordability Rental Affordability Price-to-Income Ratio Conventional 30-year Fixed Mortgage Rate Weekly Time Series Jumbo 30-year Fixed Mortgage Rates Weekly Time Series   Acknowledgements The mission of the Zillow Economic Research Team is to be the most open authoritative source for timely and accurate housing data and unbiased insight. We aim to empower consumers industry professionals policy makers and researchers looking to better understand the housing market. To see more of our mission in action we invite you to learn more about us and to check out our collection of research briefs stories data tools and past presentations at https//www.zillow.com/research/ Inspiration Zillow and the Zillow Economic Research Team firmly believe that not only do data want to be free data are going to be free. Instead of simply publishing raw data we believe in the power of pushing data up the ladder from raw data bits to actionable information and finally to unique insight. We aim to answer questions of all kinds even questions our users may not have known they had before coming to us. When done right we firmly believe this process of turning data into insight can be transformational in people's lives. Please join us on this journey and we're excited to see what insights you can discover hidden amongst our data!,CSV,,"[housing, business, demographics, economics]",Other,,,4060,40200,511,Turning on the lights in housing research.,Zillow Economics Data,https://www.kaggle.com/zillow/zecon,Thu Jan 25 2018
,Keik@,"[NU_ANO_CENSO|CO_ENTIDADE|NO_ENTIDADE|CO_ORGAO_REGIONAL|TP_SITUACAO_FUNCIONAMENTO|DT_ANO_LETIVO_INICIO|DT_ANO_LETIVO_TERMINO|CO_REGIAO|CO_MESORREGIAO|CO_MICRORREGIAO|CO_UF|CO_MUNICIPIO|CO_DISTRITO|TP_DEPENDENCIA|TP_LOCALIZACAO|TP_CATEGORIA_ESCOLA_PRIVADA|I, CO_ENTIDADE, NO_ENTIDADE, CO_ORGAO_REGIONAL, TP_SITUACAO_FUNCIONAMENTO, DT_ANO_LETIVO_INICIO, DT_ANO_LETIVO_TERMINO, CO_REGIAO, CO_MESORREGIAO, CO_MICRORREGIAO, CO_UF, CO_MUNICIPIO, CO_DISTRITO, TP_DEPENDENCIA, TP_LOCALIZACAO, TP_CATEGORIA_ESCOLA_PRIVADA, IN_CONVENIADA_PP, TP_CONVENIO_PODER_PUBLICO, IN_MANT_ESCOLA_PRIVADA_EMP, IN_MANT_ESCOLA_PRIVADA_ONG, IN_MANT_ESCOLA_PRIVADA_SIND, IN_MANT_ESCOLA_PRIVADA_SIST_S, IN_MANT_ESCOLA_PRIVADA_S_FINS, CO_ESCOLA_SEDE_VINCULADA, CO_IES_OFERTANTE, TP_REGULAMENTACAO, IN_LOCAL_FUNC_PREDIO_ESCOLAR, TP_OCUPACAO_PREDIO_ESCOLAR, IN_LOCAL_FUNC_SALAS_EMPRESA, IN_LOCAL_FUNC_SOCIOEDUCATIVO, IN_LOCAL_FUNC_UNID_PRISIONAL, IN_LOCAL_FUNC_PRISIONAL_SOCIO, IN_LOCAL_FUNC_TEMPLO_IGREJA, IN_LOCAL_FUNC_CASA_PROFESSOR, IN_LOCAL_FUNC_GALPAO, TP_OCUPACAO_GALPAO, IN_LOCAL_FUNC_SALAS_OUTRA_ESC, IN_LOCAL_FUNC_OUTROS, IN_PREDIO_COMPARTILHADO, IN_AGUA_FILTRADA, IN_AGUA_REDE_PUBLICA, IN_AGUA_POCO_ARTESIANO, IN_AGUA_CACIMBA, IN_AGUA_FONTE_RIO, IN_AGUA_INEXISTENTE, IN_ENERGIA_REDE_PUBLICA, IN_ENERGIA_GERADOR, IN_ENERGIA_OUTROS, IN_ENERGIA_INEXISTENTE, IN_ESGOTO_REDE_PUBLICA, IN_ESGOTO_FOSSA, IN_ESGOTO_INEXISTENTE, IN_LIXO_COLETA_PERIODICA, IN_LIXO_QUEIMA, IN_LIXO_JOGA_OUTRA_AREA, IN_LIXO_RECICLA, IN_LIXO_ENTERRA, IN_LIXO_OUTROS, IN_SALA_DIRETORIA, IN_SALA_PROFESSOR, IN_LABORATORIO_INFORMATICA, IN_LABORATORIO_CIENCIAS, IN_SALA_ATENDIMENTO_ESPECIAL, IN_QUADRA_ESPORTES_COBERTA, IN_QUADRA_ESPORTES_DESCOBERTA, IN_QUADRA_ESPORTES, IN_COZINHA, IN_BIBLIOTECA, IN_SALA_LEITURA, IN_BIBLIOTECA_SALA_LEITURA, IN_PARQUE_INFANTIL, IN_BERCARIO, IN_BANHEIRO_FORA_PREDIO, IN_BANHEIRO_DENTRO_PREDIO, IN_BANHEIRO_EI, IN_BANHEIRO_PNE, IN_DEPENDENCIAS_PNE, IN_SECRETARIA, IN_BANHEIRO_CHUVEIRO, IN_REFEITORIO, IN_DESPENSA, IN_ALMOXARIFADO, IN_AUDITORIO, IN_PATIO_COBERTO, IN_PATIO_DESCOBERTO, IN_ALOJAM_ALUNO, IN_ALOJAM_PROFESSOR, IN_AREA_VERDE, IN_LAVANDERIA, IN_DEPENDENCIAS_OUTRAS, NU_SALAS_EXISTENTES, NU_SALAS_UTILIZADAS, IN_EQUIP_TV, IN_EQUIP_VIDEOCASSETE, IN_EQUIP_DVD, IN_EQUIP_PARABOLICA, IN_EQUIP_COPIADORA, IN_EQUIP_RETROPROJETOR, IN_EQUIP_IMPRESSORA, IN_EQUIP_IMPRESSORA_MULT]","[string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",O Censo Escolar é um levantamento de dados estatístico-educacionais de âmbito nacional realizado todos os anos e coordenado pelo Inep. Ele é feito com a colaboração das secretarias estaduais e municipais de Educação e com a participação de todas as escolas públicas e privadas do país. Trata-se do principal instrumento de coleta de informações da educação básica que abrange as suas diferentes etapas e modalidades ensino regular (educação Infantil e ensinos fundamental e médio) educação especial e educação de jovens e adultos (EJA). O Censo Escolar coleta dados sobre estabelecimentos matrículas funções docentes movimento e rendimento escolar. Essas informações são utilizadas para traçar um panorama nacional da educação básica e servem de referência para a formulação de políticas públicas e execução de programas na área da educação incluindo os de transferência de recursos públicos como merenda e transporte escolar distribuição de livros e uniformes implantação de bibliotecas instalação de energia elétrica Dinheiro Direto na Escola e Fundo de Manutenção e Desenvolvimento da Educação Básica e de Valorização dos Profissionais da Educação (Fundeb). Além disso os resultados obtidos no Censo Escolar sobre o rendimento (aprovação e reprovação) e movimento (abandono) escolar dos alunos do ensino Fundamental e Médio juntamente com outras avaliações do Inep (Saeb e Prova Brasil) são utilizados para o cálculo do Índice de Desenvolvimento da Educação Básica (IDEB) indicador que serve de referência para as metas do Plano de Desenvolvimento da Educação (PDE) do Ministério da Educação. Para saber mais sobre o Censo Escolar http//portal.inep.gov.br/basica-censo Apresentados em formato ASCII os microdados são acompanhados de inputs ou seja canais de entrada para leitura dos arquivos por meio da utilização dos softwares SAS e SPSS. Os Microdados passaram a ser estruturados em formato CSV (Comma-Separated Values) e seus dados estão delimitados por Pipe ( | ) de modo a garantir que praticamente qualquer software estatístico inclusive open source consiga importar e carregar as bases de dados. Devido à amplitude de nossas bases os arquivos foram divididos por região geográfica (Norte Nordeste Sudeste Sul e Centro-Oeste) tanto para as variáveis de Matrículas quanto para as de Docentes. ,CSV,,[education],ODbL,,,49,574,92,Parcial:  Escolas + Turmas,Microdados Censo Escolar 2015,https://www.kaggle.com/lucianakeiko/microdados-censo-escolar-2015,Thu Sep 14 2017
,Rachael Tatman,[],[],Context GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus and the resulting representations showcase interesting linear substructures of the word vector space. Content This dataset contains English word vectors pre-trained on the combined Wikipedia 2014 + Gigaword 5th Edition corpora (6B tokens 400K vocab). All tokens are in lowercase. This dataset contains 50-dimensional 100-dimensional and 200-dimensional pre trained word vectors. For 300-dimensional word vectors and additional information please see the project website.  Acknowledgements This data has been released under the Open Data Commons Public Domain Dedication and License. If you use this dataset in your work please cite the following paper   Jeffrey Pennington Richard Socher and Christopher D. Manning. 2014. GloVe Global Vectors for Word Representation. URL https//nlp.stanford.edu/pubs/glove.pdf  Inspiration GloVe embeddings have been used in more than 2100 papers and counting! You can use these pre-trained embeddings whenever you need a way to quantify word co-occurrence (which also captures some aspects of word meaning.),Other,,"[languages, linguistics]",Other,,,609,5205,1024,Pre-trained word vectors from Wikipedia 2014 + Gigaword 5,GloVe: Global Vectors for Word Representation,https://www.kaggle.com/rtatman/glove-global-vectors-for-word-representation,Sat Aug 05 2017
,TruMedicines,[],[],Context 1K dataset of speckled pharmaceutical pills. Using a CNN to extract features and create binary hash code these pills can be retrieved from a mobile device for remote identification. Every pill can be tracked using a mobile phone app. Content 1 K pharmaceutical pills jpeg images that have been convoluted by rotations grey scale noise non-pill  Acknowledgements Special thanks for Funding and support of Microsoft - Paul DeBaun and  NWCadence- Steve Borg Inspiration The Pill Crisis in America 1) Fake Fentanyl             - killing young people  2) Opioid Abuse             - killing all ages of people  3) Fake Online Drugs    -  killing unknown numbers 4) Non-Compliance       -  killing older people  Non-Compliance  up to 90% of diabetics don't take their meds enough to benefit  Up to 75% of hypertensive patients do not adhere to their medicine  Less than 27% depressed patients adhere to their medication 41-59% of mentally ill take their meds infrequently or not at all 33% of patients with schizophrenia don’t take their medicine at all ,Other,,"[healthcare, image data, multiclass classification]",ODbL,,,294,3329,8,Speckled pills CNN feature extracted for unique individual identification,1k Pharmaceutical Pill Image Dataset,https://www.kaggle.com/trumedicines/1k-pharmaceutical-pill-image-dataset,Thu Jul 13 2017
,paultimothymooney,[],[],Context Invasive Ductal Carcinoma (IDC) is the most common subtype of all breast cancers. To assign an aggressiveness grade to a whole mount sample pathologists typically focus on the regions which contain the IDC. As a result one of the common pre-processing steps for automatic aggressiveness grading is to delineate the exact regions of IDC inside of a whole mount slide. Content The original dataset consisted of 162 whole mount slide images of Breast Cancer (BCa) specimens scanned at 40x. From that 277524 patches of size 50  x 50 were extracted (198738 IDC negative and 78786 IDC positive).  Each patch’s file name is of the format  u_xX_yY_classC.png   — > example 10253_idx5_x1351_y1101_class0.png . Where u is the patient ID (10253_idx5) X is the x-coordinate of where this patch was cropped from Y is the y-coordinate of where this patch was cropped from and C indicates the class where 0 is non-IDC and 1 is IDC. Acknowledgements The original files are located here http//gleason.case.edu/webdata/jpi-dl-tutorial/IDC_regular_ps50_idx5.zip Citation https//www.ncbi.nlm.nih.gov/pubmed/27563488 and http//spie.org/Publications/Proceedings/Paper/10.1117/12.2043872 Inspiration Breast cancer is the most common form of cancer in women and invasive ductal carcinoma (IDC) is the most common form of breast cancer.  Accurately identifying and categorizing breast cancer subtypes is an important clinical task and automated methods can be used to save time and reduce error.,Other,,"[medicine, machine learning, image data, binary classification]",CC0,,,361,3800,2048,IDC vs non-IDC classification,Breast Histopathology Images,https://www.kaggle.com/paultimothymooney/breast-histopathology-images,Tue Dec 19 2017
,World Bank,"[Country Name, Country Code, Indicator Name, Indicator Code, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, ]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context HealthStats provides key health nutrition and population statistics gathered from a variety of international sources. Themes include population dynamics nutrition reproductive health health financing medical resources and usage immunization infectious diseases HIV/AIDS DALY population projections and lending. HealthStats also includes health nutrition and population statistics by wealth quintiles. Content This dataset includes 345 indicators such as immunization rates malnutrition prevalence and vitamin A supplementation rates across 263 countries around the world. Data was collected on a yearly basis from 1960-2016. Inspiration  In your opinion what are some of the more surprising indicators? Are there any you would consider adding? Is there a relationship between condom use and rates of children born with HIV? How do these rates compare over time? Which countries have the highest consumption of iodized salt? Has this indicator changed over time and if so in which countries? Are there any other indicators that seem to correlate with this one?   Acknowledgements Data was acquired from the World Bank and can be accessed in multiple formats here.,CSV,,"[nutrition, health, demographics]",Other,,,2940,19631,43,State of human health across the world,Health Nutrition and Population Statistics,https://www.kaggle.com/theworldbank/health-nutrition-and-population-statistics,Fri Nov 18 2016
,smeschke,[],[],This dataset contains 16000 images of four shapes; square star circle and triangle. Each image is 200x200 pixels. The data was collected using a Garmin Virb 1080p action camera. The shapes were cut from poster board and then painted green. I held each shape in view of the camera for two minutes. While the camera was recording the shape I moved the shape around and rotated it. The four videos were then processed using OpenCV in Python. Using colorspaces the green shape is cropped out of the image and resized to 200x200 pixels. The data is arranged into four folders; square circle triangle and star. The images are labeled 0.png 1.png etc...  A fifth video was taken with all of the shapes in the frame. This fifth video is for testing purposes. The goal is to classify the shapes in the test video using a model created with the training data. These classifications were made using a model made in Keras. How is this different than the MINST handwritten digits dataset? There are 10 classes in the MINST dataset and 4 in this shapes dataset. The images in this data set are rotated and the digits in the MINST data set are not.,Other,,"[beginner, image data, multiclass classification]",CC0,,,223,2008,22,"16,000 images of four basic shapes (star, circle, square, triangle)",Four Shapes,https://www.kaggle.com/smeschke/four-shapes,Tue Nov 21 2017
,Sohier Dane,"[time_period, federal_funds, 1_month_nonfinancial_commercial_paper, 2_month_nonfinancial_commercial_paper, 3_month_nonfinancial_commercial_paper, 1_month_financial_commercial_paper, 2_month_financial_commercial_paper, 3_month_financial_commercial_paper, prime_rate, discount_rate, 4_week_treasury_bill, 3_month_treasury_bill, 6_month_treasury_bill, 1_year_treasury_bill, 1_month_treasury_constant_maturity, 3_month_treasury_constant_maturity, 6_month_treasury_constant_maturity, 1_year_treasury_constant_maturity, 2_year_treasury_constant_maturity, 3_year_treasury_constant_maturity, 5_year_treasury_constant_maturity, 7_year_treasury_constant_maturity, 10_year_treasury_constant_maturity, 20_year_treasury_constant_maturity, 30_year_treasury_constant_maturity, 5_year_inflation_indexed_treasury_constant_maturity, 7_year_inflation_indexed_treasury_constant_maturity, 10_year_inflation_indexed_treasury_constant_maturity, 20_year_inflation_indexed_treasury_constant_maturity, 30_year_inflation_indexed_treasury_constant_maturity, inflation_indexed_long_term_average]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The H15 Release from the federal reserve provides daily interest rate information for a variety of core interest rates such as T bills and the federal funds rate. For some rates this dataset extends all the way back to 1954. The rates included are  time_period federal_funds 1_month_nonfinancial_commercial_paper 2_month_nonfinancial_commercial_paper 3_month_nonfinancial_commercial_paper 1_month_financial_commercial_paper 2_month_financial_commercial_paper 3_month_financial_commercial_paper prime_rate discount_rate 4_week_treasury_bill 3_month_treasury_bill 6_month_treasury_bill 1_year_treasury_bill 1_month_treasury_constant_maturity 3_month_treasury_constant_maturity 6_month_treasury_constant_maturity 1_year_treasury_constant_maturity 2_year_treasury_constant_maturity 3_year_treasury_constant_maturity 5_year_treasury_constant_maturity 7_year_treasury_constant_maturity 10_year_treasury_constant_maturity 20_year_treasury_constant_maturity 30_year_treasury_constant_maturity 5_year_inflation_indexed_treasury_constant_maturity 7_year_inflation_indexed_treasury_constant_maturity 10_year_inflation_indexed_treasury_constant_maturity 20_year_inflation_indexed_treasury_constant_maturity 30_year_inflation_indexed_treasury_constant_maturity inflation_indexed_long_term_average  Please see https//www.federalreserve.gov/releases/h15/ for notices caveats and newly released data.,CSV,,"[finance, government, banking]",Other,,,315,2005,2,Historic H15 Release Data from the Federal Reserve,Interest Rate Records,https://www.kaggle.com/sohier/interest-rate-records,Sat Aug 19 2017
,National Health Service,"[CHEM SUB, NAME]","[numeric, string]",Context The British National Health Service releases data covering every public sector prescription made in the country. This covers a single year of that data. Content Covering all general practices in England the data includes figures on the number of prescription items that are dispensed each month and information relating to costs. For each GP practice the total number of items that were prescribed and then dispensed is shown. The total Net Ingredient Cost and the total Actual Cost of these items is shown. Chemical level All prescribed and dispensed medicines (by chemical name) dressings and appliances (at section level) are listed for each GP practice. Presentation level All prescribed and dispensed medicines dressings and appliances are listed at presentation level for each GP practice. (Presentation level gives the individual drug name the form and strength or size accordingly). The total quantity of drugs dispensed (in terms of number of tablets or millilitres for example) is shown. This data does not list each individual prescription and does not contain any patient identifiable data. The data have been edited from their original version. During the data preparation process I  Dropped obsolete and redundant columns. Normalized the BNF (British National Formulary) codes BNF names and practice codes. These steps reduced the total file size by roughly 75% at the cost of requiring one table join to access some of the data.  For further details please see  FAQ Glossary of Terms  Acknowledgements This dataset was kindly released by the United Kingdom's National Health Service under their government open data license v3. You can find this and other datasets at their open data site. Inspiration  What trends can you see in the data? For example can you identify the onset of winter based on the types of drugs being prescribed? The BNF Name entries contain dosage data that I haven't yet cleaned and extracted. Can you unpack that field into item dispensed units and dosage? If so let me know in the forums and I'll add it to the dataset! Per this blog from Oxford the raw BNF codes contain quite a bit of information about a drug's function. Can you find a source of open data for translating these codes? It's probable that one exists somewhere at https//www.nhsbsa.nhs.uk/nhs-prescription-services. ,CSV,,[healthcare],Other,,,257,2794,4096,One year of British National Health Service Prescription data,General Practice Prescribing Data,https://www.kaggle.com/nhs/general-practice-prescribing-data,Thu Aug 10 2017
,Jacob Boysen,"[lsoa_code, borough, major_category, minor_category, value, year, month]","[string, string, string, string, numeric, numeric, numeric]",Context Crime in major metropolitan areas such as London occurs in distinct patterns. This data covers the number of criminal reports by month LSOA borough and major/minor category from Jan 2008-Dec 2016. Content 13M rows containing counts of criminal reports by month LSOA borough and major/minor category. Acknowledgements Txt file was pulled from Google Cloud Platform and converted to csv. Photo by James Sutton. Inspiration Are there seasonal or time-of-week/day changes in crime occurrences? Any boroughs where particular crimes are increasing or decreasing? Policy makers use this data to plan upcoming budgets and deployment--can you use previous year crime reports to reliably predict later trends? If you normalize by borough population can you find any areas where crime is more or less likely?,CSV,,[crime],CC0,,,647,4503,890,"13M Rows of Crime Counts, by Borough, Category, and Month","London Crime Data, 2008-2016",https://www.kaggle.com/jboysen/london-crime,Thu Aug 03 2017
,David Cohen,[],[],"Context Analyze the world's largest rock climbing logbook! Who's the biggest downgrader? Is it better to be short tall or average height? How many years does it take the average climber to send her first 5.12? After countless crag debates over these and similar topics I set out to find the answers. Now you can prove statistically why that dude who tagged your multi-year project ""Soft"" is wrong. (Sorry he might actually be right.) Content I used Python3 to build a web-scraper to collect all of the user and ascent information from the world's largest rock climbing logbook https//www.8a.nu/. I actually ended up scraping their beta site https//beta.8a.nu/ as it provided well-formed JSON objects. The scraper dumps all of the data into an SQLite database. Check out https//github.com/dcohen21/8a.nu-Scraper for more information about the project. This dataset was collected on 9/13/2017. The database consists of four tables User Ascent Method and Grade.  Acknowledgements Thanks to Andrew Cassidy (https//github.com/andrewcassidy) for the idea and mentorship. Thanks to Jens Larssen and 8a.nu for creating the logbook and maintaining a thriving community. Inspiration The sky's the limit!",SQLite,,"[sports, internet]",Other,,,200,7174,445,Analyze the world's largest rock climbing logbook!,8a.nu Climbing Logbook,https://www.kaggle.com/dcohen21/8anu-climbing-logbook,Wed Sep 27 2017
,Rachael Tatman,[],[],Context Word embeddings define the similarity between two words by the normalised inner product of their vectors. The matrices in this repository place languages in a single space without changing any of these monolingual similarity relationships. When you use the resulting multilingual vectors for monolingual tasks they will perform exactly the same as the original vectors.  Facebook recently open-sourced word vectors in 89 languages. However these vectors are monolingual; meaning that while similar words within a language share similar vectors translation words from different languages do not have similar vectors. In this dataset are 78 matrices which can be used to align the majority of the fastText languages in a single space. Contents This repository contains 78 matrices which can be used to align the majority of the fastText languages in a single space. This dataset was obtained by first getting the 10000 most common words in the English fastText vocabulary and then using the Google Translate API to translate these words into the 78 languages available. This vocabulary was then split in two assigning the first 5000 words to the training dictionary and the second 5000 to the test dictionary. The alignment procedure is discribed in this blog. It takes two sets of word vectors and a small bilingual dictionary of translation pairs in two languages; and generates a matrix which aligns the source language with the target. Sometimes Google translates an English word to a non-English phrase in these cases we average the word vectors contained in the phrase. To place all 78 languages in a single space every matrix is aligned to the English vectors (the English matrix is the identity). You can find more information on this dataset in the authors’ GitHub repository here. Acknowledgements This dataset was produced by Samuel Smith David Turban Steven Hamblin and Nils Hammerly. If you use this repository please cite Offline bilingual word vectors orthogonal transformations and the inverted softmax. Samuel L. Smith David H. P. Turban Steven Hamblin and Nils Y. Hammerla. ICLR 2017 (conference track) Inspiration  Can you use the word embeddings in this dataset to cluster languages into their families? Can you create a visualization of the relationship between words for similar concepts across languages? ,Other,,"[languages, linguistics]",CC4,,,171,2348,168,fastText vectors of 78 languages,Multilingual word vectors in 78 languages,https://www.kaggle.com/rtatman/multilingual-word-vectors-in-78-languages,Fri Sep 01 2017
,Jose Berengueres,"[, emoji, s.writer, s.reader, sd, count, description, , diff]","[string, string, numeric, numeric, numeric, numeric, string, string, numeric]",Are people that use emoji happier? paper --> https//arxiv.org/abs/1710.00888 At ASONAM2017 PydataDubai vol 1.0 @ AWOK PyDataBCN2017 @ EASDE we have presented the paper Happiness inside a job?... Many people in the various audiences asked why we avoid using emojis to predict and profile employees. The answer is that  we prefer to use  links of likes because they are more authentic than words or emojis. In the same way that google page rank is more effective when it looks at links between pages rather than content inside the pages. ... Still people keep asking about it. But there is one thing emoji are good at estimating author sentiment and that is just possible thanks to the unique characteristics of the dataset at hand. Previous research has traditionally analyzed emoji sentiment from the point of view of the reader of the content not the author. Here we analyze emoji sentiment from the author point of view and present a benchmark that was built from an employee happiness dataset where emoji happen to be annotated with daily happiness of the author of the comment. We also found out that people that use emoji are happier!? muuch happier... But the question is what did we miss? Content The main table contains columns named after emoji hex codes a 1 means the emoji appears one time in the comment (row). This dataset is an expanded version of this one but has different formats columns and one different table that is why we decided to release it as separate dataset. as he scripts are not compatible. Other stuff The R script written on MAC OS does not work in the kaggle platform (because numbers become factors and other little changes in how the code is interpreted...) the full working script (tested on R studio MAC OS) can be found at https//github.com/orioli/emoji-writer-sentiment Thank you to Lewis Michel,CSV,,"[linguistics, internet]",GPL,,,780,7879,152,Are people that use emoji happier?,Emoji sentiment,https://www.kaggle.com/harriken/emoji-sentiment,Sun Oct 01 2017
,Stanford Open Policing Project,"[id, state, stop_date, stop_time, location_raw, county_name, county_fips, fine_grained_location, police_department, driver_gender, driver_age_raw, driver_age, driver_race_raw, driver_race, violation_raw, violation, search_conducted, search_type_raw, search_type, contraband_found, stop_outcome, is_arrested, ethnicity]","[string, string, dateTime, string, string, string, numeric, string, string, string, string, string, string, string, string, string, boolean, string, string, boolean, string, boolean, string]",Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes over 2gb of stop data from California covering all of 2013 onwards. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,CSV,,"[government agencies, crime, law]",Other,,,119,1036,2048,Data on Traffic and Pedestrian Stops by Police in California,Stanford Open Policing Project - California,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-california,Tue Jul 11 2017
,Mantas Zimnickas,"[voting_id, voter_id, time, group, voter, question, sitting_type, vote, n_eligible_voters, n_voters]","[numeric, numeric, dateTime, string, string, string, string, numeric, numeric, numeric]","Context In Lithuania Lithuanian Parliament has sittings several times per week during each sitting member of parliament vote for proposed questions. Some questions are related to law amendments other questions could be something like ""should we do a break before taking more questions"". In one they there can be several sittings usually one in the morning and another in the evening. All members of parliament (MPs) form parliamentary groups there is a group having majority of MS's and other smaller groups. In this dataset you will find all votes made by all MPs since 1997 up until 2017. So that is a lot of data and basically most of the political history of independent Lithuania. Content Each row in votes.csv file represents a vote mode by single MP on a single question. Also for each vote there is a number of meta data provided  voting_id - unique voting id can be a negative number. You can reconstruct URL to the voting page using this template http//www.lrs.lt/sip/portal.show?p_r=15275&p_k=1&p_a=sale_bals&p_bals_id={voting_id} replace {voting_id} with a voting id. voter_id - unique MP id. time - date and time when a vote was cast. group - abbreviated name of a parliamentary group. voter - full name of an MP. question - question text usually question sounds like ""do we accept this proposal or not"" unfortunately title and texts of proposed documents are not included in this dataset. sitting_type - one of rytinis - in the morning vakarinis - in the evening neeilinis - additional sitting nenumatytas - not planned vote - vote value one of 1 - aye -0.5 - abstain -1 - against n_eligible_voters - total number of eligible to vote MPs. n_eligible_voters - number of MPs who voted in a voting.  Acknowledgements All the data where scraped from Lithuanian Parliament web site. Code of web scraper can be found here https//github.com/sirex/databot-bots/blob/master/bots/lrs/balsavimai.py Inspiration One of the most interesting questions I would like to get is a way to automatically categorize all the voting by topic. Usually there are several major topics involving multiple documents and many voting sessions some times these topics can last for years. Unfortunately there is no such field with a topic in order to discover a topic one would need to analyze content of documents and probably sitting transcripts to find a topic. But in order to do this data of documents and sitting transcripts will be needed I will provide this data some time later. Other interesting things to look into is how different parliamentary groups or people relates to one another by their votes.",CSV,,"[law, politics]",ODbL,,,14,297,28,All available votes made by members of Lithuanian parliament since 1997.,Lithuanian parliament votes,https://www.kaggle.com/sirexo/lithuanian-paliament-votes,Wed Nov 29 2017
,Jeff Kao,"[docid, text_data, dupe_count]","[numeric, string, numeric]",Recent Updates (11-27-2017) I've posted a clustered full dataset. This might give you a boost on the work you're doing with the data! I've posted a vectorized subset w/ 100000 data points sampled after manual reduction of the dataset after EDA. Context Cleaned data used in researching public comments for FCC Proceeding 17-108 (Net Neutrality Repeal). Data collected from the beginning of submissions (April 2017) until Oct 27th 2017. The long-running comment scraping script suffered from a couple of disconnections and I estimate that I lost ~50000 comments because of it. Even though the Net Neutrality Public Comment Period ended on August 30 2017 the FCC ECFS system continued to take comments afterward which were included in the analysis. I did a write-up on the results here https//hackernoon.com/more-than-a-million-pro-repeal-net-neutrality-comments-were-likely-faked-e9f0e3ed36a6 Content Document id Text of the post and number of duplicates found for that text was all that was necessary to generate the results. Text is raw & unchanged from the original. I'm working hard to get online a fuller set of data w/ other important metadata fields. Cleaned-up notebooks used are available on github. I am posting the notebook for Exploratory Data Analysis first and will include others as they are cleaned up. Please share with the rest of us what interesting insights you glean from the data! Tweet at me @jeffykao.,CSV,,"[politics, linguistics, internet]",CC0,,,133,2835,198,FCC Proceeding #17-108 (text and dupe counts only),FCC Net Neutrality Comments (4/2017 - 10/2017),https://www.kaggle.com/jeffkao/proc_17_108_unique_comments_text_dupe_count,Sun Nov 26 2017
,Zeeshan-ul-hassan Usmani,"[1|1|بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ, 1, بِسْمِ اللَّهِ الرَّحْمَٰنِ الرَّحِيمِ]","[string, numeric, string]","Context The Holy Quran is the central text for 1.5 billion Muslims around the world. It literally means ""The Recitation.""  It is undoubtedly the finest work in Arabic literature and revealed by Allah (God) to His Messenger Prophet Muhammed (Peace Be Upon Him) through angel Gabriel. It was revealed verbally from December 22 609  (AD) to 632 AD (when Prophet Muhammed (Peace Be Upon Him) died) The book is divided into 30 parts 114 Chapters and 6000+ verses. There has been a lot of questions and comments on the text of this holy book given the contemporary Geo-political situation of the world wars in the Middle East and Afghanistan and the ongoing terrorism. I have put this dataset together to call my fellow data scientists to run their NLP algorithms and Kernels to find and explore the sacred text by them selves.   Content The data contains complete Holy Quran in following 21 languages (so data scientists from different parts of the world can work with it). The original text was revealed in Arabic. Other 20 files are the translations of the original text.  Arabic (Original Book by God) English (Transalation by Yusuf Ali) Persian (Makarim Sheerazi) Urdu (Jalandhari) Turkish (Y. N. Ozturk) Portuguese (El. Hayek) Dutch (Keyzer) Norwegian (Einar Berg) Italian (Piccardo) French (Hamidullah) German (Zaidan) Swedish (Rashad Kalifa) Indonesia (Bhasha Indoenisan) Bangla Chinese/Madarin Japanese Malay Malayalam Russian Tamil Uzbek  Inspiration Here are some ideas to explore  Can we make a word cloud for each chapter Can we make a word cloud of the whole book and find out the frequency of each word Can we describe or annotate subjects in each chapter and verse Can we find how many times The Quran has mentioned Humans Women Humility Heaven or Hell Can we compare the text with other famous books and see the correlation Can we compare the text with laws in multiple countries to see the resemblance  Any other ideas you can think of I am looking forward to see your work and ideas and will keep adding more ideas to explore Welcome on board to learn the finest text on earth with Data Sciences and Machine Learning! Updates Complete Verse-By-Verse Dataset has been shared. A good contribution by Zohaib Ali - https//www.kaggle.com/zohaib1111 (Nov 20 2017)",CSV,,"[languages, faith and traditions]",CC0,,,500,10244,16,Understanding God - Sacred Meanings,The Holy Quran,https://www.kaggle.com/zusmani/the-holy-quran,Mon Nov 20 2017
,Aleksey Bilogur,[],[],Context Wikipedia the world's largest encyclopedia is a crowdsourced open knowledge project and website with millions of individual web pages. This dataset is a grab of the title of every article on Wikipedia as of September 20 2017. Content This dataset is a simple newline (\n) delimited list of article titles. No distinction is made between redirects (like Schwarzenegger) and actual article pages (like Arnold Schwarzenegger). Acknowledgements This dataset was created by scraping SpecialAllPages on Wikipedia. It was originally shared here. Inspiration  What are common article title tokens? How do they compare against frequent words in the English language? What is the longest article title? The shortest? What countries are most popular within article titles? ,Other,,[],CC4,,,230,2111,296,All of the titles of articles on Wikipedia,Wikipedia Article Titles,https://www.kaggle.com/residentmario/wikipedia-article-titles,Fri Sep 22 2017
,US Bureau of Labor Statistics,[],[],Context The US Bureau of Labor Statistics monitors and collects day-to-day information about the market price of raw inputs and finished goods and publishes regularized statistical assays of this data. The Consumer Price Index and the Producer Price Index are its two most famous products. The former tracks the aggregate dollar price of consumer goods in the United States (things like onions shovels and smartphones); the latter (this dataset) tracks the cost of raw inputs to the industries producing those goods (things like raw steel bulk leather and processed chemicals). The US federal government uses this dataset to track inflation. While in the short term the raw dollar value of producer inputs may be volatile in the long term it will always go up due to inflation --- the slowly decreasing buying power of the US dollar. Content This dataset consists of a packet of files each one tracking regularized cost of inputs for certain industries. The data is tracked-month to month with an index out of 100. Acknowledgements This data is published online by the US Bureau of Labor Statistics. Inspiration  How does the Producer Price Index compare against the Consumer Price Index? What have the largest spikes in input costs been historically? Can you determine why they occurred? What is the overall price index trend amongst US producers? ,Other,,[economics],CC0,,,203,1658,136,Statistical measures of change in prices of producer goods,Producer Price Index,https://www.kaggle.com/bls/producer-price-index,Tue Sep 12 2017
,City of New York,"[CAMIS, DBA, BORO, BUILDING, STREET, ZIPCODE, PHONE, CUISINE DESCRIPTION, INSPECTION DATE, ACTION, VIOLATION CODE, VIOLATION DESCRIPTION, CRITICAL FLAG, SCORE, GRADE, GRADE DATE, RECORD DATE, INSPECTION TYPE]","[numeric, string, string, numeric, string, numeric, numeric, string, dateTime, string, string, string, string, numeric, string, dateTime, dateTime, string]",Context Restaurant inspections for permitted food establishments in NYC. Restaurants are graded on A-F scale with regular visits by city health department.  Content Dataset includes address cuisine description inspection date type action violation code and description(s). Data covers all of NYC and starts Jan 1 2010-Aug 29 2017.  Acknowledgements Data was collected by the NYC Department of Health and is available here.  Inspiration  Can you predict restaurant closings? Are certain violations more prominent in certain neighborhoods? By cuisine? Who gets worse grades--chain restaurants or independent establishments? ,CSV,,"[government agencies, food and drink, business]",CC0,,,578,3612,139,~400k Rows of Restaurant Inspections Data,NYC Restaurant Inspections,https://www.kaggle.com/new-york-city/nyc-inspections,Tue Aug 29 2017
,SciELO,[],[],Context SciELO (Scientific Electronic Library Online) is an international research communication program launched in 1998 and implemented through a decentralized network of national collections of peer reviewed journals from 15 countries – 12 from Latin America Portugal Spain and South Africa – that jointly publish over 1 thousand journals and about 50 thousand articles per year. A thematic collection on Public Health is also operated by the SciELO Program. All collections are accessible via the network portal – http//www.scielo.org. SciELO aims at the progress of research through the improvement of peer reviewed journals from all disciplines published by scientific and professional associations academic institutions and public or private research and development institutions. The specific objectives are to increase in a sustainable way the quality visibility usage impact and credibility of the indexed journals and the research they communicate. A key characteristic of SciELO is multilingual publishing so journals can publish articles in one or multiple languages including the simultaneous publishing of the same article in more than one language. SciELO Program develops itself according to three principles. First the conception that scientific knowledge is a public good and therefore should be available openly in the Web. Second the network operation envisaging to strengthen collaboration and interchange of information and experience creating scale and lessen the costs. Third quality control as an essential policy and practice at the level of articles journals and collections adoption and compliance with bibliographic and interoperability standards. SciELO operation and development are carried out following three main action lines. The first is professionalization which means to produce journals according to the state of art. The second is internationalization which means to strengthen the active participation of SciELO Journals and Program in the international flow of scientific information. The third is operational and financial sustainability which means to develop conditions to assure journals to be published on time with a well-established financial model. All collections follow the SciELO Publishing Model which comprises three main functions. First the indexing of journals with metadata of articles including the bibliographic references of the indexed articles and of the articles they cite. Second the full text of articles which are available in HTML PDF and progressively in XML JATS compatible according the SciELO Publishing Schema. Third the dissemination and interoperability of journals and articles with bibliographic indexes and systems.     SciELO Brazil led by SciELO / FAPESP Program acts as the SciELO Network secretariat and coordinates the maintenance of the methodological and technological platform while the operation of the network collections and journals are decentralized and led by national research agencies.  Content This dataset contains publication and access reports for the documents of the SciELO Brazil collection between 1998 and October 2017. The reports present metadata of journals totals of issues and published documents thematic areas authors' affiliation bibliographic references use licenses and more. Further details can be found at http//docs.scielo.org/projects/scielo-processing/pt/latest/public_reports.html (in Portuguese with notes in English).,Other,,"[research, brazil]",CC0,,,33,567,57,Publication and access reports of SciELO Brazil from 1998 to October 2017.,"Publication and usage reports, 1998-2017-10 (BR)",https://www.kaggle.com/scieloorg/publishing-and-usage-reports-1998-201710-br,Mon Nov 13 2017
,Flaredown,"[user_id, age, sex, country, checkin_date, trackable_id, trackable_type, trackable_name, trackable_value]","[string, string, string, string, dateTime, numeric, string, string, numeric]",Introduction Flaredown is an app that helps patients of chronic autoimmune and invisible illnesses improve their symptoms by avoiding triggers and evaluating their treatments. Each day patients track their symptom severity treatments and doses and any potential environmental triggers (foods stress allergens etc) they encounter. About the data Instead of coupling symptoms to a particular illness Flaredown asks users to create their unique set of conditions symptoms and treatments (“trackables”). They can then “check-in” each day and record the severity of symptoms and conditions the doses of treatments and “tag” the day with any unexpected environmental factors. User includes an ID age sex and country. Condition an illness or diagnosis for example Rheumatoid Arthritis rated on a scale of 0 (not active) to 4 (extremely active). Symptom self-explanatory also rated on a 0–4 scale. Treatment anything a patient uses to improve their symptoms along with an optional dose which is a string that describes how much they took during the day. For instance “3 x 5mg”. Tag a string representing an environmental factor that does not occur every day for example “ate dairy” or “rainy day”. Food food items were seeded from the publicly-available USDA food database. Users have also added many food items manually. Weather weather is pulled automatically for the user's postal code from the Dark Sky API. Weather parameters include a description precipitation intensity humidity pressure and min/max temperatures for the day. If users do not see a symptom treatment tag or food in our database (for instance “Abdominal Pain” as a symptom) they may add it by simply naming it. This means that the data requires some cleaning but it is patient-centered and indicates their primary concerns. Suggested Questions  Does X treatment affect Y symptom positively/negatively/not at all? What are the most strongly-correlated symptoms and treatments? Are there subsets within our current diagnoses that could more accurately represent symptoms and predict effective treatments? Can we reliably predict what triggers a flare for a given user or all users with a certain condition? Could we recommend treatments more effectively based on similarity of users rather than specific symptoms and conditions? (Netflix recommendations for treatments) Can we quantify a patient’s level of disease activity based on their symptoms? How different is it from our existing measures? Can we predict which symptom should be treated to have the greatest effect on a given illness? How accurately can we guess a condition based on a user’s symptoms? Can we detect new interactions between treatments?  Please email logan@flaredown.com if you have questions about the project,CSV,,"[healthcare, diseases, epidemiology]",CC4,,,441,3860,134,How do treatments and environmental stressors impact symptoms?,"Chronic illness: symptoms, treatments and triggers",https://www.kaggle.com/flaredown/flaredown-autoimmune-symptom-tracker,Fri Nov 03 2017
,Rachael Tatman,"[lines, words, characters, filename]","[numeric, numeric, numeric, string]","Context Building dialogue systems where a human can have a natural-feeling conversation with a virtual agent is a difficult task in Natural Language Processing and the focus of much ongoing research. Some of the challenges include linking references to the same entity over time tracking what’s happened in the conversation previously and generating appropriate responses. This corpus of naturally-occurring dialogues can be helpful for building and evaluating dialogue systems. Content The new Ubuntu Dialogue Corpus consists of almost one million two-person conversations extracted from the Ubuntu chat logs used to receive technical support for various Ubuntu-related problems. The conversations have an average of 8 turns each with a minimum of 3 turns. All conversations are carried out in text form (not audio).  The full dataset contains 930000 dialogues and over 100000000 words and is available here. This dataset contains a sample of this dataset spread across .csv files. This dataset contains more than 269 million words of text spread out over 26 million turns.   folder The folder that a dialogue comes from. Each file contains dialogues from one folder . dialogueID An ID number for a specific dialogue. Dialogue ID’s are reused across folders. date A timestamp of the time this line of dialogue was sent. from The user who sent that line of dialogue. to The user to whom they were replying. On the first turn of a dialogue this field is blank. text The text of that turn of dialogue separated by double quotes (“). Line breaks (\n) have been removed.  Acknowledgements This dataset was collected by Ryan Lowe Nissan Pow  Iulian V. Serban† and Joelle Pineau. It is made available here under the Apache License  2.0. If you use this data in your work please include the following citation  Ryan Lowe Nissan Pow Iulian V. Serban and Joelle Pineau ""The Ubuntu Dialogue Corpus A Large Dataset for Research in Unstructured Multi-Turn Dialogue Systems"" SIGDial 2015. URL http//www.sigdial.org/workshops/conference16/proceedings/pdf/SIGDIAL40.pdf Inspiration Can you use these chat logs to build a chatbot that offers help with Ubuntu?",CSV,,"[languages, linguistics, computing and society]",Other,,,367,4549,3072,26 million turns from natural two-person dialogues,Ubuntu Dialogue Corpus,https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus,Thu Aug 17 2017
,ramirobentes,[],[],"Context These are all the flights tracked by the National Civil Aviation Agency in Brazil from January 2015 to August 2017. I plan on keeping this dataset updated and on gradually adding data from previous years as well. Content The dataset is in portuguese so I had to remove some characters that were not supported on Kaggle. You can see the translation for the columns in the main dataset description.  ""Nao Identificado"" means ""Unidentified"". UF means the State where the airport is located. For every airport not located in Brazil the value is N/I.  Feel free to ask me if you need translation of any other word.",Other,,"[brazil, aviation]",CC0,,,321,2454,41,"Every flight tracked by the National Civil Aviation Agency in Brazil, 2016-17.",Flights in Brazil,https://www.kaggle.com/ramirobentes/flights-in-brazil,Wed Nov 15 2017
,Sohier Dane,[],[],The State Contract and Procurement Registration System (SCPRS) was established in 2003 as a centralized database of information on State contracts and purchases over $5000. eSCPRS represents the data captured in the State's eProcurement (eP) system Bidsync as of March 16 2009. The data provided is an extract from that system for fiscal years 2012-2013 2013-2014 and 2014-2015 Acknowledgements This dataset was kindly released by the state of California. You can find the original copy here.,Other,,"[government agencies, finance]",CC0,,,308,1761,156,All purchase orders over $5000 from 2012-2015,Large Purchases by the State of CA,https://www.kaggle.com/sohier/large-purchases-by-the-state-of-ca,Wed Aug 30 2017
,Orges Leka,[],[],"If you want to download and experiment with the Scrapy script you can do so from forum data science This page is a forum for data scientist I started in hope  that you will participate and maybe even improve the scrapy script. Over 370000 used cars scraped with Scrapy from Ebay-Kleinanzeigen. The content of the data is in german so one has to translate it first if one can not speak german. Those fields are included autos.csv  dateCrawled  when this ad was first crawled all field-values are taken from this date name  ""name"" of the car seller  private or dealer offerType price  the price on the ad to sell the car abtest vehicleType yearOfRegistration  at which year the car was first registered gearbox powerPS  power of the car in PS model kilometer  how many kilometers the car has driven monthOfRegistration  at which month the car was first registered fuelType brand notRepairedDamage  if the car has a damage which is not repaired yet dateCreated  the date for which the ad at ebay was created nrOfPictures  number of pictures in the ad (unfortunately this field contains everywhere a 0 and is thus useless (bug in crawler!) ) postalCode  lastSeenOnline  when the crawler saw this ad last online  The fields lastSeen and dateCreated could be used to estimate how long a car will be at least online before it is sold. brought to you by Orges Leka Regression on average Price per Year based on this dataset Table of value loss of an average used car per year The second file is produced in MySQL from the first one through the query select   count(*) as count   kilometer   yearOfRegistration  20*round(powerPS/20) as powerPS  min(price) as minprice  max(price) as maxPrice  avg(price) as avgPreis  sqrt(variance(price)) as sdPreis from items where       yearOfRegistration > 1990 and yearOfRegistration < 2016      and price > 100 and price < 100000      and powerPS < 600 and powerPS > 0   group by yearOfRegistration round(powerPS/20)kilometer  having count > 10  into outfile '/tmp/cnt_km_year_powerPS_minPrice_maxPrice_avgPrice_sdPrice.csv'  fields terminated by '' lines terminated by '\n';  Happy Coding!",CSV,,[automobiles],CC0,,,11549,82911,65,"Over 370,000 used cars scraped from Ebay Kleinanzeigen",Used cars database,https://www.kaggle.com/orgesleka/used-cars-database,Mon Nov 28 2016
,Zeeshan-ul-hassan Usmani,"[address, icij_id, valid_until, country_codes, countries, node_id, sourceID, note]","[string, string, string, string, string, numeric, string, string]",Context The Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5 2017. Here is a brief video about the leak. The people include Queen Elizabeth II the President of Columbia (Juan Manuel Santos) Former Prime Minister of Pakistan (Shaukat Aziz) U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group the amount of money involved is around $10 trillion. The leak contains many famous companies including Facebook Apple Uber Nike Walmart Allianz Siemens McDonald’s and Yahoo.  It also contains a lot of U. S President Donald Trump allies including Rax Tillerson Wilbur Ross Koch Brothers Paul Singer Sheldon Adelson Stephen Schwarzman Thomas Barrack and Steve Wynn etc.  The complete list of Politicians involve is avaiable here.  The Panama Papers in the cache of 38GB of data from the national corporate registry of Bahamas. It contains world’s top politicians and influential persons as head and director of offshore companies registered in Bahamas. Offshore Leaks details 13000 offshore accounts in a report. I am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye. Content The data is the effort of more than 100 journalists from 60+ countries The original data is available under creative common license and can be downloaded from this link. I will keep updating the datasets with more leaks and data as it’s available Acknowledgements International Consortium of Investigative Journalists (ICIJ) Paradise Papers Update Paradise Papers data has been uploaded as released by ICIJ on Nov 21 2017. You can find Paradise Papers zip file and six extracted files in CSV format all starting with a prefix of Paradise. Happy Coding! Inspiration Some ideas worth exploring  How many companies and individuals are there in all of the leaks data How many countries involved Total money involved What is the biggest best tax heaven Can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country Who are the biggest cheaters and where they live What role Fortune 500 companies play in this game  I need your help to make this world corruption free in the age of NLP and Big Data,CSV,,"[money, banking]",CC3,,,522,7096,134,Data Scientists United Against Corruption,Paradise-Panama-Papers,https://www.kaggle.com/zusmani/paradisepanamapapers,Tue Nov 21 2017
,Jacob Boysen,"[city, close_date, complaint_description, complaint_type, council_district_code, county, created_date, incident_address, incident_zip, last_update_date, latitude, location, longitude, map_page, map_tile, owning_department, source, state_plane_x_coordinate, state_plane_y_coordinate, status, status_change_date, street_name, street_number, unique_key]","[string, dateTime, string, string, numeric, string, dateTime, string, numeric, dateTime, numeric, string, numeric, string, string, string, string, numeric, numeric, string, dateTime, string, numeric, string]",Context 311 calls are a good snapshot of public complaints and provide interesting analytical data to predict future resource allocation by policymakers. Content Date time location description handling office and status are included. Acknowledgements This dataset was compiled by the City of Austin and published on Google Cloud Public Data. Inspiration  Any notable trends in location or volume of certain calls? Can you predict future 311 calls?  Dataset Description Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too.,CSV,,[government agencies],CC0,,,79,961,163,"463k Public Complaints, 2013-17",Austin 311 Calls,https://www.kaggle.com/jboysen/austin-calls,Fri Aug 18 2017
,Carrie,"[InvoiceNo, StockCode, Description, Quantity, InvoiceDate, UnitPrice, CustomerID, Country]","[numeric, numeric, string, numeric, dateTime, numeric, numeric, string]","Context Typically e-commerce datasets are proprietary and consequently hard to find among publicly available data.  However The UCI Machine Learning Repository has made this dataset containing actual transactions from 2010 and 2011.  The dataset is maintained  on their site where it can be found by the title ""Online Retail"". Content ""This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers."" Acknowledgements Per the UCI Machine Learning Repository this data was made available by Dr Daqing Chen Director Public Analytics group. chend '@' lsbu.ac.uk School of Engineering London South Bank University London SE1 0AA UK. Image from stocksnap.io. Inspiration Analyses for this dataset could include time series clustering classification and more. ",CSV,,[],Other,,,4145,26173,43,Actual transactions from UK retailer,E-Commerce Data,https://www.kaggle.com/carrie1/ecommerce-data,Thu Aug 17 2017
,Chris Crawford,"[barren_land, 1, 0, 0, 0]","[string, numeric, numeric, numeric, numeric]",DeepSat SAT-4  Originally images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points. The images consist of 4 bands - red green blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas urban areas densely forested mountainous terrain small to large water bodies agricultural areas etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.  Once labeled 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch. Content  Each sample image is 28x28 pixels and consists of 4 bands - red green blue and near infrared.  The training and test labels are one-hot encoded 1x4 vectors The four classes represent the four broad land covers which include barren land trees grassland and a class that consists of all land cover classes other than the above three.  Training and test datasets belong to disjoint set of image tiles.  Each image patch is size normalized to 28x28 pixels. Once generated both the training and testing datasets were randomized using a pseudo-random number generator.   CSV files  X_train_sat4.csv 400000 training images 28x28 images each with 4 channels  y_train_sat4.csv 400000 training labels 1x4 one-hot encoded vectors  X_test_sat4.csv 100000 training images 28x28 images each with 4 channels  y_test_sat4.csv 100000 training labels 1x4 one-hot encoded vectors   The original MAT file   train_x  28x28x4x400000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels) train_y  400000x4 uint8 (containing 4x1 vectors having labels for the 400000 training samples) test_x   28x28x4x100000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels) test_y   100000x4 uint8 (containing 4x1 vectors having labels for the 100000 test samples)  Acknowledgements The original MATLAB file was converted to multiple CSV files  The original SAT-4 and SAT-6 airborne datasets can be found here http//csc.lsu.edu/~saikat/deepsat/ Thanks to Saikat Basu Robert DiBiano Manohar Karki and Supratik Mukhopadhyay Louisiana State University Sangram Ganguly Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani NASA Advanced Supercomputing Division NASA Ames Research Center,CSV,,[machine learning],CC0,,,56,969,3072,"500,000 image patches covering four broad land cover classes",DeepSat (SAT-4) Airborne Dataset,https://www.kaggle.com/crawford/deepsat-sat4,Wed Jan 17 2018
,katzwigmore,"[Record ID, Report Date, Report Time, Major Offense Type, Address, Neighborhood, Police Precinct, Police District, X Coordinate, Y Coordinate]","[numeric, dateTime, dateTime, string, string, string, string, numeric, numeric, numeric]",Content The contents of this data set comes from public data available on the city of Portland website. Each individual crime reported is lists the location time and date of the incident as well as a the neighborhood in which the event occurred.   All data prior to 2015 has the same general format but the newer 2015-17 data needs to be reformatted for easier comparison since it does not match the older organizational scheme.  To this end I will be adding new .csv with 2015  2016 and 2017 YTD data broken out.  Coordinate data will also be added to make the data sets more easily comparable and mappable. Update  I created new .csv for each year 2015-2017 changing the formatting from the Portland Police Department's tab separated values to the standard comma separated values.  The pre-2015 data still isn't comparable because of the differences in the crime categorization but I will work creating some sort of key so that the full data set can be analyzed as a single batch of information.  Acknowledgements Banner image by Zack Spear on Unsplash. All data gathered from portlandoregon.gov and civicapps.org,CSV,,[crime],CC0,,,231,1863,131,"By Year, Event, and Location",Portland Oregon Crime Data,https://www.kaggle.com/katzwigmore/portland-oregon-crime-data,Mon Sep 25 2017
,Jacob Boysen,"[adm0_id, adm0_name, adm1_id, adm1_name, mkt_id, mkt_name, cm_id, cm_name, cur_id, cur_name, pt_id, pt_name, um_id, um_name, mp_month, mp_year, mp_price, mp_commoditysource]","[numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, numeric, numeric, string]",Context Global food price fluctuations can cause famine and large population shifts. Price changes are increasingly critical to policymakers as global warming threatens to destabilize the food supply. Content Over 740k rows of prices obtained in developing world markets for various goods. Data includes information on country market price of good in local currency quantity of good and month recorded. Acknowledgements Compiled by the World Food Program and distributed by HDX. Inspiration This data would be particularly interesting to pair with currency fluctuations weather patterns and/or refugee movements--do any price changes in certain staples predict population upheaval? Do certain weather conditions influence market prices? License Released under CC BY-IGO.,CSV,,"[food and drink, economics]",Other,,,1274,7735,83,743k Rows of Monthly Market Food Prices Across Developing Countries,Global Food Prices,https://www.kaggle.com/jboysen/global-food-prices,Fri Aug 04 2017
,World Bank,"[As of Date, Fiscal Year, Region, Borrower Country, Borrower Country Code, Project ID, Project Name, Procurement Type, Procurement Category, Procurement Method, Product line, Major Sector, WB Contract Number, Contract Description, Contract Signing Date, Supplier, Supplier Country, Supplier Country Code, Total Contract Amount (USD), Borrower Contract Reference Number]","[dateTime, numeric, string, string, string, string, string, string, string, string, string, string, numeric, string, dateTime, string, string, string, string, string]","This set of contract awards includes data on commitments against contracts that were reviewed by the Bank before they were awarded (prior-reviewed Bank-funded contracts) under IDA/IBRD investment projects and related Trust Funds. This dataset does not list all contracts awarded by the Bank and should be viewed only as a guide to determine the distribution of major contract commitments among the Bank's member countries. ""Supplier Country"" represents place of supplier registration which may or not be the supplier's actual country of origin. Information does not include awards to subcontractors nor account for cofinancing. The Procurement Policy and Services Group does not guarantee the data included in this publication and accepts no responsibility whatsoever for any consequences of its use. The World Bank complies with all sanctions applicable to World Bank transactions. Acknowledgements This dataset was kindly made available by the World Bank. You can find the original dataset here. Inspiration  How do the contract awards compare to each nations's voting rights? Are there any unexpected consistent preferences? ",CSV,,"[supply chain, international relations]",CC0,,,187,1845,51,The largest supply contracts awarded by the World Bank,World Bank's Major Contracts ,https://www.kaggle.com/theworldbank/world-banks-major-contracts,Wed Sep 13 2017
,Datafiniti,"[id, asins, brand, categories, colors, count, dateAdded, dateUpdated, descriptions, dimension, ean, features, flavors, imageURLs, isbn, keys, manufacturer, manufacturerNumber, merchants, name, prices.amountMin, prices.amountMax, prices.availability, prices.color, prices.condition, prices.count, prices.currency, prices.dateAdded, prices.dateSeen, prices.flavor, prices.isSale, prices.merchant, prices.offer, prices.returnPolicy, prices.shipping, prices.size, prices.source, prices.sourceURLs, prices.warranty, quantities, reviews, sizes, skus, sourceURLs, upc, vin, websiteIDs, weight, , , , ]","[string, string, string, string, string, string, dateTime, dateTime, string, string, numeric, string, string, string, string, numeric, string, numeric, string, string, numeric, numeric, string, string, string, string, string, dateTime, dateTime, string, boolean, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string]",About This Data This is a list of 10000 women's shoes and their product information provided by Datafiniti's Product Database.  The dataset includes shoe name brand price and more. Note that each shoe will have an entry for each price found for it and some shoes may have multiple entries.  What You Can Do with This Data You can use this data to determine brand markups pricing strategies and trends for luxury shoes. E.g.  What is the average price of each distinct brand listed? Which brands have the highest prices?   Which ones have the widest distribution of prices? Is there a typical price distribution (e.g. normal) across brands or within specific brands?  Further processing data would also let you  Correlate specific product features with changes in price. You can cross-reference this data with a sample of our Men's Shoe Prices to see if there are any differences between women's brands and men's brands.  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[clothing, business]",CC4,,,2006,13595,102,"A list of 10,000 women's shoes and the prices at which they are sold.",Women's Shoe Prices,https://www.kaggle.com/datafiniti/womens-shoes-prices,Mon May 15 2017
,Jacob Boysen,[],[],Context The People with significant control (PSC) snapshot is a data snapshot containing the full list of PSC's provided to Companies House. The Prime Minister first put corporate transparency on the international agenda when he chaired the G8 summit in Lough Erne and secured commitment to action the commitment to enhance corporate transparency in the UK was reaffirmed at London’s International Anti-Corruption Summit in May 2016. Since then the EU and G20 countries have also agreed to act. The UK is the first country in the G20 to create a public register of this kind. The UK has high standards of business behaviour and corporate governance. The overwhelming majority of UK companies contribute productively to the UK economy abide by the law and make a valuable contribution to society. But there are exceptions. Some of the features of the company structure which make it good for business also make it attractive to criminals. Companies can be misused to facilitate a range of criminal activities - from money laundering to tax evasion corruption to terrorist financing. Sometimes those individuals running companies will not conduct themselves in accordance with the high standards we expect in the UK posing a risk to other companies and consumers alike. Information about the ownership and control of UK corporate entities will bring benefits for law enforcement business civil society and citizens. By making this information publicly available free of charge the government is setting a standard that we are persuading other countries to follow. Content A person of significant control is someone that holds more than 25% of shares or voting rights in a company has the right to appoint or remove the majority of the board of directors or otherwise exercises significant influence or control. This is a snapshot of data in zipped JSON form as of Aug 23 2017. Daily updated snapshots and streaming API details can be found here. The People with Significant Control (PSC) register includes information about the individuals who own or control companies including their name month and year of birth nationality and details of their interest in the company. From 30 June 2016 UK companies (except listed companies) and limited liability partnerships (LLPs) need to declare this information when issuing their annual confirmation statement to Companies House. Acknowledgements Guidance here. The data is collected by UK government. Inspiration  Who owns the most businesses? In certain areas? Any weird looking situations where ownership might be obscured? ,Other,,[business],CC0,,,244,2070,3072,Snapshot of UK Business Ownership Details,Who's the Boss? People with Significant Control,https://www.kaggle.com/jboysen/uk-psc,Wed Aug 23 2017
,Chris Crawford,"[EIN, NAME, ICO, STREET, CITY, STATE, ZIP, GROUP, SUBSECTION, AFFILIATION, CLASSIFICATION, RULING, DEDUCTIBILITY, FOUNDATION, ACTIVITY, ORGANIZATION, STATUS, TAX_PERIOD, ASSET_CD, INCOME_CD, FILING_REQ_CD, PF_FILING_REQ_CD, ACCT_PD, ASSET_AMT, INCOME_AMT, REVENUE_AMT, NTEE_CD, SORT_NAME]","[numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]","Context This dataset comes from ""The Exempt Organization Business Master File Extract"" (EO BMF) which includes cumulative information on tax-exempt organizations.  Content Data is current up to 8/14/2017  EIN Employer Identification Number (EIN) NAME Primary Name of Organization ICO In Care of Name STREET Street Address CITY City STATE State ZIP Zip Code GROUP Group Exemption Number SUBSECTION Subsection Code AFFILIATION Affiliation Code CLASSIFICATION Classification Code(s) RULING Ruling Date DEDUCTIBILITY Deductibility Code FOUNDATION Foundation Code ACTIVITY Activity Codes ORGANIZATION Organization Code STATUS Exempt Organization Status Code TAX_PERIOD Tax Period ASSET_CD Asset Code INCOME_CD Income Code FILING_REQ_CD Filing Requirement Code PF_FILING_REQ_CD PF Filing Requirement Code ACCT_PD Accounting Period ASSET_AMT Asset Amount INCOME_AMT Income Amount (includes negative sign if amount is negative) REVENUE_AMT Form 990 Revenue Amount (includes negative sign if amount is negative) NTEE_CD National Taxonomy of Exempt Entities (NTEE) Code SORT_NAME Sort Name (Secondary Name Line)  There are six data files separated by regions eo1  CT MA ME NH NJ NY RI VT  eo2  DC DE IA IL IN KY MD MI MN NC ND NE OH PA SC SD VA WI WV  eo3  AK AL AR AZ CA CO FL GA HI ID KS LA MO MS MT NM NV OK OR TN TX UT WA WY  eo4  AA AE AP AS FM GU MH MP PR PW VI  eo_pr  Puerto Rico  eo_xx Various international non-profits (too many countries to list). See columns 5 and 6. Acknowledgements More information and updated data an be found here",CSV,,[],CC0,,,155,1271,275,All of the charities and non-profits registered with the IRS,U.S. Charities and Non-profits,https://www.kaggle.com/crawford/us-charities-and-nonprofits,Sat Aug 19 2017
,Alin Secareanu,[],[],Context Most publicly available football (soccer) statistics are limited to aggregated data such as Goals Shots Fouls Cards. When assessing performance or building predictive models this simple aggregation without any context can be misleading. For example a team that produced 10 shots on target from long range has a lower chance of scoring than a club that produced the same amount of shots from inside the box. However metrics derived from this simple count of shots will similarly asses the two teams. A football game generates much more events and it is very important and interesting to take into account the context in which those events were generated. This dataset should keep sports analytics enthusiasts awake for long hours as the number of questions that can be asked is huge. Content This dataset is a result of a very tiresome effort of webscraping and integrating different data sources. The central element is the text commentary. All the events were derived by reverse engineering the text commentary using regex. Using this I was able to derive 11 types of events as well as the main player and secondary player involved in those events and many other statistics. In case I've missed extracting some useful information you are gladly invited to do so and share your findings. The dataset provides a granular view of 9074 games totaling 941009 events from the biggest 5 European football (soccer) leagues England Spain Germany Italy France from 2011/2012 season to 2016/2017 season as of 25.01.2017. There are games that have been played during these seasons for which I could not collect detailed data. Overall over 90% of the played games during these seasons have event data. The dataset is organized in 3 files  events.csv contains event data about each game. Text commentary was scraped from bbc.com espn.com and onefootball.com ginf.csv - contains metadata and market odds about each game. odds were collected from oddsportal.com dictionary.txt contains a dictionary with the textual description of each categorical variable coded with integers  Past Research I have used this data to  create predictive models for football games in order to bet on football outcomes. make visualizations about upcoming games build expected goals models and compare players  Inspiration There are tons of interesting questions a sports enthusiast can answer with this dataset. For example  What is the value of a shot? Or what is the probability of a shot being a goal given it's location shooter league assist method gamestate number of players on the pitch time - known as expected goals (xG) models When are teams more likely to score? Which teams are the best or sloppiest at holding the lead? Which teams or players make the best use of set pieces? In which leagues is the referee more likely to give a card? How do players compare when they shoot with their week foot versus strong foot? Or which players are ambidextrous? Identify different styles of plays (shooting from long range vs shooting from the box crossing the ball vs passing the ball use of headers) Which teams have a bias for attacking on a particular flank?  And many many more...,CSV,,[association football],Other,,,9501,49659,174,"More than 900,000 events from 9,074 football games across Europe",Football Events,https://www.kaggle.com/secareanualin/football-events,Wed Jan 25 2017
,freeCodeCamp,[],[],"Context Student data of those taking an online education course. The files contains data about how freeCodeCamp 103000 registered students progressed through the freeCodeCamp curricula during 2015. Records are about anonymised students the freeCodeCamp ""challenge"" they completed together a simple description of the challenge when it was completed (Unix timestamp in milliseconds) and the solution in either code or URL to the hosting platform. It does not include data from users who had opted out of data sharing. The curriculum was subject to several modifications one particularly important during mid-2015. More information about the rationale of this project can be found on this Medium article.  Content  output.json dataset a json file with list of lists each nested list representing a user and having as elements objects of his/her activity. Activity was recorded as objects of 3 datapoints. Example   [   {     “name” “Waypoint Say Hello to HTML Elements”     “completedDate” 1445854025698     “solution” “<h1>Hello World</h1>\n”   } ]  Also available at Academic Torrents.  outputsample.json dataset a json file each nested list representing a user and having as elements objects of his/her activity; names of each object is the index of the record as found in the big dump. a random sample of the previous dataset with a 1% of the data only for records with at least 3 datapoints; empty records or with less of 3 datapoints were excluded. Below example of format   {1 [   {     “name” “Waypoint Say Hello to HTML Elements”     “completedDate” 1445854025698     “solution” “<h1>Hello World</h1>\n”   }   {...} ] 30[...] ... }  Acknowledgements The output.json dataset was prepared by Quincy Larson Nathan Leniz Berkeley Martinez and Ben McMahon. The outputsample.json was later prepared by Evaristo Caraballo. Thanks to all freeCodeCamp students who kindly allowed to share their personal progress. Publications Some preliminary analyses using this dataset can be found at the freeCodeCamp Github repository for the Open Data Initiative. Terms of Use This Free Code Camp Dataset is made available under the Open Database License http//opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License http//opendatacommons.org/licenses/dbcl/1.0/ Bibtex  @article{ title= {freeCodeCamp Data Jan-Dec 2015} keywords= {} journal= {} author= {Quincy Larson Nathan Leniz Berkeley Martinez Ben McMahon} year= {} url= {} license= {} abstract= {It’s one big JSON array. Inside that array are 103000 subarrays — one for each camper who’s completed at least one challenge since we overhauled our schema a few months ago.  Each subarray contains an individual camper’s completed challenges as a series of JavaScript objects.  Each of these objects has the name of the challenge the date they first completed it (as a Unix timestamp in milliseconds) and their solution which will either be code or a URL for their solution on CodePen Heroku or GitHub. It does not include data from users who have opted out of data sharing. } superseded= {} terms= {This Free Code Camp Dataset is made available under the Open Database License http//opendatacommons.org/licenses/odbl/1.0/. Any rights in individual contents of the database are licensed under the Database Contents License http//opendatacommons.org/licenses/dbcl/1.0/} }  Questions? If you have questions about this dataset please contact team@freecodecamp.com or get in touch with us through https//gitter.im/FreeCodeCamp/DataScience (Gitter registration might be required).",{}JSON,,"[education, internet]",ODbL,,,31,546,345,students records extracted from the freeCodeCamp database in 2015,freeCodeCamp Students Data Jan-Dec 2015,https://www.kaggle.com/free-code-camp/students-data-2015,Thu Nov 23 2017
,Manjeet Singh,"[Store, Date, Temperature, Fuel_Price, MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5, CPI, Unemployment, IsHoliday]","[numeric, dateTime, numeric, numeric, string, string, string, string, string, numeric, numeric, boolean]",Context The Challenge - One challenge of modeling retail data is the need to make decisions based on limited history. Holidays and select major events come once a year and so does the chance to see how strategic decisions impacted the bottom line. In addition markdowns are known to affect sales – the challenge is to predict which departments will be affected and to what extent.   Content You are provided with historical sales data for 45 stores located in different regions - each store contains a number of departments.  The company also runs several promotional markdown events throughout the year. These markdowns precede prominent holidays the four largest of which are the Super Bowl Labor Day Thanksgiving and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Within the Excel Sheet there are 3 Tabs – Stores Features and Sales  Stores Anonymized information about the 45 stores indicating the type and size of store Features Contains additional data related to the store department and regional activity for the given dates.   Store - the store number Date - the week Temperature - average temperature in the region Fuel_Price - cost of fuel in the region MarkDown1-5 - anonymized data related to promotional markdowns. MarkDown data is only available after Nov 2011 and is not available for all stores all the time. Any missing value is marked with an NA CPI - the consumer price index Unemployment - the unemployment rate IsHoliday - whether the week is a special holiday week  Sales Historical sales data which covers to 2010-02-05 to 2012-11-01. Within this tab you will find the following fields  Store - the store number Dept - the department number Date - the week Weekly_Sales -  sales for the given department in the given store IsHoliday - whether the week is a special holiday week  The Task  Predict the department-wide sales for each store for the following year Model the effects of markdowns on holiday weeks Provide recommended actions based on the insights drawn with prioritization placed on largest business impact ,CSV,,[],CC0,,,2887,18573,13,Historical sales data from 45 stores,Retail Data Analytics,https://www.kaggle.com/manjeetsingh/retaildataset,Fri Sep 01 2017
,FuzzyFrogHunter,[],[],"Context This is a collection of all the works of Charles Darwin that are available through Project Gutenberg. This dataset is subject to the Project Gutenberg license. It is very possible that I have missed some of his works. Please add them and update the ""Last updated"" date below if you get the chance. Otherwise if you leave a comment on this dataset I will try and do so myself. Content Last updated October 13 2017 This dataset consists of the following works in TXT format  ""On the Origin of Species by Means of Natural Selection or the Preservation of Favoured Races in the Struggle for Life."" - pg22764.txt ""The Formation of Vegetable Mould through the action of worms with observations on their habits"" - 2355-0.txt ""The Descent of Man and Selection in Relation to Sex Vol. I (1st edition)"" - 34967-0.txt ""The Descent of Man and Selection in Relation to Sex Volume II (1st Edition)"" - 36520-0.txt ""A Monograph on the Sub-class Cirripedia (Volume 1 of 2)"" - pg31558.txt ""A Monograph on the Sub-class Cirripedia (Volume 2 of 2)"" - 46408-0.txt ""The Foundations of the Origin of Species"" - pg22728.txt ""Coral Reefs"" - pg2690.txt ""More Letters of Charles Darwin Volume II"" - pg2740.txt ""The Variation of Animals and Plants under Domestication Volume I"" - pg2871.txt ""The Variation of Animals and Plants under Domestication Volume II"" - pg2872.txt ""South American Geology"" or ""Geological Observations On South America"" - pg3620.txt ""The Different Forms of Flowers on Plants of the Same Species"" - pg3807.txt ""The Effects of Cross & Self-Fertilisation in the Vegetable Kingdom"" - pg4346.txt ""The Movement and Habits of Climbing Plants"" - 2485-0.txt ""The Expression of Emotion in Man and Animals"" - pg1227.txt ""The Life and Letters of Charles Darwin Volume I (of II)"" - pg2087.txt ""The Life and Letters of Charles Darwin Volume II (of II)"" - pg2088.txt ""More Letters of Charles Darwin Volume I (of II)"" - pg2739.txt ""A Naturalist's Voyage Round the World The Voyage Of The Beagle"" - pg3704.txt ""Charles Darwin His Life in an Autobiographical Chapter and in a Selected Series of His Published Letters"" - pg38629.txt ""Insectivorous Plants"" - pg5765.txt  Acknowledgements Content taken from Project Gutenberg Image taken from Wikimedia Commons Inspiration Making this data available for any kind of textual analysis. I intend for this to be part of a series.",Other,,"[books, biology]",Other,,,65,1003,20,Collected from Project Gutenberg [text],The Works of Charles Darwin,https://www.kaggle.com/fuzzyfroghunter/darwin,Sat Oct 14 2017
,Rachael Tatman,[],[],Context The Leipzig Corpora Collection presents corpora in different languages using the same format and comparable sources. All data are available as plain text files and can be imported into a MySQL database by using the provided import script. They are intended both for scientific use by corpus linguists as well as for applications such as knowledge extraction programs.  Content This dataset contains 3 million sentences taken from newspaper texts in 2015. Non-sentences and foreign language material was removed. In addition to the sentences themselves this dataset contains information on the frequency of each word. More information about the format and content of these files can be found here. The corpora are automatically collected from carefully selected public sources without considering in detail the content of the contained text. No responsibility is taken for the content of the data. In particular the views and opinions expressed in specific parts of the data remain exclusively with the authors. Acknowledgements This dataset is released under a CC-BY 4.0 license. If you use this dataset in your work please cite the following paper D. Goldhahn T. Eckart & U. Quasthoff Building Large Monolingual Dictionaries at the Leipzig Corpora Collection From 100 to 200 Languages. In Proceedings of the 8th International Language Resources and Evaluation (LREC'12) 2012,Other,,"[languages, linguistics]",Other,,,140,1290,382,German language data from the Leipzig Corpus Collection,3 Million German Sentences,https://www.kaggle.com/rtatman/3-million-german-sentences,Wed Aug 16 2017
,Motaz Saad,[],[],"Context Egyptian is an Arabic dialect and it is the only Arabic dialect that has articles on Wikipedia. That is why I decided to extract Arabic-Egyptian comparable corpus from Wikipedia to make these resources available for linguists and computational linguists.  Content The dataset is composed of a set of text documents in both Arabic (Modern Standard) and Egyptian dialect aligned at document level. comparable documents share the same document ID.  Acknowledgements Thanks to Wikipedia and Wikipedia contributors who make these resource available. This corpus was collected by M. Saad and B. O. Alijla ""WikiDocsAligner An Off-the-Shelf Wikipedia Documents Alignment Tool"" 2017 Palestinian International Conference on Information and Communication Technology (PICICT) Gaza Palestine 2017 pp. 34-39. doi 10.1109/PICICT.2017.27 Inspiration What are the most common words in Egyptian and Arabic? What are the most frequent words in Egyptian and Arabic? What are the least frequent (rare) words in Egyptian and Arabic?",{}JSON,,"[languages, linguistics]",CC4,,,72,850,264,Arabic (Modern Standard) and Egyptian Arabic dialect comparable documents,Arabic - Egyptian comparable Wikipedia corpus,https://www.kaggle.com/mksaad/arb-egy-cmp-corpus,Fri Sep 29 2017
,US Department of Energy,[],[],The TMY3s are data sets of hourly values of solar radiation and meteorological elements for a 1-year period. Their intended use is for computer simulations of solar energy conversion systems and building systems to facilitate performance comparisons of different system types configurations and locations in the United States and its territories. Because they represent typical rather than extreme conditions they are not suited for designing systems to meet the worst-case conditions occurring at a location. Please note that TMY3 is NOT the state of the art solar data.  It was used as a key component of investment analyses for several years but NREL has released a more recent version based on satellite data and updated meteorological models that provides coverage for the entire United States. That dataset is much too large to publish here but is highly recommended if you need the best information. Content  Please see the pdf manual for full details of each field; there are several dozen of them.  It's important to know that nearly all of the solar data is modeled based on estimates of cloud cover; less than 1% of the stations directly measured sunlight. This data is not appropriate for time series analysis. A typical meteorological year is literally twelve months of real data from twelve different years. Please see the manual for further details.  Acknowledgements This dataset was made available by the National Renewable Energy Laboratory. You can find the original dataset here. If you like If you liked this dataset you might also enjoy  Google Project Sunroof 30 Years of European Wind Generation 30 Years of European Solar Generation ,CSV,,[energy],CC0,,,223,1600,2048,One Year of Typical Hourly Solar & Weather Data for +1000 US Locations,TMY3 Solar,https://www.kaggle.com/us-doe/tmy3-solar,Fri Sep 15 2017
,Keras,[],[],Inception-Resnet-V2  Inception-v4 Inception-ResNet and the Impact of Residual Connections on Learning Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge Authors Christian Szegedy Sergey Ioffe Vincent Vanhoucke Alex Alemi https//arxiv.org/abs/1602.07261  InceptionResnetV2 Architecture   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,19,390,392,InceptionResNetV2 Pre-trained Model for Keras,InceptionResNetV2,https://www.kaggle.com/keras/inceptionresnetv2,Tue Dec 12 2017
,Brian Roach,"[16.0000, 100.0000, 1.0000, 1.0000, -11.4336, -11.5865, -0.3657, 12.1542, 12.1083, 23.0913, 2.9400, 2.1203, 8.0126, 11.1846, 9.2980, 37.4620, 23.0685, 9.1054, 11.2053, 14.4939, 19.6752, 22.5268, 5.8674, 23.6633, 31.1399, 28.8796, 9.0602, 14.6499, 23.6764, 34.6535, 31.8239, 16.1440, 36.3840, 38.8325, 47.0992, 34.4035, -17.1579, -8.1754, 0.8432, -5.9440, -13.4556, -12.6293, -5.8640, -3.0849, 12.8907, 0.7508, 26.1668, 14.0396, 31.6723, 10.3824, 9.8335, 19.0244, 9.8999, 19.6063, 12.4894, 25.0452, 34.3579, 36.2412, 6.9570, 21.1968, 31.2997, 25.5969, 30.4795, 34.2454, 36.3866, 36.6116, 32.5585, 55.6669, 8.4273, 0.7109, 33.6411, 38.4445, 23.9526, 81.3624]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Humans (and many other animals) have the ability to reduce or suppress their brains' responses to sensory consequences that are a result of their own actions.  The nervous system accomplishes this with a  corollary discharge forward model system in which an ""efference copy"" of an impending motor plan is transmitted from motor to sensory cortex where it generates a ""corollary discharge"" representation of the expected sensory consequences of the imminent motor act.  For example when you move your eyes from left to right your brain knows the environment is not shifting.  When you speak your auditory cortex has a reduced response to the expected sound of your voice. Schizophrenia is a chronic mental illness that affects about 1% of people across the globe.  One possible explanation for some of the symptoms of schizophrenia is that one or more problems with the corollary discharge process in the nervous system makes it difficult for patients to differentiate between internally and externally generated stimuli.  Therefore studying this process and its relationship to symptoms in the illness might allow us to better understand abnormal brain processes in patients with this diagnosis. In a previously published EEG experiment (full report) we used a simple button pressing task in which subjects either (1) pressed a button to immediately generated a tone (2) passively listened to the same tone or (3) pressed a button without generating a tone to study the corollary discharge in people with schizophrenia and comparison controls.  We found that comparison controls suppressed the N100 a negative deflection in EEG brain wave 100 milliseconds after the onset of a sound when they pressed a button to generate a tone compared to passive playback but patients with schizophrenia did not.  This data set is a larger sample replication of that previous study.  Specifically EEG data from 22 controls and 36 patients with schizophrenia have been combined with 10 controls and 13 patients from the previous report. Methods Due to the size of the raw EEG data some pre-processing was done prior to upload.  EEG data acquisition parameters and the experimental task was identical to that described in our paper.  However pre-processing differed.  All individual subject data had at least the following data processing steps applied in this order  Re-reference to averaged ear lobes 0.1 Hz high-pass filter Interpolation of outlier channels in the continuous EEG data (outliers defined as in this paper) Chop continous data into single trial epochs 1.5 seconds before and after task events (3s total) Baseline correction -100 to 0ms Canonical correlation analysis to remove muscle and high-frequency white noise artifacts Rejection of outlier single trials (outliers defined as in this paper) Removal of outlier components from a spatial independent components analysis (outliers defined as in this paper) Interpolation of outlier channels within single trials (outliers defined as in this paper)  Derived data includes event-related potential (ERP) averages for 9 electrode sites analyzed in our previous report including Fz FCz Cz FC3 FC4 C3 C4 CP3 CP4 (pictured below)  The ERPs are calculated by averaging across trials for every sample in the time series separately for each subject electrode and condition. Content The single trial data from all 64 channels are too large to be uploaded for all 81 subjects but those interested in that type of data will find one subject (subject 21 in 21.csv) among the data files.  This includes all his data after the pre-processing step 9 listed above. For those interested in comparing patients with schizophrenia to control subjects the ERPdata.csv file contains the averaged ERP time series for all subjects conditions and the 9 electrodes mentioned above.  These data along with the subject information in demographic.csv could be used to replicate the analyses in our prior report. For those interested in single trial categorization/prediction like the grasp-and-lift challenge or the face decoding challenge the mergedTrialData.csv contains summary measurements from nearly 24000 individual trials (all subjects and conditions are included). Acknowledgements Funding for the study procedures initial analyses and publications came from the National Institute of Mental Health.  Please see grant info for additional details and cite this NIMH project number (R01MH058262) in any work related to these data.  All study participants gave written informed consent to participate in this study which received Institutional Review Board approval.",CSV,,"[mental health, time series, neuroscience]",CC4,,,237,3421,2048,Button press and auditory tone event related potentials from 81 human subjects,EEG data from basic sensory task in Schizophrenia,https://www.kaggle.com/broach/button-tone-sz,Fri Nov 17 2017
,Jens Laufer,"[, area_id, area_names, state, registered.voters, total_votes, invalid_first_votes, invalid_second_votes, valid_first_votes, valid_second_votes]","[numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context The dataset reflects the votes for the different areas in Germany for the 2017 federal  elections. See German Federal Election 2017 for more details Content The data was aquired from govdata.de which is state website offering interesting datasets. The original dataset was not easy to use therefore I did some reshaping without changing the  data Acknowledgements The dataset is originally from https//www.govdata.de/web/guest/apps/-/details/bundestagswahl-2017 Inspiration The data is interesting as it reflects the votes for all areas in Germany,Other,,[politics],Other,,,306,2472,10,Results  in the different areas,German Federal Elections 2017,https://www.kaggle.com/jenslaufer/german-election-2017,Sat Sep 30 2017
,Álvaro López García,"[dc:identifier, dc:modified, ayto:WKT, ayto:Estado, uri, wkt_wsg84]","[numeric, dateTime, string, string, string, string]",Description This is a collection of the Santander (Spain) bike-sharing open data facility named Tusbic operated by JCDecaux. I will be updating this dataset form time to time added new data as I collect it. Format Bike sharing system data The bikes.csv file contains the information from the bike sharing system. Data is structured as follows  number number of the station contract_name name of the contract of the station name name of the station address address of the station (raw) lat latitude of the station in WGS84 format lng latitude of the station in WGS84 format banking indicates whether this station has a payment terminal bonus indicates whether this is a bonus station status indicates whether this station is CLOSEDor OPEN bike_stands the number of operational bike stands at this station available_bike_stands the number of available bike stands at this station available_bikes the number of available and operational bikes at this station last_update timestamp indicating the last update time in milliseconds since Epoch  Bike lane geometries The bike_lanes.csv file contains the geometries of bike lanes in Santander city as published by the Santander City Council in its open data platform.  aytoWKT contains the geometry in WKT format using ED50 UTM coordinates (zone 30N). wkt_wsg84 contains the geometry in WKT format using WGS84 coordinates. aytoEstado shows the status of the bike lane. EJECUTADO means that is has been built and it is operative.  License The bike sharing data is being collected from the JCDecaux Developer Open Data platform and is licensed under the Etalab Open License compatbile with the standards of Open Data licenses (ODC-BY CC-BY 2.0). The bike lane geometry is being collected form the Santander Open Data Platform and is licensed under a CC BY 4.0 license. Dataste Kaggle logo is a photo licensed under a CC-BY-SA 3.0 authored by Tiia Monto.,CSV,,"[cities, cycling]",Other,,,67,958,2,Santander bike sharing system,Tusbic Santander ,https://www.kaggle.com/alvarolopez/tusbic,Thu Nov 16 2017
,Department of Transportation,"[IATA_CODE, AIRLINE]","[string, string]",Context The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time delayed canceled and diverted flights is published in DOT's monthly Air Travel Consumer Report and in this dataset of 2015 flight delays and cancellations. Acknowledgements The flight delay and cancellation data was collected and published by the DOT's Bureau of Transportation Statistics.,CSV,,[aviation],CC0,,,12561,64557,565,Which airline should you fly on to avoid significant delays?,2015 Flight Delays and Cancellations,https://www.kaggle.com/usdot/flight-delays,Fri Feb 10 2017
,joshkyh,[],[],The below information is from the project page https//nlp.stanford.edu/projects/glove/ Context GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus and the resulting representations showcase interesting linear substructures of the word vector space. Content Due to size constraints only the 25 dimension version is uploaded. Please visit the project page for GloVe of other dimensions. This dataset (https//www.kaggle.com/rtatman/glove-global-vectors-for-word-representation) contains GloVe extracted from Wikipedia 2014 + Gigaword 5.  Nearest neighbors The Euclidean distance (or cosine similarity) between two word vectors provides an effective method for measuring the linguistic or semantic similarity of the corresponding words. Sometimes the nearest neighbors according to this metric reveal rare but relevant words that lie outside an average human's vocabulary. Linear substructures The similarity metrics used for nearest neighbor evaluations produce a single scalar that quantifies the relatedness of two words. This simplicity can be problematic since two given words almost always exhibit more intricate relationships than can be captured by a single number. For example man may be regarded as similar to woman in that both words describe human beings; on the other hand the two words are often considered opposites since they highlight a primary axis along which humans differ from one another.  In order to capture in a quantitative way the nuance necessary to distinguish man from woman it is necessary for a model to associate more than a single number to the word pair. A natural and simple candidate for an enlarged set of discriminative numbers is the vector difference between the two word vectors. GloVe is designed in order that such vector differences capture as much as possible the meaning specified by the juxtaposition of two words. Acknowledgements Jeffrey Pennington Richard Socher and Christopher D. Manning. 2014. Inspiration The dataset specifically includes tokens extracted from Twitter which unlike tokens from Wikipedia include many abbreviations that have interesting content.,Other,,"[languages, linguistics, twitter]",Other,,,59,1016,246,Pre-trained word vectors from Twitter,GloVe: Global Vectors for Word Representation,https://www.kaggle.com/joshkyh/glove-twitter,Tue Sep 19 2017
,Keras,[],[],InceptionV3  Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training) computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.  Authors Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens Zbigniew Wojna https//arxiv.org/abs/1512.00567  InceptionV3 Architecture   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,26,672,162,InceptionV3 Pre-trained Model for Keras,InceptionV3,https://www.kaggle.com/keras/inceptionv3,Tue Dec 12 2017
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, numeric, string, string, string, string, string, numeric, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one data file for each of these countries States included in this dataset Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets to map weather crime or how your next canoing trip.,CSV,,[],Other,,,213,1143,7168,Addresses and geolocations for European countries,OpenAddresses - Europe,https://www.kaggle.com/openaddresses/openaddresses-europe,Thu Aug 03 2017
,marc moreaux,[],[],"Context With this dataset we hope to do a nice cheeky wink to the ""cats and dogs"" image dataset. In fact this dataset is aimed to be the audio counterpart of the famous ""cats and dogs"" image classification task here available on Kaggle. Content The dataset consists in many ""wav"" files for both the cat and dog classes    cat has 164 WAV files to which corresponds 1323 sec of audio dog has 113 WAV files to which corresponds 598 sec of audio  You can have an visual description of the Wav here  Visualizing woofs & meows 🐱. In Accessing the Dataset 2 we propose a train / test split which can be used. All the WAV files contains 16KHz audio and have variable length. Acknowledgements We have not much credit in proposing the dataset here. Much of the work have been done by the AE-Dataset creator (From which we extracted the two classes) and by the humans behind FreeSound From which was extracted the AE-Dataset. Inspiration You might use this dataset to test raw audio classification challenge ;)  A more challenging dataset is available here",Other,,"[animals, acoustics]",CC3,,,493,6439,59,Classify raw sound events,Audio Cats and Dogs,https://www.kaggle.com/mmoreaux/audio-cats-and-dogs,Thu Oct 05 2017
,Chaitanya Bapat,[],[],"Context Fantasy Premier League is the online global competition for Football Enthusiasts to try their luck at picking up their ""Dream Team"" and collect points. It involves a deep understanding of the Sport Clubs Players and the Fixtures apart from many other things. All this makes for a compelling Data Science (read Machine Learning Problem). English Premier League (EPL) - one of the famous leagues in the sport of football. Most viewed and followed across the globe. FPL provides an opportunity for enthusiasts to try their hand at decision making. To predict the best set of players who perform every game. Points are given based on various parameters. Goal - To get the maximum Team Score every week Content Time Period - Year 2016-17 Season Dataset consists of  FPL users data (basic information) Fixtures (Week-wise) Points earned by every user (Gameweek-wise)  Data Extraction Detailed Information about the Code Github repo - https//github.com/ChaiBapchya/fantasypremierleague-datascience Acknowledgements Thanks to Fantasy Premier League without which this data would not have been available. Inspiration  Diego Costa scores a goal every 15 minutes of the match he plays.  Harry Kane is the youngest player to have scored 100 Premier League goals.   Such statistics (if true) are a compelling read. But to know -   Nathaniel Chalobah has 70% probability of scoring against Stoke City this weekend.  Alexis Sanchez will score around 2 goals this month. Cesc Fabregas is going to assist this weekend.  There's 70% David De Gea is going to have a clean-sheet.  Such statistics and much more lend so much credibility to decision making. It would enable Fantasy Premier League team owners to decide - a. When to choose which player? b. When is the right time to use the WildCard? c. If it is time to be patient with a signing? d. Who to watch out for? In order to do this one needs data to back your predictions. Hence I was keen on retrieving all this data.  Future Scope -  Predict the best team for the upcoming week. Predict the best player (Highest value for money) (Goalkeeper Defender MId-fielder Attacker) Suggest possible changes in formation if need be. ",Other,,[association football],CC0,,,812,7361,685,"Dataset regarding users, fixtures, points of Fantasy English Premier League",Fantasy Premier League,https://www.kaggle.com/chaibapat/fantasy-premier-league,Wed May 17 2017
,casimian2000,"[datetime, host, src, proto, type, spt, dpt, srcstr, cc, country, locale, localeabbr, postalcode, latitude, longitude, ]","[dateTime, string, numeric, string, string, numeric, numeric, string, string, string, string, string, string, numeric, numeric, string]",Context (U) My purpose is to analyze Amazon Web Services (AWS) honeypot data for any trends and/or correlations that could possibly be used in predictive cyber threat vectors.  I spent a lot of time looking for data sets and most of the ones I found had no documentation and the data was hard to interpret just from the file.  This data is well formatted and straight forward. Content (U) The AWS Honeypot Database is an open-source database including information on cyber attacks/attempts.  (U) Data has 451581 data points collected from 953pm on 3 March 2013 to 555am on 8 September 2013.  Acknowledgements http//datadrivensecurity.info/blog/pages/dds-dataset-collection.html Jay Jacobs & Bob Rudis Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?,CSV,,"[crime, internet]",CC0,,,165,1807,6,Visualizing Cyber Attacks,AWS Honeypot Attack Data,https://www.kaggle.com/casimian2000/aws-honeypot-attack-data,Fri Jan 26 2018
,Paolo Campanelli,"[name, id]","[string, numeric]",League of Legends Ranked Matches Data about 184070 League of Legends ranked solo games spanning across several years Content  Matches Player and team stats Bans  Acknowledgements I found this data on a SQL database and exported it to CSV. All data belongs ultimately to Riot Games and their data policies applies. These files are presented only as a simpler way to obtain a large dataset without stressing the Riot API and are in no way associated with Riot Games. The data is provided as-is without any warranty on its correctness. If your algorithm catches fire don't blame me or Riot. If you are Rito and are opposed to sharing this data here contact me and it will be removed immediately. Possible questions  Can we predict the winner given the teams? Can ranked matchmaking be assumed to be unbiased (or adjusted for red-side advantage)? Does the region affect significantly win rates? Can we compare the data in relation to competitive data (also available on Kaggle)? Can we assess information on the different metas? ,CSV,,[video games],Other,,,1114,8724,696,180000 ranked games of League of Legends starting from 2014,League of Legends Ranked Matches,https://www.kaggle.com/paololol/league-of-legends-ranked-matches,Thu Oct 26 2017
,Datafiniti,"[address, categories, city, country, latitude, longitude, name, postalCode, province, reviews.date, reviews.dateAdded, reviews.doRecommend, reviews.id, reviews.rating, reviews.text, reviews.title, reviews.userCity, reviews.username, reviews.userProvince]","[string, string, string, string, numeric, numeric, string, numeric, string, dateTime, dateTime, string, string, numeric, string, string, string, string, string]",About This Data This is a list of 1000 hotels and their reviews provided by Datafiniti's Business Database. The dataset includes hotel location name rating review data title username and more.  What You Can Do With This Data You can use this data to compare hotel reviews on a state-by-state basis; experiment with sentiment scoring and other natural language processing techniques. The review data lets you correlate keywords in the review text with ratings. E.g.  What are the bottom and top states for hotel reviews by average rating? What is the correlation between a state’s population and their number of hotel reviews? What is the correlation between a state’s tourism budget and their number of hotel reviews?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, hotels, linguistics, internet]",CC4,,,1487,10610,16,"A list of 1,000 hotels and their online reviews.",Hotel Reviews,https://www.kaggle.com/datafiniti/hotel-reviews,Mon May 29 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,[],[],Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context Ministry of Human Resource Development (MHRD) Govt of India has initiated an All India Survey on Higher Education (AISHE) in the year 2010-11 to build a robust database and to assess the correct picture of higher Education in the country.  The main objectives of the survey was to  identify & capture all the institutions of higher learning in the country Collect the data from all the higher education institutions on various aspects of higher education.    Data was collected on following broad items  Institution’s Basic Details Teacher’s Details Details of Non-Teaching Staff Programme conducted under various Faculties/Schools & Departments/Centres Students enrolled in these Programme Examination result of terminal year of each Programme Financial Information such as Receipt and Expenditure under various heads Availability of Infrastructure Scholarships Loans & Accreditation   source AISHE(pdf) Content This dataset contains unit level data from AISHE from 2011-12 to 2015-16. csv file list  accreditation.csv college.csv college_institution.csv college_institution_accreditation.csv college_institution_department.csv college_institution_faculty.csv college_institution_non_teaching_staff_count.csv college_institution_student_hostel.csv college_institution_teaching_staff.csv college_institution_teaching_staff_sanctioned_strength.csv course.csv course_enrolled_foreign_student_count.csv course_enrolled_student_count.csv course_examination_result.csv department.csv educational_institution_course.csv enrolled_distance_student_university.csv enrolled_distance_student_university_count.csv enrolled_foreign_student_count.csv enrolled_student_count.csv examination_result.csv faculty.csv faculty_department.csv infrastructure.csv loan.csv MetaData.csv non_teaching_staff_count.csv other_minority_college_regular .csv other_minority_standalone_distance.csv other_minority_standalone_regular .csv other_minority_university_distance.csv other_minority_university_regular .csv persons_count_by_category.csv private_students_result.csv ref_broad_discipline_group.csv ref_broad_discipline_group_category.csv ref_college_institution_statutory_body.csv ref_count_by_category_remarks.csv ref_country.csv ref_course_level.csv ref_course_mode.csv ref_course_type.csv ref_diploma_course.csv ref_district.csv ref_examination_system.csv ref_institute_type.csv ref_institution_management.csv ref_non_teaching_staff_group.csv ref_non_teaching_staff_type.csv ref_programme.csv ref_programme_broad_discipline_group_and_category.csv ref_programme_statutory_body.csv ref_speciality.csv ref_standalone_institution.csv ref_state.csv ref_state_body.csv ref_student_hostel_type.csv ref_teaching_staff_designation.csv ref_teaching_staff_selection_mode.csv ref_university.csv ref_university_college_type.csv ref_university_type.csv regional_center.csv scholarship.csv staff_quarter.csv standalone_institution.csv standalone_institution_accreditation.csv standalone_institution_department.csv standalone_institution_faculty.csv standalone_institution_non_teaching_staff_count.csv standalone_institution_student_hostel.csv standalone_institution_teaching_staff.csv standalone_institution_teaching_staff_sanctioned_strength.csv student_hostel.csv teaching_staff.csv teaching_staff_count.csv teaching_staff_sanctioned_strength.csv university.csv university_accreditation.csv university_department.csv university_enrolled_distance_student.csv university_faculty.csv university_non_teaching_staff_count.csv university_private_students_result.csv university_student_hostel.csv university_teaching_staff.csv university_teaching_staff_sanctioned_strength.csv  Acknowledgements Ministry of Human Resource Development (MHRD) Govt of India has published this dataset on Open Govt Data India Platform under Govt. open data license - India. MHRD has also published some reports from this survey. Inspiration This is an interesting dataset to get the holistic picture of higher education system in India. One of the main objective of dept. of higher education is to increase the gross enrolment ratio (GRT) to 15% by 2011-12 and to 21% by 12th five year plan (2012-17).  One can look at things like the objective like this has been achieved or can be achieved based on the progress of past data. There are several other things that can be analysed from this dataset.  Pupil-Teacher Ratio (PTR) Out-Turn Gender Parity Index (GPI) etc. ,Other,,"[india, education]",CC4,,,548,5072,2048,Unit level survey data from 2011-12 to 2015-16.,Higher Education Analytics,https://www.kaggle.com/rajanand/aishe,Sun Aug 13 2017
,Jacob Boysen,"[ACTOR1, ACTOR1_ID, ACTOR2, ACTOR2_ID, ACTOR_DYAD_ID, ADMIN1, ADMIN2, ADMIN3, ALLY_ACTOR_1, ALLY_ACTOR_2, COUNTRY, EVENT_DATE, EVENT_ID_CNTY, EVENT_ID_NO_CNTY, EVENT_TYPE, FATALITIES, GEO_PRECISION, GWNO, INTER1, INTER2, INTERACTION, LATITUDE, LOCATION, LONGITUDE, NOTES, SOURCE, TIME_PRECISION, YEAR]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, numeric]",Context The Armed Conflict Location and Event Data Project is designed for disaggregated conflict analysis and crisis mapping. This dataset codes the dates and locations of all reported political violence and protest events in dozens of developing countries in Africa. Political violence and protest includes events that occur within civil wars and periods of instability public protest and regime breakdown. The project covers all African countries from 1997 to the present. Content These data contain information on  Dates and locations of conflict events; Specific types of events including battles civilian killings riots protests and recruitment activities; Events by a range of actors including rebels governments militias armed groups protesters and civilians; Changes in territorial control; and Reported fatalities.  Event data are derived from a variety of sources including reports from developing countries and local media humanitarian agencies and research publications. Please review the codebook and user guide for additional information the codebook is for coders and users of ACLED whereas the brief guide for users reviews important information for downloading reviewing and using ACLED data. A specific user guide for development and humanitarian practitioners is also available as is a guide to our sourcing materials. Acknowledgements ACLED is directed by Prof. Clionadh Raleigh (University of Sussex). It is operated by senior research manager Andrea Carboni (University of Sussex) for Africa and Hillary Tanoff for South and South-East Asia. The data collection involves several research analysts including Charles Vannice James Moody Daniel Wigmore-Shepherd Andrea Carboni Matt Batten-Carew Margaux Pinaud Roudabeh Kishi Helen Morris Braden Fuller Daniel Moody and others. Please cite Raleigh Clionadh Andrew Linke Håvard Hegre and Joakim Karlsen. 2010. Introducing ACLED-Armed Conflict Location and Event Data. Journal of Peace Research 47(5) 651-660. Inspiration Do conflicts in one region predict future flare-ups? How do the individual actors interact across time? Do some sources report more often on certain actors?,CSV,,[war],CC0,,,95,1138,59,Details on 165k Conflicts Across Africa Over Twenty Years,"ACLED African Conflicts, 1997-2017",https://www.kaggle.com/jboysen/african-conflicts,Tue Aug 08 2017
,Abhinav Walia,"[, lemma, next-lemma, next-next-lemma, next-next-pos, next-next-shape, next-next-word, next-pos, next-shape, next-word, pos, prev-iob, prev-lemma, prev-pos, prev-prev-iob, prev-prev-lemma, prev-prev-pos, prev-prev-shape, prev-prev-word, prev-shape, prev-word, sentence_idx, shape, word, tag]","[numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string]","Context Annotated Corpus for Named Entity Recognition using GMB(Groningen Meaning Bank) corpus for entity classification with enhanced and popular features by Natural Language Processing applied to the data set.  Tip Use Pandas Dataframe to load dataset if using Python for convenience.  Content This is the extract from GMB corpus which is tagged annotated and built specifically to train the classifier to predict named entities such as name location etc.  Number of tagged entities  'O' 1146068' geo-nam' 58388 'org-nam' 48034 'per-nam' 23790 'gpe-nam' 20680 'tim-dat' 12786 'tim-dow' 11404 'per-tit' 9800 'per-fam' 8152 'tim-yoc' 5290 'tim-moy' 4262 'per-giv' 2413 'tim-clo' 891 'art-nam' 866 'eve-nam' 602 'nat-nam' 300 'tim-nam' 146 'eve-ord' 107 'per-ini' 60 'org-leg' 60 'per-ord' 38 'tim-dom' 10 'per-mid' 1 'art-add' 1  Essential info about entities  geo = Geographical Entity org = Organization per = Person gpe = Geopolitical Entity tim = Time indicator art = Artifact eve = Event nat = Natural Phenomenon  Total Words Count = 1354149 Target Data Column ""tag"" Inspiration This dataset is getting more interested because of more features added to the recent version of this dataset. Also it helps to create a broad view of Feature Engineering with respect to this dataset. Why this dataset is helpful or playful? It might not sound so interested for earlier versions but when you are able to pick intent and custom named entities from your own sentence with more features then it is getting interested and helps you solve real business problems(like picking entities from Electronic Medical Records etc) Please feel free to ask questions do variations and let's play together!",CSV,,[linguistics],ODbL,,,1741,16166,164,Corpus (CoNLL 2002) annotated with IOB and POS tags,Annotated Corpus for Named Entity Recognition,https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus,Thu Sep 21 2017
,United Nations,"[session, year, country, text]","[numeric, numeric, string, string]","Context Every year since 1947 representatives of UN member states gather at the annual sessions of the United Nations General Assembly. The centrepiece of each session is the General Debate. This is a forum at which leaders and other senior officials deliver statements that present their government’s perspective on the major issues in world politics. These statements are akin to the annual legislative state-of-the-union addresses in domestic politics. This dataset the UN General Debate Corpus (UNGDC) includes the corpus of texts of General Debate statements from 1970 (Session 25) to 2016 (Session 71). Content This dataset includes the text of each country’s statement from the general debate separated by country session and year and tagged for each. The text was scanned from PDFs of transcripts of the UN general sessions. As a result the original scans included  page numbers in the text from OCR (Optical character recognition) scans which have been removed. This dataset only includes English. Acknowledgements This dataset was prepared by  Alexander Baturo Niheer Dasandi and Slava Mikhaylov and is presented in the paper ""Understanding State Preferences With Text As Data Introducing the UN General Debate Corpus"" Research & Politics 2017. Inspiration This dataset includes over forty years of data from different countries which allows for the exploration of differences between countries and over time. This allows you to ask both country-specific and longitudinal questions. Some questions that might be interesting  How has the sentiment of each country’s general debate changed over time?  What topics have been more or less popular over time and by region?  Can you build a classifier which identifies which country a given text is from?  Are there lexical or syntactic changes over time or differences between region?  How does the latitude of a country affect lexical complexity? ",CSV,,"[international relations, linguistics]",CC0,,,250,2800,129,Transcriptions of general debates at the UN from 1970 to 2016,UN General Debates,https://www.kaggle.com/unitednations/un-general-debates,Wed Sep 06 2017
,JustinMoore,"[player_name, tracker_id, solo_KillDeathRatio, solo_WinRatio, solo_TimeSurvived, solo_RoundsPlayed, solo_Wins, solo_WinTop10Ratio, solo_Top10s, solo_Top10Ratio, solo_Losses, solo_Rating, solo_BestRating, solo_DamagePg, solo_HeadshotKillsPg, solo_HealsPg, solo_KillsPg, solo_MoveDistancePg, solo_RevivesPg, solo_RoadKillsPg, solo_TeamKillsPg, solo_TimeSurvivedPg, solo_Top10sPg, solo_Kills, solo_Assists, solo_Suicides, solo_TeamKills, solo_HeadshotKills, solo_HeadshotKillRatio, solo_VehicleDestroys, solo_RoadKills, solo_DailyKills, solo_WeeklyKills, solo_RoundMostKills, solo_MaxKillStreaks, solo_WeaponAcquired, solo_Days, solo_LongestTimeSurvived, solo_MostSurvivalTime, solo_AvgSurvivalTime, solo_WinPoints, solo_WalkDistance, solo_RideDistance, solo_MoveDistance, solo_AvgWalkDistance, solo_AvgRideDistance, solo_LongestKill, solo_Heals, solo_Revives, solo_Boosts, solo_DamageDealt, solo_DBNOs, duo_KillDeathRatio, duo_WinRatio, duo_TimeSurvived, duo_RoundsPlayed, duo_Wins, duo_WinTop10Ratio, duo_Top10s, duo_Top10Ratio, duo_Losses, duo_Rating, duo_BestRating, duo_DamagePg, duo_HeadshotKillsPg, duo_HealsPg, duo_KillsPg, duo_MoveDistancePg, duo_RevivesPg, duo_RoadKillsPg, duo_TeamKillsPg, duo_TimeSurvivedPg, duo_Top10sPg, duo_Kills, duo_Assists, duo_Suicides, duo_TeamKills, duo_HeadshotKills, duo_HeadshotKillRatio, duo_VehicleDestroys, duo_RoadKills, duo_DailyKills, duo_WeeklyKills, duo_RoundMostKills, duo_MaxKillStreaks, duo_WeaponAcquired, duo_Days, duo_LongestTimeSurvived, duo_MostSurvivalTime, duo_AvgSurvivalTime, duo_WinPoints, duo_WalkDistance, duo_RideDistance, duo_MoveDistance, duo_AvgWalkDistance, duo_AvgRideDistance, duo_LongestKill, duo_Heals, duo_Revives, duo_Boosts]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Grab your pans... Player statistics for approximately 85000 of the top PUBG players (as tracked by https//pubgtracker.com/). All statistics were gathered using aggregate region filters (all regions) and feature labels are subdivided by server type solo duo and squad. 87898 players with 150 numerical game-play features per player (+2 for player name and PUBG Tracker ID). Content Features include KD ratios wins losses damage wins top 10's and movement characteristics (walking/riding distance etc...) Acknowledgements Special thanks to pubgtracker.com for their support and aid with gathering this data. More information can be found here https//pubgtracker.com/ PLAYERUNKNOWN'S BATTLEGROUNDS is a registered trademark trademark or service mark of Bluehole Inc. and its affiliates https//www.playbattlegrounds.com/main.pu Inspiration As a gamer addicted to PUBG it was a blast putting this data set together. Some great project ideas include a. Visualizations of player skill vs. specific strategies b. Unsupervised clustering of players based on strategy (for matchmaking or team building) c. Prediction of features based on player skill and/or strategies,CSV,,"[games and toys, video games]",CC0,,,732,7896,62,Player statistics for PUBG. 150 features per player.,PLAYERUNKNOWN'S BATTLEGROUNDS Player Statistics,https://www.kaggle.com/lazyjustin/pubgplayerstats,Tue Aug 01 2017
,Keras,[],[],ResNet-50  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,333,2120,174,ResNet-50 Pre-trained Model for Keras,ResNet-50,https://www.kaggle.com/keras/resnet50,Tue Dec 12 2017
,Seattle Public Library,"[BibNumber, ItemBarcode, ItemType, Collection, CallNumber, CheckoutDateTime]","[numeric, numeric, string, string, string, dateTime]",Context This dataset includes a log of all physical item checkouts from Seattle Public Library. The dataset begins with checkouts occurring in April 2005 and is regularly updated. Renewals are not included. Content The dataset contains several types of files  Checkout records hold the raw data. I've dropped several columns from these files in order to shrink the total dataset size down from a couple of dozen gigabytes; they can be rebuilt by merging with the library collection inventory. The data dictionary allows you to decode the 'ItemType' column from the checkout records. The library collection inventory is a dataset in its own right and stores important metadata about each title such as the author and subjects.  Inspiration  Can you predict which books will be checked out in the coming month? SPL posts fresh data every month so you can check your forecast by downloading the new data from them.  With a bit of imagination this can be a great dataset for logistics modeling. Make some assumptions about each location's storage capacity and where a book was checked out from and you've got a warehouse resource allocation problem.   Acknowledgements This dataset was kindly made available by the Seattle Public Library. You can find the original copies of the three component datasets at the following links - Collection data - Data dictionary - Checkout records Disclaimer The data made available here has been modified for use from its original source which is the City of Seattle. Neither the City of Seattle nor the Office of the Chief Technology Officer (OCTO) makes any claims as to the completeness timeliness accuracy or content of any data contained in this application; makes any representation of any kind including but not limited to warranty of the accuracy or fitness for a particular use; nor are any such warranties to be implied or inferred with respect to the information or data furnished herein. The data is subject to change as modifications and updates are complete. It is understood that the information contained in the web feed is being used at one's own risk. For the complete terms of use please see the City of Seattle Data Policy.,CSV,,"[books, libraries]",Other,,,404,2928,7168,Twelve years of checkout records,Seattle Library Checkout Records,https://www.kaggle.com/seattle-public-library/seattle-library-checkout-records,Tue Oct 24 2017
,Steven Coleman,"[Flight Number, Date, Time (UTC), Booster Version, Launch Site, Payload, Payload Mass (kg), Orbit, Customer, Mission Outcome, Landing Outcome]","[numeric, dateTime, dateTime, string, string, string, string, string, string, string, string]",Context SpaceX designs manufactures and launches advanced rockets and spacecraft. The company was founded in 2002 to revolutionize space technology with the ultimate goal of enabling people to live on other planets - SpaceX Content The dataset contains mission information for rocket launches conducted by SpaceX (Space Exploration Technologies Corp). Acknowledgements Data was obtained via Wikipedia's entry for Falcon 9 and Falcon Heavy launches. Inspiration Do you anticipate an increase in launches with the introduction of the Falcon Heavy? How has launch rate increased over time? Do you predict a shift in payload orbits for upcoming launches? How has the customer diversity changed over the years?,CSV,,"[space, spaceflight]",CC3,,,261,1776,0.005859375,"Launch, payload, and outcome information for SpaceX missions",SpaceX Launch Data,https://www.kaggle.com/scoleman/spacex-launch-data,Mon Feb 12 2018
,Datafiniti,"[id, address, categories, city, country, keys, latitude, longitude, menuPageURL, menus.amountMax, menus.amountMin, menus.currency, menus.dateSeen, menus.description, menus.name, name, postalCode, priceRangeCurrency, priceRangeMin, priceRangeMax, province]","[string, string, string, string, string, string, numeric, numeric, string, numeric, numeric, string, dateTime, string, string, string, numeric, string, numeric, numeric, string]",About this Data This is a list of over 3500 pizzas from multiple restaurants provided by Datafiniti's Business Database. The dataset includes the category name address city state menu information price range and more for each pizza restaurant.  What You Can Do with this Data You can use this data to discover how much you can expect to pay for pizza across the country. E.g.  What are the least and most expensive cities for pizza? What is the number of restaurants serving pizza per capita (100000 residents) across the U.S.? What is the median price of a large plain pizza across the U.S.? Which cities have the most restaurants serving pizza per capita (100000 residents)?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, food and drink, business, internet]",CC4,,,747,4480,1,"A list of pizza restaurants,  3,500 pizzas, and their menu prices.",Pizza Restaurants and the Pizza They Sell,https://www.kaggle.com/datafiniti/pizza-restaurants-and-the-pizza-they-sell,Sat Sep 30 2017
,HugoDarwood,[],[],Context I created this dataset to explore different factors affecting people's enjoyment of food and/or cooking! Content Over 20k recipes listed by recipe rating nutritional information and assigned category (sparse). I may later upload a version binned by recipe creation date and also including recipe ingredients. Use the 'full_format_recipes.json' file to interact with all recipe data 'epi_r.csv' drops ingredients and directions in favour of sparse category dummies.  Acknowledgements Recipe information lifted from http//www.epicurious.com/recipes-menus,Other,,"[food and drink, nutrition]",Other,,,2491,16920,86,"Recipes from Epicurious by rating, nutritional content, and categories",Epicurious - Recipes with Rating and Nutrition,https://www.kaggle.com/hugodarwood/epirecipes,Tue Feb 21 2017
,Sohier Dane,"[Date, Value, Variable]","[dateTime, numeric, string]",Context This data set covers all aspects of the pre-WWI and interwar economies including production construction employment money prices asset market transactions foreign trade and government activity. Many series are highly disaggregated and many exist at the monthly or quarterly frequency. The data set has some coverage of the United Kingdom France and Germany although it predominantly covers the United States. For information see  Improving the Accessibility of the NBER's Historical Data  by Daniel Feenberg and Jeff Miron. (NBER Working Paper #5186). Published in the Journal of Business and Economic Statistics Volume 15 Number 3 (July 1997) pages 293-299.  Information about seasonal adjustments is available but in most cases only unadjusted series have been made available here. Content The data.csv is organized in a long format with columns for the date variable and value.  The dates are always the beginning of period date for whatever period existed in the original data. This means that '1920' was converted to January 1st 1920 while Q2 1920 was converted to April 1 1920. This is intended as a convenience to make it easier to work with multiple time series from the original mixed frequency data. The data is currently organized into 16 chapters  Chapter1 Production of Commodities Chapter2 Construction Chapter3 Transportation and Public Utilities Chapter4 Prices Chapter5 Stocks of Commodities Chapter6 Distribution of Commodities Chapter7 Foreign Trade Chapter8 Income and Employment Chapter9 Financial Status of Business Chapter10 Savings and Investment Chapter11 Security Markets Chapter12 Volume of Transactions Chapter13 Interest Rates Chapter14 Money and Banking Chapter15 Government and Finance Chapter16 Leading Indicators  The dataset has been transformed from its original format. You can find the data preparation code here. Acknowledgements This dataset was kindly made available by the National Bureau of Economic Research (NBER). You can find the original dataset here. Inspiration  Which major historical events can you detect from the data? With roughly 3500 time series in the dataset finding relevant information can be challenging. Can you find a better way of organizing or indexing the data?  ,CSV,,"[history, economics]",CC0,,,90,987,31,Western economic history data spanning 1785-1974,NBER Macrohistory Database,https://www.kaggle.com/sohier/nber-macrohistory-database,Thu Oct 12 2017
,GyanendraMishra,"[index, song, year, artist, genre, lyrics]","[numeric, string, numeric, string, string, string]",Context I tried to gather as many lyrics as I could. I ran my code on a a free ec2 instance and ran out of storage space. I have attached the code below so if any one wants to try out it and get all lyrics please do. Content There are around 380000+ lyrics in the data set from a lot of different artists from a lot of different genres arranged by year. Structure is artist/year/song. Every artist folder has a genre.txt that tells what is the genre of the musician. Find the crawler here. Acknowledgements I would like to thank Shruti Jasoria SJasoria on GitHub for writing the multi-threaded version. Inspiration I wanted to find out what genre and what artist abuses what substance. Do rapstars like cocaine or liquor? If liquor then what Liquor? Does Eminem prefer Hennesy over Jack Daniels? Do Rockstars love pot?,CSV,,"[writing, music, linguistics]",CC4,,,1629,11932,310,"Lyrics, Artist , Genre, Year","380,000+ lyrics from MetroLyrics",https://www.kaggle.com/gyani95/380000-lyrics-from-metrolyrics,Wed Jan 11 2017
,Mirko Mälicke,[],[],Context I have desk job. A very interesting desk job but nevertheless a desk job. Therefore I started doing sports a while ago and now I can't stop anymore. Like every other Geek I need a gadet for every hobby I have and in this case it was a GPS-Sport-Smartwatch The vivoactive and the vivoactive HR by Garmin. They also offer a data analysis center and as a data scientist I of course had to export the data and employ some analysis that go beyond bar charts. This dataset is basically the bulk-downloaded-not-cleaned-dataset from the mentioned data center. For those interested there is a nice github project for bulk downloading from garmin connect https//github.com/kjkjava/garmin-connect-export. The weather data has to be included by hand. Content You can find 155 samples each one representing one sport activity mainly from the black forest. There are a lot of useless colmuns which either contain no data or the same value for every sample. You will have to identify these columns in any case and remove them.  The ZIP includes the GPX tracks of all activities and can be used as well. The two devices use different GPS sensors and are from my feeling of different precision and reliability (untested). Additionally the HR device had a lot connectivity problems since summer 2017. The devices lost the signal during a numerous amount of runs and therefore the distance value is not always correct. Usually the start and end timestamps are correct (except for one case) and the GPX files might help to figure out which track I was using. With a single exception all start and ending points are the same in all tracks. This means I started and ended recording at the same cross-roads not exactly the same position. If you decide to open the GPX files in a GIS you should be able to repair the affected datasets. I did this using QGIS. You will find one instance with two activities on a single day. This is actually the same activity where I went to the peak of Rosskopf in the Black Forest. Because my brain was undersupplied after runnning up there I ended the activity instead of pausing it that's why I ended up with two activities that have to be merged together.  Soft data For the dataset there is also some soft data which might be helpful  I commute 130 km to work since June 2016 usually on Mon Tue Wed. Possibly I spent less time on sports since then on these days. I wrote my Master thesis from Sep / 2015 until Mar 2016. Maybe I was doing more sports during the thesis (except for the last two weeks?) Since I commute I would say I do sports less frequently but on longer distances and over higher elevation gains I bought new shoes in August 2016 which are WAY more comfortable  Acknowledgements The bulk downloading script for garmin connect was really helpful https//github.com/kjkjava/garmin-connect-export. Without this tool I most likely would not have created this dataset. Inspiration I am personally very interested if my running performance is dependent on specific weather conditions and eventually predictable. Another interesting thing would be to see how other people rate the performance based on the given data. I use this dataset also during my teaching in a Python and a statistics class at University. I decided to upload the data and some of my teaching notebooks to Kaggle (in the near future) in oder to give my students access to external comments on my kernels givem them the opportunity to upload their solutions to a bigger community and eventually scan your kernels on the data.,{}JSON,,"[running, sports, weather]",CC4,,,163,1590,3,Can you predict sport performance from the weather?,Run Activities,https://www.kaggle.com/mmaelicke/run-activites,Mon Nov 27 2017
,Zeeshan-ul-hassan Usmani,"[node_1, rel_type, node_2, r.sourceID, r.valid_until, r.start_date, r.end_date]","[numeric, string, numeric, string, string, string, string]",Context I've uploaded a dataset previously that contains Paradise Papers Panama Papers Bahamas and Offshore Leaks. I've been getting a lot of requests to upload Paradise papers alone to make it less confusing for my fellow data scientists. Here it is the complete cache of Paradise Papers released so far. I will keep updating it. The Paradise Papers is a cache of some 13GB of data that contains 13.4 million confidential records of offshore investment by 120000 people and companies in 19 tax jurisdictions (Tax Heavens - an awesome video to understand this); that was published by the International Consortium of Investigative Journalists (ICIJ) on November 5 2017. Subsequent data was released on November 20 2017. Here is a brief video about the leak. The people include Queen Elizabeth II the President of Columbia (Juan Manuel Santos) Former Prime Minister of Pakistan (Shaukat Aziz) U.S Secretary of Commerce (Wilbur Ross) and many more. According to an estimate by the Boston Consulting Group the amount of money involved is around $10 trillion. The leak contains many famous companies including Facebook Apple Uber Nike Walmart Allianz Siemens McDonald’s and Yahoo. It also contains a lot of U. S President Donald Trump allies including Rax Tillerson Wilbur Ross Koch Brothers Paul Singer Sheldon Adelson Stephen Schwarzman Thomas Barrack and Steve Wynn etc. The complete list of Politicians involve is available here. I am calling all data scientists to help me stop the corruption and reveal the patterns and linkages invisible for the untrained eye. Content The data is the effort of more than 100 journalists from 60+ countries The original data is available under creative common license and can be downloaded from this link. I will keep updating the datasets with more leaks and data as it’s available Acknowledgements International Consortium of Investigative Journalists (ICIJ) Inspiration Some ideas worth exploring How many companies and individuals are there in all of the leaks data  How many countries involved  Total money involved  What is the biggest best tax heaven  Can we compare the corruption with human development index and make an argument that would correlate corruption with bad conditions in that country  Who are the biggest cheaters and where they live  What role Fortune 500 companies play in this game  I need your help to make this world corruption free in the age of NLP and Big Data,CSV,,"[covariance and correlation, money, politics]",CC3,,,112,1728,7,Data Scientists Against Corruption,Paradise Papers,https://www.kaggle.com/zusmani/paradise-papers,Tue Nov 21 2017
,Federal Reserve,"[Series Description, SPOT EXCHANGE RATE - EURO AREA , UNITED KINGDOM -- SPOT EXCHANGE RATE, US$/POUND (1/RXI_N.B.UK), SPOT EXCHANGE RATE - BRAZIL , CHINA -- SPOT EXCHANGE RATE, YUAN/US$ P.R. , DENMARK -- SPOT EXCHANGE RATE, KRONER/US$ , INDIA -- SPOT EXCHANGE RATE, RUPEES/US$ , JAPAN -- SPOT EXCHANGE RATE, YEN/US$ , KOREA -- SPOT EXCHANGE RATE, WON/US$ , MALAYSIA -- SPOT EXCHANGE RATE, RINGGIT/US$ , MEXICO -- SPOT EXCHANGE RATE, PESOS/US$ , NORWAY -- SPOT EXCHANGE RATE, KRONER/US$ , SWEDEN -- SPOT EXCHANGE RATE, KRONOR/US$ , SOUTH AFRICA -- SPOT EXCHANGE RATE, RAND/$US, SINGAPORE -- SPOT EXCHANGE RATE, SINGAPORE $/US$ , SWITZERLAND -- SPOT EXCHANGE RATE, FRANCS/US$ , TAIWAN -- SPOT EXCHANGE RATE, NT$/US$ , THAILAND -- SPOT EXCHANGE RATE, BAHT/US$ , SPOT EXCHANGE RATE - VENEZUELA , Nominal Broad Dollar Index , Nominal Major Currencies Dollar Index , Nominal Other Important Trading Partners Dollar Index , AUSTRALIA -- SPOT EXCHANGE RATE US$/AU$ (RECIPROCAL OF RXI_N.B.AL), NEW ZEALAND -- SPOT EXCHANGE RATE, US$/NZ$ RECIPROCAL OF RXI_N.B.NZ , CANADA -- SPOT EXCHANGE RATE, CANADIAN $/US$ , HONG KONG -- SPOT EXCHANGE RATE, HK$/US$ , SRI LANKA -- SPOT EXCHANGE RATE, RUPEES/US$ ]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The Federal Reserve's H.10 statistical release provides data on exchange rates between the US dollar 23 other currencies and three benchmark indexes. The data extend back to 1971 for several of these. Please note that the csv has six header rows. The first contains the  Acknowledgements This dataset was provided by the US Federal Reserve. If you need the current version you can find their weekly updates here. Inspiration  Venezuela is both unusually dependent on oil revenues and experiencing unusual degrees of political upheaval. Can you determine which movements in their currency were driven by changes in the oil price and which were driven by political events? ,CSV,,"[finance, economics]",CC0,,,367,2295,2,Exchange rates as far back as 1971 between the USA and 23 countries,Exchange Rates,https://www.kaggle.com/federalreserve/exchange-rates,Wed Sep 06 2017
,Keras,[],[],VGG16  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,79,2493,542,VGG-16 Pre-trained Model for Keras,VGG-16 ,https://www.kaggle.com/keras/vgg16,Tue Dec 12 2017
,Securities and Exchange Commission,[],[],"The Financial Statement Data Sets below provide numeric information from the face financials of all financial statements.  This data is extracted from exhibits to corporate financial reports filed with the Commission using eXtensible Business Reporting Language (XBRL).  As compared to the more extensive Financial Statement and Notes Data Sets which provide the numeric and narrative disclosures from all financial statements and their notes the Financial Statement Data Sets are more compact. The information is presented without change from the ""as filed"" financial reports submitted by each registrant. The data is presented in a flattened format to help users analyze and compare corporate disclosure information over time and across registrants. The data sets also contain additional fields including a company's Standard Industrial Classification to facilitate the data's use. Content Each quarter's data is stored as a json of the original text files. This was necessary to limit the overall number of files. The num.txt file will likely be of most interest. Acknowledgements This dataset was kindly made available by the SEC. You can find the original dataset which is updated quarterly here.",{}JSON,,[finance],CC0,,,533,3640,3072,Data extracted from filings companies make to the SEC 2015-2017,Financial Statement Extracts,https://www.kaggle.com/securities-exchange-commission/financial-statement-extracts,Thu Sep 14 2017
,Jacob Boysen,"[address, care_assistant, care_taker, dbh, latitude, legal_status, location, longitude, permit_notes, plant_date, plant_type, plot_size, site_info, site_order, species, tree_id, x_coordinate, y_coordinate]","[string, string, string, numeric, numeric, string, string, numeric, string, dateTime, string, string, string, numeric, string, numeric, numeric, numeric]",Context Prop. E was a measure on the November 8 2016 San Francisco ballot regarding responsibility for maintaining street trees and surrounding sidewalks. Voters were asked if the City should amend the City Charter to transfer responsibility from property owners to the City for maintaining trees on sidewalks adjacent to their property as well for repairing sidewalks damaged by the trees. Prop E passed with almost 80% of the voters’ support. As part of this proposition a SF tree census was conducted by SF Public Works. Content 18 columns of data includes address (including lat/longs) caretaker details legal details size of plot time of planting surrounding site context and species. Acknowledgements This data was collected by SF Public Works. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too. Inspiration  SF is notorious for it’s microclimates--can you identify zones from particular trees that thrive there? Combine this data with the NYC Tree Census--which species are most common? least? ,CSV,,"[geography, climate, civil engineering]",CC0,,,63,840,48,188k Street Trees Around SF,SF Street Trees,https://www.kaggle.com/jboysen/sf-street-trees,Fri Sep 01 2017
,Aman Shrivastava,"[, Name, Age, Photo, Nationality, Flag, Overall, Potential, Club, Club Logo, Value, Wage, Special, Acceleration, Aggression, Agility, Balance, Ball control, Composure, Crossing, Curve, Dribbling, Finishing, Free kick accuracy, GK diving, GK handling, GK kicking, GK positioning, GK reflexes, Heading accuracy, Interceptions, Jumping, Long passing, Long shots, Marking, Penalties, Positioning, Reactions, Short passing, Shot power, Sliding tackle, Sprint speed, Stamina, Standing tackle, Strength, Vision, Volleys, CAM, CB, CDM, CF, CM, ID, LAM, LB, LCB, LCM, LDM, LF, LM, LS, LW, LWB, Preferred Positions, RAM, RB, RCB, RCM, RDM, RF, RM, RS, RW, RWB, ST]","[numeric, string, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",The Dataset you can play with.   Context Dataset for people who love data science and have grown up playing FIFA.  Content  Every player featuring in FIFA 18 70+ attributes  Player and Flag Images Playing Position Data Attributes based on actual data of the latest  EA's FIFA 18 game Attributes include on all player style statistics like Dribbling Aggression GK Skills etc. Player personal data like Nationality Photo Club Age Wage Salary etc.   Upcoming Update will Include   Team (National and Club) Data Player Images in Zip folder Betting Odds  The dataset contains all the statistics and playing attributes of all the players in the Full version of FIFA 18.  Data Source The data is scraped from the website https//sofifa.com by extracting the Player personal data and Player Ids and then the playing and style statistics.    Github Project  Possible Explorations  Make your dream team Analyse which Club or National Team has the best-rated players Assess the strength of a team at a particular position Analyse the team with the best dribbling speed Co-relate between Age and Overall rating Co-relate between Age and Nationality  Co-relate between Age and Potential   Could prove of immense value to Fantasy Premier League enthusiasts. These are just basic examples sky is the limit.   Acknowledgements The data has been crawled from the https//sofifa.com website.  Inspiration Several insights and correlations between player value wage age and performance can be derived from the dataset. Furthermore how do the players in this dataset compare against themselves in last year's dataset?  Contributing Changes and Improvement suggestions are welcome. Feel free to comment new additions that you think are useful or drop a PR on the github project.,CSV,,"[popular culture, video games, association football]",CC4,,,5308,45419,15,"17k+ players, 70+ attributes extracted from the latest edition of FIFA",FIFA 18 Complete Player Dataset,https://www.kaggle.com/thec03u5/fifa-18-demo-player-dataset,Mon Oct 30 2017
,Karolina Wullum,"[Geographic Area, City, Median Income]","[string, string, numeric]","The 2014 killing of Michael Brown in Ferguson Missouri began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide.  Since Jan. 1 2015 The Washington Post has been compiling a database of every fatal shooting in the US by a police officer in the line of duty.  It's difficult to find reliable data from before this period as police killings haven't been comprehensively documented and the statistics on police brutality are much less available. As a result a vast number of cases go unreported. The Washington Post is tracking more than a dozen details about each killing - including the race age and gender of the deceased whether the person was armed and whether the victim was experiencing a mental-health crisis. They have gathered this information from law enforcement websites local new reports social media and by monitoring independent databases such as ""Killed by police"" and ""Fatal Encounters"". The Post has also conducted additional reporting in many cases. There are four additional datasets. These are US census data on poverty rate high school graduation rate median household income and racial demographics.  Source of census data https//factfinder.census.gov/faces/nav/jsf/pages/community_facts.xhtml",CSV,,"[united states, death, crime, violence]",CC4,,,1607,4826,3,Fatal police shootings in the US since 2015 with additional US census data,Fatal Police Shootings in the US,https://www.kaggle.com/kwullum/fatal-police-shootings-in-the-us,Sat Sep 23 2017
,Rohk,"[publish_time, feed_code, source_url, headline_text]","[dateTime, string, string, string]","Context This dataset is a snapshot of most of the new news content published online over one week (August 24 2017 through August 30 2017).  Prepared by Rohit Kulkarni It includes approximately 1.4 million articles with 20000 news sources and 20+ languages. This dataset has just four fields (as per the column metadata)  publish_time - earliest known time of the url appearing online in yyyyMMddHHmm format IST timezone feed_code - unique identifier for the publisher or domain source_url - url of the article headline_text - Headline of the article (UTF8 20 possible languages)  See the ""Basic Data Exploration"" notebook for a quick look at the dataset contents. Inspiration The sources include news feeds news websites government agencies tech journals company websites blogs and wikipedia updates. The data has been collected by polling RSS feeds and by crawling other large news aggregators.  This 7 day slice was selected as there wasn't any downtime or internet outage during the interval. New news content is produced at this rate by publishers everyday throughout the year. Several other News datasets exploring other attributes countries and topics can be seen on my profile. Acknowledgements This dataset is free to use with the following citation  Rohit Kulkarni (2017) One Week of Global Feeds [News CSV Dataset] doi10.7910/DVN/ILAT5B Retrieved from [this url]",CSV,,"[news agencies, historiography, linguistics, internet]",CC4,,,378,6225,280,7 days of tracking 20k news feeds worldwide,One Week of Global News Feeds,https://www.kaggle.com/therohk/global-news-week,Sat Sep 30 2017
,Chris Cross,[],[],Context Having such a task as predicting the travel time of taxis it can be insightful to have a deeper look at the underlying street network of the city. Network Analysis can enable us to get insights for why certain taxi trips take longer than others given some basic network properties. Examples for the analysis can be calculate the shortest path measure the influence of specific streets on the robustness of the network or find out which streets are key points in the network when it comes to traffic flow. Content This dataset contains one large Graph for the Street Network of New York City in GraphML format and a subgraph for the area of Manhattan for fast testing of your Analysis.  Each Graph was created with the awesome python package https//github.com/gboeing/osmnx which is not available on Kaggle. The Graphs nodes attributes are taken from OSM and contain information to which other nodes they are connected how long the connection is which speed limit it has etc. Acknowledgements https//github.com/gboeing/osmnx Inspiration Explore the New York Street Network gain a deeper understanding for network analysis and craft some useful Features for the Taxi Trip Prediction Competition!,Other,,[networks],ODbL,,,207,1771,59,Analyse the New York City Street Network!,Street Network of New York in GraphML,https://www.kaggle.com/crailtap/street-network-of-new-york-in-graphml,Thu Aug 03 2017
,Alan Du,[],[],"Context This is an aggregate of the data I studied for my thesis titled ""Data Mining in Presidential Debates and Speeches How Campaign Rhetoric Shaped Voter Opinion in the 2016 U.S. Presidential Race"". The goal of my thesis was to use NLP techniques to understand how Donald Trump’s rhetoric impacted the opinions of various voter groups throughout his campaign. Here is a summary of my findings  Trump’s words were typically more common in an American English corpus and more extreme on both ends of the sentiment spectrum Trump not only used rhetorical devices for persuasion but also adeptly coupled these devices with the right talking points based on the composition of his audience Precise execution of the above strategy garnered him an unexpectedly large number of votes from the white female and Hispanic demographics  I hope that others can use this dataset to answer questions of their own about the 2016 presidential campaign. Content Collection of data from the 2016 U.S. Presidential Election Campaign containing  Transcripts of the three presidential debates divided into separate Trump and Clinton text files Transcripts of Trump's 64 speeches delivered after the RNC and Clinton's 35 speeches delivered after the DNC Transcripts of select speeches delivered by candidates during the primary campaigns USC Dornsife/LA Times Presidential Election Poll with daily breakdown by voter groups Five Thirty Eight Election Poll containing daily data from numerous pollsters  Acknowledgements Debate and speech texts scraped from the American Presidency Project website.",Other,,"[politics, demographics, linguistics, political science]",CC0,,,90,771,2,Debate and Speech Transcripts & Voter Group Polls,2016 U.S. Presidential Campaign Texts and Polls,https://www.kaggle.com/alandu20/2016-us-presidential-campaign-texts-and-polls,Sat Dec 30 2017
,Rounak Banik,"[abilities, against_bug, against_dark, against_dragon, against_electric, against_fairy, against_fight, against_fire, against_flying, against_ghost, against_grass, against_ground, against_ice, against_normal, against_poison, against_psychic, against_rock, against_steel, against_water, attack, base_egg_steps, base_happiness, base_total, capture_rate, classfication, defense, experience_growth, height_m, hp, japanese_name, name, percentage_male, pokedex_number, sp_attack, sp_defense, speed, type1, type2, weight_kg, generation, is_legendary]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric]",Context This dataset contains information on all 802 Pokemon from all Seven Generations of Pokemon. The information contained in this dataset include Base Stats Performance against Other Types Height Weight Classification Egg Steps Experience Points Abilities etc. The information was scraped from http//serebii.net/ Content  name The English name of the Pokemon japanese_name The Original Japanese name of the Pokemon pokedex_number The entry number of the Pokemon in the National Pokedex percentage_male The percentage of the species that are male. Blank if the Pokemon is genderless. type1 The Primary Type of the Pokemon type2 The Secondary Type of the Pokemon classification The Classification of the Pokemon as described by the Sun and Moon Pokedex height_m Height of the Pokemon in metres weight_kg The Weight of the Pokemon in kilograms capture_rate Capture Rate of the Pokemon base_egg_steps The number of steps required to hatch an egg of the Pokemon abilities A stringified list of abilities that the Pokemon is capable of having experience_growth The Experience Growth of the Pokemon base_happiness Base Happiness of the Pokemon against_? Eighteen features that denote the amount of damage taken against an attack of a particular type hp The Base HP of the Pokemon attack The Base Attack of the Pokemon defense The Base Defense of the Pokemon sp_attack The Base Special Attack of the Pokemon sp_defense The Base Special Defense of the Pokemon speed The Base Speed of the Pokemon generation The numbered generation which the Pokemon was first introduced is_legendary Denotes if the Pokemon is legendary.  Acknowledgements The data was scraped from http//serebii.net/. Inspiration Pokemon holds a very special place in my heart as it is probably the only video game I have judiciously followed for more than 10 years. With this dataset I wanted to be able to answer the following questions  Is it possible to build a classifier to identify legendary Pokemon? How does height and weight of a Pokemon correlate with its various base stats? What factors influence the Experience Growth and Egg Steps? Are these quantities correlated? Which type is the strongest overall? Which is the weakest? Which type is the most likely to be a legendary Pokemon? Can you build a Pokemon dream team? A team of 6 Pokemon that inflicts the most damage while remaining relatively impervious to any other team of 6 Pokemon. ,CSV,,"[popular culture, video games]",CC0,,,1557,15649,0.1533203125,Data on more than 800 Pokemon from all 7 Generations.,The Complete Pokemon Dataset,https://www.kaggle.com/rounakbanik/pokemon,Sat Sep 30 2017
,UCI Machine Learning,"[patientId, value0, value1, value2, value3, value4, value5, value6, value7, value8, value9, value10, value11, value12, value13, value14, value15, value16, value17, value18, value19, value20, value21, value22, value23, value24, value25, value26, value27, value28, value29, value30, value31, value32, value33, value34, value35, value36, value37, value38, value39, value40, value41, value42, value43, value44, value45, value46, value47, value48, value49, value50, value51, value52, value53, value54, value55, value56, value57, value58, value59, value60, value61, value62, value63, value64, value65, value66, value67, value68, value69, value70, value71, value72, value73, value74, value75, value76, value77, value78, value79, value80, value81, value82, value83, value84, value85, value86, value87, value88, value89, value90, value91, value92, value93, value94, value95, value96, value97, value98]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset consists of 384 features extracted from CT images. The class variable is numeric and denotes the relative location of the CT slice on the axial axis of the human body. The data was retrieved from a set of 53500 CT images from 74 different  patients (43 male 31 female).   Content Each CT slice is described by two histograms in polar space. The first histogram describes the location of bone structures in the image the second the location of air inclusions inside of the body. Both histograms are concatenated to form the final feature vector. Bins that are outside of the image are marked with the value -0.25.  The class variable (relative location of an image on the axial axis) was constructed by manually annotating up to 10 different distinct landmarks in each CT Volume with known location. The location of slices in between landmarks was interpolated. Field Descriptions  patientId Each ID identifies a different patient  value[1-241] Histogram describing bone structures  value[242 - 385] Histogram describing air inclusions  386 Relative location of the image on the axial axis (class value).   Values are in the range [0; 180] where 0 denotes the top of the head and 180 the soles of the feet. Acknowledgements Original dataset was downloaded from UCI Machine learning Repository Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. Banner image acknowledgement   Self Portre on cat scan 1997 Title ""Soon I will be there"" Date 8 April 1997 Author   Sérgio Valle Duarte License CC BY 3.0 Source Wikipedia  Inspiration Predict the relative location of CT slices on the axial axis",CSV,,"[healthcare, biology, health]",CC0,,,101,2216,78,384 features extracted from CT images,CAT Scan Localization,https://www.kaggle.com/uciml/ct-slice-localization,Thu Sep 07 2017
,Neha,[],[],,CSV,,"[film, visual arts]",Other,,,0,0,26,Predicting the genre of a movie by analyzing its poster,Movie Genre from Its Poster,https://www.kaggle.com/neha1703/movie-genre-from-its-poster,Fri Jun 09 2017
,Nadin Tamer,"[id, name, artists, danceability, energy, key, loudness, mode, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration_ms, time_signature]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Top Spotify Tracks of 2017 At the end of each year Spotify compiles a playlist of the songs streamed most often over the course of that year. This year's playlist (Top Tracks of 2017) included 100 songs. The question is What do these top songs have in common? Why do people like them? Original Data Source  The audio features for each song were extracted using the Spotify Web API and the spotipy Python library. Credit goes to Spotify for calculating the audio feature values. Data Description There is one .csv file in the dataset. (featuresdf.csv) This file includes  Spotify URI for the song Name of the song Artist(s) of the song Audio features for the song (such as danceability tempo key etc.)  A more detailed explanation of the audio features can be found in the Metadata tab. Exploring the Data  Some suggestions for what to do with the data  Look for patterns in the audio features of the songs. Why do people stream these songs the most? Try to predict one audio feature based on the others See which features correlate the most ,CSV,,"[popular culture, music]",Other,,,591,3593,0.0126953125,Audio features of top Spotify songs,Top Spotify Tracks of 2017,https://www.kaggle.com/nadintamer/top-tracks-of-2017,Wed Dec 20 2017
,Chicago Police Department,"[INTERSECTION, CAMERA ID, ADDRESS, VIOLATION DATE, VIOLATIONS, X COORDINATE, Y COORDINATE, LATITUDE, LONGITUDE, LOCATION]","[string, numeric, string, dateTime, numeric, string, string, string, string, string]",Context This dataset reflects the daily volume of violations created by the City of Chicago Red Light Program for each camera. The data reflects violations that occurred from July 1 2014 until present minus the most recent 14 days. This data may change due to occasional time lags between the capturing of a potential violation and the processing and determination of a violation. The most recent 14 days are not shown due to revised data being submitted to the City of Chicago during this period. The reported violations are those that have been collected by the camera system and reviewed by two separate City contractors. In some instances due to the inability the registered owner of the offending vehicle the violation may not be issued as a citation. However this dataset contains all violations regardless of whether a citation was actually issued which provides an accurate view into the Red Light Program. Because of occasional time lags between the capturing of a potential violation and the processing and determination of a violation as well as the occasional revision of the determination of a violation this data may change.  Content More information on the Red Light Program can be found here. Data covers July 1 2014 to Sept 7 2017. Rows include  Intersection Intersection of the location of the red light enforcement camera(s). There may be more than one camera at each intersection. Camera ID A unique ID for each physical camera at an intersection which may contain more than one camera. Address The address of the physical camera (CAMERA ID). The address may be the same for all cameras or different based on the physical installation of each camera. Violation Date The date of when the violations occurred. NOTE The citation may be issued on a different date. Violations Number of violations for each camera on a particular day. X Coordinate The X Coordinate measured in feet of the location of the camera. Geocoded using Illinois State Plane East (ESRI102671). Y Coordinate The Y Coordinate measured in feet of the location of the camera. Geocoded using Illinois State Plane East (ESRI102671). Latitude The latitude of the physical location of the camera(s) based on the ADDRESS column. Geocoded using the WGS84. Longitutde The longitude of the physical location of the camera(s) based on the ADDRESS column. Geocoded using the WGS84. Location The coordinates of the camera(s) based on the LATITUDE and LONGITUDE columns. Geocoded using the WGS84.  Acknowledgements Dataset compiled by City of Chicago here.  Inspiration  Which intersections have the most violations? When do most violations occur? ,CSV,,"[government agencies, government, law]",CC0,,,122,1444,46,309k Records of Violations in Safety Zones,Chicago Red Light Violations,https://www.kaggle.com/chicagopolice/chicago-red-light-violations,Tue Sep 12 2017
,Kevin Mader,[],[],Context Competitions like LUNA (http//luna16.grand-challenge.org) and the Kaggle Data Science Bowl 2017 (https//www.kaggle.com/c/data-science-bowl-2017) involve processing and trying to find lesions in CT images of the lungs. In order to find disease in these images well it is important to first find the lungs well. This dataset is a collection of 2D and 3D images with manually segmented lungs. Challenge Come up with an algorithm for accurately segmenting lungs and measuring important clinical parameters (lung volume PD etc) Percentile Density (PD) The PD is the density (in Hounsfield units) the given percentile of pixels fall below in the image. The table includes 5 and 95% for reference. For smokers this value is often high indicating the build up of other things in the lungs.,Other,,[healthcare],Other,,,1408,8043,632,"A collection of CT images, manually segmented lungs and measurements in 2/3D",Finding and Measuring Lungs in CT Data,https://www.kaggle.com/kmader/finding-lungs-in-ct-data,Wed Apr 26 2017
,Jose Berengueres,"[screen_name, name, twitter_id, description, year, month, date, time, tweet_id, tweet]","[string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, string]","What I talk about when I talk about Catalonia This is my grain of sand to help in the Catalonia Independence crisis. The Iberian media has been a key driver to incubate disaffection between Catalonia and Spain. For example a leading newspaper tweeted boycott and Catalonia 9 times in the last month. In this dataset we analyze  tweets  topics of tweets and  sentiment differentials in news    Context The dramatic Catalonia independence crisis offers a unique opportunity to analyze bias in news reporting as opinions on the issue are quite polarized (See #catalanReferendum on twitter). In this dataset we compare how different newspapers (NYT Washington-Post Bloomberg...) have reported one singular specific event in the saga The reply of the Spanish Government to M.H. Puigdemont speech of October 11th of 2017.  For each of the 30 newspapers considered the most popular news article that reported this news is represented as a row in the dataset.  Why this news? The Spanish government published a pdf called (""requerimiento"") which was faxed to Puigdemont. The document requires that  Puigdemont reply in five days a clarification of the meaning of his speech.  This ""clean"" news offers a rare example where the news is about a written document rather than a speech or an action (usually subjected to more interpretations and biases) Content  news_...csv each row contains the news article and its translation to English. all3.csv contains 100k tweets.  Acknowledgements All the journalists who made this dataset possible. Thanks to @DataCanary for helping make the visualizations better! Inspiration I always thought that sentiment analysis was a useless topic but here there is a chance to use an objective measure to show how polarized reporting has become (even if sentiment does not account for fakenews nuances or sarcasm). The linear regressions shows that news written in Spanish language are less positive about the event than the global mean. In other words sentiment seems strongly biased by language. Bias by location of the newspapers is also analyzed.  Disclaimer Note that the 'bing' scale is used. Other scales such AFINN might yield different results.",CSV,,[journalism],CC0,,,123,2239,72,Sentiment Bias of News on Catalonia Independence Crisis,Bias Media CAT,https://www.kaggle.com/harriken/bias-media-cat,Sat Oct 28 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes 1.6 gb of stop data from North Carolina covering all of 2010 onwards. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[crime, law]",Other,,,67,740,1024,Data on Traffic and Pedestrian Stops by Police in North Carolina,Stanford Open Policing Project - North Carolina,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-north-carolina,Tue Jul 11 2017
,VincentLa,"[id, user_id, user_name, screen_name, user_statuses_count, user_favorites_count, friends_count, followers_count, user_location, user_description, user_time_zone, user_profile_text_color, user_profile_background_color, full_text, created_at, is_retweet, retweeted_status_text, retweeted_status_id, quoted_status_text, quoted_status_id, in_reply_to_screen_name, in_reply_to_status_id, in_reply_to_user_id, hashtags]","[numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, string, dateTime, string, string, string, string, numeric, string, numeric, numeric, string]","Charlottesville Virgina Charlottesville is home to a statue of Robert E. Lee which is slated to be removed. (For those unfamiliar with American history Robert E. Lee was a US Army general who defected to the Confederacy during the American Civil War and was considered to be one of their best military leaders.) While many Americans support the move believing the main purpose of the Confederacy was to defend the institution of slavery many others do not share this view. Furthermore believing Confederate symbols to be merely an expression of Southern pride many have not taken its planned removal lightly. As a result many people--including white nationalists and neo-Nazis--have descended to Charlottesville to protest its removal. This in turn attracted many counter-protestors. Tragically one of the counter-protestors--Heather Heyer--was killed and many others injured after a man intentionally rammed his car into them. In response President Trump blamed ""both sides"" for the chaos in Charlottesville leading many Americans to denounce him for what they see as a soft-handed approach to what some have called an act of ""domestic terrorism."" This dataset below captures the discussion--and copious amounts of anger--revolving around this past week's events. The Data Description This data set consists of a random sample of 50000 tweets per day (in accordance with the Twitter Developer Agreement) of tweets mentioning Charlottesville or containing ""#charlottesville"" extracted via the Twitter Streaming API starting on August 15.  The files were copied from a large Postgres database containing--currently--over 2 million tweets. Finally a table of tweet counts per timestamp was created using the whole database (not just the Kaggle sample). The data description PDF provides a full summary of the attributes found in the CSV files.  Note While the tweet timestamps are in UTC the cutoffs were based on Eastern Standard Time so the August 16 file will have timestamps ranging from 2017-08-16 40000 UTC to 2017-08-17 40000 UTC. Format The dataset is available as either separate CSV files or a single SQLite database. License I'm releasing the dataset under the CC BY-SA 4.0 license. Furthermore because this data was extracted via the Twitter Streaming API its use must abide by the Twitter Developer Agreement. Most notably the display of individual tweets should satisfy these requirements. More information can be found in the data description file or on Twitter's website. Acknowledgements Obviously I would like to thank Twitter for providing a fast and reliable streaming service. I'd also like to thank the developers of the Python programming language psycopg2 and Postgres for creating amazing software with which this data set would not exist. Image Credit The banner above is a personal modification of these images  Evan Nesterak Image Source Image License Wikipedia user Cville Dog Image Source The Associated Press Image Source  Inspiration I almost removed the header ""inspiration"" from this section because this is a rather sad and dark data set. However this is preciously why this is an important data set to analyze. Good history books have never shied away from unpleasant events and never should we. This data set provides a rich opportunity for many types of research including  Natural language processing Sentiment analysis Data visualization  Furthermore given the political nature of this dataset there are a lot of social science questions that can potentially be answered or at least piqued by this data.",CSV,,"[politics, linguistics, twitter, internet]",CC4,,,300,2884,178,A snapshot of American history in the making,#Charlottesville on Twitter,https://www.kaggle.com/vincela9/charlottesville-on-twitter,Tue Aug 22 2017
,Google News Lab,"[id, googleTopic, week_id, value]","[string, string, dateTime, numeric]",This is the data behind the Rhythm of Food visualisation by Moritz Stefaner. It shows seasonal food searches in different food types around the world since 2004. The data is indexed with 0 being the least and 100 being the highest search interest. Find out more here http//rhythm-of-food.net/,CSV,,"[journalism, food and drink, internet]",CC4,,,472,3364,4,How do we search for food? Google search interest can reveal key food trends over the years.,Food searches on Google since 2004,https://www.kaggle.com/GoogleNewsLab/food-searches-on-google-since-2004,Wed Nov 01 2017
,Thought Vector,"[tweet_id, author_id, inbound, created_at, text, response_tweet_id, in_response_to_tweet_id]","[numeric, string, boolean, string, string, string, string]","The Customer Support on Twitter dataset is a large modern corpus of tweets and replies to aid innovation in natural language understanding and conversational models and for study of modern customer support practices and impact.  Context Natural language remains the densest encoding of human experience we have and innovation in NLP has accelerated to power understanding of that data but the datasets driving this innovation don't match the real language in use today.  The Customer Support on Twitter dataset offers a large corpus of modern English (mostly) conversations between consumers and customer support agents on Twitter and has three important advantages over other conversational text datasets  Focused - Consumers contact customer support to have a specific problem solved and the manifold of problems to be discussed is relatively small especially compared to unconstrained conversational datasets like the reddit Corpus. Natural - Consumers in this dataset come from a much broader segment than those in the Ubuntu Dialogue Corpus and have much more natural and recent use of typed text than the Cornell Movie Dialogs Corpus. Succinct - Twitter's brevity causes more natural responses from support agents (rather than scripted) and to-the-point descriptions of problems and solutions.  Also its convenient in allowing for a relatively low message limit size for recurrent nets.  Inspiration The size and breadth of this dataset inspires many interesting questions  Can we predict company responses? Given the bounded set of subjects handled by each company the answer seems like yes! Do requests get stale?  How quickly do the best companies respond compared to the worst? Can we learn high quality dense embeddings or representations of similarity for topical clustering? How does tone affect the customer support conversation?  Does saying sorry help? Can we help companies identify new problems or ones most affecting their customers?  Content The dataset is a CSV where each row is a tweet.  The different columns are described below.  Every conversation included has at least one request from a consumer and at least one response from a company.  Which user IDs are company user IDs can be calculated using the inbound field. tweet_id A unique anonymized ID for the Tweet.  Referenced by response_tweet_id and in_response_to_tweet_id. author_id A unique anonymized user ID.  @s in the dataset have been replaced with their associated anonymized user ID. inbound Whether the tweet is ""inbound"" to a company doing customer support on Twitter.  This feature is useful when re-organizing data for training conversational models. created_at Date and time when the tweet was sent. text Tweet content.  Sensitive information like phone numbers and email addresses are replaced with mask values like __email__. response_tweet_id IDs of tweets that are responses to this tweet comma-separated. in_response_to_tweet_id ID of the tweet this tweet is in response to if any. Contributing Know of other brands the dataset should include?  Found something that needs to be fixed?  Start a discussion or email me directly at $FIRSTNAME@$LASTNAME.com! Acknowledgements A huge thank you to my friends who helped bootstrap the list of companies that  do customer support on Twitter!  There are many rocks that would have been left un-turned were it not for your suggestions! Relevant Resources  NLTK - casual_tokenize for social media text tokenizing vader sentiment analysis for social media text SciKit Learn - BoW Count Vectorizer Multinomial Naive Bayes Classifier Topic Modeling via Phrase detection with gensim facebook research - fastText text classifier ",Other,,"[business, communication, linguistics, twitter]",CC4,,,713,10758,167,Over 3 million tweets and replies from the biggest brands on Twitter,Customer Support on Twitter,https://www.kaggle.com/thoughtvector/customer-support-on-twitter,Mon Dec 04 2017
,Documenting the American South (DocSouth),[],[],"""First-Person Narratives of the American South"" is a collection of diaries autobiographies memoirs travel accounts and ex-slave narratives written by Southerners. The majority of materials in this collection are written by those Southerners whose voices were less prominent in their time including African Americans women enlisted men laborers and Native Americans. The narratives available in this collection offer personal accounts of Southern life between 1860 and 1920 a period of enormous change. At the end of the Civil War the South faced the enormous challenge of re-creating their society after their land had been ravaged by war many of their men were dead or injured and the economic and social system of slavery had been abolished. Many farmers confronted by periodic depressions and market turmoil joined political and social protest movements. For African Americans the end of slavery brought hope for unprecedented control of their own lives but whether they stayed in the South or moved north or west they continued to face social and political oppression. Most African Americans in the South were pulled into a Darwinistic sharecropper system and saw their lives circumscribed by the rise of segregation. As conservative views faced a growing challenge from Modernist thought Southern arts sciences and religion also reflected the considerable tensions manifested throughout Southern society. Admidst these dramatic changes Southerners who had lived in the antebellum South and soldiers who had fought for the Confederacy wrote memoirs that and strived to preserve a memory of many different experiences. Southerners recorded their stories of these tumultuous times in print and in diaries and letters but few first-person narratives other than those written by the social and economic elite found their way into the national print culture. In this online collection accounts of life on the farm or in the servants' quarters or in the cotton mill have priority over accounts of public lives and leading military battles. Each narrative offers a unique perspective on life in the South and serves as an important primary resource for the study of the American South. The original texts for ""First-Person Narratives of the American South"" come from the University Library of the University of North Carolina at Chapel Hill which includes the Southern Historical Collection one of the largest collections of Southern manuscripts in the country and the North Carolina Collection the most complete printed documentation of a single state anywhere. The DocSouth Editorial Board composed of faculty and librarians at UNC and staff from the UNC Press oversees this collection and all other collections on Documenting the American South. Context The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world. The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives. More information about the collection and access to individual page images can be be found here http//docsouth.unc.edu/neh The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives. The .csv file acts as a table of contents for the collection and includes Title Author Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant http//voyant-tools.org/).  Copyright Statement and Acknowledgements With the exception of ""Fields's Observation The Slave Narrative of a Nineteenth-Century Virginian"" which has no known rights the texts encoding and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0http//creativecommons.org/licenses/by/4.0/). Users are free to copy share adapt and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available. If you make use of this data considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu. About the DocSouth Data Project Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools. Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured corrected and machine readable text. Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http//voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling named-entity recognition or sentiment analysis.",CSV,,"[united states, history, linguistics]",Other,,,56,577,43,Personal accounts of Southern life between 1860 and 1920,First Person Narratives of the American South,https://www.kaggle.com/docsouth-data/first-person-narratives-of-the-american-south,Tue Aug 15 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes over 1 gb of stop data from Florida. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[government agencies, crime, law]",Other,,,90,949,1008,Data on Traffic and Pedestrian Stops by Police in Florida,Stanford Open Policing Project - Florida,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-florida,Fri Jul 21 2017
,Kevin Mader,[],[],Overview The dataset contains a number of different subsets of the full food-101 data. The idea is to make a more exciting simple training set for image analysis than CIFAR10 or MNIST. For this reason the data includes massively downscaled versions of the images to enable quick tests. The data has been reformatted as HDF5 and specifically Keras HDF5Matrix which allows them to be easily read in. The file names indicate the contents of the file. For example  food_c101_n1000_r384x384x3.h5 means there are 101 categories represented with n=1000 images that have a resolution of 384x384x3 (RGB uint8) food_test_c101_n1000_r32x32x1.h5 means the data is part of the validation set has 101 categories represented with n=1000 images that have a resolution of 32x32x1 (float32 from -1 to 1)  Challenge The first goal is to be able to automatically classify an unknown image using the dataset but beyond this there are a number of possibilities for looking at what regions / image components are important for making classifications identify new types of food as combinations of existing tags build object detectors which can find similar objects in a full scene. Data Acknowledgement The data was repackaged from the original source (gzip) available at https//www.vision.ee.ethz.ch/datasets_extra/food-101/ License  The Food-101 data set consists of images from Foodspotting [1]. Any use beyond scientific fair use must be negotiated with the respective picture owners according to the Foodspotting terms of use [2].  [1] http//www.foodspotting.com/ [2] http//www.foodspotting.com/terms/,Other,,"[food and drink, popular culture, image data, multiclass classification]",Other,,,1566,10155,663,Labeled food images in 101 categories from apple pies to waffles,Food Images (Food-101),https://www.kaggle.com/kmader/food41,Tue Aug 08 2017
,Sohier Dane,[],[],This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output. The overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of wind power production in the latest decades. For this reason the hourly wind power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the wind installed capacity. Thus the installed capacity considered is fixed as the one installed at the end of 2015. For this reason data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.  Content  The data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units. Please see the manual for the technical details of how these estimates were generated.  This product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.  Acknowledgements This dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here. Inspiration  How clean is the dataset? What does a typical year look like? One common approach is to stitch together 12 months of raw data using the 12 most typical months per this ISO standard. Can you identify more useful geographical areas for this sort of analysis such as valleys that would share similar wind patterns?  If you like If you like this dataset you might also enjoy -  30 years of European solar - Google's Project Sunroof data,CSV,,[energy],CC0,,,312,2365,710,Hourly energy potential for 1986-2015,30 Years of European Wind Generation,https://www.kaggle.com/sohier/30-years-of-european-wind-generation,Fri Sep 15 2017
,Dor Oppenheim,"[Area Abbreviation, Area Code, Area, Item Code, Item, Element Code, Element, Unit, latitude, longitude, Y1961, Y1962, Y1963, Y1964, Y1965, Y1966, Y1967, Y1968, Y1969, Y1970, Y1971, Y1972, Y1973, Y1974, Y1975, Y1976, Y1977, Y1978, Y1979, Y1980, Y1981, Y1982, Y1983, Y1984, Y1985, Y1986, Y1987, Y1988, Y1989, Y1990, Y1991, Y1992, Y1993, Y1994, Y1995, Y1996, Y1997, Y1998, Y1999, Y2000, Y2001, Y2002, Y2003, Y2004, Y2005, Y2006, Y2007, Y2008, Y2009, Y2010, Y2011, Y2012, Y2013]","[string, numeric, string, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Our world population is expected to grow from 7.3 billion today to 9.7 billion in the year 2050. Finding solutions for feeding the growing world population has become a hot topic for food and agriculture organizations entrepreneurs and philanthropists. These solutions range from changing the way we grow our food to changing the way we eat. To make things harder the world's climate is changing and it is both affecting and affected by the way we grow our food – agriculture.  This dataset provides an insight on our worldwide food production - focusing on a comparison between food produced for human consumption and feed produced for animals. Content The Food and Agriculture Organization of the United Nations provides free access to food and agriculture data for over 245 countries and territories from the year 1961 to the most recent update (depends on the dataset). One dataset from the FAO's database is the Food Balance Sheets. It presents a comprehensive picture of the pattern of a country's food supply during a specified reference period the last time an update was loaded to the FAO database was in 2013. The food balance sheet shows for each food item the sources of supply and its utilization. This chunk of the dataset is focused on two utilizations of each food item available  Food - refers to the total amount of the food item available as human food during the reference period. Feed - refers to the quantity of the food item available for feeding to the livestock and poultry during the reference period.  Dataset's attributes  Area code - Country name abbreviation Area - County name Item - Food item Element - Food or Feed Latitude - geographic coordinate that specifies the north–south position of a point on the Earth's surface Longitude - geographic coordinate that specifies the east-west position of a point on the Earth's surface Production per year - Amount of food item produced in 1000 tonnes  Acknowledgements This dataset was meticulously gathered organized and published by the Food and Agriculture Organization of the United Nations.  Inspiration Animal agriculture and factory farming is a a growing interest of the public and of world leaders.  Can you find interesting outliers in the data? What are the fastest growing countries in terms of food production\consumption? Compare between food and feed consumption. ,CSV,,"[animals, agriculture]",CC0,,,639,4204,0.853515625,"Worldwide food\feed production and distribution, 1961-2013 ",Who eats the food we grow?,https://www.kaggle.com/dorbicycle/world-foodfeed-production,Thu Nov 30 2017
,Liling Tan,[],[],"Context The Daikon Corpus was created during the Diachronic Text Evaluation task in SemEval-2015. The task was to create a system that can date a piece of text.  For example given a text snippet  “Dictator Saddam Hussein ordered his troops to march into Kuwait.   After the invasion is condemned by the UN Security Council the US has   forged a coalition with allies. Today American troops are sent to   Saudi Arabia in Operation Desert Shield protecting Saudi Arabia from   possible attack.”  The text has clear temporal evidence with reference to a  historical figure (“Saddam Hussein”)  notable organization (“UN Security Council”)  factual event (“Operation Desert Shield”).  Historically we know that   Saddam Hussein lived between 1937 to 2006  UN Security Council has existed since 1946  Operation Desert Shield (i.e. the Gulf War) occurred between 1990-1991  Given the specific chronic deicticity (“today”) that indicates that the text is published during the Gulf War we can conceive that the text snippet should be dated 1990-1991. Content The Daikon Corpus is made up of articles from the British Spectator news magazine from year 828 to 2008. The corpus contains 24280 articles with 19 million tokens; the token count is calculated by summing the number of whitespaces plus 1 for each paragraph. The Daikon corpus is saved in the JSON format where the outer most-structure is a list and the inner data structure is a key-value dictionary/hashmap that contains the  url URL where the original article resides date Date of the article body A list of paragraphs title Title of the text   Note If the url is broken try removing the .html suffix of the url. e.g. change  http//archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house.html   to  http//archive.spectator.co.uk/article/24th-september-2005/57/doctor-in-the-house  Citations Liling Tan and Noam Ordan. 2015.  USAAR-CHRONOS  Crawling the Web for Temporal Annotations.  In Proceedings of Ninth International Workshop on  Semantic Evaluation (SemEval 2015). Denver USA.  Task reference Octavian Popescu and Carlo Strapparava.  SemEval 2015 Task 7 Diachronic Text Evaluation.  In Proceedings of Ninth International Workshop on  Semantic Evaluation (SemEval 2015). Denver USA.  Dataset image comes from Jonathan Pielmayer Inspiration Let's make an artificially intelligent ""Flynn Carsen"" !!",{}JSON,,[],CC0,,,22,551,113,Historic texts from the British Spector news magazine,Daikon (Diachronic Corpus),https://www.kaggle.com/alvations/daikon,Thu Aug 17 2017
,Rob Harrand,[],[],Context Data downloaded from the EMPRES Global Animal Disease Information System. Content Data shows the when where and what of animal disease outbreaks from the last 2 years including African swine fever Foot and mouth disease and bird-flu. Numbers of cases deaths etc are also included. Acknowledgements This data is from the Food and Agriculture Organization of the United Nations. The EMPRES-i system can be access here Read more about the details of the system here,CSV,,"[animals, medicine]",ODbL,,,155,935,3,Global animal disease outbreaks from the last 2 years,EMPRES Global Animal Disease Surveillance,https://www.kaggle.com/tentotheminus9/empres-global-animal-disease-surveillance,Thu Aug 24 2017
,Rachael Tatman,[],[],"Context Fraudulent e-mails contain criminally deceptive information usually with the intent of convincing the recipient to give the sender a large amount of money. Perhaps the best known type of fraudulent e-mails is the Nigerian Letter or “419” Fraud. Content This dataset is a collection of more than 2500 ""Nigerian"" Fraud Letters dating from 1998 to 2007.  These emails are in a single text file. Each e-mail has a header which includes the following information   Return-Path address the email was sent from X-Sieve the X-Sieve host (always cmu-sieve 2.0) Message-Id a unique identifier for each message From the message sender (sometimes blank) Reply-To the email address to which replies will be sent To the email address to which the e-mail was originally set (some are truncated for anonymity) Date Date e-mail was sent Subject Subject line of e-mail X-Mailer The platform the e-mail was sent from MIME-Version The Multipurpose Internet Mail Extension version Content-Type type of content & character encoding Content-Transfer-Encoding encoding in bits X-MIME-Autoconverted the type of autoconversion done Status r (read) and o (opened)  Acknowledgements If you use this collection of fraud email in your research please include the following citation in any resulting papers  Radev D. (2008) CLAIR collection of fraud email ACL Data and Code Repository ADCR2008T001 http//aclweb.org/aclwiki  Inspiration  This dataset contains fraudulent e-mails sent over a period of years. Has the language used in fraudulent E-mails changed over time?  Are there any words or phrases that are particularly common in this type of e-mail? (You might compare it with the Enron email corpus linked below)  Related datasets  https//www.kaggle.com/wcukierski/enron-email-dataset https//www.kaggle.com/uciml/sms-spam-collection-dataset ",Other,,"[crime, linguistics, internet]",CC4,,,560,4869,17,"CLAIR collection of ""Nigerian"" fraud emails",Fraudulent E-mail Corpus,https://www.kaggle.com/rtatman/fraudulent-email-corpus,Wed Jul 26 2017
,Joerg Simon Wicker,"[ row ,  label ,  sub-label ]","[numeric, string, string]",Context While the physiological response of humans to emotional events or stimuli is well-investigated for many modalities (like EEG skin resistance ...) surprisingly little is known about the exhalation of so-called Volatile Organic Compounds (VOCs) at quite low concentrations in response to such stimuli. VOCs are molecules of relatively small mass that quickly evaporate or sublimate and can be detected in the air that surrounds us. The project introduces a new field of application for data mining where trace gas responses of people reacting on-line to films shown in cinemas (or movie theaters) are related to the semantic content of the films themselves. To do so we measured the VOCs from a movie theater over a whole month in intervals of thirty seconds and annotated the screened films by a controlled vocabulary compiled from multiple sources.  Content The data set consists of two parts first the measured VOCs and second the information about the movies. The VOCs are given in the file TOF_CO2_data_30sec.arff which is simply the time of the measurement in the first column then all measured 400+ VOCs in the other columns. Roughly one measurement was carried out every 30 seconds. The information which movies were shown is given in the file screenings.csv. It gives start time end time movie title and how many visitors were in the screening. Additionally the folder labels_aggregated give a consensus labelling of multiple annotators for the movies. The labels describe the scenes each label represented by a row then each column showing if the label is active (1) or not (0). This is available for 6 movies in the data set.  The goal of our initial analysis was the identification of markers that is finding certain VOCs that have a relation to certain labels and therefore emotions. For example given the scene label blood is there any increase or decrease in the concentration of a specific VOC? Further information is available in our publications https//doi.org/10.1145/2783258.2783404 and https//doi.org/10.1038/srep25464 Acknowledgements If you use this data set please cite  Jörg Wicker Nicolas Krauter Bettina Derstorff Christof Stönner   Efstratios Bourtsoukidis Thomas Klüpfel Jonathan Williams and   Stefan Kramer. 2015. Cinema Data Mining The Smell of Fear. In   Proceedings of the 21th ACM SIGKDD International Conference on   Knowledge Discovery and Data Mining (KDD '15). ACM New York NY USA   1295-1304. DOI https//doi.org/10.1145/2783258.2783404  Inspiration While the first analysis gave already interesting results we believe that this data set has a high potential for further analysis. We are currently working on increasing the size of the data. Additionally multiple follow-up publications are being prepared. There are many posssible tasks we focus mainly on the identification of markers in the VOC data but there are many potential interesting findings in the data set. Are movies related based on the VOCs? Could we identify similar scenes based on the VOCs? ,CSV,,"[film, time series]",CC4,,,105,1847,94,Identification of markers for human emotions in breath,The Smell of Fear,https://www.kaggle.com/jswicker/the-smell-of-fear,Tue Nov 28 2017
,Max Horowitz,"[Date, GameID, Drive, qtr, down, time, TimeUnder, TimeSecs, PlayTimeDiff, SideofField, yrdln, yrdline100, ydstogo, ydsnet, GoalToGo, FirstDown, posteam, DefensiveTeam, desc, PlayAttempted, Yards.Gained, sp, Touchdown, ExPointResult, TwoPointConv, DefTwoPoint, Safety, Onsidekick, PuntResult, PlayType, Passer, Passer_ID, PassAttempt, PassOutcome, PassLength, AirYards, YardsAfterCatch, QBHit, PassLocation, InterceptionThrown, Interceptor, Rusher, Rusher_ID, RushAttempt, RunLocation, RunGap, Receiver, Receiver_ID, Reception, ReturnResult, Returner, BlockingPlayer, Tackler1, Tackler2, FieldGoalResult, FieldGoalDistance, Fumble, RecFumbTeam, RecFumbPlayer, Sack, Challenge.Replay, ChalReplayResult, Accepted.Penalty, PenalizedTeam, PenaltyType, PenalizedPlayer, Penalty.Yards, PosTeamScore, DefTeamScore, ScoreDiff, AbsScoreDiff, HomeTeam, AwayTeam, Timeout_Indicator, Timeout_Team, posteam_timeouts_pre, HomeTimeouts_Remaining_Pre, AwayTimeouts_Remaining_Pre, HomeTimeouts_Remaining_Post, AwayTimeouts_Remaining_Post, No_Score_Prob, Opp_Field_Goal_Prob, Opp_Safety_Prob, Opp_Touchdown_Prob, Field_Goal_Prob, Safety_Prob, Touchdown_Prob, ExPoint_Prob, TwoPoint_Prob, ExpPts, EPA, airEPA, yacEPA, Home_WP_pre, Away_WP_pre, Home_WP_post, Away_WP_post, Win_Prob, WPA, airWPA]","[dateTime, numeric, numeric, numeric, string, dateTime, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric, string, string, numeric, numeric, numeric, string, numeric, string, string, string, numeric, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, string, numeric, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string]",Introduction The lack of publicly available National Football League (NFL) data sources has been a major obstacle in the creation of modern reproducible research in football analytics.  While clean play-by-play data is available via open-source software packages in other sports (e.g. nhlscrapr for hockey; PitchF/x data in baseball; the Basketball Reference for basketball) the equivalent datasets are not freely available for researchers interested in the statistical analysis of the NFL.  To solve this issue a group of Carnegie Mellon University statistical researchers including Maksim Horowitz Ron Yurko and Sam Ventura built and released nflscrapR an R package which uses an API maintained by the NFL to scrape clean parse and output clean datasets at the individual play player game and season levels.  Using the data outputted by the package the trio went on to develop reproducible methods for building expected point and win probability models for the NFL. The outputs of these models are included in this dataset and can be accessed using the nflscrapR package. Content The dataset made available on Kaggle contains all the regular season plays from the 2009-2016 NFL seasons. The dataset has 356768 rows and 100 columns. Each play is broken down into great detail containing information on game situation players involved results and advanced metrics such as expected point and win probability values. Detailed information about the dataset can be found at the following web page along with more NFL data https//github.com/ryurko/nflscrapR-data. Acknowledgements This dataset was compiled by Ron Yurko Sam Ventura and myself. Special shout-out to Ron for improving our current expected points and win probability models and compiling this dataset. All three of us are proud founders of the Carnegie Mellon Sports Analytics Club. Inspiration This dataset is meant to both grow and bring together the community of sports analytics by providing clean and easily accessible NFL data that has never been availabe on this scale for free.,CSV,,"[american football, sports]",Other,,,1153,11564,67,nflscrapR generated NFL dataset wiith expected points and win probability,Detailed NFL Play-by-Play Data 2009-2016,https://www.kaggle.com/maxhorowitz/nflplaybyplay2009to2016,Wed Jan 10 2018
,PyTorch,[],[],ResNet-101  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,7,475,159,ResNet-101 Pre-trained Model for PyTorch,ResNet-101,https://www.kaggle.com/pytorch/resnet101,Thu Dec 14 2017
,Jason Liu,"[Hotel_Address, Additional_Number_of_Scoring, Review_Date, Average_Score, Hotel_Name, Reviewer_Nationality, Negative_Review, Review_Total_Negative_Word_Counts, Total_Number_of_Reviews, Positive_Review, Review_Total_Positive_Word_Counts, Total_Number_of_Reviews_Reviewer_Has_Given, Reviewer_Score, Tags, days_since_review, lat, lng]","[string, numeric, dateTime, numeric, string, string, string, numeric, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric]",Acknowledgements The data was scraped from Booking.com. All data in the file is publicly available to everyone already. Data is originally owned by Booking.com. Please contact me through my profile if you want to use this dataset somewhere else. Data Context This dataset contains 515000 customer reviews and scoring of 1493 luxury hotels across Europe. Meanwhile the geographical location of hotels are also provided for further analysis. Data Content The csv file contains 17 fields. The description of each field is as below  Hotel_Address Address of hotel.  Review_Date Date when reviewer posted the corresponding review. Average_Score Average Score of the hotel calculated based on the latest comment in the last year. Hotel_Name Name of Hotel Reviewer_Nationality Nationality of Reviewer Negative_Review Negative Review the reviewer gave to the hotel. If the reviewer does not give the negative review then it should be 'No Negative' Review_Total_Negative_Word_Counts Total number of words in the negative review. Positive_Review Positive Review the reviewer gave to the hotel. If the reviewer does not give the negative review then it should be 'No Positive' Review_Total_Positive_Word_Counts Total number of words in the positive review. Reviewer_Score Score the reviewer has given to the hotel based on his/her experience Total_Number_of_Reviews_Reviewer_Has_Given Number of Reviews the reviewers has given in the past. Total_Number_of_Reviews Total number of valid reviews the hotel has. Tags Tags reviewer gave the hotel. days_since_review Duration between the review date and scrape date. Additional_Number_of_Scoring There are also some guests who just made a scoring on the service rather than a review. This number indicates how many valid scores without review in there. lat Latitude of the hotel lng longtitude of the hotel  In order to keep the text data clean I removed unicode and punctuation in the text data and transform text into lower case. No other preprocessing was performed. Inspiration The dataset is large and informative I believe you can have a lot of fun with it! Let me put some ideas below to futher inspire kagglers!  Fit a regression model on reviews and score to see which words are more indicative to a higher/lower score Perform a sentiment analysis on the reviews Find correlation between reviewer's nationality and scores. Beautiful and informative visualization on the dataset. Clustering hotels based on reviews Simple recommendation engine to the guest who is fond of a special characteristic of hotel.  The idea is unlimited! Please have a look into data generate some ideas and leave a master kernel here! I am ready to upvote your ideas and kernels! Cheers!,CSV,,"[life, hotels]",CC0,,,1322,9840,227,Can you make your trip more cozy by using data science?,515K Hotel Reviews Data in Europe,https://www.kaggle.com/jiashenliu/515k-hotel-reviews-data-in-europe,Tue Aug 22 2017
,Google News Lab,"[dma, geoCode, 2004+cancer, 2004+cardiovascular, 2004+stroke, 2004+depression, 2004+rehab, 2004+vaccine, 2004+diarrhea, 2004+obesity, 2004+diabetes, 2005+cancer, 2005+cardiovascular, 2005+stroke, 2005+depression, 2005+rehab, 2005+vaccine, 2005+diarrhea, 2005+obesity, 2005+diabetes, 2006+cancer, 2006+cardiovascular, 2006+stroke, 2006+depression, 2006+rehab, 2006+vaccine, 2006+diarrhea, 2006+obesity, 2006+diabetes, 2007+cancer, 2007+cardiovascular, 2007+stroke, 2007+depression, 2007+rehab, 2007+vaccine, 2007+diarrhea, 2007+obesity, 2007+diabetes, 2008+cancer, 2008+cardiovascular, 2008+stroke, 2008+depression, 2008+rehab, 2008+vaccine, 2008+diarrhea, 2008+obesity, 2008+diabetes, 2009+cancer, 2009+cardiovascular, 2009+stroke, 2009+depression, 2009+rehab, 2009+vaccine, 2009+diarrhea, 2009+obesity, 2009+diabetes, 2010+cancer, 2010+cardiovascular, 2010+stroke, 2010+depression, 2010+rehab, 2010+vaccine, 2010+diarrhea, 2010+obesity, 2010+diabetes, 2011+cancer, 2011+cardiovascular, 2011+stroke, 2011+depression, 2011+rehab, 2011+vaccine, 2011+diarrhea, 2011+obesity, 2011+diabetes, 2012+cancer, 2012+cardiovascular, 2012+stroke, 2012+depression, 2012+rehab, 2012+vaccine, 2012+diarrhea, 2012+obesity, 2012+diabetes, 2013+cancer, 2013+cardiovascular, 2013+stroke, 2013+depression, 2013+rehab, 2013+vaccine, 2013+diarrhea, 2013+obesity, 2013+diabetes, 2014+cancer, 2014+cardiovascular, 2014+stroke, 2014+depression, 2014+rehab, 2014+vaccine, 2014+diarrhea, 2014+obesity]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This is the Google Search interest data that powers the Visualisation Searching For Health. Google Trends data allows us to see what people are searching for at a very local level. This visualization tracks the top searches for common health issues in the United States from Cancer to Diabetes and compares them with the actual location of occurrences for those same health conditions to understand how search data reflects life for millions of Americans. How does search interest for top health issues change over time? From 2004–2017 the data shows that search interest gradually increased over the past few years. Certain regions show a more significant increase in search interest than others. The increase in search activity is greatest in the Midwest and Northeast while the changes are noticeably less dramatic in California Texas and Idaho. Are people generally becoming more aware of health conditions and health risks? The search interest data was collected using the Google Trends API. The visualisation also brings in incidences of each condition so they can be compared. The health conditions were hand-selected from the Community Health Status Indicators (CHSI) which provides key indicators for local communities in the United States. The CHSI dataset includes more than 200 measures for each of the 3141 United States counties. More information about the CHSI can be found on healthdata.gov.  Many striking similarities exist between searches and actual conditions—but the relationship between the Obesity and Diabetes maps stands out the most. “There are many risk factors for type 2 diabetes such as age race pregnancy stress certain medications genetics or family history high cholesterol and obesity. However the single best predictor of type 2 diabetes is overweight or obesity. Almost 90% of people living with type 2 diabetes are overweight or have obesity. People who are overweight or have obesity have added pressure on their body's ability to use insulin to properly control blood sugar levels and are therefore more likely to develop diabetes.”  —Obesity Society via obesity.org,CSV,,"[journalism, healthcare, diseases]",CC4,,,475,3047,0.08203125,Data from Google trends showing who searches for what and where,"Health searches by US Metropolitan Area, 2005-2017",https://www.kaggle.com/GoogleNewsLab/health-searches-us-county,Sat Nov 04 2017
,NYPD,"[UNIQUE KEY, DATE, TIME, BOROUGH, ZIP CODE, LATITUDE, LONGITUDE, LOCATION, ON STREET NAME, CROSS STREET NAME, OFF STREET NAME, PERSONS INJURED, PERSONS KILLED, PEDESTRIANS INJURED, PEDESTRIANS KILLED, CYCLISTS INJURED, CYCLISTS KILLED, MOTORISTS INJURED, MOTORISTS KILLED, VEHICLE 1 TYPE, VEHICLE 2 TYPE, VEHICLE 3 TYPE, VEHICLE 4 TYPE, VEHICLE 5 TYPE, VEHICLE 1 FACTOR, VEHICLE 2 FACTOR, VEHICLE 3 FACTOR, VEHICLE 4 FACTOR, VEHICLE 5 FACTOR]","[numeric, dateTime, dateTime, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string]",Content The motor vehicle collision database includes the date and time location (as borough street names zip code and latitude and longitude coordinates) injuries and fatalities vehicle number and types and related factors for all 65500 collisions in New York City during 2015 and 2016. Acknowledgements The vehicle collision data was collected by the NYPD and published by NYC OpenData.,CSV,,"[walking, road transport]",CC0,,,1371,12151,85,Where are the most pedestrians struck by vehicles in New York City?,"Vehicle Collisions in NYC, 2015-Present",https://www.kaggle.com/nypd/vehicle-collisions,Thu Mar 09 2017
,Zielak,"[Timestamp, Open, High, Low, Close, Volume_(BTC), Volume_(Currency), Weighted_Price]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Bitcoin is the longest running and most well known cryptocurrency first released as open source in 2009 by the anonymous Satoshi Nakamoto. Bitcoin serves as a decentralized medium of digital exchange with transactions verified and recorded in a public distributed ledger (the blockchain) without the need for a trusted record keeping authority or central intermediary. Transaction blocks contain a SHA-256 cryptographic hash of previous transaction blocks and are thus ""chained"" together serving as an immutable record of all transactions that have ever occurred. As with any currency/commodity on the market bitcoin trading and financial instruments soon followed public adoption of bitcoin and continue to grow. Included here is historical bitcoin market data at 1-min intervals for select bitcoin exchanges where trading takes place. Happy (data) mining!  Content coincheckJPY_1-min_data_2014-10-31_to_2018-01-08.csv bitflyerJPY_1-min_data_2017-07-04_to_2018-01-08.csv coinbaseUSD_1-min_data_2014-12-01_to_2018-01-08.csv bitstampUSD_1-min_data_2012-01-01_to_2018-01-08.csv  CSV files for select bitcoin exchanges for the time period of Jan 2012 to Jan 2018 with minute to minute updates of OHLC (Open High Low Close) Volume in BTC and indicated currency and weighted bitcoin price.  Timestamps are in Unix time.  Timestamps without any trades or activity have their data fields populated with NaNs. If a timestamp is missing or if there are jumps this may be because the exchange (or its API) was down the exchange (or its API) did not exist or some other unforseen technical error in data reporting or gathering. All effort has been made to deduplicate entries and verify the contents are correct and complete to the best of my ability but obviously trust at your own risk.  Acknowledgements and Inspiration The various exchange APIs for making it difficult or unintuitive enough to get OHLC and volume data at 1-min intervals that I set out on this data scraping project. Satoshi Nakamoto and the novel core concept of the blockchain as well as its first execution via the bitcoin protocol. I'd also like to thank viewers like you! Can't wait to see what code or insights you all have to share.  I am a lowly Ph.D. student who did this for fun in my meager spare time. If you find this data interesting and you can spare a coffee to fuel my science send it my way and I'd be immensely grateful! 1kmWmcQa8qN9ZrdGfdkw8EHKBgugKBRcF",CSV,,"[history, finance]",CC4,,,17046,147433,119,"Bitcoin data at 1-min intervals from select exchanges, Jan 2012 to Jan 2018",Bitcoin Historical Data,https://www.kaggle.com/mczielinski/bitcoin-historical-data,Wed Jan 10 2018
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, numeric, string, string, string, string, string, numeric, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one datafile for each state in the U.S. West region. States included in this dataset  Alaska - ak.csv Arizona - az.csv California - ca.csv Colorado - co.csv Hawaii - hi.csv Idaho - id.csv Montana - mt.csv New Mexico - nm.csv Nevada - nv.csv Oregon - or.csv Utah - ut.csv Washington - wa.csv Wyoming - wy.csv  Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets for crime or weather,CSV,,[],Other,,,139,857,2048,Addresses and geo-locations for the U.S. West,OpenAddresses - U.S. West,https://www.kaggle.com/openaddresses/openaddresses-us-west,Thu Aug 03 2017
,City of New York,"[Borough, Block, Lot, CD, CT2010, CB2010, SchoolDist, Council, ZipCode, FireComp, PolicePrct, HealthArea, SanitBoro, SanitDistrict, SanitSub, Address, ZoneDist1, ZoneDist2, ZoneDist3, ZoneDist4, Overlay1, Overlay2, SPDist1, SPDist2, SPDist3, LtdHeight, SplitZone, BldgClass, LandUse, Easements, OwnerType, OwnerName, LotArea, BldgArea, ComArea, ResArea, OfficeArea, RetailArea, GarageArea, StrgeArea, FactryArea, OtherArea, AreaSource, NumBldgs, NumFloors, UnitsRes, UnitsTotal, LotFront, LotDepth, BldgFront, BldgDepth, Ext, ProxCode, IrrLotCode, LotType, BsmtCode, AssessLand, AssessTot, ExemptLand, ExemptTot, YearBuilt, YearAlter1, YearAlter2, HistDist, Landmark, BuiltFAR, ResidFAR, CommFAR, FacilFAR, BoroCode, BBL, CondoNo, Tract2010, XCoord, YCoord, ZoneMap, ZMCode, Sanborn, TaxMap, EDesigNum, APPBBL, APPDate, PLUTOMapID, Version]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, dateTime, numeric, string]",Context PLUTO is a master record of the locations and characteristics of buildings in New York City. It’s published by the New York City Department of City Planning on an approximately quarterly-to-half-yearly basis and is one of the more important datasets for civic analysis in New York City. Content PLUTO includes information on building height square footage location type landmark status number of units owner year of construction and other related fields. Acknowledgements This dataset is published as-is by the New York City Department of Planning. Inspiration  What is the distribution of the heights of buildings in New York City? The age? Can you define neighborhoods by clustering similar buildings within them? What (and where) is the split between commercial residential and office space in New York City? ,CSV,,"[cities, civil engineering]",CC0,,,359,2531,290,The PLUTO master record of buildings in New York City.,New York City - Buildings Database,https://www.kaggle.com/new-york-city/nyc-buildings,Fri Sep 01 2017
,World Bank,"[Country Name, Country Code, Indicator Name, Indicator Code, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2020, 2025, 2030, 2035, 2040, 2045, 2050, 2055, 2060, 2065, 2070, 2075, 2080, 2085, 2090, 2095, 2100, ]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The World Bank EdStats All Indicator Query holds over 4000 internationally comparable indicators that describe education access progression completion literacy teachers population and expenditures. The indicators cover the education cycle from pre-primary to vocational and tertiary education. Content In addition to the above mentioned indicators this dataset also holds learning outcome data from international and regional learning assessments (e.g. PISA TIMSS PIRLS) equity data from household surveys and projection/attainment data to 2050. For further information please visit the EdStats website. Inspiration  In your opinion what are some of the more surprising indicators? Are there any you would consider adding? Which countries have the highest adult illiteracy rates? Have they changed over time and do rates vary based on age bracket? Do school enrollment rates have an impact on adult illiteracy rates? If so can you determine approximately how long a change in enrollment takes in order to impact illiteracy? Does this change vary among countries and if so can you point to changes in policies or NGO efforts that might play a role?  Acknowledgements Data was acquired from the World Bank and can be accessed in multiple formats here.,CSV,,"[countries, education]",Other,,,2162,16297,296,Studies of education status indicators around the world,Education Statistics,https://www.kaggle.com/theworldbank/education-statistics,Fri Nov 18 2016
,Recruit Institute of Technology,"[hmid, wid, reflection_period, original_hm, cleaned_hm, modified, num_sentence, ground_truth_category, predicted_category]","[numeric, numeric, string, string, string, boolean, numeric, string, string]",Description HappyDB is a corpus of more than 100000 happy moments crowd-sourced via Amazon’s Mechanical Turk.  Each worker is given the following task   What made you happy today? Reflect on the past 24 hours and recall three actual events that happened to you that made you happy. Write down your happy moment in a complete sentence.   (Write three such moments.)  The goal of the corpus is to advance the understanding of the causes of happiness through text-based reflection. More information is available on the HappyDB website (https//rit-public.github.io/HappyDB/). Content  cleaned_hm.csv the cleaned-up corpus of 100000 crowd-sourced happy moments. For cleaning up we have done spell checking over the whole corpus and remove empty or one-word statements. The raw happy moments are retained for reference and the author of each happy moment is represented by the his/her worker ID. demographic.csv the demographic information of the worker who provided the moment.  The information includes worker id age country gender marital status and status of parenthood.  Have fun with the data! Feel free to contact us with any questions. Sample questions To provide some inspiration here are a few sample interesting exploration questions.  What are the popular sports/movies/books/purchased products/tourist destinations/... that make people happy? Can we predict gender/marriage status/parenthood/age groups based on happy moment texts? How many indoor and outdoor activities are in the corpus respectively? Can we find interesting ways of clustering happy moments?  Citation Please cite the following publication if you are using the dataset for your work  HappyDB A Corpus of 100000 Crowdsourced Happy Moments LREC 2018 (to appear)  Akari Asai Sara Evensen Behzad Golshan Alon Halevy Vivian Li Andrei Lopatenko Daniela Stepanov Yoshihiko Suhara Wang-Chiew Tan and Yinzhan Xu,CSV,,"[emotion, linguistics]",CC0,,,165,2886,5,"A Corpus of 100,000 Crowdsourced Happy Moments",HappyDB,https://www.kaggle.com/ritresearch/happydb,Sat Jan 13 2018
,PromptCloud,"[Restaurant ID, Restaurant URL, Name, Address, Phone, City, State, Country, Neighbourhood, Email ID, Menu, Website, Latitude, Longitude, About Restaurant, Cuisine, Good for(suitable), Price, Currency, Rating, Ranking, Deal(Promotion), Total Review, Last Reviewed, Recommended, Dining Option, Award, Uniq Id]","[numeric, string, string, string, numeric, string, string, string, string, string, string, string, numeric, numeric, string, string, string, dateTime, string, string, string, string, numeric, dateTime, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 1492992 restaurants) that was created by extracting data from TripAdvisor.com. Content This dataset has following fields  Restaurant URL Name Address Phone City State Country Neighbourhood Email ID Menu Website Latitude Longitude About Restaurant Cuisine Good for(suitable) Price Currency Rating Ranking Deal(Promotion) Total Review Last Reviewed Recommended Dining Option Award  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration The country-wise analyses of cuisine rating ranking etc. can be performed.,CSV,,[internet],CC4,,,1315,8636,7,"Data on 18,000 restaurants",Restaurants on TripAdvisor,https://www.kaggle.com/PromptCloudHQ/restaurants-on-tripadvisor,Fri Sep 15 2017
,Liling Tan,"[1	77, 77]","[string, numeric]",Context Tatoeba is a crowd-sourced dataset made up of example sentences and their translations.  This dump uploaded on Kaggle is downloaded some time in Oct/Nov 2017. The latest dumps can be downloaded from https//tatoeba.org/eng/downloads Acknowledgements Credits goes to the maintainers and the crowd on https//tatoeba.org Credits of the banner image goes to Patrick Tomasso License The official license is CC BY-SA 2.0,CSV,,"[languages, linguistics]",CC3,,,30,710,639,Crowd-source Example Sentence and Translations,Tatoeba,https://www.kaggle.com/alvations/tatoeba,Fri Jan 05 2018
,"Open Sourcing Mental Illness, LTD",[],[],OSMI Mental Health in Tech Survey 2016 Currently over 1400 responses the ongoing 2016 survey aims to measure attitudes towards mental health in the tech workplace and examine the frequency of mental health disorders among tech workers. How Will This Data Be Used? We are interested in gauging how mental health is viewed within the tech/IT workplace and the prevalence of certain mental health disorders within the tech industry. The Open Sourcing Mental Illness team of volunteers will use this data to drive our work in raising awareness and improving conditions for those with mental health disorders in the IT workplace.,Other,,"[mental health, employment]",CC4,,,2100,12761,80,Data on prevalence and attitudes towards mental health among tech workers,OSMI Mental Health in Tech Survey 2016,https://www.kaggle.com/osmi/mental-health-in-tech-2016,Mon Nov 14 2016
,Max Mind,[],[],Context This dataset is meant to be used with other datasets that have features like country and city but no latitude/longitude. It is simply a list of cities in the world. Being able to put cities on a map will help people tell their stories more effectively. Another way to think about it is that you can use this make more pretty graphs! Content Fields   city region country  population latitude  longitude  Acknowledgements These data come from Maxmind.com and have not been altered. The original source can be found by clicking here Additionally the Maxmind sharing license has been included. Inspiration I wanted to analyze a dataset and make a map but I was only given a city name without any latitude or longotude coordinates. I found this dataset very helpful and I hoe you do too!,Other,,"[world, cities, countries]",Other,,,1053,8653,157,A database of coordinates for countries and cities,World Cities Database,https://www.kaggle.com/max-mind/world-cities-database,Thu Aug 24 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes stop data from MS MT ND NH NJ NV OR RI SD TN  VA V WI and WY. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,CSV,,"[government agencies, crime, law]",Other,,,120,683,1024,Data on Traffic and Pedestrian Stops by Police in many states,Stanford Open Policing Project - Bundle 2,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-bundle-2,Tue Aug 01 2017
,Manas,"[match_id, inning, batting_team, bowling_team, over, ball, batsman, non_striker, bowler, is_super_over, wide_runs, bye_runs, legbye_runs, noball_runs, penalty_runs, batsman_runs, extra_runs, total_runs, player_dismissed, dismissal_kind, fielder]","[numeric, numeric, string, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string]",This is the ball by ball data of all the IPL cricket matches till season 9.   Source http//cricsheet.org/ (data is available on this website in the YAML format. This is converted to CSV format by the contributors)  The dataset contains 2 files deliveries.csv and matches.csv. matches.csv contains details related to the match such as location contesting teams umpires results etc. deliveries.csv is the ball-by-ball data of all the IPL matches including data of the batting team batsman bowler non-striker  runs scored etc.  Research scope Predicting the winner of the next season of IPL based on past data  Visualizations Perspectives etc.,CSV,,"[cricket, india]",CC4,,,9038,51176,1,Data for all the IPL seasons,Indian Premier League,https://www.kaggle.com/manasgarg/ipl,Thu Nov 23 2017
,Arizona Secretary of State,"[County, PrecinctID, PrecinctName, ContestID, ContestTitle, CandidateID, CandidateName, TotalVotes, PollVotes, EarlyVotes, Provisionals, LatePoll, LateEarly, Registered, Undervote, ContestTotal, CandidateParty, TotalTurnout, EDTurnout]","[string, string, string, numeric, string, numeric, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, string, string, numeric, numeric]","Context The 2016 Statewide General Election results for Arizona. Arizona's 15 counties are required by statute to publish tabulated General Election results by precinct. This file represents a standardized and aggregated version of all 15 files. Please note that while the file is mostly standardized many of the attributes are relatable accross counties via a fuzzy match (the keyword ""Congress"" etc...). Content  County Abbreviation of Arizona's 15 counties AP Apache CH Cochise CN Coconino GI Gila GM Graham GN Greenlee LP La Paz MC Maricopa MO Mohave NA Navajo PM Pima PN Pinal SC Santa Cruz YA Yavapai YU Yuma PrecinctID Precinct identification number designated by the counties. County shorthand has been added. PrecinctName Precinct Name designated by the counties. This is directly passed from the tabulation files.  ContestID Contest Identification number designated by the counties. This is directly passed from the tabulation files and may not be standardized across counties. ContestTitle Title of race as designated by counties. This is directly passed from the tabulation files and may not be standardized across counties. CandidateID Candidate identification number designated by the counties. This is directly passed form the tabulation files and may not be standardized across counties. CandidateName Name of Candidate as desingated by the counties. This is directly passed form the tabulation files and may not be standardized across counties. TotalVotes Vote Total aggregated from the attributes ""PollVotes EarlyVotes Provisionals LatePoll LateEarly"".  PollVotes Total votes tabulated at a designated precinct location on election day. EarlyVotes Total votes tabulated by the counties during the 29 day early voting period. Provisionals Total votes tabulated at a designated precinct location that were deemed acceptable provisional ballots. 12 LateEarly Total votes tabulated by the counties of early votes that were dropped off at designated polling locations rather than received in the mail. (Note only a few counties separated this number from EarlyVote in their tabulation files). Registered The number of registered voters at the time of the election in each designated precinct. Undervote The number of ballots that did note cast the allowed number of votes for any given race. (Example voters are allowed to ""vote for 2"" in the Arizona House of Representatives race in this case these ballots were either left blank or only voted for 1) ContestTotal Total votes cast in a given contest. CandidateParty Party of candidate in a given contest. REP Republican DEM Democrat NP No Party GRN Green LBT Libertarian TotalTurnout Total turnout for a designated precinct. EDTurnout Total turnout for a designated precinct on election day. EarlyTurnout Total turnout for a designated precinct during the 29 day early voting period. (Note this number will include early ballots dropped off at the designated polling location.)  Final Note There are certain records in the file that are not part of any contest. They are normally designated by a contest ID that begins with a ""999"" These are records that the tabulators append to every file to provide background on each of the designated precincts.",CSV,,[politics],CC0,,,102,1354,65,"November 8, 2016 General Election for the State of Arizona",General Election Results,https://www.kaggle.com/arizonaSecofState/2016-statewide-general-election-results,Fri Sep 22 2017
,MarcSchroeder,"[ra_review_id, release_type, artist, release_title, label, release_month, release_year, style, num_comments, rating, review_published, author, review_body, tracklist]","[numeric, string, string, string, string, string, numeric, string, numeric, numeric, dateTime, string, string, string]",Context Started in 2001 Resident Advisor (RA) has become the web's largest resource for information about underground electronic music around the world. The site maintains a huge database of music and tech reviews artists labels news podcasts and events. Content Gathered using a web-scraping script the dataset below is of the site's entire collection of music reviews from the start of the site through the end of 2017. It contains the following fields  Release Type (album or single) Artist Release Title Label Release Month Release Year Style (genres of release listed by RA) Rating (score out of 5 given by RA) Date of Review Review Author Body of Review Release Tracklist  Acknowledgements Thanks to the RA team for the journalism over the years and for (hopefully) being cool with this dataset being published here.,CSV,,"[journalism, music]",Other,,,76,928,15,16972 Rows of Underground Electronic Music Reviews,17 Years of Resident Advisor Reviews,https://www.kaggle.com/marcschroeder/17-years-of-resident-advisor-reviews,Wed Jan 03 2018
,PromptCloud,"[uniq_id, sku, name_title, description, list_price, sale_price, category, category_tree, average_product_rating, product_url, product_image_urls, brand, total_number_reviews, Reviews]","[string, string, string, string, numeric, numeric, string, string, string, string, string, string, numeric, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 3.7 million products) that was created by extracting data from jcpenney.com a well known retailer. Content This dataset has following fields  sku name_title description list_price sale_price category category_tree average_product_rating product_url product_image_urls brand total_number_reviews reviews  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of list price  sale price rating and reviews can be performed.,CSV,,[internet],CC4,,,458,3322,23,"20,000 product listings from JCPenney",JCPenney products,https://www.kaggle.com/PromptCloudHQ/all-jc-penny-products,Fri Sep 15 2017
,Carlos Paradis,"[Field, Data Item, Data Type, Definition, Examples]","[string, string, string, string, numeric]","https//www.youtube.com/watch?v=A8syQeFtBKc  Context The Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center. While not an exhaustive collection of mass shootings it is a high-quality dataset ranging from 1966 to 2016 with well-defined methodology definitions and source URLs for user validation.  This dataset can be used to validate other datasets such as us-mass-shootings-last-50-years which contains more recent data or conduct other analysis as more information is provided.  Content This dataset contains data by the MSA project both from it's website and from it's Github account. The difference between the two sources is only on the data format (i.e. .csv versus .geojson for the data or .csv versus .pdf for the dictionary).  mass_shooting_events_stanford_msa_release_06142016 Contains a nonexaustive list of US Mass Shootings from 1966 to 2016 in both .csv and .geojson formats. dictionary_stanford_msa_release_06142016 Contains the data dictionary in .csv and .pdf formats. Note the .pdf format provides an easier way to visualize sub-fields.  Note the data was reproduced here without any modifications other than file renaming for clarity the content is the same as in the source. The following sections are reproduced from the dataset creators website. For more details please see the source. Project background The Stanford Mass Shootings of America (MSA) data project began in 2012 in reaction to the mass shooting in Sandy Hook CT. In our initial attempts to map this phenomena it was determined that no comprehensive collection of these incidents existed online. The Stanford Geospatial Center set out to create as best we could a single point repository for as many mass shooting events as could be collected via online media. The result was the Stanford MSA. What the Stanford MSA is The Stanford MSA is a data aggregation effort. It is a curated set of spatial and temporal data about mass shootings in America taken from online media sources. It is an attempt to facilitate research on gun violence in the US by making raw data more accessible. What the Stanford MSA is not The Stanford MSA is not a comprehensive longitudinal research project. The data collected in the MSA are not investigated past the assessment for inclusion in the database. The MSA is not an attempt to answer specific questions about gun violence or gun laws. The Stanford Geospatial Center does not provide analysis or commentary on the contents of this database or any derivatives produced with it. Data collection methodology The information collected for the Stanford MSA is limited to online resources. An initial intensive investigation was completed looking back over existing online reports to fill in the historic record going back to 1966. Contemporary records come in as new events occur and are cross referenced against a number of online reporting sources. In general a minimum of three corroborating sources are required to add the full record into the MSA (as many as 6 or 7 sources may have been consulted in many cases). All sources for each event are listed in the database. Due to the time involved in vetting the details of any new incident there is often a 2 to 4 week lag between a mass shooting event and its inclusion in the public release database. It is important to note the records in the Stanford MSA span a time from well before the advent of online media reporting through its infancy to the modern era of web based news and information resources. Researchers using this database need to be aware of the reporting bias these changes in technology present. A spike in incidents for recent years is likely due to increased online reporting and not necessarily indicative of the rate of mass shootings alone. Researchers should look at this database as a curated collection of quality checked data regarding mass shootings and not an exhaustive research data set itself. Independent verification and analysis will be required to use this data in examining trends in mass shootings over time. Definition of Mass Shooting The definition of mass shooting used for the Stanford database is 3 or more shooting victims (not necessarily fatalities) not including the shooter. The shooting must not be identifiably gang drug or organized crime related. Acknowledgements The Stanford Mass Shootings in America (MSA) is a dataset released under Creative Commons Attribution 4.0 international license by the Stanford Geospatial Center. How to cite the MSA The Stanford MSA is released under a Creative Commons Attribution 4.0 international license. Please cite the MSA as “Stanford Mass Shootings in America courtesy of the Stanford Geospatial Center and Stanford Libraries”. Inspiration There is already a great number of interesting datasets in Kaggle surrounding the subject of Mass Shootings however little has been done leveraging information from multiple sources. Can you see a story among them? Can we learn anything for example comparing the different sources by city or state?  From a bigger picture  Leading Causes of Death in US https//www.kaggle.com/cdc/mortality Gun Violence Database https//www.kaggle.com/gunviolencearchive/gun-violence-database Gun Deaths in US https//www.kaggle.com/hakabuk/gun-deaths-in-the-us Homicide Reports https//www.kaggle.com/murderaccountability/homicide-reports Global Terrorism Database https//www.kaggle.com/START-UMD/gtd Crime Rates in America https//www.kaggle.com/marshallproject/crime-rates  ""Prevention?  Firearms Provisions in US States https//www.kaggle.com/jboysen/state-firearms Trial and Terror https//www.kaggle.com/jboysen/trial-and-terror Connecticut Inmates Waiting trial https//www.kaggle.com/Connecticut-open-data/connecticut-inmates-awaiting-trial  Are there warning signs?  Mental Health in Tech Survey https//www.kaggle.com/osmi/mental-health-in-tech-2016/data (Not directly related but can be used to make a parenthesis about mental health being an issue in our surroundings). ",CSV,,"[united states, crime, violence, terrorism]",Other,,,489,3008,2,"A high quality dataset from 1966-2016 with method, definitions and references",Stanford Mass Shootings in America (MSA),https://www.kaggle.com/carlosparadis/stanford-msa,Sun Oct 08 2017
,breandan,[],[],"LELÚ is a French dialog corpus that contains a rich collection of human-human spontaneous written conversations extracted from Reddit’s public dataset available through Google BigQuery. Our corpus is composed of 556621 conversations with 1583083 utterances in total. The code to generate this dataset can be found in our GitHub Repository. The archive spf.tar.gz contains Reddit discussions in an XML file with the following format <dialog>     <s link_id=""4rqtz"" subreddit_id=""2qhjz"">         <utt uid=""1"" comment_id=""123"" parent_id=""4rqtz"" score=""1"" create_utc=""12458129356"">Hey how are you?</utt>         <utt uid=""2"" comment_id=""124"" parent_id=""123"" score=""1"" create_utc=""12458129486"">I’m fine thank you!</utt>         <utt uid=""1"" comment_id=""125"" parent_id=""124"" score=""1"" create_utc=""12458139804"">Nice!</utt>     </s>     <s link_id=""8y1br"" subreddit_id=""2qhjz"">         <utt uid=""1"" comment_id=""126"" parent_id=""124""  score=""1"" create_utc=""12458129310"">Who’s around for lunch?</utt>         <utt uid=""2"" comment_id=""127"" parent_id=""126"" score=""1"" create_utc=""12458139345"">Me!</utt>         <utt uid=""3"" comment_id=""128"" parent_id=""127"" score=""1"" create_utc=""12458149382"">Me too!</utt>     </s> </dialog>  The tag attributes can be described as follows  link_id ID of the parent Reddit post. subreddit_id ID of the subreddit. uid ID of the comment author. comment_id ID of the Reddit comment. parent_id ID of the parent Reddit comment.  We have split up the conversation trees into short sequential conversations using a heuristic described in our paper LELÚ A French Dialog Corpus from Reddit however the full conversation trees can be reconstructed using the comment_id and parent_id attributes of the <utt> tag. Data was collected from the following subreddits /r/france /r/FrancaisCanadien /r/truefrance /r/paslegorafi and /r/rance.",Other,,"[languages, social groups, demographics]",CC0,,,68,1071,211,"A French written dialog corpus from Reddit, with 1,583,083 utterances in total.",French Reddit Discussion,https://www.kaggle.com/breandan/french-reddit-discussion,Thu Sep 28 2017
,Olga Belitskaya,"[country_label, country, decor_label, decor, type_label, type, file]","[numeric, string, numeric, string, numeric, string, string]",History I have made the database of photos sorted by countries and pattern types. Screenshots were performed only on official websites. Content The main dataset (decor.zip) is 485 color images (150x150x3) of traditional decor patterns and the file with labels decor.csv. Photo files are in the .png format and the labels are integers and values. The file DecorColorImages.h5 consists of preprocessing images of this set image tensors and targets (labels). Acknowledgements I have published the data for absolutely free usage by any site visitor. But this database contains the names of famous traditional decor styles so it can not be used for commercial purposes. Usage Classification image recognition or generation colorizing etc. in a case of a small number of images are useful exercises. There are three kinds of classification here by country by pattern by types of image (pattern itself or product). Improvement It's possible to find lots of ways for improving this set and the machine learning algorithms applying to it because of many traditional patterns in the world.,Other,,"[photography, classification, deep learning]",Other,,,66,988,48,Pattern Classification,Traditional Decor Patterns,https://www.kaggle.com/olgabelitskaya/traditional-decor-patterns,Mon Jan 15 2018
,Luiz Gerosa,"[AnoCalendario, DataArquivamento, DataAbertura, CodigoRegiao, Regiao, UF, strRazaoSocial, strNomeFantasia, Tipo, NumeroCNPJ, RadicalCNPJ, RazaoSocialRFB, NomeFantasiaRFB, CNAEPrincipal, DescCNAEPrincipal, Atendida, CodigoAssunto, DescricaoAssunto, CodigoProblema, DescricaoProblema, SexoConsumidor, FaixaEtariaConsumidor, CEPConsumidor]","[numeric, dateTime, dateTime, numeric, string, string, string, string, numeric, numeric, numeric, string, string, numeric, string, string, numeric, string, numeric, string, string, string, numeric]",Context When Brazilian consumers need to resolve a dispute with business the first step is to go to a local Procon (Consumer Protection Agency) and file a complaint. The Procon assists the consumer and intermediates the resolution with the company. Content This dataset contains information about complaints filed in Procons between 2012 and 2016.  This data was download from official Brazilian government open data website,CSV,,"[brazil, business]",CC4,,,196,2274,406,Consumer complaints about issues with business in Brazil,Consumer Business Complaints in Brazil,https://www.kaggle.com/gerosa/procon,Thu Oct 12 2017
,Hazrat Ali,[],[],Context This dataset presents speech files recorded for isolated words of Urdu.  Language resources for Urdu language are not well developed. In this work we summarize our work on the development of Urdu speech corpus for isolated words. The Corpus comprises of 250 isolated words of Urdu recorded by ten individuals. The speakers include both native and non-native male and female individuals. The corpus can be used for both speech and speaker recognition tasks. The sampling frequency is 16000 Hz. Content Each folder name refers to a single speaker.  The folder name gives information about the characteristics of each speaker. Each folder contains 250 isolated files i.e. 250 isolated words. Speaker Name AA AB AC . . . AK Gender M for male  F for female Native /Non-Native Y for Native  N for Non-Native Age Group G1 G2 G3 G4 Example AAMNG1 Speaker Name    =   AA Gender      =   Male N       =   Non-Native G1      =   Age Group G1 Acknowledgements All the volunteers community who recorded for us.,Other,,[linguistics],CC4,,,57,1102,40,"2,500 Urdu audio samples",Urdu Speech Dataset,https://www.kaggle.com/hazrat/urdu-speech-dataset,Wed Nov 15 2017
,The Flying Munkey,"[year, month, port_of_landing, port_nationality, vessel_nationality, length_group, gear_category, species_code, species_name, species, species_group, live_weight, landed_weight, value_gbp]","[numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric]",Context Data taken from the Marine Management Organisation on all UK vessels landing in ports or foreign vessels landing in UK ports. This dataset contains data on the catch also the weight and value (£) of the catch. Content  Year year of landing 2008-2015 Month calendar months 1-12 Port of Landing name of port Port Nationality nationality of the port of landing Vessel Nationality nationality of the vessel Length Group length of the vessel 10m or under/Over 10m Gear Category gear carried by the vessel Species Code three letter code of the catch Species Name name of the catch Species as shown in publication name of the catch with fewer subcategories Species Group catch species Live Weight (tonnes) weight of live catch Landed Weight (tonnes) landed weight of catch Value (£) the value of the catch in GBP most likely without any inflation adjustment  Points to note  Data on Port Nationality and Vessel Nationality for 2008 were supplied as 3 letter codes not all of which matched standard country codes. I've tried to clean these up as best I can to match with standard country codes but ones that couldn't be matched have been left as-is. Species Code and Species Name were not supplied for 2008. You may be able to infer some from the 2009-2015 data.  Inspiration  Which UK ports see the greatest activity? Which foreign ports see the greatest number of UK vessels? Has the price-paid per tonne of goods varied during the data collection period? Has there been any major changes in activity for particular species?  Acknowledgements Data taken from the Office of National Statistics and is part of the UK Sea Fisheries Annual Statistics. Data are available under a Open Government Licence v3.0.,CSV,,"[government agencies, fishing, business]",Other,,,55,830,81,Data from 2008-2015 on fishing vessels either from the UK or landing in the UK,UK fleet and foreign fleet landings by port,https://www.kaggle.com/theflyingmunkey/uk-fleet-landings,Thu Aug 24 2017
,PyTorch,[],[],VGG-16  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,7,561,490,VGG-16 Pre-trained model with batch normalization for PyTorch,VGG-16 with batch normalization,https://www.kaggle.com/pytorch/vgg16bn,Sat Dec 16 2017
,Fabiola,"[, , 12_lt26_mf, , , , , , 12_lt16_mf, , , , , , 16_lt18_mf, , , , , , 18_lt22_mf, , , , , , 22_lt26_mf, , , , , , 12_lt26_m, , , , , , 12_lt16_m, , , , , , 16_lt18_m, , , , , , 18_lt22_m, , , , , , 22_lt26_m, , , , , , 12_lt26_f, , , , , , 12_lt16_f, , , , , , 16_lt18_f, , , , , , 18_lt22_f, , , , , , 22_lt26_f, , , , , ]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The data was collected by social science research institutes 1973 bis 1993 Institut für Jugendforschung GmbH (IJF) München; 1997 GfM-GETAS/WBA GmbH Hamburg 2001 bis 2015 forsa Gesellschaft für Sozialforschung und statistische Analysen mbH Dortmund und Berlin.  Long term studies regarding the alcohol and (illegal) drug consumption habits of 12 to 25 year old German teens were taken. The motivations and influences in drug consumption were studied. The aim was to develop preventional means and ways of communication with drug consuming teenagers. Content Young Germans between 12 and 25 years were asked about their drug consumptions in the last 12 months. The survey was taken from the 70's until today. Different datasets were provided and for some years features are missing. Acknowledgements The data is provided by German Bundeszentrale für gesundheitliche Aufklärung (Ministry of Health Education) and can be accessed at http//www.gbe-bund.de. Photo by Stas Svechnikov on Unsplash. Inspiration While growing up in the late 2000's in Germany I had the impression that teenagers smoke less cigarettes and drink less alcohol than they used to in the 90's. Instead they consumed more cannabis. I wonder if I was right ...,CSV,,"[alcohol, public health, children]",Other,,,448,3066,0.013671875,Do modern teens prefer weed over cigarettes?,Alcohol and Drug Consumption of German Teens,https://www.kaggle.com/fabiolabusch/alcohol-and-drug-consumption-of-german-teens,Fri Sep 15 2017
,US Department of Agriculture,"[Commodity Name, City Name, Type, Package, Variety, Sub Variety, Grade, Date, Low Price, High Price, Mostly Low, Mostly High, Origin, Origin District, Item Size, Color, Environment, Unit of Sale, Quality, Condition, Appearance, Storage, Crop, Repack, Trans Mode]","[string, string, string, string, string, string, string, dateTime, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context Over 1.5 billions pounds of pumpkin are grown annually in the United States. Where are they sold and for how much? This dataset contains prices for which pumpkins were sold at selected U.S. cities’ terminal markets. Prices are differentiated by the commodities’ growing origin variety size package and grade. Content This dataset contains terminal market prices for different pumpkin crops in 13 cities in the United States from September 24 2016 to September 30 2017. In keeping with the structure of the original source data information on each city has been uploaded as a separate file.  Atlanta GA Baltimore MD Boston MA Chicago IL Columbia SC Dallas TX Detroit MI Los Angeles CA Miami FL New York NY Philadelphia PA San Francisco CA Saint Louis MO  Data for each city includes the following columns (although not all information is available for every city)   Commodity Name Always pumpkin since this is a pumpkin-only dataset City Name City where the pumpkin was sold Type Package Variety Sub Variety Grade In the US usually only canned pumpkin is graded Date Date of sale (rounded up to the nearest Saturday) Low Price High Price Mostly Low Mostly High Origin Where the pumpkins were grown Origin District Item Size Color Environment Unit of Sale Quality Condition Appearance Storage Crop Repack Whether the pumpkin has been repackaged before sale Trans Mode  Acknowledgements This dataset is based on Specialty Crops Terminal Markets Standard Reports distributed by the United States Department of Agriculture. Up-to-date reports can be generated here. This data is in the public domain. Inspiration  Which states produce the most pumpkin? Where are pumpkin prices highest? How does pumpkin size relate to price? Which pumpkin variety is the most expensive? Least expensive? ,CSV,,"[food and drink, united states, finance, agriculture]",CC0,,,629,4026,0.1796875,Pumpkin Prices in 13 US Cities: 2016-2017,A Year of Pumpkin Prices,https://www.kaggle.com/usda/a-year-of-pumpkin-prices,Tue Oct 24 2017
,PromptCloud,"[title, url, date]","[string, string, dateTime]",Context Web data extraction or web scraping can be a great business tool for trend spotting via media monitoring. Essentially leading media outlets can be tracked to unveil the top buzzwords and the number of mentions companies (including their products) garner over specific time period. We wanted to apply this method to understand the tech landscape and its coverage in 2017. Hence we deployed PromptCloud’s in-house web crawler to extract the article titles from two popular outlets (TechCrunch and VentureBeat) and performed text mining on the dataset to uncover the top buzzwords companies and products. Content The dataset contains following 3 fields  URL Title Date of publication  Acknowledgements This dataset was created by using PromptCloud's in-house web scraping service. Inspiration Initial Analysis can be found here. It includes the following findings  Top companies/products that were covered by media over the year Top tech trends over the year ,CSV,,"[journalism, mass media, internet]",CC4,,,36,610,1,"Titles of 22,000+ article published on two of the top media websites",Article Titles from TechCrunch and VentureBeat,https://www.kaggle.com/PromptCloudHQ/titles-by-techcrunch-and-venturebeat-in-2017,Wed Dec 20 2017
,Jacob Boysen,[],[],Context Occupational Safety and Health Administration aka OSHA requires employers to report all severe work-related injuries defined as an amputation in-patient hospitalization or loss of an eye. The requirement began on January 1 2015.  Content This dataset provides information from those reports including a description of the incident and the name and address of the establishment where it happened. Injuries are coded using the Occupational Injury and Illness Classification System. Data covers ~22k incidents Jan 1 2015-Feb 28 2017. 26 columns describe incident parties involved employer injury sustained and final outcome. Acknowledgements This dataset was created by [OSHA](https//www.osha.gov/severeinjury/index.html ) and released to the public. Inspiration  Which industries have the highest rate of worker injuries? Most severe injuries? Can you predict injuries for 2016 based on 2015 data? In which regions are injuries most common? ,CSV,,"[healthcare, employment]",CC0,,,394,2518,11,"~22k Injury Reports for US Workers, 2015-2017",Severely Injured Workers,https://www.kaggle.com/jboysen/injured-workers,Wed Sep 06 2017
,Currie32,"[, ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrest, Domestic, Beat, District, Ward, Community Area, FBI Code, X Coordinate, Y Coordinate, Year, Updated On, Latitude, Longitude, Location]","[numeric, numeric, string, dateTime, string, numeric, string, string, string, boolean, boolean, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, numeric, string]","Context This dataset reflects reported incidents of crime (with the exception of murders where data exists for each victim) that occurred in the City of Chicago from 2001 to present minus the most recent seven days. Data is extracted from the Chicago Police Department's CLEAR (Citizen Law Enforcement Analysis and Reporting) system. In order to protect the privacy of crime victims addresses are shown at the block level only and specific locations are not identified. Should you have questions about this dataset you may contact the Research & Development Division of the Chicago Police Department at 312.745.6071 or RDAnalysis@chicagopolice.org. Disclaimer These crimes may be based upon preliminary information supplied to the Police Department by the reporting parties that have not been verified. The preliminary crime classifications may be changed at a later date based upon additional investigation and there is always the possibility of mechanical or human error. Therefore the Chicago Police Department does not guarantee (either expressed or implied) the accuracy completeness timeliness or correct sequencing of the information and the information should not be used for comparison purposes over time. The Chicago Police Department will not be responsible for any error or omission or for the use of or the results obtained from the use of this information. All data visualizations on maps should be considered approximate and attempts to derive specific addresses are strictly prohibited. The Chicago Police Department is not responsible for the content of any off-site pages that are referenced by or that reference this web page other than an official City of Chicago or Chicago Police Department web page. The user specifically acknowledges that the Chicago Police Department is not responsible for any defamatory offensive misleading or illegal conduct of other users links or third parties and that the risk of injury from the foregoing rests entirely with the user. The unauthorized use of the words ""Chicago Police Department"" ""Chicago Police"" or any colorable imitation of these words or the unauthorized use of the Chicago Police Department logo is unlawful. This web page does not in any way authorize such use. Data are updated daily. The dataset contains more than 6000000 records/rows of data and cannot be viewed in full in Microsoft Excel.  To access a list of Chicago Police Department - Illinois Uniform Crime Reporting (IUCR) codes go to http//data.cityofchicago.org/Public-Safety/Chicago-Police-Department-Illinois-Uniform-Crime-R/c7ck-438e Content ID - Unique identifier for the record. Case Number - The Chicago Police Department RD Number (Records Division Number) which is unique to the incident. Date - Date when the incident occurred. this is sometimes a best estimate. Block - The partially redacted address where the incident occurred placing it on the same block as the actual address. IUCR - The Illinois Unifrom Crime Reporting code. This is directly linked to the Primary Type and Description. See the list of IUCR codes at https//data.cityofchicago.org/d/c7ck-438e. Primary Type - The primary description of the IUCR code. Description - The secondary description of the IUCR code a subcategory of the primary description. Location Description - Description of the location where the incident occurred. Arrest - Indicates whether an arrest was made. Domestic - Indicates whether the incident was domestic-related as defined by the Illinois Domestic Violence Act. Beat - Indicates the beat where the incident occurred. A beat is the smallest police geographic area – each beat has a dedicated police beat car. Three to five beats make up a police sector and three sectors make up a police district. The Chicago Police Department has 22 police districts. See the beats at https//data.cityofchicago.org/d/aerh-rz74. District - Indicates the police district where the incident occurred. See the districts at https//data.cityofchicago.org/d/fthy-xz3r. Ward - The ward (City Council district) where the incident occurred. See the wards at https//data.cityofchicago.org/d/sp34-6z76. Community Area - Indicates the community area where the incident occurred. Chicago has 77 community areas. See the community areas at https//data.cityofchicago.org/d/cauq-8yn6. FBI Code - Indicates the crime classification as outlined in the FBI's National Incident-Based Reporting System (NIBRS). See the Chicago Police Department listing of these classifications at http//gis.chicagopolice.org/clearmap_crime_sums/crime_types.html. X Coordinate - The x coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block. Y Coordinate - The y coordinate of the location where the incident occurred in State Plane Illinois East NAD 1983 projection. This location is shifted from the actual location for partial redaction but falls on the same block. Year - Year the incident occurred. Updated On - Date and time the record was last updated. Latitude - The latitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block. Longitude - The longitude of the location where the incident occurred. This location is shifted from the actual location for partial redaction but falls on the same block. Location - The location where the incident occurred in a format that allows for creation of maps and other geographic operations on this data portal. This location is shifted from the actual location for partial redaction but falls on the same block. Acknowledgements I really want to say thank you to the City of Chicago and the Chicago Police Department for making this comprehensive data set available to everyone!  Inspiration How has crime changed over the years? Is it possible to predict where or when a crime will be committed? Which areas of the city have evolved over this time span?",CSV,,[crime],Other,,,6896,42294,2048,"An extensive dataset of crimes in Chicago (2001-2017), by City of Chicago",Crimes in Chicago,https://www.kaggle.com/currie32/crimes-in-chicago,Sat Jan 28 2017
,Marco Molina,[],[],The dataset contains the following variables water consumption per user (cubic meters) from 2009 to 2016 land use type of user (e.g. industrial housing public infrastructure etc.) zip code and others. The challenge is to treat NAs in a way that do not distort the overall dataset. You should also check whether there are any missing values. If so can you ﬁll them in and do you understand why they are missing? This dataset is property of a local water provider called AguaH and its part of a research developed between 2014 and 2016.,CSV,,[],ODbL,,,1331,9365,45,This dataset contains water consumption per capita from late 2000s to 2016.,Water Consumption in a Median Size City,https://www.kaggle.com/marcomolina/water-consumption-in-a-median-size-city,Thu Sep 15 2016
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,[],[],,CSV,,[india],CC4,,,0,0,0.193359375,Traffic accidents in each states/u.t of India from 2001-14.,Accidents in India,https://www.kaggle.com/rajanand/accidents-in-india,Sun Jul 23 2017
,Rachael Tatman,"[ean, name]","[numeric, string]",Context “The Universal Product Code (UPC) is a barcode symbology that is widely used in the United States Canada United Kingdom Australia New Zealand in Europe and other countries for tracking trade items in stores. “UPC (technically refers to UPC-A) consists of 12 numeric digits that are uniquely assigned to each trade item. Along with the related EAN barcode the UPC is the barcode mainly used for scanning of trade items at the point of sale per GS1 specifications.[1] UPC data structures are a component of GTINs and follow the global GS1 specification which is based on international standards. But some retailers (clothing furniture) do not use the GS1 system (rather other barcode symbologies or article number systems). On the other hand some retailers use the EAN/UPC barcode symbology but without using a GTIN (for products brands sold at such retailers only).” -- Tate. (n.d.). In Wikipedia. Retrieved August 18 2017 from https//en.wikipedia.org/wiki/Plagiarism. Text reproduced here under a CC-BY-SA 3.0 license. Content This dataset contains just over 1 million UPC codes and the names of the products associated with them. Acknowledgements While UPC’s themselves are not copyrightable the brand names and trademarks in this dataset remain the property of their respective owners. Inspiration  Can you use this dataset to generate new product names? Can you use this in conjunction with other datasets to disambiguate products? ,CSV,,"[business, supply chain, product]",CC0,,,264,2774,60,One million products & their UPC codes,Universal Product Code Database,https://www.kaggle.com/rtatman/universal-product-code-database,Sat Aug 19 2017
,Jacob Boysen,[],[],Context The Home Mortgage Disclosure Act (HMDA) requires many financial institutions to maintain report and publicly disclose information about mortgages. Content This dataset covers all mortgage decisions made in 2015 for the state of New York. Data for additional states and years can be accessed here. Acknowledgements This dataset was compiled by the Consumer Finance Protection Board. Inspiration  Where are mortgages most likely to be approved? Can you predict mortgage decisions based on the criteria provided here? ,CSV,,[],CC0,,,181,1567,338,Data on ~440k Home Mortgage Decisions in NY,"Home Mortgage Disclosure Act Data, NY, 2015",https://www.kaggle.com/jboysen/ny-home-mortgage,Thu Aug 17 2017
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, string, string, string, string, string, string, string, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one data file for each of these countries in Asia and Oceania.  United Arab Emirates - arab_emirates.csv Australia - australia.csv China - china.csv Iceland - iceland.csv Israel - israel.csv Japan - japan.csv Kazakhstan - kazakhstan.csv Kuwait - kuwait.csv New Caldonia - new_caldonia.csv New Zealand - new_zealand.csv Qatar - qatar.csv Saudi Arabia - saudiarabia.csv Singapore - singapore.csv South Korea - south_korea.csv Taiwan - taiwan.csv  Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets to map weather crime or how your next canoing trip.,CSV,,[],Other,,,154,1025,4096,Addresses and geolocations for Asia and Oceania,OpenAddresses - Asia and Oceania,https://www.kaggle.com/openaddresses/openaddresses-asia-and-oceania,Thu Aug 03 2017
,KP,[],[],Introduction Video games are a rich area for data extraction due to its digital nature.  Notable examples such as the complex EVE Online economy World of Warcraft corrupted blood incident and even Grand Theft Auto self-driving cars tells us that fiction is closer to reality than we really think.  Data scientists can gain insight on the logic and decision-making that the players face when put in hypothetical and virtual scenarios.  In this Kaggle Dataset I provide over 720000 competitive matches from the popular game PlayerUnknown's Battlegrounds.  The data was extracted from pubg.op.gg a game tracker website.  I intend for this data-set to be purely exploratory however users are free to create their own predictive models they see fit.   PlayerUnknown's Battlegrounds PUBG is a first/third-person shooter battle royale style game that matches over 90 players on a large island where teams and players fight to the death until one remains.  Players are airdropped from an airplane onto the island where they are to scavenge towns and buildings for weapons ammo armor and first-aid.  Players will then decide to either fight or hide with the ultimate goal of being the last one standing.  A bluezone (see below) will appear a few minutes into the game to corral players closer and closer together by dealing damage to anyone that stands within the bluezone and sparing whoever is within the safe zone.  Read more about PUBG here The Dataset This dataset provides two zips aggregate and deaths.   In deaths the files record every death that occurred within the 720k matches.  That is each row documents an event where a player has died in the match. In aggregate each match's meta information and player statistics are summarized (as provided by pubg).  It includes various aggregate statistics such as player kills damage distance walked etc as well as metadata on the match itself such as queue size fpp/tpp date etc.  The uncompressed data is divided into 5 chunks of approximately 2gb each.  For details on columns please see the file descriptions. Interpreting Positional Data The XY coordinates are all in in-game coordinates and need to be linearly scaled to be plotted on square erangel and miramar maps.  The coordinate minmax are 0800000 respectively. Potential Bias in the Data The scraping methodology first starts with an initial seed player I chose this to be my own account (a rather low rank individual).  I then use the seed player to scrape for all players that it has encountered in its historical matches.  I then take a random subset of 5000 players from this and then scrape for their historical games for the final dataset.  What this could produce is an unrepresentative sample of all games played as it is more likely that I queued and matched with lower rated players and those players more than likely also got matched against lower rated players as well.  Thus the matches and deaths are more representative of lower tier gameplay but given the simplicity of the dataset this shouldn't be an issue. Acknowledgements  Pubg.op.gg if this is against the TOS please let me know and I will take it down ,Other,,"[video games, geography, demographics]",CC0,,,619,8706,4096,Over 65 million death entries for PlayerUnknown Battleground's matches,PUBG Match Deaths and Statistics,https://www.kaggle.com/skihikingkevin/pubg-match-deaths,Fri Jan 12 2018
,Sohier Dane,[],[],"This dataset contains 28 million recommendation and click/no click pairs from users of the Sowiport library. From the abstract of the pre-print discussing the dataset Stereotype and most-popular recommendations are widely neglected in the research-paper recommender-system and digital-library community. In other domains such as movie recommendations and hotel search however these recommendation approaches have proven their effectiveness. We were interested to find out how stereotype and most-popular recommendations would perform in the scenario of a digital library. Therefore we implemented the two approaches in the recommender system of GESIS’ digital library Sowiport in cooperation with the recommendations-as-aservice provider Mr. DLib. We measured the effectiveness of most-popular and stereotype recommendations with click-through rate (CTR) based on 28 million delivered recommendations. Most-popular recommendations achieved a CTR of 0.11% and stereotype recommendations achieved a CTR of 0.124%. Compared to a “random recommendations” baseline (CTR 0.12%) and a content-based filtering baseline (CTR 0.145%) the results are discouraging. However for reasons explained in the paper we concluded that more research is necessary about the effectiveness of stereotype and most-popular recommendations in digital libraries.  This dataset was kindly made available by the authors of ""Stereotype and Most-Popular Recommendations in the Digital Library Sowiport"" under the CC-BY 3.0 license. You can find additional information at http//mr-dlib.org/.",Other,,[],Other,,,170,1983,2048,User behavior for multiple recommender systems,Recommender Click Logs- Sowiport,https://www.kaggle.com/sohier/recommender-click-logs-sowiport,Mon Aug 14 2017
,Jean-Michel D.,[],[],Context To better follow the energy consumption the government wants energy suppliers to install smart meters in every home in England Wales and Scotland. There are more than 26 million homes for the energy suppliers to get to with the goal of every home having a smart meter by 2020. This roll out of meter is lead by the European Union who asked all member governments to look at smart meters as part of measures to upgrade our energy supply and tackle climate change. After an initial study the British government decided to adopt smart meters as part of their plan to update our ageing energy system. In this dataset you will find a refactorised version of the data from the London data store that contains the energy consumption readings for a sample of 5567 London Households that took part in the UK Power Networks led Low Carbon London project between November 2011 and February 2014. The data from the smart meters seems associated only to the electrical consumption. There is infomations on the ACORN classification details that you can find in this report or the website of CACI. I added weather data for London area I used the darksky api to collect this data. Content There is 19 files in this dataset   informations_households.csv  this file that contains all the information on the households in the panel (their acorn group their tariff) and in which block.csv.gz file their data are stored halfhourly_dataset.zip Zip file that contains the block files with the half-hourly smart meter measurement daily_dataset.zip Zip file that contains the block files with the daily information like the number of measures minimum maximum mean median sum and std. acorn_details.csv  Details on the acorn groups and their profile of the people in the group it's come from this xlsx spreadsheet.The first three columns are the attributes studied the ACORN-X is the index of the attribute. At a national scale the index is 100 if for one column the value is 150 it means that there are 1.5 times more people with this attribute in the ACORN group than at the national scale. You can find an explanation on the CACI website weather_daily_darksky.csv  that contains the daily data from darksky api. You can find more details about the parameters in the documentation of the api weather_hourly_darksky.csv  that contains the hourly data from darksky api. You can find more details about the parameters in the documentation of the api  Acknowledgements All the big work of data collection has been done by the UK power networks for the smart meter data. The details related at the acorn group are provided by the CACI. The weather data are from darksky. Inspiration For me some ideas to analyze the data  Segmentation of the consumption daily pattern Disaggregation of the electricity load curve Cross the consumption result and the acorn information Forecast the electricity consumption of a household I wrote an article on this subject What if I add electrical heating system ? an EV battery system ? Forecast at a global scale (London consumption) ,CSV,,"[weather, home, demographics, energy]",ODbL,,,1079,11138,1024,Smart meter data from London area,Smart meters in London,https://www.kaggle.com/jeanmidev/smart-meters-in-london,Wed Dec 13 2017
,Democracy Fund,[],[],"Context The Democracy Fund Voter Study Group is using a unique longitudinal data set that most recently surveyed 8000 adults (age 18+) in December 2016 via YouGov. Participants were identified from a pool of respondents who participated in a similar survey in December 2011 as well as a second pre-election interview in 2012 and a third interview following the 2012 presidential election. For these 8000 respondents we have measures of their political attitudes values and affinities in 2011 as well as self-reports of their turnout and vote choice in November 2012. Content The VOTER (Views of the Electorate Research) Survey was conducted by the survey firm YouGov. In total 8000 adults (age 18+) with internet access took the survey online between November 29 and December 29 2016. The estimated margin of error is plus or minus 2.2 percent. YouGov also supplied measures of primary voting behavior from the end of the primary period (July 2016) when these respondents had been contacted as part of a different survey project.  These respondents were originally interviewed by YouGov in 2011 to 2012 as part of the 2012 Cooperative Campaign Analysis Project (CCAP). In that survey 45000 respondents were first interviewed in December 2011 and were interviewed a second time in one of the 45 weekly surveys between January 1 and November 8 2012. After the November election 35408 respondents were interviewed a third time. We invited 11168 panelists from the 2012 CCAP. Of those invited 8637 (77 percent) completed the 2016 VOTER Survey.  The 2012 CCAP was constructed using YouGov’s sample matching procedure. A stratified sample is drawn from YouGov’s panel which consists of people who have agreed to take occasional surveys. The strata are defined by the combination of age gender race and education and each stratum is sampled in proportion to its size in the U.S. population. Then each element of this sample is matched to a synthetic sampling frame that is constructed from the U.S. Census Bureau’s American Community Survey the Current Population Survey Voting and Registration Supplement and other databases. The matching procedure finds the observation in the sample from YouGov’s panel that most closely matches each observation in the synthetic sampling frame on a set of demographic characteristics. The resulting sample is then weighted by a set of demographic and non-demographic variables.  Information on variables can be found in the ""Guide to the 2016 Voter Survey"" and included in the dataset. Acknowledgements The Democracy Fund a charitable foundation committed to the protection and enhancement of democratic values found the rise of these movements and candidates worthy of analysis. In May 2016 the Democracy Fund chose to begin a rigorous project that would supply hard data to test proposed theories to explain these phenomenon. Working with Henry Olsen of the Ethics and Public Policy Center and John Sides of George Washington University the Democracy Fund assembled a diverse group of scholars and analysts representing political viewpoints from all angles. Additional information on participants can be found here. Data was originally published here. To reference the VOTER survey please use this protocol Democracy Fund Voter Study Group. VIEWS OF THE ELECTORATE RESEARCH SURVEY December 2016. [Computer File] Release 1 August 28 2017. Washington DC Democracy Fund Voter Study Group [producer] https//www.voterstudygroup.org/. Inspiration The Democracy Fund Voter Study Group has made available a series of insights on the dataset--read them for further data inspiration  Executive Summary Political Divisions in 2016 and Beyond Race Religion and Immigration in 2016 The Story of Trump's Appeal The Five Types of Trump Voters Methodology Read the latest news and updates from (and about) the Democracy Fund Voter Study Group. ",Other,,[],CC4,,,254,1713,60,"Unique Longitudinal Data Set on ~8k Voters, 2011-16",2016 VOTER Survey Data Set,https://www.kaggle.com/democracy-fund/2016-voter-survey,Fri Sep 22 2017
,Olga Belitskaya,[],[],History I made the database from my own photos of Russian lowercase letters written by hand.  Content The GitHub repository with examples GitHub  The main dataset (letters.zip)  1650 (50x33) color images (32x32x3) with 33 letters and the file with labels letters.txt.  Photo files are in the .png format and the labels are integers and values. Additional letters.csv file. The file LetterColorImages.h5 consists of preprocessing images of this set image tensors and targets (labels) The additional dataset (letters2.zip) 5940 (180x33) color images (32x32x3) with 33 letters and the file with labels letters2.txt.  Photo files are in the .png format and the labels are integers and values. Additional letters2.csv file. The file LetterColorImages2.h5 consists of preprocessing images of this set image tensors and targets (labels)   Letter Symbols => Letter Labels  а=>1 б=>2 в=>3 г=>4 д=>5 е=>6 ё=>7 ж=>8 з=>9 и=>10 й=>11 к=>12 л=>13 м=>14 н=>15 о=>16 п=>17 р=>18 с=>19 т=>20 у=>21 ф=>22 х=>23 ц=>24 ч=>25 ш=>26 щ=>27 ъ=>28 ы=>29 ь=>30 э=>31 ю=>32 я=>33  Background Images => Background Labels  striped=>0 gridded=>1 no background=>2 Acknowledgements As an owner of this database I have published it for absolutely free using by any site visitor. Usage Classification image generation etc. in a case of handwritten letters with a small number of images are useful exercises. Improvement There are lots of ways for increasing this set and the machine learning algorithms applying to it. For example add the same images but written by other person or add capital letters. ,Other,,"[languages, photography, deep learning]",Other,,,275,3526,73,Images of Russian Letters,Classification of Handwritten Letters,https://www.kaggle.com/olgabelitskaya/classification-of-handwritten-letters,Mon Dec 04 2017
,Mohammad Kachuee,[],[],Data Set Information The main goal of this data set is providing clean and valid signals for designing cuff-less blood pressure estimation algorithms. The raw electrocardiogram (ECG) photoplethysmograph (PPG) and arterial blood pressure (ABP) signals are originally collected from the physionet.org and then some preprocessing and validation performed on them. (For more information about the process please refer to our paper) Attribute Information This database consists of a cell array of matrices each cell is one record part.  In each matrix each row corresponds to one signal channel 1 PPG signal FS=125Hz; photoplethysmograph from fingertip 2 ABP signal FS=125Hz; invasive arterial blood pressure (mmHg) 3 ECG signal FS=125Hz; electrocardiogram from channel II Relevant Papers M. Kachuee M. M. Kiani H. Mohammadzade M. Shabany Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time IEEE International Symposium on Circuits and Systems (ISCAS'15) 2015.  A. Goldberger L. Amaral L. Glass J. Hausdorff P. Ivanov R. Mark J.Mietus G. Moody C. Peng and H. Stanley â€œPhysiobank physiotoolkit and physionet components of a new research resource for complex physiologic signalsâ€ Circulation vol. 101 no. 23 pp. 215â€“220 2000.  Citation Request If you found this data set useful please cite the following  M. Kachuee M. M. Kiani H. Mohammadzade M. Shabany Cuff-Less High-Accuracy Calibration-Free Blood Pressure Estimation Using Pulse Transit Time IEEE International Symposium on Circuits and Systems (ISCAS'15) 2015. M. Kachuee M. M. Kiani H. Mohammadzadeh M. Shabany Cuff-Less Blood Pressure Estimation Algorithms for Continuous Health-Care Monitoring IEEE Transactions on Biomedical Engineering 2016.,Other,,"[healthcare, health]",Other,,,619,5907,5120,Pre-processed and cleaned vital signals for cuff-less BP estimation,Cuff-Less Blood Pressure Estimation,https://www.kaggle.com/mkachuee/BloodPressureDataset,Sun Jun 04 2017
,UCI Machine Learning,"[id, diagnosis, radius_mean, texture_mean, perimeter_mean, area_mean, smoothness_mean, compactness_mean, concavity_mean, concave points_mean, symmetry_mean, fractal_dimension_mean, radius_se, texture_se, perimeter_se, area_se, smoothness_se, compactness_se, concavity_se, concave points_se, symmetry_se, fractal_dimension_se, radius_worst, texture_worst, perimeter_worst, area_worst, smoothness_worst, compactness_worst, concavity_worst, concave points_worst, symmetry_worst, fractal_dimension_worst, ]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]","Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.  n the 3-dimensional space is that described in [K. P. Bennett and O. L. Mangasarian ""Robust Linear Programming Discrimination of Two Linearly Inseparable Sets"" Optimization Methods and Software 1 1992 23-34].  This database is also available through the UW CS ftp server  ftp ftp.cs.wisc.edu  cd math-prog/cpo-dataset/machine-learn/WDBC/ Also can be found on UCI Machine Learning Repository https//archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29 Attribute Information 1) ID number  2) Diagnosis (M = malignant B = benign)  3-32)  Ten real-valued features are computed for each cell nucleus  a) radius (mean of distances from center to points on the perimeter)  b) texture (standard deviation of gray-scale values)  c) perimeter  d) area  e) smoothness (local variation in radius lengths)  f) compactness (perimeter^2 / area - 1.0)  g) concavity (severity of concave portions of the contour)  h) concave points (number of concave portions of the contour)  i) symmetry  j) fractal dimension (""coastline approximation"" - 1) The mean standard error and ""worst"" or largest (mean of the three largest values) of these features were computed for each image resulting in 30 features.  For instance field 3 is Mean Radius field 13 is Radius SE field 23 is Worst Radius. All feature values are recoded with four significant digits. Missing attribute values none Class distribution 357 benign 212 malignant",CSV,,[healthcare],CC4,,,17579,127483,0.119140625,Predict whether the cancer is benign or malignant,Breast Cancer Wisconsin (Diagnostic) Data Set,https://www.kaggle.com/uciml/breast-cancer-wisconsin-data,Sun Sep 25 2016
,Serhiy Subota,[],[],Context Data on parliamentary agendas and actions taken by the Verkhovna Rada (Верховна Рада) the Ukrainian parliament in 2014 through 2017. Content For the period of 27th Nov 2014 through 17th Oct 2017  List of deputies List of parliamentary fractions Session Days Daily agenda results including total voting result individual deputy votes speech authors and timings no full text registration performed as the session day starts  Acknowledgements Sourced from http//data.rada.gov.ua/open/data/ppz-skl8 on the Ukrainian government open data portal. Thanks to everyone who made this open data possible. Photo by Illia Cherednychenko on Unsplash. Inspiration  How are the Ukranian parliamentary factions structured? Does parliamentary activity in this dataset reflect the ongoing political events in the country? ,{}JSON,,"[politicians, politics]",CC0,,,33,639,254,Sessions of the Verkhovna Rada,Ukrainian Parliament Daily Agenda Results,https://www.kaggle.com/subota/ukrainian-parliament-daily-agenda-result,Wed Oct 18 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[Area_Name, Year, Group_Name, Sub_Group_Name, Cases_Property_Recovered, Cases_Property_Stolen, Value_of_Property_Recovered, Value_of_Property_Stolen]","[string, numeric, string, string, numeric, numeric, numeric, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context This dataset contains complete information about various aspects of crimes happened in India from 2001. There are many factors that can be analysed from this dataset. Over all I hope this dataset helps us to understand better about India. Content  I  Cases Reported and their Disposal by Police and Court Indian Penal Code Special & Local Laws IA  SC/ST Cases Reported and their Disposal by Police and Court Crime against SCs Crime against STs IB  Children Cases Reported and their Disposal by Police and Court Abetment of Suicide (Section 305 IPC) Buying of Girls for Prostitution (Section 373 IPC) Child Marriage Restraint Act 1929 Exposure and Abandonment (Section 317 IPC) Foeticide (Section 315 and 316 IPC) Infanticide (Section 315 IPC) Kidnapping & Abduction (Section 360361363363-A 363 read with Section 384 366 367 & 369 IPC) Murder (Section 302 315 IPC) Other Crimes against Children  Other Murder of Children (Section 302 IPC) Procuration of Minor Girls (Section 366-A IPC) Rape (Section 376 IPC) Selling of Girls for Prostitution (Section 372 IPC) Total Crimes against Children  II  Persons Arrested and their Disposal by Police and Court Indian Penal Code Special and Local Laws IIA  SC/ST Persons Arrested and their Disposal by Police and Court Crime against SCs Crime against STs IIB  Children Persons Arrested and their Disposal by Police and Court Abetment of suicide (Section 305 IPC) Buying of girls for prostitution (Section 373 IPC) Child Marriage Restraint Act 1929 Exposure and Abandonment (Section 317 IPC) Foeticide (Section 315 and 316 IPC) Kidnapping & Abduction (Section 360361363363-A 366 367 & 369 IPC) Murder - Infanticide (Section 315 IPC) Murder - Other Murder of Children  Murder (Section 302 315 IPC) Other Crimes against Children Procuration of minor girls (Section 366-A IPC) Rape (Section 376 IPC) Selling of girls for prostitution (Section 372 IPC) Total Crimes against Children IV  Persons Arrested by Sex and Age Group Indian Penal Code Special & Local Laws V  Juveniles Apprehended Indian Penal Code Special & Local Laws VI  Juveniles Arrested and their Disposal VII  Property Stolen & Recovered (Crime Head) Dacoity Robbery Burglary Theft Criminal Breach of Trust Other Property Total Property Stolen & Recovered VIII  Property Stolen & Recovered (Nature of Property) Communation and Electricity Wire Cattle Cycle Motor Vehicles Motor Vehicles - Motor Cycle/Scooters Motor Vehicles - Motor Car/Taxi/Jeep Motor Vehicles - Other Motor Vehicles Fire Arms Explosives/Explosive Substances Electronic Components Cultural Property including Antiques Other kinds of Property Total Property Stolen & Recovered IX  Police Strength (Actual & Sanctioned) A) Actual Civil Police (Incl. District Armed Police and Women Police) A)  Acual Armed Police (Incl. Women Police) A)  Actual Police Strength (Incl. Women) B) Acual Women Civil Police (Incl. District Armed Force) B) Actual Women Armed Police B) Actual Women Police Strength C) Sanctioned Civil Police (Incl. District Armed Police) C) Santioned Armed Police (Incl. Women Police) C) Santioned Police Strength (Incl. Women) D) Sanctioned Women Civil Police (Incl. District Armed Police) D) Sanctioned Women Armed Police D) Sanctioned Women Police Strength X  Police Personnel Killed or Injured on duty Constables Head Constables Assistant Sub-Inspectos Sub-Inspectors Inspectors Gazetted Officers Total Police Killed or Injured X-B  Age Profile of Police Personnel Killed on Duty X-C  Natural Deaths and Suicides of Police Personnel Natural Deaths of Police Personnel (while in service) Police Personnel Committed Suicide XI  Casualties under Police Firing and LathiCharge Riot Control Anti Dacoity Operations Against Extremists & Terrorists Against Others Total Casualties XII  Cases Reported Value of Property Stolen under Dacoity Robbery Burglary and Theft by Place of Occurance Residential Premises Highways River and Sea Railways 4.1 In Running Trains 4.2 Others Banks Commercial Establishments (Shops etc.) Other Places Total  XIII  Particulars of Juveniles Arrested Education Economic Setup Family Background Recidivism XIV  Motive/Cause of Murder and Culpable Homicide not Amounting to Murder XV  Victims of Rape(Age Group-wise) Incest Rape Cases Other Rape Cases (Otherthan Incest) Total Rape Cases XV-A  Rape Offenders relation nearness to Rape Victims XVI  Persons Arrested under Recidivism XVII  Anti Corruption - Cases XVIII  Anti Corruption - Arrests XIX  Complaints/Cases Against Police Personnel Complaints Received/Cases Registered Police Personnel Involved/Action Taken Departmental Action/Punishments *XX  Police Budget and Infrastructure Equipments and Transport Support Distribution of Police Stations by Crime Incidences Distribution of Police Stations by Police Strength Organisational Set Up SCs/STs and Muslims in Police Force (Actual) XXI  1.  Nature of Complaints Received by Police XXI  2. Trial of Violent Crimes by Courts Murder Attempt to Murder C H Not Amounting to Murder Rape Kidnapping & Abduction 5.1 Kidnapping & Abduction of Women & Girls 5.2 Kidnapping & Abduction of Others Dacoity Preparation & Assembly for Dacoity Robbery Riots Arson Dowry Deaths Total Trials (Sum of 1-11 Above) XXI  3. Period of Trials by Courts District/Session Judge Additional Session Judge Chief Judicial Magistrate Judicial Magistrate (I) Judicial Magistrate (II) Special Judicial Magistrate Other courts  Total Trials (Sum of 1-7 Above) XXI  4.1 Autho Theft (Stolen & Recovered) Motor Cycles/ Scooters Motor Car/Taxi/Jeep Buses Goods carrying vehicles (Trucks/Tempo etc) Other Motor vehicles Total (Sum of 1-5 Above) XXI  4.2 Serious Fraud Criminal Breach of Trust Cheating  XXI  5.1 Victims of Murder (Age & Sex-Wise) Male Victims Female Victims Total XXI  5.2 Victims of CH not Amounting to Murder (Age & Sex-wise) Male Victims Female Victims Total XXI  5.3 Use of FireArms in Murder Cases XXI  6. Human Rights Violation by Police Disappearance of Persons Illegal Detention/Arrests Fake Encounter Killings Violation Against Terrorists/Extremists Extortion Torture False Implication Failure in Taking Action Indignity to Women Atrocities on SC/ST Others Total (Sum of 1-11 Above) XXI  7. Police Housing For Officers (Dy.SP & Above) Upper SubOrdinates (ASI to Inspectos) Lower SubOrdinates (Constables Head Constables & Class-IV Subordinate Staff) XXI  8. Home Guards and Auxilliary force XXI  9. Unidentified Deadbodies Recovered & Inquest conducted XXI  10. Victims of Kidnapping & Abduction for Specific Purpose For Adoption For Begging for Camel Racing For Illicit Intercourse For Marriage For Prostitution For Ransom For Revenge For Sale For Selling Bodyparts For Slavery For Unlawful Activity Other Purposes Total (Sum of 1-13 Above) XXI  11. Custodial Deaths Deaths in Custody/Lockup of Persons Remanded to Police Custody by Court Deaths in Custody/Lockup of Persons Not Remanded to Police Custody by Court Deaths in Custody during production/process in courts/journey connected with investigation Deaths during Hospitalisation/Treatment Deaths due to Other Reasons XXI  12. Escapes from Police Custody Cases under Crime Against Women Rape Kidnapping & Abduction of Women & Girls Dowry Deaths Molestation Sexual Harassment Cruelty by Husband and Relatives Importation of Girls Immoral Traffic Prevention Act 1956 Dowry Prohibition Act 1961 Indecent Representation of Women(Prohibition) Act 1986 Sati Prevention Act 1987 Total Crimes Against Women Arrests under Crime Against Women Rape Kidnapping & Abduction of Women & Girls Dowry Deaths Molestation Sexual Harassment Cruelty by Husband and Relatives Importation of Girls Immoral Traffic Prevention Act 1956 Dowry Prohibition 1961 Indecent Representation of Women(Prohibition) Act 1986 Sati Prevention Act 1987 Total Crimes Against Women  Some of the data contains district level data. The districts are police districts and also include special police unit. Therefore these may be different from revenue districts. Most of the data is from 2001 to 2010. But there are few files which has data only from 2011 and few are having 2001-14. Inspiration There could be many things one can understand by analyzing this dataset. Few inspirations for you to start with.  What is the major reason people being kidnapped in each and every state? Offenders relation to the rape victim Juveniles family background education and economic setup. Which state has more crime against children and women? Age group wise murder victim Crime by place of occurrence. Anti corruption cases vs arrests. Which state has more number of complaints against police? Which state is the safest for foreigners?  Acknowledgements National Crime Records Bureau (NCRB) Govt of India has published this dataset  on their website  and also has shared on Open Govt Data Platform India portal under Govt. Open Data License - India.,CSV,,"[india, crime]",CC4,,,1675,9987,12,State-wise data from 2001 is classified according to 40+factors. (75+ csv files),Crime in India,https://www.kaggle.com/rajanand/crime-in-india,Fri Sep 01 2017
,Gabriele Baldassarre,[],[],About A dataset containing the attributes and the ratings for around 94000 among board games and expansions as get from BoardGameGeek. Resources The same data and the scripts used to crawl it from BoardGameGeek are available in the form of R package on Github.,SQLite,,[board games],Other,,,2582,19416,140,What makes a game a good game?,Board Games Dataset,https://www.kaggle.com/gabrio/board-games-dataset,Sun Jun 25 2017
,US Census Bureau,"[time_series_code, date, value]","[string, dateTime, numeric]",Context Along with their core mission of counting the US population the United States Census Bureau gathers a wide range of economic data. This dataset covers 16 of their economic reports and surveys  Advance Monthly Sales for Retail and Food Services Construction Spending Housing Vacancies and Homeownership Manufactured Housing Survey (1980-2013) Manufactured Housing Survey (Current) Manufacturers' Shipments Inventories and Orders Manufacturing and Trade Inventories and Sales Monthly Retail Trade and Food Services Monthly Wholesale Trade Sales and Inventories New Home Sales New Residential Construction Quarterly Financial Report Quarterly Services Survey Quarterly Summary of State & Local Taxes Quarterly Survey of Public Pensions U.S. International Trade in Goods and Services  Content  The data csv is arranged in a long format with the time_series_code column tying it back to the metadata csv. If you're trying to figure out what data is available you'll want to start with the metadata. Just over a third of the time series store error codes usually confidence intervals rather than actual values. The metadata for these time series will have values in the columns et_code et_desc and et_unit. All of the dates are stored as complete beginning of the period dates but all of the time series are at either monthly quarterly or annual resolution. Exact days and months are provided for convenience when aligning time series and so that you don't have to unpack period codes like 'Q22009'. There may be many time series bundled under a given data category or description. For example the largest category (taxes) contains dozens of types of tax categories and each of those contains a separate time series for each state in the country.  Two of the error code time series have non-numeric values. To convert the values column into reasonable units you'll need to drop all entries equal to the string Less than .05 percent. The data have been substantially reformatted from how they are provided by the Census Bureau. You can find the script I used to prepare the data here.  Acknowledgements This data was kindly made available by the United States Census. You can find the original data here. If you enjoyed this dataset you might also like one of the  other US Census datasets available on Kaggle. Inspiration  The National Bureau of Economic Research's macroeconomic history of the United States covers many similar time series but before the census data was reported. Can you integrate it with this census data? This should allow you to generate many time series stretching from the present back to the 19th century. ,CSV,,"[finance, economics]",CC0,,,431,4220,40,"7,000 economics time series for 1956-2017",Business and Industry Reports,https://www.kaggle.com/census/business-and-industry-reports,Wed Oct 18 2017
,Shruti Bhargava,"[stn_code, sampling_date, state, location, agency, type, so2, no2, rspm, spm, location_monitoring_station, pm2_5, date]","[numeric, string, string, string, string, string, numeric, numeric, string, string, string, string, dateTime]",Context Since industrialization there has been an increasing concern about environmental pollution. As mentioned in the WHO report 7 million premature deaths annually linked to air pollution  air pollution is the world's largest single environmental risk. Moreover as reported in the NY Times article India’s Air Pollution Rivals China’s as World’s Deadliest it has been found that India's air pollution is deadlier than even China's. Using this dataset one can explore India's air pollution levels at a more granular scale. Content This data is combined(across the years and states) and largely clean version of the Historical Daily Ambient Air Quality Data released by the Ministry of Environment and Forests and Central Pollution Control Board of India under the National Data Sharing and Accessibility Policy (NDSAP). Visualization of the Mean RSPM values over the years Inspiration Can we detect local trends? Can we relate the air quality changes to changes in Environmental policy in India? Acknowledgements Vishal Subbiah (Data downloading),CSV,,"[india, pollution]",Other,,,1141,6406,60,India's air pollution levels over the years,India Air Quality Data,https://www.kaggle.com/shrutibhargava94/india-air-quality-data,Sat Jul 22 2017
,Aleksey Bilogur,[],[],"Context r/Place was a wildly successful April Fool's joke perpetrated by Reddit over the course of 72 hours April 1-3 2017. The rules of Place quoting u/Drunken_Economist were  There is an empty canvas. You may place a tile upon it but you must wait to place another. Individually you can create something. Together you can create something more.  1.2 million redditors used these premises to build the largest collaborative art project in history painting (and often re-painting) a million-pixel canvas with 16.5 million tiles in 16 colors. The canvas started out completely blank and ended looking like this  How did that happen? Content This dataset is a full time placement history for r/place over time. Each record is a single move one user changing one pixel to one of 15 different colors. Acknowledgements This data was published as-is by Reddit. Inspiration Users were heavily rate-limited in their ability to place pixels so this dataset shows what happens when users of similar stripes ""band together"" to build something greater than themselves. With a pixel-by-pixel history what can you tell about the relative popularity of different regions in the figure? Can you use image analysis techniques to segment the image into different regions and measure what happens to them over time?",CSV,,"[popular culture, internet]",Other,,,47,1001,494,All 16 million moves in the r/place collaborative art project,Reddit r/Place History,https://www.kaggle.com/residentmario/reddit-rplace-history,Wed Sep 20 2017
,oscarleo,"[BOROUGH, ZIP CODE, LATITUDE, LONGITUDE, LOCATION, ON STREET NAME, CROSS STREET NAME, OFF STREET NAME, NUMBER OF PERSONS INJURED, NUMBER OF PERSONS KILLED, NUMBER OF PEDESTRIANS INJURED, NUMBER OF PEDESTRIANS KILLED, NUMBER OF CYCLIST INJURED, NUMBER OF CYCLIST KILLED, NUMBER OF MOTORIST INJURED, NUMBER OF MOTORIST KILLED, CONTRIBUTING FACTOR VEHICLE 1, CONTRIBUTING FACTOR VEHICLE 2, CONTRIBUTING FACTOR VEHICLE 3, CONTRIBUTING FACTOR VEHICLE 4, CONTRIBUTING FACTOR VEHICLE 5, UNIQUE KEY, VEHICLE TYPE CODE 1, VEHICLE TYPE CODE 2, VEHICLE TYPE CODE 3, VEHICLE TYPE CODE 4, VEHICLE TYPE CODE 5, datetime]","[string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, dateTime]",Context I created this data set to help with the New York City Taxi Trip Duration playground. I used OSRM to extract information about the fastest routes for each data point.  I think it will be very useful for anyone doing work in that competition. Please try it out and tell me what you want me to add. I intend to improve and add features. Content starting_street The street where the taxi-trip starts. In version 1 this field contained a lot of empty values. That has been dealt with. end_street The street where the taxi-trip ends. In version 1 this field contained a lot of empty values. That has been dealt with. total_distance The total distance is measured between the pickup coordinates and the drop-off coordinates in train.csv and test.csv. The unit is meters. total_travel_time The total travel time for that data point in seconds number_of_steps The number of steps on that trip. One step consists of some driving and an action the taxi needs to perform. It can be something like a turn or going on to a highway. See step_maneuvers for more information. street_for_each_step A list of streets where each step occurs. Multiple steps can be performed on the same street. Therefore there might the same street might occur multiple times. (The values are stored as a string separated by '|') distance_per_step The distance for each step. (The values are stored as a string separated by '|') travel_time_per_step The travel time for each step (The values are stored as a string separated by '|') step_maneuvers The action (or maneuver) performed in each step. The possible maneuvers are  turn a basic turn  new name no turn is taken/possible but the road name changes. depart The trip starts arrive The trip ends merge Merge onto a street (e.g. getting on the highway from a ramp) on ramp Entering a highway (direction given my  modifier ) off ramp Exiting a highway fork The road forks end of road The road ends in a T intersection continue Turn to stay on the same road roundabout   A roundabout rotary A traffic circle (a bigger roundabout) roundabout turn  A small roundabout that can be treated as a regular turn  (The values are stored as a string separated by '|') step_direction The direction for each action (or maneuver) step_location_list The coordinates for each action (or maneuver),CSV,,[taxi services],ODbL,,,3714,12123,2048,Helpful dataset for New York Taxi Playground,New York City Taxi with OSRM,https://www.kaggle.com/oscarleo/new-york-city-taxi-with-osrm,Sun Aug 06 2017
,City of New York,"[Year of Birth, Gender, Ethnicity, Child's First Name, Count, Rank]","[numeric, string, string, string, numeric, numeric]",Context Baby names for children recently born in New York City. This dataset is notable because it includes a breakdown by the ethnicity of the mother of the baby a source of ethnic information that is missing from many other similar datasets published on state and national levels. Content This dataset includes columns for the name year of birth sex and mother's ethnicity of the baby. It also includes a rank column (that name's popularity relative to the rest of the names on the list). Acknowledgements This data is published as-is by the City of New York. Inspiration  How do baby names in New York City differ from national trends? What names are most more or less popular amongst different ethnicities? ,CSV,,[children],CC0,,,265,1467,0.849609375,Baby names popular in New York City.,NYC Baby Names,https://www.kaggle.com/new-york-city/nyc-baby-names,Sat Sep 09 2017
,Rachael Tatman,[],[],"Context Linked data “Linked open data is linked data that is open content. In computing linked data (often capitalized as Linked Data) is a method of publishing structured data so that it can be interlinked and become more useful through semantic queries. It builds upon standard Web technologies such as HTTP RDF and URIs but rather than using them to serve web pages for human readers it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.” -- “Linked Open Data” on Wikipedia Anime “Anime is a Japanese term for hand-drawn or computer animation. The word is the abbreviated pronunciation of ""animation"" in Japanese where this term references all animation. Outside Japan anime is used to refer specifically to animation from Japan or as a Japanese-disseminated animation style often characterized by colorful graphics vibrant characters and fantastical themes.” -- “Anime” on Wikipedia This dataset is a linked open dataset that contains information on 391706 anime titles. Content This dataset contains two files. The first is the native N-Triples format which is suitable for tasks. The second is a .csv containing three columns  Anime the title of the anime Concept the concept Value the value of the concept for that anime  The .csv is not a true linked data dataset since it has removed many of the relevant URL’s. However it should prove easier for data analysis. Acknowledgements This dataset has been collected and maintained by Pieter Heyvaert. It is © Between Our Worlds and reproduced here under an MIT license. You can find more information on this dataset and the most recent version here.  Inspiration  Many anime have summaries under the “description” concept. Can you use these to identify common themes in anime? What about training an anime description generator?  Can you plot the number of titles released over time? Has the rate of anime production increased or decreased over time? ",CSV,,"[popular culture, information technology, internet]",Other,,,70,1314,97,"A Linked Open Dataset of Over 390,000 Anime",Between Our Worlds: An Anime Ontology,https://www.kaggle.com/rtatman/between-our-worlds-an-anime-ontology,Sat Sep 09 2017
,meep,"[ppd_id, PlanName, fy, system_id, ValuationID, PlanFullName, source_PlanBasics, InPFS, FiscalYearType, PlanInceptionYear, PlanClosed, PlanYearClosed, AdministeringGovt, StateAbbrev, StateName, GovtName, PlanType, EmployeeTypeCovered, SocSecCovered, SocSecCovered_verbatim, CostStructure, EmployerType, CostSharing, StateEmployers, LocalEmployers, SchoolEmployers, CoversStateEmployees, CoversLocalEmployees, CoversTeachers, StateGenEE, LocalGenEE, StatePolice, LocalPolice, StateFire, LocalFire, Teacher, SchoolEEs, JudgesAttorneys, ElectedOfficials, BenefitsWebsite, ReportingDateNotes, EEGroupID, TierID, CAFR_CY, ActRpt_CY, CAFR_AV_Conflict, ActRptDate, fye, DataEntryCode, source_GASBAssumptions, ActCostMeth_GASB, AssetValMeth_GASB, FundingMeth_GASB, COLA_Verabatim, COLA_Code, InflationAssumption_GASB, InvestmentReturnAssumption_GASB, ActCostMethCode_GASB, AssetValMethCode_GASB, AssetSmoothingPeriod_GASB, FundingMethCode1_GASB, FundingMethCode2_GASB, UAALAmortPeriod_GASB, BlendedDiscountRate, ActValDate_GASBAssumptions, source_FundingAndMethods, AssetValMeth, PhaseIn, AssetValMeth_note, ActCostMeth, ActCostMeth_note, FundingMeth, FundingMeth_note, MktAssets_Smooth, ActAssets_Smooth, NetFlows_smooth, AssetValMethCode, SmoothingReset, GainlossConcept, GainLossBase_1, GainLossBase_2, GainLoss, GainLossPeriod, PhaseInPercent, PhaseInPeriods, PhaseInType, GainLossRecognition, AssetSmoothingBaseline, ExpectedReturnMethod, AddSubtractGainLoss, UpperCorridor, LowerCorridor, ActCostMethCode, FundMethCode_1, FundMethCode_2, PayrollGrowthAssumption, TotAmortPeriod, RemainingAmortPeriod, UAALYearEstablished, WageInflation]","[numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, dateTime, dateTime, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context The Public Plans Dataset compiles publicly-available information on major U.S. public pension plans from their annual reporting.  This includes state-level plans as well as some large plans from localities such as New York City and Chicago. Content The Public Plans Dataset has data compiled from the actuarial reports and CAFRs (comprehensive annual financial reports) for major plans. It includes balance sheet (assets/liabilities) income (e.g. investment income) and cash flow (e.g. benefit payments) information. In addition there are items such as number of beneficiaries receiving payment the number of active participants the number vested etc. The current set covers fiscal years 2001 - 2016 which usually run mid-year to mid-year (this can differ by plan).   Acknowledgements Data from Public Plans Data http//publicplansdata.org/public-plans-database/ The Public Plans Data project comes out of a partnership between the Center for Retirement Research at Boston College (CRR) and the Center for State and Local Government Excellence (SLGE). The National Association of State Retirement Administrators (NASRA) which has been collecting and sharing public plan data since 2001 supports the partnership by providing review and assistance on the development of data models validation of data and development and administration of surveys. More here http//publicplansdata.org/about/our-research/ Cover Photo by ashutosh nandeshwar on Unsplash Inspiration The Center for Retirement Research often produces research briefs based on this data which can be found here http//publicplansdata.org/research/issue-briefs/?category=crr Questions I have been pursuing using this data  What has been driving decreasing funded ratios in U.S. pensions? Which pension plans are sustainable? Which are in trouble? Is there a link between pension fundedness and investment strategy?  It may be useful to link these data sets to information on various state/local revenue amounts in order to determine sustainability of pension costs.,CSV,,"[finance, government, demographics]",CC4,,,155,1308,2,"Trends in Investments, Benefits, and Funding for U.S. Public Pensions","U.S. Public Pensions Data, fiscal years 2001-2016",https://www.kaggle.com/meepbobeep/us-public-pensions-data-fiscal-years-20012016,Tue Jan 02 2018
,BurakH,[],[],Context This dataset contains ground state energies of 16242 molecules calculated by quantum mechanical simulations.  Content The data contains 1277 columns. The first 1275 columns are entries in the Coulomb matrix that act as molecular features. The 1276th column is the Pubchem Id where the molecular structures are obtained. The 1277th column is the atomization energy calculated by simulations using the Quantum Espresso package. In the csv file the first column (X1) is the data index and unused.  Past Research The data is used for a publication in Journal of Chemical Physics. A blog post was also published explaining the data and the research behind it in less technical terms.  A Github repository is available that contains the source code used for generating the data as well as some of the R scripts used for analysis.  Inspiration Simulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database of simulations. If this can be done with high accuracy properties of new molecules can be calculated using the trained model. This could open up many possibilities in computational design and discovery of molecules compounds and new drugs.  The purpose is to use the 1275 molecular features to predict the atomization energy. This is a regression problem so mean squared error is minimized during training.  I am looking for Kagglers to find the best model and reduce mean squared error as much as possible! ,CSV,,"[chemistry, physics]",CC0,,,338,3890,162,Predict molecular properties from a database of atomic level simulations,"Ground State Energies of 16,242 Molecules",https://www.kaggle.com/burakhmmtgl/energy-molecule,Wed Apr 12 2017
,PromptCloud,"[additional_info, address, area, city, country, crawl_date, guest_recommendation, hotel_brand, hotel_category, hotel_description, hotel_facilities, hotel_star_rating, image_count, latitude, locality, longitude, pageurl, point_of_interest, property_id, property_name, property_type, province, qts, query_time_stamp, review_count_by_category, room_area, room_count, room_facilities, room_type, similar_hotel, site_review_count, site_review_rating, site_stay_review_rating, sitename, state, uniq_id]","[string, string, string, string, string, dateTime, numeric, string, string, string, string, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, dateTime, dateTime, string, string, numeric, string, string, string, numeric, numeric, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 33344 hotels) that was created by extracting data from goibibo.com a leading travel site from India. Content This dataset has following fields  address area - The sub-city region that this hotel is located in geographically. city country - Always India. crawl_date guest_recommendation - How many guests that stayed here have recommended this hotels to others on the site. hotel_brand - The chain that owns this hotel if this hotel is part of a chain. hotel_category hotel_description - A hotel description as provided by the lister. hotel_facilities -  hotel_star_rating - The out-of-five star rating of this hotel. image_count - The number of images provided with the listing. latitude locality longitude pageurl point_of_interest - Nearby locations of interest. property_name property_type - The type of property. Usually a hotel. province qts - Crawl timestamp. query_time_stamp - Copy of qts. review_count_by_category - Reviews for the hotel broken across several different categories. room_area room_count room_facilities room_type similar_hotel site_review_count - The number of reviews for this hotel left on the site by users. site_review_rating - The overall rating for this hotel by users. site_stay_review_rating sitename - Always goibibo.com state uniq_id  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration  Try exploring some of the amenity categories. What do you see? Try applying some natural language processing algorithms to the hotel descriptions. What are the some common words and phrases? How do they relate to the amenities the hotel offers? What can you discover by drilling down further into hotels in different regions? ,CSV,,"[india, hotels, internet]",CC4,,,210,1493,9,"4,000 Indian hotels on Goibibo",Indian Hotels on Goibibo,https://www.kaggle.com/PromptCloudHQ/hotels-on-goibibo,Fri Sep 15 2017
,Dan,"[, ID, Likes, Replies, Retweets, Time, Tweet]","[numeric, numeric, numeric, numeric, numeric, dateTime, string]","Context Tweets containing Hurricane Harvey from the morning of 8/25/2017. I hope to keep this updated if computer problems do not persist.  *8/30 Update This update includes the most recent tweets tagged ""Tropical Storm Harvey"" which spans from 8/20 to 8/30 as well as the properly merged version of dataset including Tweets from when Harvey before it was downgraded back to a tropical storm.  Inspiration What are the popular tweets? Can we find popular news stories from this? Can we identify people likely staying or leaving and is there a difference in sentiment between the two groups? Is it possible to predict popularity with respect to retweets likes and shares?",CSV,,"[weather, internet]",CC0,,,388,4620,71,Recent tweets on Hurricane Harvey,Hurricane Harvey Tweets,https://www.kaggle.com/dan195/hurricaneharvey,Sat Sep 09 2017
,City of Chicago,"[taxi_id, trip_start_timestamp, trip_end_timestamp, trip_seconds, trip_miles, pickup_census_tract, dropoff_census_tract, pickup_community_area, dropoff_community_area, fare, tips, tolls, extras, trip_total, payment_type, company, pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude]","[numeric, dateTime, dateTime, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric]",Context This dataset includes taxi trips for 2016 reported to the City of Chicago in its role as a regulatory agency. To protect privacy but allow for aggregate analyses the Taxi ID is consistent for any given taxi medallion number but does not show the number Census Tracts are suppressed in some cases and times are rounded to the nearest 15 minutes. Due to the data reporting process not all trips are reported but the City believes that most are. See http//digital.cityofchicago.org/index.php/chicago-taxi-data-released for more information about this dataset and how it was created. Content Please see the data dictionary for details of specific fields. We also shrunk the original files by roughly two thirds by dropping redundant columns and remapping several others to use shorter IDs. For example the taxi_id column used to be a 128 character string. We’ve replaced it with an integer containing at most four digits. The redundant columns were unique_key pickup_location and dropoff_location. The remapped columns were taxi_id company pickup_census_tract dropoff_census_tract pickup_latitude pickup_longitude dropoff_latitude and dropoff_longitude. The original versions of those columns can be unpacked using the column_remapping.json. Acknowledgements This dataset was kindly made publically available by the City of Chicago at https//data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew Please note that this site provides applications using data that has been modified for use from its original source www.cityofchicago.org the official website of the City of Chicago.  The City of Chicago makes no claims as to the content accuracy timeliness or completeness of any of the data provided at this site.  The data provided at this site is subject to change at any time.  It is understood that the data provided at this site is being used at one’s own risk. Inspiration  How centralized is Chicago? In other words what portion of trips are to or from downtown? Chicago has an extensive metro system. Are taxis competing with the trains by covering similar routes or supplementing public transit by getting people to and from train stations?  Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/chicago-taxi. BigQuery hosts the full version of this dataset which extends from 2013 through the present.,CSV,,[road transport],Other,,,2566,8498,2048,Details of taxi rides in Chicago,Chicago Taxi Rides 2016,https://www.kaggle.com/chicago/chicago-taxi-rides-2016,Thu Jul 06 2017
,Rachael Tatman,[],[],"Context In working on Unicode implementations it is often useful to access the full content of the Unicode Character Database (UCD). For example in establishing mappings from characters to glyphs in fonts it is convenient to see the character scalar value the character name the character East Asian width along with the shape and metrics of the proposed glyph to map to; looking at all this data simultaneously helps in evaluating the mapping. This is a machine-readable version of the Unicode Character Database in JSON format. Content The majority of information about individual codepoints is represented using properties. Each property except for the Special_Case_Condition and Name_Alias properties is represented by an attribute. In an XML data file the absence of an attribute (may be only on some code-points) means that the document does not express the value of the corresponding property. Conversely the presence of an attribute is an expression of the corresponding property value; the implied null value is represented by the empty string. The Name_Alias property is represented by zero or more name-alias child elements. Unlike the situation for properties represented by attributes it is not possible to determine whether all of the aliases have been represented in a data file by inspecting that data file. The name of an attribute is the abbreviated name of the property as given in the file PropertyAliases.txt in version 6.1.0 of the UCD. For the Unihan properties the name is that given in the various versions of the Unihan database (some properties are no longer present in version 6.1.0). For catalog and enumerated properties the values are those listed in the file PropertyValueAliases.txt in version 6.1.0 of the UCD; if there is an abbreviated name it is used otherwise the long name is used. Note that the set of possible values for a property captured in this schema may change from one version to the next. The following properties are associated with code points  Age property Name properties Name Aliases Block General Category Combining properties Bidirectionality properties Decomposition properties Numeric Properties Joining properties Linebreak properties East Asian Width property Case properties Script properties ISO Comment properties Hangul properties Indic properties Identifier and Pattern and programming language properties Properties related to function and graphic characteristics Properties related to boundaries Properties related to ideographs Miscellaneous properties Unihan properties Tangut data Nushu data  For additional information please consult the full documentation on the Unicode website. Acknowledgements Copyright © 1991-2017 Unicode Inc. All rights reserved. Distributed under the Terms of Use in http//www.unicode.org/copyright.html. Permission is hereby granted free of charge to any person obtaining a copy of the Unicode data files and any associated documentation (the ""Data Files"") or Unicode software and any associated documentation (the ""Software"") to deal in the Data Files or Software without restriction including without limitation the rights to use copy modify merge publish distribute and/or sell copies of the Data Files or Software and to permit persons to whom the Data Files or Software are furnished to do so provided that either (a) this copyright and permission notice appear with all copies of the Data Files or Software or (b) this copyright and permission notice appear in associated Documentation.",{}JSON,,"[writing, languages, linguistics]",Other,,,25,398,30,Machine-readable information on Unicode 10.0 Characters,Unicode 10.0 Character Database in JSON,https://www.kaggle.com/rtatman/unicode-100-character-database-in-json,Sat Dec 16 2017
,Ashok Lathwal,"[project_id, final_status]","[string, numeric]",Problem Statement Kickstarter is a community of more than 10 million people comprising of creative tech enthusiasts who help in bringing creative project to life. Till now more than $3 billion dollars have been contributed by the members in fueling creative projects. The projects can be literally anything – a device a game an app a film etc. Kickstarter works on all or nothing basis i.e if a project doesn’t meet it goal the project owner gets nothing. For example if a projects’s goal is $500. Even if it gets funded till $499 the project won’t be a success. Recently Kickstarter released its public data repository to allow researchers and enthusiasts like us to help them solve a problem. Will a project get fully funded ? In this challenge you have to predict if a project will get successfully funded or not. Data Description There are three files given to download train.csv test.csv and sample_submission.csv The train data consists of sample projects from the May 2009 to May 2015. The test data consists of projects from June 2015 to March 2017.,CSV,,"[business, finance]",Other,,,1008,8322,57,Predict if a project will get successfully funded or not using labeled data,Funding Successful Projects on Kickstarter,https://www.kaggle.com/codename007/funding-successful-projects,Tue Jun 20 2017
,Rachael Tatman,[],[],Context Everyone who speaks a language speaks it with an accent. A particular accent essentially reflects a person's linguistic background. When people listen to someone speak with a different accent from their own they notice the difference and they may even make certain biased social judgments about the speaker. The speech accent archive is established to uniformly exhibit a large set of speech accents from a variety of language backgrounds. Native and non-native speakers of English all read the same English paragraph and are carefully recorded. The archive is constructed as a teaching tool and as a research tool. It is meant to be used by linguists as well as other people who simply wish to listen to and compare the accents of different English speakers. This dataset allows you to compare the demographic and linguistic backgrounds of the speakers in order to determine which variables are key predictors of each accent. The speech accent archive demonstrates that accents are systematic rather than merely mistaken speech. All of the linguistic analyses of the accents are available for public scrutiny. We welcome comments on the accuracy of our transcriptions and analyses. Content This dataset contains 2140 speech samples each from a different talker reading the same reading passage. Talkers come from 177 countries and have 214 different native languages. Each talker is speaking in English.  This dataset contains the following files  reading-passage.txt the text all speakers read speakers_all.csv demographic information on every speaker recording a zipped folder containing .mp3 files with speech  Acknowledgements This dataset was collected by many individuals (full list here) under the supervision of Steven H. Weinberger. The most up-to-date version of the archive is hosted by George Mason University. If you use this dataset in your work please include the following citation  Weinberger S. (2013). Speech accent archive. George Mason University. This datasets is distributed under a CC BY-NC-SA 2.0 license. Inspiration The following types of people may find this dataset interesting  ESL teachers who instruct non-native speakers of English Actors who need to learn an accent Engineers who train speech recognition machines Linguists who do research on foreign accent Phoneticians who teach phonetic transcription Speech pathologists Anyone who finds foreign accent to be interesting ,Other,,"[languages, acoustics, linguistics]",CC4,,,161,1849,863,Parallel English speech samples from 177 countries,Speech Accent Archive,https://www.kaggle.com/rtatman/speech-accent-archive,Tue Nov 07 2017
,Miroslav Zoricak,"[maker, model, mileage, manufacture_year, engine_displacement, engine_power, body_type, color_slug, stk_year, transmission, door_count, seat_count, fuel_type, date_created, date_last_seen, price_eur]","[string, string, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, string, dateTime, dateTime, numeric]",Context The data was scraped from several websites in Czech Republic and Germany over a period of more than a year. Originally I wanted to build a model for estimating whether a car is a good buy or a bad buy based on the posting. But I was unable to create a model I could be satisfied with and now have no use for this data. I'm a great believer in open data so here goes. Content The scrapers were tuned slowly over the course of the year and some of the sources were completely unstructured so as a result the data is dirty there are missing values and some values are very obviously wrong (e.g. phone numbers scraped as mileage etc.) There are roughly 35 Million rows and the following columns  maker - normalized all lowercase model - normalized all lowercase mileage - in KM manufacture_year engine_displacement - in ccm engine_power - in kW body_type - almost never present but I scraped only personal cars no motorcycles or utility vehicles color_slug - also almost never present stk_year - year of the last emission control transmission - automatic or manual door_count seat_count fuel_type - gasoline diesel cng lpg electric date_created - when the ad was scraped date_last_seen - when the ad was last seen. Our policy was to remove all ads older than 60 days price_eur - list price converted to EUR  Inspiration  Which factors determine the price of a car? With what accuracy can the price be predicted? Can a model trained on all cars be used to accurately predict prices of models with only a few samples?  In my analysis there is too much variance even within individual models to reliably predict the price can you prove me wrong? I would love to understand what I did wrong if you can.,CSV,,[automobiles],CC0,,,1774,10620,400,Used cars for sale in Germany and Czech Republic since 2015,Classified Ads for Cars,https://www.kaggle.com/mirosval/personal-cars-classifieds,Fri Mar 17 2017
,PromptCloud,"[uniq_id, url, restaurant_id, restaurant_location, name, category, title, review_date, review_text, author, author_url, location, rating, food, value, service, visited_on]","[string, string, string, string, string, string, string, dateTime, string, string, string, string, string, string, string, string, dateTime]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 1.8 million restaurants) that was created by extracting data from Tripadvisor.co.uk. Content This dataset has following fields  uniq_id url restaurant_id restaurant_location name category title review_date review_text author author_url location rating food value service visited_on  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of the restaurant reviews and ratings can be performed.,CSV,,[internet],CC4,,,423,2722,15,"20,000 restaurant's reviews on TripAdvisor.co.uk",London-based restaurants' reviews on TripAdvisor,https://www.kaggle.com/PromptCloudHQ/londonbased-restaurants-reviews-on-tripadvisor,Sat Sep 16 2017
,Adam Mathias Bittlingmayer,[],[],This dataset consists of a few million Amazon customer reviews (input text) and star ratings (output labels) for learning how to train fastText for sentiment analysis. The idea here is a dataset is more than a toy - real business data on a reasonable scale - but can be trained in minutes on a modest laptop. Content The fastText supervised learning tutorial requires data in the following format __label__<X> __label__<Y> ... <Text>  where X and Y are the class names.  No quotes all on one line. In this case the classes are  __label__1 and  __label__2 and there is only one class per row. __label__1 corresponds to 1- and 2-star reviews and  __label__2 corresponds to 4- and 5-star reviews. (3-star reviews i.e. reviews with neutral sentiment were not included in the original)  The review titles followed by '' and a space are prepended to the text. Most of the reviews are in English but there are a few in other languages like Spanish. Source The data was lifted from Xiang Zhang's Google Drive dir but it was in .csv format not suitable for fastText. Training and Testing Follow the basic instructions at fastText supervised learning tutorial to set up the directory. To train  ./fasttext supervised -input train.ft.txt -output model_amzn  This should take a few minutes. To test ./fasttext test model_amzn.bin test.ft.txt  Expect precision and recall of 0.916 if all is in order. You can also train and test in Python see Kernel.,Other,,"[business, linguistics, internet]",Other,,,3358,28177,493,A few million Amazon reviews in fastText format,Amazon Reviews for Sentiment Analysis,https://www.kaggle.com/bittlingmayer/amazonreviews,Wed May 24 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[State, Year, Type_code, Type, Gender, Age_group, Total]","[string, numeric, string, string, string, string, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context This data set contains yearly suicide detail of all the states/u.t of India by various parameters from 2001 to 2012. Content Time Period 2001 - 2012  Granularity Yearly Location States and U.T's of India  Parameters a) Suicide causes b) Education status c) By means adopted d) Professional profile e) Social status Acknowledgements National Crime Records Bureau (NCRB) Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared the historical data on their website,CSV,,"[india, mental health, death, health]",CC4,,,1949,11635,15,Sucides in each state is classified according to various parameters from 2001-12,Suicides in India,https://www.kaggle.com/rajanand/suicides-in-india,Sun Jul 23 2017
,Facebook,[],[],English Word Vectors  About fastText fastText is a library for efficient learning of word representations and sentence classification. One of the key features of fastText word representation is its ability to produce vectors for any words even made-up ones. Indeed fastText word vectors are built from vectors of substrings of characters contained in it. This allows you to build vectors even for misspelled words or concatenation of words. About the vectors These pre-trained vectors contain 1 million word vectors that were learned using Wikipedia 2017 the UMBC webbase corpus and the statmt.org news dataset. In total it contains 16B tokens. The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency. Acknowledgements These word vectors are distributed under the Creative Commons Attribution-Share-Alike License 3.0. P. Bojanowski* E. Grave* A. Joulin T. Mikolov Enriching Word Vectors with Subword Information A. Joulin E. Grave P. Bojanowski T. Mikolov Bag of Tricks for Efficient Text Classification A. Joulin E. Grave P. Bojanowski M. Douze H. Jégou T. Mikolov FastText.zip Compressing text classification models (* These authors contributed equally.),Other,,"[machine learning, pre-trained model]",CC3,,,29,638,658,"Word vectors trained on Wikipedia 2017, UMBC webbase corpus, and statmt.org ",fastText English Word Vectors,https://www.kaggle.com/facebook/fasttext-wikinews,Wed Jan 10 2018
,Golden Oak Research Group,[],[],New Upload Added +32000 more locations. For information on data calculations please refer to the methodology pdf document. Information on how to calculate the data your self is also provided as well as how to buy data for $1.29 dollars. What you get The database contains 32000 records on US Household Income Statistics & Geo Locations. The field description of the database is documented in the attached pdf file. To access all 348893 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to up vote. Up vote right now please. Enjoy! Household & Geographic Statistics  Mean Household Income (double) Median Household Income (double) Standard Deviation of Household Income (double) Number of Households (double)  Square area of land at location (double)  Square area of water at location (double)   Geographic Location  Longitude (double) Latitude (double) State Name (character) State abbreviated (character)  State_Code (character)  County Name (character) City Name (character) Name of city town village or CPD  (character) Primary Defines if the location is a track and block group. Zip Code (character) Area Code (character)                        Abstract The dataset originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36000 files and covers 348893 location records. License Only proper citing is required please see the documentation for details. Have Fun!!! Golden Oak Research Group LLC. “U.S. Income Database Kaggle”. Publication 5 August 2017. Accessed day month year. Sources don't have 2 dollars? Get the full information yourself! 2011-2015 ACS 5-Year Documentation was provided by the U.S. Census Reports. Retrieved August 2 2017 from https//www2.census.gov/programs-surveys/acs/summary_file/2015/data/5_year_by_state/ Found Errors? Please tell us so we may provide you the most accurate data possible. You may reach us at  research_development@goldenoakresearch.com  for any questions you can reach me on at 585-626-2965 please note it is my personal number and email is preferred Check our data's accuracy Census Fact Checker Access all 348893 location records and more Don't settle. Go big and win big.  Optimize your potential. Overcome limitation and outperform expectation. Access all household income records on a scale roughly equivalent to a neighborhood see link below Website Golden Oak Research Kaggle Deals all databases $1.29 Limited time only A small startup with big dreams giving the every day up and coming data scientist professional grade data at affordable prices It's what we do. ,Other,,"[geography, income, finance]",Other,,,1220,6871,6,"+32,000 records, with grandularity on a neighborhood scale (mean, median, Stdev)",US Household Income Statistics,https://www.kaggle.com/goldenoakresearch/us-household-income-stats-geo-locations,Sun Aug 20 2017
,Stack Overflow,"[source, target, value]","[string, string, numeric]",Context On the data team at Stack Overflow we spend a lot of time and energy thinking about tech ecosystems and how technologies are related to each other.  One way to get at this idea of relationships between technologies is tag correlations how often technology tags at Stack Overflow appear together relative to how often they appear separately. One place we see developers using tags at Stack Overflow is on their Developer Stories or professional profiles/CVs/resumes. If we are interested in how technologies are connected and how they are used together developers' own descriptions of their work and careers is a great place to get that. Content A network of technology tags from Developer Stories on the Stack Overflow online developer community website. This is organized as two tables stack_network_links contains links of the network the source and target tech tags plus the value of the the link between each pair stack_network_nodes contains nodes of the network the name of each node which group that node belongs to (calculated via a cluster walktrap) and a node size based on how often that technology tag is used Acknowledgements All Stack Overflow user contributions are licensed under CC-BY-SA 3.0 with attribution required.,CSV,,"[internet, programming languages, networks]",CC3,,,519,4210,0.017578125,Network (links and nodes) of Stack Overflow tags based on Developer Stories,Stack Overflow Tag Network,https://www.kaggle.com/stackoverflow/stack-overflow-tag-network,Fri Sep 29 2017
,PyTorch,[],[],ResNet-34  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,8,494,77,ResNet-34 Pre-trained Model for PyTorch,ResNet-34,https://www.kaggle.com/pytorch/resnet34,Thu Dec 14 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,[],[],Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context When India got independence from British in 1947 the literacy rate was 12.2% and as per the recent census 2011 it is 74.0%. Although it looks an accomplishment still many people are there without access to education.  It would be interesting to know the current status of the Indian education system. Content This dataset contains district and state wise Indian primary and secondary school education data for 2015-16. Granularity Annual List of files   2015_16_Districtwise.csv ( 680 observations and 819 variables ) 2015_16_Statewise_Elementary.csv ( 36 observations and 816 variables ) 2015_16_Statewise_Secondary.csv ( 36 observations and 630 variables  )  Acknowledgements Ministry of Human Resource Development (DISE) has shared the dataset here and also published some reports. Inspiration This dataset provides the complete information about primary and secondary education. There are many inferences can be made from this dataset. There are few things I would like to understand from this dataset.   Drop out ratio in primary and secondary education. (Govt. has made law that every child under age 14 should get free compulsary education.) Various factors affecting examination results of the students. What are all the factors that makes the difference (in literacy rate) between Kerala and Bihar? What could be done to improve the female literacy rate and literacy rate in rural area? ,CSV,,"[india, education]",CC4,,,915,5206,2,"District and state-wise primary & secondary school education data, 2015-16",Education in India,https://www.kaggle.com/rajanand/education-in-india,Mon Aug 14 2017
,Internal Revenue Service,"[STATEFIPS, STATE, zipcode, agi_stub, N1, mars1, MARS2, MARS4, PREP, N2, NUMDEP, TOTAL_VITA, VITA, TCE, VITA_EIC, RAL, RAC, ELDERLY, A00100, N02650, A02650, N00200, A00200, N00300, A00300, N00600, A00600, N00650, A00650, N00700, A00700, N00900, A00900, N01000, A01000, N01400, A01400, N01700, A01700, SCHF, N02300, A02300, N02500, A02500, N26270, A26270, N02900, A02900, N03220, A03220, N03300, A03300, N03270, A03270, N03150, A03150, N03210, A03210, N03230, A03230, N03240, A03240, N04470, A04470, A00101, N18425, A18425, N18450, A18450, N18500, A18500, N18300, A18300, N19300, A19300, N19700, A19700, N04800, A04800, N05800, A05800, N09600, A09600, N05780, A05780, N07100, A07100, N07300, A07300, N07180, A07180, N07230, A07230, N07240, A07240, N07220, A07220, N07260, A07260, N09400]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",ZIP Code data show selected income and tax items classified by State ZIP Code and size of adjusted gross income. Data are based on individual income tax returns filed with the IRS. The data include items such as  Number of returns which approximates the number of households Number of personal exemptions which approximates the population Adjusted gross income  Wages and salaries Dividends before exclusion Interest received    Content For details of the exact fields available please see the field_definitions.csv. Please note that the exact fields available can change from year to year this definitions file was generated by retaining only the most recent year's entry from the years which had pdf manuals. The associated IRS form numbers are the most likely to change over time.  Acknowledgements This data was generated by the Internal Revenue Service.,CSV,,"[finance, government]",Other,,,587,3905,838,Summaries of individual income tax returns by zip code,Individual Income Tax Statistics,https://www.kaggle.com/irs/individual-income-tax-statistics,Wed Sep 06 2017
,Rachael Tatman,"[year, month, day, paper, fileName, text]","[numeric, numeric, numeric, string, string, string]","Context ""Tulip mania tulipmania or tulipomania (Dutch names include tulpenmanie tulpomanie tulpenwoede tulpengekte and bollengekte) was a period in the Dutch Golden Age during which contract prices for bulbs of the recently introduced tulip reached extraordinarily high levels and then dramatically collapsed in February 1637. It is generally considered the first recorded speculative bubble (or economic bubble)."" -- From Wikipedia CC BY-SA Market forecasting is difficult. There are many factors that may affect the market and a high degree of uncertainty. One thing that some researchers have been investigating is whether natural language processing (NLP) of news texts can help with market forecasting. Recent publications suggest that it can be.  Peng Y. & Jiang H. (2016). Leverage Financial News to Predict Stock Price Movements Using Word Embeddings and Deep Neural Networks. In Proceedings of NAACL-HLT (pp. 374-379). Fraiberger S. P. (2016). News Sentiment and Cross-Country Fluctuations. NLP+ CSS 2016 125.  This dataset an interesting test case for these methodologies. It contains Dutch-language newspapers from the years immediately preceding and following tulip mania. Can you use NLP techniques to model the tulip market over time? Content This dataset contains the texts of 8559 newspaper deliveries from the 17th century from June 14th 1618 to December 31 1699. The text is in Dutch. Since the text was scraped from old newspapers using OCR (optical character recognition) there are some errors in the text. Acknowledgments This dataset was compiled by Delpher an archive service provided by the National Library of the Netherlands. It is provided under a CC-BY 4.0 license. For more information and newspapers from other years please visit their website (in Dutch). If you use this dataset in your work please include this citation Delpher open newspaper archive (1.0). Creative Commons Attribution 4.0  The Hague 2017 .",Other,,"[languages, europe, finance]",Other,,,29,741,144,Can you identify linguistic features that predict a market crash?,Delpher Dutch Newspaper Archive (1618-1699),https://www.kaggle.com/rtatman/delpher-dutch-newspaper-archive-16181699,Thu Aug 10 2017
,AIFirst,[],[],Context It is difficult to determine how many coins are in a large pile of money we want to make an app to automatically count them. Content Each folder has a different type of coin (1fr = 1 franc 50rp = 50 rappen / cents) and since all of the franc coins have the same back we have a fr_back folder and all of the rappen have the same we have a rp_back Acknowledgements The data was collected by https//github.com/Zarkonnen/aimeetup_coins Inspiration Making a tool to classify coints,Other,,"[money, image data, multiclass classification]",CC0,,,98,961,33,Count and classify swiss coins from images,Swiss Coins,https://www.kaggle.com/ai-first/swisscoins,Sat Sep 23 2017
,Yonatan Vaizman,"[timestamp, raw_acc:magnitude_stats:mean, raw_acc:magnitude_stats:std, raw_acc:magnitude_stats:moment3, raw_acc:magnitude_stats:moment4, raw_acc:magnitude_stats:percentile25, raw_acc:magnitude_stats:percentile50, raw_acc:magnitude_stats:percentile75, raw_acc:magnitude_stats:value_entropy, raw_acc:magnitude_stats:time_entropy, raw_acc:magnitude_spectrum:log_energy_band0, raw_acc:magnitude_spectrum:log_energy_band1, raw_acc:magnitude_spectrum:log_energy_band2, raw_acc:magnitude_spectrum:log_energy_band3, raw_acc:magnitude_spectrum:log_energy_band4, raw_acc:magnitude_spectrum:spectral_entropy, raw_acc:magnitude_autocorrelation:period, raw_acc:magnitude_autocorrelation:normalized_ac, raw_acc:3d:mean_x, raw_acc:3d:mean_y, raw_acc:3d:mean_z, raw_acc:3d:std_x, raw_acc:3d:std_y, raw_acc:3d:std_z, raw_acc:3d:ro_xy, raw_acc:3d:ro_xz, raw_acc:3d:ro_yz, proc_gyro:magnitude_stats:mean, proc_gyro:magnitude_stats:std, proc_gyro:magnitude_stats:moment3, proc_gyro:magnitude_stats:moment4, proc_gyro:magnitude_stats:percentile25, proc_gyro:magnitude_stats:percentile50, proc_gyro:magnitude_stats:percentile75, proc_gyro:magnitude_stats:value_entropy, proc_gyro:magnitude_stats:time_entropy, proc_gyro:magnitude_spectrum:log_energy_band0, proc_gyro:magnitude_spectrum:log_energy_band1, proc_gyro:magnitude_spectrum:log_energy_band2, proc_gyro:magnitude_spectrum:log_energy_band3, proc_gyro:magnitude_spectrum:log_energy_band4, proc_gyro:magnitude_spectrum:spectral_entropy, proc_gyro:magnitude_autocorrelation:period, proc_gyro:magnitude_autocorrelation:normalized_ac, proc_gyro:3d:mean_x, proc_gyro:3d:mean_y, proc_gyro:3d:mean_z, proc_gyro:3d:std_x, proc_gyro:3d:std_y, proc_gyro:3d:std_z, proc_gyro:3d:ro_xy, proc_gyro:3d:ro_xz, proc_gyro:3d:ro_yz, raw_magnet:magnitude_stats:mean, raw_magnet:magnitude_stats:std, raw_magnet:magnitude_stats:moment3, raw_magnet:magnitude_stats:moment4, raw_magnet:magnitude_stats:percentile25, raw_magnet:magnitude_stats:percentile50, raw_magnet:magnitude_stats:percentile75, raw_magnet:magnitude_stats:value_entropy, raw_magnet:magnitude_stats:time_entropy, raw_magnet:magnitude_spectrum:log_energy_band0, raw_magnet:magnitude_spectrum:log_energy_band1, raw_magnet:magnitude_spectrum:log_energy_band2, raw_magnet:magnitude_spectrum:log_energy_band3, raw_magnet:magnitude_spectrum:log_energy_band4, raw_magnet:magnitude_spectrum:spectral_entropy, raw_magnet:magnitude_autocorrelation:period, raw_magnet:magnitude_autocorrelation:normalized_ac, raw_magnet:3d:mean_x, raw_magnet:3d:mean_y, raw_magnet:3d:mean_z, raw_magnet:3d:std_x, raw_magnet:3d:std_y, raw_magnet:3d:std_z, raw_magnet:3d:ro_xy, raw_magnet:3d:ro_xz, raw_magnet:3d:ro_yz, raw_magnet:avr_cosine_similarity_lag_range0, raw_magnet:avr_cosine_similarity_lag_range1, raw_magnet:avr_cosine_similarity_lag_range2, raw_magnet:avr_cosine_similarity_lag_range3, raw_magnet:avr_cosine_similarity_lag_range4, watch_acceleration:magnitude_stats:mean, watch_acceleration:magnitude_stats:std, watch_acceleration:magnitude_stats:moment3, watch_acceleration:magnitude_stats:moment4, watch_acceleration:magnitude_stats:percentile25, watch_acceleration:magnitude_stats:percentile50, watch_acceleration:magnitude_stats:percentile75, watch_acceleration:magnitude_stats:value_entropy, watch_acceleration:magnitude_stats:time_entropy, watch_acceleration:magnitude_spectrum:log_energy_band0, watch_acceleration:magnitude_spectrum:log_energy_band1, watch_acceleration:magnitude_spectrum:log_energy_band2, watch_acceleration:magnitude_spectrum:log_energy_band3, watch_acceleration:magnitude_spectrum:log_energy_band4, watch_acceleration:magnitude_spectrum:spectral_entropy, watch_acceleration:magnitude_autocorrelation:period]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context Behavioral Context refers to a wide range of attributes describing what is going on with you where you are (home school work at the beach at a restaurant) what you are doing (sleeping eating in a meeting computer work exercising shower) who you are with (family friends co-workers) your body posture state (sitting standing walking running) and so on. The ability to automatically (effortlessly frequently objectively) recognize behavioral context can serve many domains. Medical applications can monitor physical activity or eating habits; aging-at-home programs can log older adults' physical social and mental behavior; personal assistant systems can better server the user if they are aware of the context. In-the-wild (in real life) natural behavior is complex composed of different aspects and has high variability. You can run outside at the beach with friends with your phone in the pocket; you can also run indoors at the gym on a treadmill with your phone motionless next to you. This high variability makes context-recognition a hard task to perform in-the-wild. Content The ExtraSensory Dataset was collected from 60 participants where each person participated approximately 7 days. We installed our data-collection mobile app on their personal phone and it was used to collect both sensor-measurements and context-labels. The sensor-measurements were recorded automatically for a window of 20-seconds every minute. This included accelerometer gyroscope magnetometer audio location and phone-state from the person's phone as well as accelerometer and compass from an additional smartwatch that we provided. In addition the app's interface had many mechanisms for self-reporting the relevant context-labels including reporting past context near future responding to notifications and more. The flexible interface allowed to collect many labels with minimal effort and interaction-time to avoid interfering with the natural behavior. The data was collected in-the-wild participants used their phone in any way that was convenient to them they engaged in their regular behavior and reported an combinations of labels that fit their context. For every participant (or ""user"") the dataset has a CSV file with pre-computed features that we extracted from the sensors and with labels.  Each row has a separate example (representing 1 minute) and is indexed by the timestamp (seconds since the epoch). There are columns for the sensor-features with the prefix of the column name indicating the sensor it came from (e.g. prefix ""raw_acc"" indicating a feature came from the raw phone accelerometer measurements). There are columns for 51 diverse context-labels and the value for an example-label pair is either 1 (the label is relevant for the example) 0 (the label is not relevant) or 'NaN' (missing information). Here we provide data for 2 of the 60 participants. You can use this partial data to get familiar with the data and practice algorithms. The full dataset is publicly available at http//extrasensory.ucsd.edu. The website has additional parts of the data (such as a wider range of the original reported labels location coordinates mood labels from part of the participants). If you use the data for your publications you are required to cite our original paper Vaizman Y. Ellis K. and Lanckriet G. ""Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches"". IEEE Pervasive Computing vol. 16 no. 4 October-December 2017 pp. 62-74. Read the information at http//extrasensory.ucsd.edu and the original paper for more details. Acknowledgements The dataset was collected by Yonatan Vaizman and Katherine Ellis under the supervision of prof. Gert Lanckriet all from the department of Electrical and Computer Engineering University of California San Diego. Inspiration The ExtraSensory Dataset can serve as a benchmark to compare methods for context-recognition (or context-awareness activity recognition daily activity detection). You can focus on specific sensors or on specific context-labels. You can suggest new models and classifiers train them on the data and evaluate their performance on the data.",CSV,,[psychology],CC4,,,211,2850,24,Behavioral Context Recognition In-the-Wild,The ExtraSensory Dataset,https://www.kaggle.com/yvaizman/the-extrasensory-dataset,Wed Jun 07 2017
,Mathijs Waegemakers,"[date, maximum temperature, minimum temperature, average temperature, precipitation, snow fall, snow depth]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric]",Context As a former transportation student I know how the weather can influence traffic. Both the increase of traffic as well as the decrease of road conditions increases the travel time. Content Weather data collected from the National Weather Service. It contains the first six months of 2016 for a weather station in central park. It contains for each day the minimum temperature maximum temperature average temperature precipitation new snow fall and current snow depth. The temperature is measured in Fahrenheit and the depth is measured in inches. T means that there is a trace of precipitation. Acknowledgements The data was retrieved on 20th of July 2017 on the website http//w2.weather.gov/climate/xmacis.php?wfo=okx.,CSV,,[weather],Other,,,1285,8007,0.0107421875,"Added for the ""New York City Taxi Trip Duration"" challenge",Weather data in New York City - 2016,https://www.kaggle.com/mathijs/weather-data-in-new-york-city-2016,Sun Sep 24 2017
,FelixZhao,"[Product_Code, Warehouse, Product_Category, Date, Order_Demand]","[string, string, string, dateTime, numeric]",Context The dataset contains historical product demand for a manufacturing company with footprints globally. The company provides thousands of products within dozens of product categories. There are four central warehouses to ship products within the region it is responsible for. Since the products are manufactured in different locations all over the world it normally takes more than one month to ship products via ocean to different central warehouses. If forecasts for each product in different central with reasonable accuracy for the monthly demand for month after next can be achieved it would be beneficial to the company in multiple ways. This dataset is all real-life data and products/warehouse and category information encoded.  Content Product_Code The product name encoded. Warehouse Warehouse name encoded. Product_Category Product Category for each Product_Code encoded. Date The date customer needs the product. Order_Demand single order qty. Inspiration Is it possible to make forecasts for thousands of products (some of them are highly variable in terms of monthly demand) for the the month after next?,CSV,,"[business, supply chain, product]",GPL,,,863,6698,49,Make Accurate Forecasts for Thousands of Different Products,Forecasts for Product Demand,https://www.kaggle.com/felixzhao/productdemandforecasting,Fri Aug 25 2017
,4d4stra,"[FIPS_Block_Group, State, State_name, County, County_name, Tract, Block_Group, Flag, LAND_AREA, AIAN_LAND, URBANIZED_AREA_POP_CEN_2010, URBAN_CLUSTER_POP_CEN_2010, RURAL_POP_CEN_2010, Tot_Population_CEN_2010, Tot_Population_ACS_09_13, Tot_Population_ACSMOE_09_13, Males_CEN_2010, Males_ACS_09_13, Males_ACSMOE_09_13, Females_CEN_2010, Females_ACS_09_13, Females_ACSMOE_09_13, Pop_under_5_CEN_2010, Pop_under_5_ACS_09_13, Pop_under_5_ACSMOE_09_13, Pop_5_17_CEN_2010, Pop_5_17_ACS_09_13, Pop_5_17_ACSMOE_09_13, Pop_18_24_CEN_2010, Pop_18_24_ACS_09_13, Pop_18_24_ACSMOE_09_13, Pop_25_44_CEN_2010, Pop_25_44_ACS_09_13, Pop_25_44_ACSMOE_09_13, Pop_45_64_CEN_2010, Pop_45_64_ACS_09_13, Pop_45_64_ACSMOE_09_13, Pop_65plus_CEN_2010, Pop_65plus_ACS_09_13, Pop_65plus_ACSMOE_09_13, Tot_GQ_CEN_2010, Inst_GQ_CEN_2010, Non_Inst_GQ_CEN_2010, Hispanic_CEN_2010, Hispanic_ACS_09_13, Hispanic_ACSMOE_09_13, NH_White_alone_CEN_2010, NH_White_alone_ACS_09_13, NH_White_alone_ACSMOE_09_13, NH_Blk_alone_CEN_2010, NH_Blk_alone_ACS_09_13, NH_Blk_alone_ACSMOE_09_13, NH_AIAN_alone_CEN_2010, NH_AIAN_alone_ACS_09_13, NH_AIAN_alone_ACSMOE_09_13, NH_Asian_alone_CEN_2010, NH_Asian_alone_ACS_09_13, NH_Asian_alone_ACSMOE_09_13, NH_NHOPI_alone_CEN_2010, NH_NHOPI_alone_ACS_09_13, NH_NHOPI_alone_ACSMOE_09_13, NH_SOR_alone_CEN_2010, NH_SOR_alone_ACS_09_13, NH_SOR_alone_ACSMOE_09_13, Pop_5yrs_Over_ACS_09_13, Pop_5yrs_Over_ACSMOE_09_13, Othr_Lang_ACS_09_13, Othr_Lang_ACSMOE_09_13, Pop_25yrs_Over_ACS_09_13, Pop_25yrs_Over_ACSMOE_09_13, Not_HS_Grad_ACS_09_13, Not_HS_Grad_ACSMOE_09_13, College_ACS_09_13, College_ACSMOE_09_13, Pov_Univ_ACS_09_13, Pov_Univ_ACSMOE_09_13, Prs_Blw_Pov_Lev_ACS_09_13, Prs_Blw_Pov_Lev_ACSMOE_09_13, One_Health_Ins_ACS_09_13, One_Health_Ins_ACSMOE_09_13, Two_Plus_Health_Ins_ACS_09_13, Two_Plus_Health_Ins_ACSMOE_09_13, No_Health_Ins_ACS_09_13, No_Health_Ins_ACSMOE_09_13, Pop_1yr_Over_ACS_09_13, Pop_1yr_Over_ACSMOE_09_13, Diff_HU_1yr_Ago_ACS_09_13, Diff_HU_1yr_Ago_ACSMOE_09_13, ENG_VW_SPAN_ACS_09_13, ENG_VW_SPAN_ACSMOE_09_13, ENG_VW_INDO_EURO_ACS_09_13, ENG_VW_INDO_EURO_ACSMOE_09_13, ENG_VW_API_ACS_09_13, ENG_VW_API_ACSMOE_09_13, ENG_VW_OTHER_ACS_09_13, ENG_VW_OTHER_ACSMOE_09_13, ENG_VW_ACS_09_13, ENG_VW_ACSMOE_09_13, Rel_Family_HHDS_CEN_2010, Rel_Family_HHD_ACS_09_13]","[numeric, numeric, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Federal Superfund sites are some of the most polluted in the United States.  This dataset contains a multifaceted view of Superfunds including free-form text descriptions geography demographics and socioeconomics. Content The core data was scraped from the National Priorities List (NPL) provided by the U.S. Environmental Protection Agency (EPA).  This table provides basic information such as site name site score date added and links to a site description and current status.  Apache Tika was used to extract text from the site description pdfs.  The addresses were scraped from site status pages and used to geocode to latitude and longitude and Census block group.  The block group assignment was used to join with the Census Bureau's planning database a rich source of nationwide demographic and socioeconomic data.  The full source code used to generate the data can be found here on github. I have provided three separate downloads to explore  priorities_list_full.json the NPL containing all geographic site information text descriptions and Census Bureau data from the relevant block groups. pdb_tract.csv the planning database aggregated on the tract level with an additional indicator (has_superfund) noting whether or not the tract contains the address of a Superfund site. pdb_block_group.csv the planning database aggregated on the block group level with an additional indicator (has_superfund) noting whether or not the block group contains the address of a Superfund site.  Some caveats  The planning database contains 300+ columns.  For a full description of these columns please see the documentation here. Since the Google geocoder is relatively aggressive in providing address matches geocoding was done through a hierarchy of queries (full address city-state-zip and zipcode only) to prevent gross errors.  The address string used to geocode is noted through the 'geocode_source' column. While this data is linked to demographic and socioeconomic data based on either the block group (tract for pdb_tract.csv) the impacts of a particular site's pollution may extend beyond these geographic regions.  Acknowledgements I would like to thank the EPA and the Census Bureau for making such detailed information publicly available.  For relevant academic work please see Burwell-Naney et al. (2013) and references both to and therein. Please let me know if you have any suggestions for improving the dataset!,CSV,,"[pollution, demographics, linguistics, sociology]",CC0,,,39,513,306,"The most polluted sites in the US, from free-form text to socioeconomics",U.S. Federal Superfund Sites,https://www.kaggle.com/srrobert50/federal-superfunds,Fri Nov 17 2017
,marc moreaux,[],[],Context Audio classification is often proposed as MFCC classification problem. With this dataset we intend to give attention to raw audio classification as performed in the Wavenet network. Content The dataset consists in 50 WAV files sampled at 16KHz for 50 different classes.  To each one of the classes corresponds 40 audio sample of 5 seconds each. All of these audio files have been concatenated by class in order to have 50 wave files of 3 min. 20sec. In our example notebook we show how to access the data and visualize a piece of it. Acknowledgements We have not much credit in proposing the dataset here. Much of the work have been done by the authors of the ESC-50 Dataset for Environmental Sound Classification. In order to fit on Kaggle we processed the files with the to_wav.py file present in the original repository. You might also notice that we transformed the data from OGG to WAV as the former didn't seem to be supported in Anaconda. Inspiration You might use this dataset to challenge your algorithms in classifying from raw audio ;),Other,,"[categorical data, acoustics]",CC4,,,143,2207,153,raw audio classification of environmental sounds,Environmental Sound Classification 50,https://www.kaggle.com/mmoreaux/environmental-sound-classification-50,Tue Oct 17 2017
,usfundamentals,"[company_id, name_latest, names_previous]","[numeric, string, string]",This dataset contains US stocks fundamental data such as income statement balance sheet and cash flows.  12129 companies 8526 unique indicators ~20 indicators comparable across most companies Five years of data yearly  The data is provided by http//usfundamentals.com.,CSV,,[finance],CC0,,,3667,24030,145,"Fundamental data for 12,129 companies based on XBRL",US Stocks Fundamentals (XBRL),https://www.kaggle.com/usfundamentals/us-stocks-fundamentals,Tue Sep 06 2016
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes over 1 gb of stop data from Illinois covering all of 2010 onwards. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[crime, law]",Other,,,57,616,1017,Data on Traffic and Pedestrian Stops by Police in Illinois,Stanford Open Policing Project - Illinois,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-illinois,Fri Jul 21 2017
,Liling Tan,"[Language, Source, Date, Text]","[string, string, dateTime, string]",Context The HC Corpora was a great resource that contains natural language text from various newspapers social media posts and blog pages in multiple languages. This is a cleaned version of the raw data from newspaper subset of the HC corpus.  Originally this subset was created for a language identification task for similar languages Content The columns of each row in the .tsv file are  Langauge Language of the text. Source Newspaper from which the text is from. Date Date of the article that contains the text. Text Sentence/paragraph from the newspaper  The corpus contains 16806041 sentences/paragraphs in 67 languages  Afrikaans Albanian Amharic Arabic Armenian Azerbaijan Bengali Bosnian Catalan Chinese (Simplified) Chinese (Traditional) Croatian Welsh Czech German Danish Danish English Spanish Spanish (South America) Finnish French Georgian Galician Greek Hebrew Hindi Hungarian Icelandic Indonesian Italian Japanese Khmer Kannada Korean Kazakh Lithuanian Latvian Macedonian Malayalam Mongolian Malay Nepali Dutch Norwegian (Bokmal) Punjabi Farsi Polish Portuguese (Brazil) Portuguese (EU) Romanian Russian Serbian Sinhalese Slovak Slovenian Swahili Swedish Tamil Telugu Tagalog Thai Turkish Ukranian Urdu Uzbek Vietnamese  Languages in HC Corpora but not in this (yet)  Estonian Greenlandic Gujarati   Acknowledge All credits goes to Hans Christensen the creator of HC Corpora. Dataset image is from Philip Strong. Inspire Use this dataset to  create a language identifier / detector exploratory corpus linguistics (It’s one capstone project from Coursera’s data science specialization ) ,Other,,"[news agencies, history, linguistics, internet]",CC0,,,321,2696,2048,A cleaned subset of HC Corpora newspapers,Old Newspapers,https://www.kaggle.com/alvations/old-newspapers,Thu Nov 16 2017
,US Environmental Protection Agency,"[YEAR, TRI_FACILITY_ID, FRS_ID, FACILITY_NAME, STREET_ADDRESS, CITY, COUNTY, ST, ZIP, BIA_CODE, TRIBE, LATITUDE, LONGITUDE, FEDERAL_FACILITY, INDUSTRY_SECTOR_CODE, INDUSTRY_SECTOR, PRIMARY_SIC, SIC_2, SIC_3, SIC_4, SIC_5, SIC_6, PRIMARY_NAICS, NAICS_2, NAICS_3, NAICS_4, NAICS_5, NAICS_6, DOC_CTRL_NUM, CHEMICAL, CAS_#/COMPOUND_ID, SRS_ID, CLEAR_AIR_ACT_CHEMICAL, CLASSIFICATION, METAL, METAL_CATEGORY, CARCINOGEN, FORM_TYPE, UNIT_OF_MEASURE, 5.1_FUGITIVE_AIR, 5.2_STACK_AIR, 5.3_WATER, 5.4_UNDERGROUND, 5.4.1_UNDERGROUND_CLASS_I, 5.4.2_UNDERGROUND_CLASS_II-V, 5.5.1_LANDFILLS, 5.5.1A_RCRA_C_LANDFILLS, 5.5.1B_OTHER_LANDFILLS, 5.5.2_LAND_TREATMENT, 5.5.3_SURFACE_IMPOUNDMENT, 5.5.3A_RCRA_C_SURFACE_IMP., 5.5.3B_Other_SURFACE_IMP., 5.5.4_OTHER_DISPOSAL, ON-SITE_RELEASE_TOTAL, 6.1_POTW-TRANSFERS_FOR_RELEASE, 6.1_POTW-TRANSFERS_FOR_TREATM., 6.1_POTW-TOTAL_TRANSFERS, 6.2_M10, 6.2_M41, 6.2_M62, 6.2_M71, 6.2_M81, 6.2_M82, 6.2_M72, 6.2_M63, 6.2_M66, 6.2_M67, 6.2_M64, 6.2_M65, 6.2_M73, 6.2_M79, 6.2_M90, 6.2_M94, 6.2_M99, OFF-SITE_RELEASE_TOTAL, 6.2_M20, 6.2_M24, 6.2_M26, 6.2_M28, 6.2_M93, OFF-SITE_RECYCLED_TOTAL, 6.2_M56, 6.2_M92, OFF-SITE_RECOVERY_TOTAL, 6.2_M40, 6.2_M50, 6.2_M54, 6.2_M61, 6.2_M69, 6.2_M95, OFF-SITE_TREATED_TOTAL, TOTAL_RELEASES, 8.1_RELEASES, 8.1A_ON-SITE_CONTAINED_REL., 8.1B_ON-SITE_OTHER_RELEASES, 8.1C_OFF-SITE_CONTAINED_REL., 8.1D_OFF-SITE_OTHER_RELEASES, 8.2_ENERGY_RECOVERY_ON-SITE, 8.3_ENERGY_RECOVERY_OFF-SITE, 8.4_RECYCLING_ON-SITE]","[numeric, string, numeric, string, string, string, string, string, numeric, string, string, numeric, numeric, string, numeric, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string, numeric, string, numeric, numeric, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This database is managed by the  US Environmental Protection Agency and contains information reported annually by some industry groups as well as federal facilities. Each year companies across a wide range of industries (including chemical mining paper oil and gas industries) that produce more than 25000 pounds or handle more than 10000 pounds of a listed toxic chemical must report it to the TRI. The TRI threshold was initially set at 75000 pounds annually. If the company treats recycles disposes or releases more than 500 pounds of that chemical into the environment (as opposed to just handling it) then they must provide a detailed inventory of that chemical's inventory. Content  There are roughly 100 columns in this dataset; please see the tri_basic_data_file_format_v15.pdf for details. You may also wish to consult factors_to_consider_6.15.15_final.pdf for general background about interpreting the data. I've merged all of the TRI basic data files into a single large csv. You will probably need to process it in batches or use a tool like Dask to stay within kernel memory limits. Please note that the 2016 data remains preliminary at the time of this release.  Acknowledgements This dataset was released by the US EPA. You can find the original dataset more detailed versions of the data  and a great deal of background information here https//www.epa.gov/toxics-release-inventory-tri-program/tri-data-and-tools Inspiration The EPA runs an annual university contest. Their list of previous winners contains a lot of great ideas that people have had for this dataset in the past. The 2017 competition is already over but you can find the rules here.,Other,,"[government agencies, pollution]",Other,,,231,1801,2048,US EPA data on release of toxic chemicals for 1987-2016,Toxic Release Inventory,https://www.kaggle.com/epa/toxic-release-inventory,Wed Aug 23 2017
,IHME,[],[],Context IHME United States Mortality Rates by County 1980-2014 National - All.  (Deaths per 100000 population)  To quickly get started creating maps like the one below see the Quick Start R kernel.  How the Dataset was Created This Dataset was created from the Excel Spreadsheet which can be found in the download.  Or you can view the source here.   If you take a look at the row for United States for the column Mortality Rate 1980* you'll see the set of numbers  1.52 (1.44 1.61).  Numbers in parentheses are 95% uncertainty.  The 1.52 is an age-standardized mortality rate for both sexes combined (deaths per 100000 population). In this Dataset 1.44 will be placed in the named column Mortality Rage 1989 (Min)* and 1.61 is in column named  Mortality Rate 1980 (Max)* .  For information on how these Age-standardized mortality rates were calculated see the  December JAMA 2016  article which you can download for free.  Reference JAMA Full Article Video Describing this Study  (Short and this is worth viewing) Data Resources How Americans Die May Depend On Where They Live by Anna Maria Barry-Jester (FiveThirtyEight) Interactive Map from healthdata.org IHME Data Acknowledgements This Dataset was provided by  IHME Institute for Health Metrics and Evaluation 2301 Fifth Ave. Suite 600 Seattle WA 98121 USA Tel +1.206.897.2800 Fax +1.206.897.2899 © 2016 University of Washington,Other,,[demographics],Other,,,1353,11656,24,United States Mortality Rates by County 1980-2014,US county-level mortality,https://www.kaggle.com/IHME/us-countylevel-mortality,Wed Dec 28 2016
,PyTorch,[],[],ResNet-50  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,4,584,91,ResNet-50 Pre-trained Model for PyTorch,ResNet-50,https://www.kaggle.com/pytorch/resnet50,Thu Dec 14 2017
,The Museum of Modern Art,"[Artist ID, Name, Nationality, Gender, Birth Year, Death Year]","[numeric, string, string, string, numeric, numeric]",Context The Museum of Modern Art (MoMA) acquired its first artworks in 1929 the year it was established. Today the Museum’s evolving collection contains almost 200000 works from around the world spanning the last 150 years. The collection includes an ever-expanding range of visual expression including painting sculpture printmaking drawing photography architecture design film and media and performance art. Content MoMA is committed to helping everyone understand enjoy and use our collection. The Museum’s website features 72706 artworks from 20956 artists. The artworks dataset contains 130262 records representing all of the works that have been accessioned into MoMA’s collection and cataloged in our database. It includes basic metadata for each work including title artist date medium dimensions and date acquired by the Museum. Some of these records have incomplete information and are noted as “not curator approved.” The artists dataset contains 15091 records representing all the artists who have work in MoMA's collection and have been cataloged in our database. It includes basic metadata for each artist including name nationality gender birth year and death year. Inspiration Which artist has the most works in the museum collection or on display? What is the largest work of art in the collection? How many pieces in the collection were made during your birth year? What gift or donation is responsible for the most artwork in the collection?,CSV,,"[museums, visual arts]",CC0,,,514,4179,33,"Title, artist, date, and medium of every artwork in the MoMA collection",Museum of Modern Art Collection,https://www.kaggle.com/momanyc/museum-collection,Wed Feb 15 2017
,NLTK Data,[],[],"Context NLTK provides an interface to the BLLIP reranking parser (aka Charniak-Johnson parser Charniak parser Brown reranking parser).  NLTK redistribute the pre-trained model trained on the WSJ section of the Penn Treebank without auxillary.  The full list of models can be found on https//github.com/BLLIP/bllip-parser/blob/master/MODELS.rst Acknowledgements Parser and reranker Eugene Charniak and Mark Johnson. 2005. ""Coarse-to-fine n-best  parsing and MaxEnt discriminative reranking."" In ACL.  Eugene Charniak. 2000. ""A maximum-entropy-inspired parser."" In ACL.  Self-training David McClosky Eugene Charniak and Mark Johnson. 2006.  ""Effective Self-Training for Parsing."" In HLT-NAACL.  Syntactic fusion Do Kook Choe David McClosky and Eugene Charniak. 2015.  ""Syntactic Parse Fusion."" In EMNLP. ",Other,,[],Other,,,8,396,52,Charniak-Johnson parser,BLLIP Parser Model,https://www.kaggle.com/nltkdata/bllip,Sun Aug 20 2017
,Rohk,"[word_id, word, up_votes, down_votes, author, definition]","[numeric, string, numeric, numeric, string, string]",Context This dataset contains 2.6 million words from Urban Dictionary including their definitions and votes in CSV format. Source https//www.urbandictionary.com To lookup a word in the dataset via urban dictionary api  http//api.urbandictionary.com/v0/define?defid={word_id} Warning that this dataset contains a lot of profanity and racial slurs. Content Rows 2606522 Column 1 word_id - for usage in urban dictionary api Column 2 word - the text being defined Column 3 up_votes - thumbs up count as of may 2016 Column 4 down_votes - thumbs down count as of may 2016 Column 5 author - hash of username of submitter Column 6 definition - text with possible utf8 chars double semi-colon denotes a newline Acknowledgements Remixed from data posted anonymously on reddit. https//archive.org/details/UrbanDictionary1999-May2016DefinitionsCorpus Inspiration Modified and cleaned the original source which is in a very un-friendly format.,CSV,,"[linguistics, internet]",CC0,,,204,2609,238,Corpus of 2.6 million words with ratings from urban dictionary,Urban Dictionary Words And Definitions,https://www.kaggle.com/therohk/urban-dictionary-words-dataset,Thu Nov 16 2017
,Mad Hab,"[jobpost, date, Title, Company, AnnouncementCode, Term, Eligibility, Audience, StartDate, Duration, Location, JobDescription, JobRequirment, RequiredQual, Salary, ApplicationP, OpeningDate, Deadline, Notes, AboutC, Attach, Year, Month, IT]","[string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, dateTime, string, string, string, numeric, numeric, boolean]",Job Posts dataset The dataset consists of 19000 job postings that were posted through the Armenian human resource portal CareerCenter. The data was extracted from the Yahoo! mailing group https//groups.yahoo.com/neo/groups/careercenter-am. This was the only online human resource portal in the early 2000s.  A job posting usually has some structure although some fields of the posting are not necessarily filled out by the client (poster). The data was cleaned by removing posts that were not job related or had no structure. The data consists of job posts from 2004-2015     Content jobpost – The original job post  date – Date it was posted in the group  Title – Job title  Company - employer  AnnouncementCode – Announcement code (some internal code is usually missing)  Term – Full-Time Part-time etc  Eligibility -- Eligibility of the candidates  Audience  --- Who can apply?  StartDate – Start date of work  Duration  - Duration of the employment  Location – Employment location  JobDescription – Job Description  JobRequirment  - Job requirements  RequiredQual   -Required Qualification  Salary      - Salary  ApplicationP – Application Procedure  OpeningDate – Opening date of the job announcement  Deadline – Deadline for the job announcement  Notes   - Additional Notes  AboutC  - About the company  Attach  - Attachments  Year -  Year of the announcement (derived from the field date)  Month - Month of the announcement (derived from the field date)  IT – TRUE if the job is an IT job. This variable is created by a simple     search of IT job titles within column “Title”    Acknowledgements The data collection and initial research was funded by the American University of Armenia’s research grant (2015).  Inspiration The online job market is a good indicator of overall demand for labor in the local economy. In addition online job postings data are easier and quicker to collect and they can be a richer source of information than more traditional job postings such as those found in printed newspapers.  The data can be used in the following ways  -Understand the demand for certain professions job titles or industries -Help universities with curriculum development -Identify skills that are most frequently required by employers and how the distribution of    necessary skills changes over time -Make recommendations to job seekers and employers Past research We have used association rules mining and simple text mining techniques to analyze the data. Some results can be found here (https//www.slideshare.net/HabetMadoyan/it-skills-analysis-63686238).,CSV,,"[employment, linguistics]",Other,,,1814,11129,92,"Dataset of 19,000 online job posts from 2004 to 2015",Online Job Postings,https://www.kaggle.com/madhab/jobposts,Sat Apr 22 2017
,BrendaSo,"[, State Code, County Code, Site Num, Address, State, County, City, Date Local, NO2 Units, NO2 Mean, NO2 1st Max Value, NO2 1st Max Hour, NO2 AQI, O3 Units, O3 Mean, O3 1st Max Value, O3 1st Max Hour, O3 AQI, SO2 Units, SO2 Mean, SO2 1st Max Value, SO2 1st Max Hour, SO2 AQI, CO Units, CO Mean, CO 1st Max Value, CO 1st Max Hour, CO AQI]","[numeric, numeric, numeric, numeric, string, string, string, string, dateTime, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, string]",Context This dataset deals with pollution in the U.S. Pollution in the U.S. has been well documented by the U.S. EPA but it is a pain to download all the data and arrange them in a format that interests data scientists. Hence I gathered four major pollutants (Nitrogen Dioxide Sulphur Dioxide Carbon Monoxide and Ozone) for every day from 2000 - 2016 and place them neatly in a CSV file.  Content There is a total of 28 fields. The four pollutants (NO2 O3 SO2 and O3) each has 5 specific columns. Observations totaled to over 1.4 million. This kernel provides a good introduction to this dataset! For observations on specific columns visit the Column Metadata on the Data tab. Acknowledgements All the data is scraped from the database of U.S. EPA  https//aqsdr1.epa.gov/aqsweb/aqstmp/airdata/download_files.html  Inspiration I did a related project with some of my friends in college and decided to open source our dataset so that data scientists don't need to re-scrape the U.S. EPA site for historical pollution data.,CSV,,"[environment, pollution]",ODbL,,,7279,57036,382,Pollution in the U.S. since 2000,U.S. Pollution Data,https://www.kaggle.com/sogun3/uspollution,Fri Nov 04 2016
,National Institutes of Health Chest X-Ray Dataset,[],[],"NIH Chest X-ray Dataset Sample  National Institutes of Health Chest X-Ray Dataset Chest X-ray exams are one of the most frequent and cost-effective medical imaging examinations available. However clinical diagnosis of a chest X-ray can be challenging and sometimes more difficult than diagnosis via chest CT imaging. The lack of large publicly available datasets with annotations means it is still very difficult if not impossible to achieve clinically relevant computer-aided detection and diagnosis (CAD) in real world medical sites with chest X-rays. One major hurdle in creating large X-ray image datasets is the lack resources for labeling so many images. Prior to the release of this dataset Openi was the largest publicly available source of chest X-ray images with 4143 images available. This NIH Chest X-ray Dataset is comprised of 112120 X-ray images with disease labels from 30805 unique patients. To create these labels the authors used Natural Language Processing to text-mine disease classifications from the associated radiological reports. The labels are expected to be >90% accurate and suitable for weakly-supervised learning. The original radiology reports are not publicly available but you can find more details on the labeling process in this Open Access paper ""ChestX-ray8 Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases."" (Wang et al.) Link to paper  File contents - This is a random sample (5%) of the full dataset  sample.zip Contains 5606 images with size 1024 x 1024 sample_labels.csv Class labels and patient data for the entire dataset Image Index File name Finding Labels Disease type (Class label) Follow-up #  Patient ID Patient Age Patient Gender View Position X-ray orientation OriginalImageWidth OriginalImageHeight OriginalImagePixelSpacing_x OriginalImagePixelSpacing_y   Class descriptions There are 15 classes (14 diseases and one for ""No findings"") in the full dataset but since this is drastically reduced version of the full dataset some of the classes are sparse with the labeled as ""No findings""  Hernia  - 13 images Pneumonia  - 62 images Fibrosis  - 84 images Edema  - 118 images Emphysema  - 127 images Cardiomegaly  - 141 images Pleural_Thickening  - 176 images Consolidation  - 226 images Pneumothorax  - 271 images Mass  - 284 images Nodule  - 313 images Atelectasis  - 508 images Effusion  - 644 images Infiltration  - 967 images No Finding - 3044 images   Full Dataset Content The full dataset can be found here. There are 12 zip files in total and range from ~2 gb to 4 gb in size.   Data limitations  The image labels are NLP extracted so there could be some erroneous labels but the NLP labeling accuracy is estimated to be >90%.  Very limited numbers of disease region bounding boxes (See BBox_list_2017.csv) Chest x-ray radiology reports are not anticipated to be publicly shared. Parties who use this public dataset are encouraged to share their “updated” image labels and/or new bounding boxes in their own studied later maybe through manual annotation   Modifications to original data  Original TAR archives were converted to ZIP archives to be compatible with the Kaggle platform CSV headers slightly modified to be more explicit in comma separation and also to allow fields to be self-explanatory   Citations  Wang X Peng Y Lu L Lu Z Bagheri M Summers RM. ChestX-ray8 Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases. IEEE CVPR 2017 ChestX-ray8_Hospital-Scale_Chest_CVPR_2017_paper.pdf NIH News release NIH Clinical Center provides one of the largest publicly available chest x-ray datasets to scientific community Original source files and documents https//nihcc.app.box.com/v/ChestXray-NIHCC/folder/36938765345   Acknowledgements This work was supported by the Intramural Research Program of the NClinical Center (clinicalcenter.nih.gov) and National Library of Medicine (www.nlm.nih.gov). ",Other,,"[healthcare, health, biotechnology]",CC0,,,717,5371,2048,"5,606 images and labels sampled from the NIH Chest X-ray Dataset",Random Sample of NIH Chest X-ray Dataset,https://www.kaggle.com/nih-chest-xrays/sample,Thu Nov 23 2017
,Dylan,"[0XX, 1.50406699716e-06]","[string, string]",About This Dataset You can use this fonts file to generate some Chinese character. Use this image can train a machine learning model to recognize text. Dataset is updating Tell me if you have other font file or anything related to this topic.,Other,,"[writing, linguistics]",Other,,,232,3336,199,"Chinese fonts dataset, which can be used for Chinese text OCR",Chinese Characters Generator,https://www.kaggle.com/dylanli/chinesecharacter,Fri Jul 14 2017
,Noah Gift,"[PLAYER_ID, PLAYER_NAME, TEAM_ID, TEAM_ABBREVIATION, AGE, GP, W, L, W_PCT, MIN, OFF_RATING, DEF_RATING, NET_RATING, AST_PCT, AST_TO, AST_RATIO, OREB_PCT, DREB_PCT, REB_PCT, TM_TOV_PCT, EFG_PCT, TS_PCT, USG_PCT, PACE, PIE, FGM, FGA, FGM_PG, FGA_PG, FG_PCT, GP_RANK, W_RANK, L_RANK, W_PCT_RANK, MIN_RANK, OFF_RATING_RANK, DEF_RATING_RANK, NET_RATING_RANK, AST_PCT_RANK, AST_TO_RANK, AST_RATIO_RANK, OREB_PCT_RANK, DREB_PCT_RANK, REB_PCT_RANK, TM_TOV_PCT_RANK, EFG_PCT_RANK, TS_PCT_RANK, USG_PCT_RANK, PACE_RANK, PIE_RANK, FGM_RANK, FGA_RANK, FGM_PG_RANK, FGA_PG_RANK, FG_PCT_RANK, CFID, CFPARAMS, WIKIPEDIA_HANDLE, TWITTER_HANDLE, SALARY_MILLIONS, PTS, ACTIVE_TWITTER_LAST_YEAR, TWITTER_FOLLOWER_COUNT_MILLIONS]","[numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric]","Context This data set contains combined on-court performance data for NBA players in the 2016-2017 season alongside salary Twitter engagement and Wikipedia traffic data. Further information can be found in a series of articles for IBM Developerworks ""Explore valuation and attendance using data science and machine learning"" and ""Exploring the individual NBA players"".  Acknowledgement Data sources include ESPN Basketball-Reference Twitter Five-ThirtyEight and Wikipedia. The source code for this dataset (in Python and R) can be found on GitHub. Links to more writing can be found at noahgift.com. Inspiration  Do NBA fans know more about who the best players are or do owners?   What is the true worth of the social media presence of athletes in the NBA? ",CSV,,"[basketball, social groups]",CC4,,,1925,10224,8,"NBA on the court performance with Social Influence, Popularity and Power",Social Power NBA,https://www.kaggle.com/noahgift/social-power-nba,Tue Aug 01 2017
,Marlesson,"[title, text, date, category, subcategory, link]","[string, string, dateTime, string, string, string]",Content The dataset consists of 167.053 examples and contains Headlines Url of Article Complete Article and Category. I gathered the summarized news from Inshorts and only scraped the news articles from Folha de São Paulo - http//www.folha.uol.com.br/ (Brazilian Newspaper). Time period ranges is between January 2015 and September 2017.,CSV,,"[journalism, languages, brazil]",CC0,,,123,925,480,167.053 news of the site Folha de São Paulo (Brazilian Newspaper),News of the Brazilian Newspaper,https://www.kaggle.com/marlesson/news-of-the-site-folhauol,Mon Oct 30 2017
,Centers for Medicare & Medicaid Services,"[Organization legal name or 'doing business as' name, Group PAC ID, State, Participating in PQRS, Measure Identifier, Measure Title, Inverse Measure, Measure Performance Rate, Footnote, Reporting Mechanism, Reported on PC Live Site]","[string, numeric, string, string, string, string, string, numeric, string, string, string]",The Physician Compare website was created by the Centers for Medicare & Medicaid Services (CMS) in December 2010 as required by the Affordable Care Act (ACA) of 2010 to help patients assess and find doctors and hospitals. This dataset contains the information supplied to patients via that website including patient satisfaction surveys and performance scores across over 100 metrics. Acknowledgements This dataset was kindly released by the Centers for Medicare & Medicaid Services. You can find the original copy of the dataset here.,CSV,,"[healthcare, public health]",CC0,,,307,2391,689,The 2017 Physican Compare Database,Medicare's Doctor Comparison Scores,https://www.kaggle.com/cms/medicares-doctor-comparison-scores,Wed Aug 30 2017
,Erik van de Ven,"[likes, timestamp, text, retweets, replies, fullname, id, user]","[numeric, dateTime, string, numeric, numeric, string, numeric, string]","""Zwarte Piet"" or ""Black Pete"" is originally part of an annually recurring party for children called ""Sinterklaas"". Every single year they arrive by boat in the Netherlands bringing toys and candy for all kind children. But since 2011 (if my research is correct) resistance started against ""Zwarte Piet"". An annually recurring discussion arose is ""Black Pete"" just wrong or not and should it be forbidden or not?  As far as my research is correct it all started in 2011 and ever since people are demonstrating each year. So I thought this would be an interesting topic for a dataset. I collected all tweets with hastags ""sinterklaas"" or ""zwartepiet"" from 13-05-2017 till 23-11-2017. I tried to receive tweets from the whole year (23-11-2016 till 23-11-2017) but somehow I was just able to receive it from 13-05-2017 till 23-11-2017. Perhaps it's even more interesting to just collect all tweets of every year when the party ""lives"" around all people in the Netherlands (from around September till January) so we're even able to see changes over the past years. Maybe an idea for a next update.",CSV,,[internet],CC0,,,22,267,2,All tweets from past months about this annually recurring party.,"""Zwarte Piet"" Tweets",https://www.kaggle.com/erikvdven/zwarte-piet-tweets,Fri Nov 24 2017
,Rachael Tatman,[],[],"Context Being able to automatically answer questions accurately remains a difficult problem in natural language processing. This dataset has everything you need to try your own hand at this task. Can you correctly generate the answer to questions given the Wikipedia article text the question was originally generated from? Content There are three question files one for each year of students S08 S09 and S10 as well as 690000 words worth of cleaned text from Wikipedia that was used to generate the questions. The ""question_answer_pairs.txt"" files contain both the questions and answers. The columns in this file are as follows                    ArticleTitle is the name of the Wikipedia article from which questions and answers initially came. Question is the question. Answer is the answer. DifficultyFromQuestioner is the prescribed difficulty rating for the question as given to the question-writer.  DifficultyFromAnswerer is a difficulty rating assigned by the individual who evaluated and answered the question which may differ from the difficulty in field 4. ArticleFile is the name of the file with the relevant article  Questions that were judged to be poor were discarded from this data set. There are frequently multiple lines with the same question which appear if those questions were answered by multiple individuals.  Acknowledgements These data were collected by Noah Smith Michael Heilman Rebecca Hwa Shay Cohen Kevin Gimpel and many students at Carnegie Mellon University and the University of Pittsburgh between 2008 and 2010. It is released here under CC BY_SA 3.0. Please cite this paper if you write any papers involving the use of the data above Smith N. A. Heilman M. & Hwa R. (2008 September). Question generation as a competitive undergraduate course project. In Proceedings of the NSF Workshop on the Question Generation Shared Task and Evaluation Challenge. You may also like  Question-Answer Jokes Jokes of the question-answer form from Reddit's r/jokes Stanford Question Answering Dataset New Reading Comprehension Dataset on 100000+ Question-Answer Pairs Question Pairs Dataset Can you identify duplicate questions? ",Other,,"[languages, linguistics, artificial intelligence]",CC3,,,409,3480,5,Can you use NLP to answer these questions?,Question-Answer Dataset,https://www.kaggle.com/rtatman/questionanswer-dataset,Fri Sep 29 2017
,United Nations,"[country_or_area, year, value, category]","[string, numeric, numeric, string]",The Greenhouse Gas (GHG) Inventory Data contains the most recently submitted information covering the period from 1990 to the latest available year to the extent the data have been provided. The GHG data contain information on anthropogenic emissions by sources and removals by sinks of the following GHGs (carbon dioxide (CO2) methane (CH4) nitrous oxide (N2O) hydrofluorocarbons (HFCs) perfluorocarbons (PFCs) unspecified mix of HFCs and PFCs sulphur hexafluoride (SF6) and nitrogen triflouride (NF3)) that are not controlled by the Montreal Protocol. GHG emission inventories are developed by Parties to the Convention using scientific and methodological guidance from the Intergovernmental Panel on Climate Change (IPCC) such as 2006 IPCC Guidelines for National Greenhouse Gas Inventories Revised Guidelines for National Greenhouse Gas Inventories (1996) IPCC Good Practice Guidance and Uncertainty Management in National Greenhouse Gas Inventories (2000) and IPCC Good Practice Guidance on Land Use Land-use Change and Forestry (2003). Last update in UNdata 23 Mar 2017 with data released in Nov 2016. Acknowledgements This dataset was kindly published by the United Nation on the UNData site. You can find the original dataset here. License Per the UNData terms of use all data and metadata provided on UNdata’s website are available free of charge and may be copied freely duplicated and further distributed provided that UNdata is cited as the reference. ,CSV,,"[earth sciences, climate]",Other,,,269,1664,0.9658203125,A global GHG inventory from 1990-2017,International Greenhouse Gas Emissions,https://www.kaggle.com/unitednations/international-greenhouse-gas-emissions,Thu Nov 16 2017
,Jacob Boysen,"[latitude, location, longitude, name, station_id, status]","[numeric, string, numeric, string, numeric, string]",Context Bike shares are becoming a popular alternative means of transportation. The City of Austin makes data available on >649k bike trips over 2013-2017. Content This data includes information on bike trip start location stop location duration type of bike share user. Bike station location data is also provided. Dataset Description Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too. *austin_bikeshare_trips.csv*  bikeid integer id of bike  checkout_time HHMMSS see start time for date stamp duration_minutes int minutes of trip duration end_station_id integer id of end station end_station_name string of end station name month month integer start_station_id integer id of start station start_station_name string of start station name start_time YYYY-MM-DD HHMMSS subscriber_type membership typ e.g. walk up annual other bike share etc trip_id unique trip id int year year of trip int  *austin_bikeshare_stations.csv*  latitude geospatial latitude precision to 5 places location (lat long) longitude geospatial longitude precision to 5 places name station name str station_id unique station id int status station status (active closed moved ACL-only)  Acknowledgements This dataset is available from Google Public Data. Inspiration  What stations are most popular? At certain times? What are the average user trip?  Can you predict station usage to improve the ability of bike share employees to supply high-use stations? ,CSV,,[cycling],CC0,,,337,2176,83,Information on 649k Bike Rides Across Austin,Austin Bike Share Trips,https://www.kaggle.com/jboysen/austin-bike,Thu Aug 10 2017
,UCI Machine Learning,"[Age, Gender, Total_Bilirubin, Direct_Bilirubin, Alkaline_Phosphotase, Alamine_Aminotransferase, Aspartate_Aminotransferase, Total_Protiens, Albumin, Albumin_and_Globulin_Ratio, Dataset]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Patients with Liver disease have been continuously increasing because of excessive consumption of alcohol inhale of harmful gases intake of contaminated food pickles and drugs. This dataset was used to evaluate prediction algorithms in an effort to  reduce burden on doctors.  Content This data set contains 416 liver patient records and 167 non liver patient records collected from North East of Andhra Pradesh India.  The ""Dataset"" column is a class label used to divide groups into liver patient (liver disease) or not (no disease). This data set contains 441 male patient records and 142 female patient records.  Any patient whose age exceeded 89 is listed as being of age ""90"". Columns  Age of the patient  Gender of the patient  Total Bilirubin  Direct Bilirubin  Alkaline Phosphotase  Alamine Aminotransferase  Aspartate Aminotransferase  Total Protiens  Albumin  Albumin and Globulin Ratio  Dataset field used to split the data into two sets (patient with liver disease or no disease)  Acknowledgements This dataset was downloaded from the UCI ML Repository Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. Inspiration Use these patient records to determine which patients have liver disease and which ones do not. ",CSV,,"[healthcare, health sciences, health, medicine]",CC0,,,731,5692,0.0224609375,"Patient records collected from North East of Andhra Pradesh, India",Indian Liver Patient Records,https://www.kaggle.com/uciml/indian-liver-patient-records,Wed Sep 20 2017
,Amir Malekpour,[],[],"Context In 2007 I wrote my masters thesis on indoor location determination using Wi-Fi received signal strength indicator (RSSI). As a part of my research I gathered 120K RSSI samples from 4 access points on floor 1 and 2 of the building of my faculty. I wrote a custom software to do this (no reliable open source software for this at the time) and sampling was a long and painful process. With my limited ML learning skills at the time I came up with a simple algorithm that was able to find the location of a Wi-Fi device in that building with 85% accuracy.  I imagine this is a solved problem now and state of the art indoor positioning systems are way more accurate today. Yet I decided to find and publish this data set because it's small and simple enough for practice and at the same time it has some peculiarity for advanced data science and ML fun.  Content The samples were taken in a two-level building. Each row in the dataset is a single RSSI sample from one of the 4 access points. Access points are identified by one of the letters A B C or D. Physical coordinates of the location where each sample was taken is identified by XY Z coordinates with Z being the floor (1 or 2). At each coordinate multiple samples are taken where sample number is identified by a field name sequence.  The reason for taking multiple samples is that signal strength fluctuates due to things like scattering and reflection specially in buildings with moving objects and people. So to have reliable measurements one needs 10s of samples from each access points to make for the variability. Having- said this each row of the sample set has the format ap signal sequence xyz where ap Access point identifier. one of A B C or D signal Signal strength from access point ap.  Note RSSI values in the table are negated. That is smaller values in this cell mean stronger signal reception from the access point. sequence The sequence of sample from this particular access point at this particular coordinate xyz Coordinates where this sample was taken Note do not assume all locations have the same number of samples from all access points. For example at location (x=1y=1z=1) we might have  a samples from A b samples from B c samples from C and d samples from D and a != b != c!= d. Why? That's because in some areas we might have poor reception (or complete lack thereof) from an access point and so we end up with fewer or no samples.  Here's the floor plan of the building where sampling took place    These are the locations of the access points {""A"" (23 17 2) ""B"" (23 41 2) ""C""  (1 15 2) ""D"" (1 41 2)} You can also find the location of access points on the map.  By poking at this data and comparing it with the floor plan you'll learn interesting things about Wi-Fi radio signals (in 2.4 GHz frequency) and how they behave in indoors. One challenge is to come up with a ML model that given RSSI from the 4 access points finds the (xyz) coordinates of the location.",CSV,,[internet],CC4,,,57,623,0.37109375,"120K RSSI samples to determine X,Y,Z coordinates in a two-level building",Indoor location determination with RSSI,https://www.kaggle.com/amirma/indoor-location-determination-with-rssi,Fri Jan 19 2018
,Myles O'Neill,[],[],"Overwatch is a team-based multiplayer online first-person shooter video game developed and published by Blizzard Entertainment. It was released in May 2016 for Windows PlayStation 4 and Xbox One. Overwatch assigns players into two teams of six with each player selecting from a roster of over 20 characters known in-game as ""heroes"" each with a unique style of play whose roles are divided into four general categories Offense Defense Tank and Support. Players on a team work together to secure and defend control points on a map or escort a payload across the map in a limited amount of time.  I discovered this dataset on the Overwatch Subreddit here https//www.reddit.com/r/Overwatch/comments/7o8hmg/my_friend_has_recorded_every_game_hes_played/ It represents a ridiculous amount of effort in terms of manually recording game results. This data whilst in some places incomplete gives an unprecedented view into the experience of a single overwatch player over the course of years of gameplay. From it you can track the ups and downs shifts in hero preference and all sorts of other exciting in game trends. Thanks to JustWingIt for their amazing collecting this data. I cleaned the data a little and put it into a single CSV.",CSV,,"[games and toys, video games]",CC0,,,144,1409,0.6767578125,"One player, thousands of games, stats meticulously recorded!",Overwatch Game Records,https://www.kaggle.com/mylesoneill/overwatch-game-records,Fri Jan 05 2018
,Sohier Dane,[],[],This dataset contains hourly estimates of an area's energy potential for 1986-2015 as a percentage of a power plant's maximum output. The overall scope of EMHIRES is to allow users to assess the impact of meteorological and climate variability on the generation of solar power in Europe and not to mime the actual evolution of solar power production in the latest decades. For this reason the hourly solar power generation time series are released for meteorological conditions of the years 1986-2015 (30 years) without considering any changes in the solar installed capacity. Thus the installed capacity considered is fixed as the one installed at the end of 2015. For this reason data from EMHIRES should not be compared with actual power generation data other than referring to the reference year 2015.  Content  The data is available at both the national level and the NUTS 2 level. The NUTS 2 system divides the EU into 276 statistical units. Please see the manual for the technical details of how these estimates were generated.  This product is intended for policy analysis over a wide area and is not the best for estimating the output from a single system. Please don't use it commercially.  Acknowledgements This dataset was kindly made available by the European Commission's STETIS program. You can find the original dataset here. Inspiration  How clean is the dataset? Older solar estimates used to contain impossible values around sunset (ie more energy than the sun releases) or negative sunlight. What does a typical year look like? One common approach is to stitch together 12 months of raw data using the 12 most typical months per this ISO standard.  If you like If you like this dataset you might also enjoy  - 30 years of European wind  - Google's Project Sunroof ,CSV,,[energy],CC0,,,379,3024,564,Hourly energy potential for 1986-2015,30 Years of European Solar Generation,https://www.kaggle.com/sohier/30-years-of-european-solar-generation,Fri Sep 15 2017
,US Bureau of Labor Statistics,"[area_code, area_text, display_level, selectable, sort_sequence]","[numeric, string, numeric, numeric, numeric]",Context The Occupational Employment Statistics (OES) and National Compensation Survey (NCS) programs have produced estimates by borrowing from the strength and breadth of each survey to provide more details on occupational wages than either program provides individually. Modeled wage estimates provide annual estimates of average hourly wages for occupations by selected job characteristics and within  geographical location. The job characteristics include bargaining status (union and nonunion) part- and full-time work status incentive- and time-based pay and work levels by occupation. Direct estimates are based on survey responses only from the particular geographic area to which the estimate refers. In contrast modeled wage estimates use survey responses from larger areas to fill in information for smaller areas where the sample size is not sufficient to produce direct estimates. Modeled wage estimates require the assumption that the patterns to responses in the larger area hold in the smaller area. The sample size for the NCS is not large enough to produce direct estimates by area occupation and job characteristic for all of the areas for which the OES publishes estimates by area and occupation. The NCS sample consists of 6 private industry panels with approximately 3300 establishments sampled per panel and 1600 sampled state and local government units. The OES full six-panel sample consists of nearly 1.2 million establishments.  The sample establishments are classified in industry categories based on the North American Industry Classification System (NAICS). Within an establishment specific job categories are selected to represent broader occupational definitions. Jobs are classified according to the Standard Occupational Classification (SOC) system. Content Summary Average hourly wage estimates for civilian workers in occupations by job characteristic and work levels. These data are available at the national state metropolitan and nonmetropolitan area levels. Frequency of Observations Data are available on an annual basis typically in May.  Data Characteristics All hourly wages are published to the nearest cent. Acknowledgements This dataset was taken directly from the Bureau of Labor Statistics and converted to CSV format.  Inspiration This dataset contains the estimated wages of civilian workers in the United States. Wage changes in certain industries may be indicators for growth or decline. Which industries have had the greatest increases in wages? Combine this dataset with the Bureau of Labor Statistics Consumer Price Index dataset and find out what kinds of jobs you would need to afford your snacks and instant coffee!,CSV,,[income],CC0,,,303,2725,60,Modeled wage estimates of average hourly wages,Wage Estimates,https://www.kaggle.com/bls/wage-estimates,Thu Jun 29 2017
,Rachael Tatman,"[NAME, GENDER, AGE, HOMETOWN, HOMESTATE, CURRENT STATE, EDUCATION, YEARS OF EDUCATION, OCUPATION, ETHNICITY, RECORDING]","[string, string, numeric, string, string, string, string, numeric, string, string, numeric]",Context The Santa Barbara Corpus of Spoken American English is based on hundreds of recordings of natural speech from all over the United States representing a wide variety of people of different regional origins ages occupations and ethnic and social backgrounds. It reflects many ways that people use language in their lives conversation gossip arguments on-the-job talk card games city council meetings sales pitches classroom lectures political speeches bedtime stories sermons weddings and more. The corpus was collected by the University of California Santa Barbara Center for the Study of Discourse Director John W. Du Bois (UCSB) Associate Editors Wallace L. Chafe (UCSB) Charles Meyer (UMass Boston) and Sandra A. Thompson (UCSB). Each speech file is accompanied by a transcript in which phrases are time stamped with respect to the audio recording. Personal names place names phone numbers etc. in the transcripts have been altered to preserve the anonymity of the speakers and their acquaintances and the audio files have been filtered to make these portions of the recordings unrecognizable. Pitch information is still recoverable from these filtered portions of the recordings but the amplitude levels in these regions have been reduced relative to the original signal. The audio data consists of MP3 format speech files recorded in two-channel pcm at 22050Hz. Contents This dataset contains part one of the corpus. The other three parts and additional information can be found here. The following information is included in this dataset  Recordings 14 recordings as .mp3 files Transcripts Time-aligned transcripts for all 14 recordings in the CHAT format Metadata A .csv with demographic information on speakers as well as which recordings they appear in. (Some talkers appear in more than one recording.)  Acknowledgements The Santa Barbara Corpus was compiled by researchers in the Linguistics Department of the University of California Santa Barbara. The Director of the Santa Barbara Corpus is John W. Du Bois working with Associate Editors Wallace L. Chafe and Sandra A. Thompson (all of UC Santa Barbara) and Charles Meyer (UMass Boston). For the publication of Parts 3 and 4 the authors are John W. Du Bois and Robert Englebretson. It is distributed here under an CC BY-ND 3.0 US license. Inspiration  Currently the transcriptions are close transcriptions and include disfluencies and overlaps. Can you use NLP to convert them to broad transcriptions without this information? Can you create a phone-aligned transcription of this dataset? You might find it helpful to use forced alignment.  ,Other,,"[languages, linguistics]",Other,,,97,2761,2048,"Transcribed recordings of natural, conversational speech",Santa Barbara Corpus of Spoken American English,https://www.kaggle.com/rtatman/santa-barbara-corpus-of-spoken-american-english,Thu Sep 14 2017
,Kevin Mader,[],[],"Content The data is images and labels / annotations for mammography scans. More about the database can be found at MIAS. The 'Preview' kernel shows how the Info.txt and PGM files can be parsed correctly. Labels  1st column MIAS database reference number. 2nd column Character of background tissue F  Fatty G  Fatty-glandular D  Dense-glandular 3rd column Class of abnormality present CALC  Calcification CIRC  Well-defined/circumscribed masses SPIC  Spiculated masses MISC  Other ill-defined masses ARCH  Architectural distortion ASYM  Asymmetry NORM  Normal 4th column Severity of abnormality; B  Benign M  Malignant 5th 6th columns xy image-coordinates of centre of abnormality. 7th column Approximate radius (in pixels) of a circle enclosing the abnormality. There are also several things you should note  The list is arranged in pairs of films where each pair represents the left (even filename numbers) and right mammograms (odd filename numbers) of a single patient. The size of all the images is 1024 pixels x 1024 pixels. The images have been centered in the matrix. When calcifications are present centre locations and radii apply to clusters rather than individual calcifications. Coordinate system origin is the bottom-left corner. In some cases calcifications are widely distributed throughout the image rather than concentrated at a single site. In these cases centre locations and radii are inappropriate and have been omitted. Acknowledgements/LICENCE MAMMOGRAPHIC IMAGE ANALYSIS SOCIETY                      MiniMammographic Database                    LICENCE AGREEMENT  This is a legal agreement between you the end user and the Mammographic Image Analysis Society (""MIAS""). Upon installing the  MiniMammographic database (the ""DATABASE"") on your system you are agreeing to be bound by the terms of this Agreement.  GRANT OF LICENCE MIAS grants you the right to use the DATABASE for research purposes  ONLY. For this purpose you may edit format or otherwise modify the DATABASE provided that the unmodified portions of the DATABASE included in a modified work shall remain subject to the terms of this Agreement. COPYRIGHT The DATABASE is owned by MIAS and is protected by United Kingdom copyright laws international treaty provisions and all other applicable national laws. Therefore you must treat the DATABASE like any other copyrighted material. If the DATABASE is used in any publications then reference must be made to the DATABASE within that publication. OTHER RESTRICTIONS You may not rent lease or sell the DATABASE. LIABILITY To the maximum extent permitted by applicable law MIAS shall not be liable for damages other than death or personal injury whatsoever (including without limitation damages for negligence loss of business profits business interruption loss of business information or other pecuniary loss) arising out of the use of or inability to use this DATABASE even if MIAS has been advised of the possibility of such damages. In any case MIAS's entire liability under this Agreement shall be limited to the amount actually paid by you or your assignor as the case may be for the DATABASE.  Inspiration Automatically finding lesions would be a very helpful tool for physicians also predicting malignancy based on a found/marked lesion",Other,,"[healthcare, health]",Other,,,320,2727,206,Looking for breast cancer,MIAS Mammography,https://www.kaggle.com/kmader/mias-mammography,Wed Nov 01 2017
,Andrew Thompson,[],[],Context I wanted to see how articles clustered together if the articles were rendered into document-term matrices---would there be greater affinity among political affiliations or medium subject matter etc. The data was scraped using BeautifulSoup and stored in Sqlite but I've chopped it up into three separate CSVs here because the entire Sqlite database came out to about 1.2 gb beyond Kaggle's max. Content Each row contains  an id for the Sqlite database author name full date month year title publication name article url (not available for all articles) full article content  The publications include the New York Times Breitbart CNN Business Insider the Atlantic Fox News Talking Points Memo Buzzfeed News National Review New York Post the Guardian NPR Reuters Vox and the Washington Post. Sampling wasn't quite scientific; I chose publications based on my familiarity of the domain and tried to get a range of political alignments as well as a mix of print and digital publications. By count the publications break down accordingly  It's not entirely even---this was something of a collect-it-all approach and some sites are more prolific than others and some have data that maintains integrity after scraping more easily than others. For each publication I used archive.org to grab the past year-and-a-half of either home-page headlines or RSS feeds and ran those links through the scraper. That is the articles are not the product of scraping an entire site but rather their more prominently placed articles. For example CNN's articles from 5/6/16 were what appeared on the homepage of CNN.com proper not everything within the CNN.com domain. Vox's articles from 5/6/16 were everything that appeared in the Vox RSS reader. on 5/6/16 and so on. RSS readers are a breeze to scrape and so I used them when possible but not every publication uses them or makes them easy to find. The data primarily falls between the years of 2016 and July 2017 although there is a not-insignificant number of articles from 2015 and a possibly insignificant number from before then. A note there are some stray spaces between non-word characters at times as well as some other minor blemishes and imperfections here and there the result of cleaning a very messy dataset.  Acknowledgements Thanks mostly go to the maesters of Stack Overflow. ,CSV,,[journalism],Other,,,1191,9432,639,"143,000 articles from 15 American publications",All the news,https://www.kaggle.com/snapcrack/all-the-news,Sun Aug 20 2017
,Oh InQueue,[],[],Context I got all these .csv files using pandas data reader but getting every single kospi data through pandas data reader is annoying. so I decided to share this files. Content Files kospi.csv contains average kospi price. you can use this for checking whether if korean stock is day-off or not. xxxxxx.csv contains each single price records. xxxxxx is it's unique ticker. Columns Date  format - \d{4}-\d{2}-\d{2}  Open  format - \d{1}\.\d{1}  High  format - \d{1}\.\d{1}  Low  format - \d{1}\.\d{1}  Close  format - \d{1}\.\d{1}  Adj Close  format - \d{1}\.\d{1}  Volume  format - \d+  Acknowledgements blog post which describes how i got these data's. you might need this to update csv files. git repository git repository  Inspiration Good luck.,Other,,"[finance, economics]",CC0,,,129,1525,155,Korean Stock Kospi Prices,Kospi Stock Price,https://www.kaggle.com/gomjellie/kospi-price-data,Sat Sep 30 2017
,Rachael Tatman,"[, Wikipedia.Language.Code, Language.name..English., Language.name..native., Last.Updated]","[numeric, string, string, string, dateTime]",Context Sentiment analysis the task of automatically detecting whether a piece of text is positive or negative generally relies on a hand-curated list of words with positive sentiment (good great awesome) and negative sentiment (bad gross awful). This dataset contains both positive and negative sentiment lexicons for 81 languages. Content The sentiment lexicons in this dataset were generated via graph propagation based on a knowledge graph--a graphical representation of real-world entities and the links between them. The general intuition is that words which are closely linked on a  knowledge graph probably have similar sentiment polarities. For this project sentiments were generated based on English sentiment lexicons.  This dataset contains sentiment lexicons for the following languages  Afrikaans Albanian Arabic Aragonese Armenian Azerbaijani Basque Belarusian Bengali Bosnian Breton Bulgarian Catalan Chinese Croatian Czech Danish Dutch Esperanto Estonian Faroese Finnish French Galician Georgian German Greek Gujarati Haitian Creole Hebrew Hindi Hungarian Icelandic Ido Indonesian Interlingua Irish Italian Kannada Khmer Kirghiz Korean Kurdish Latin Latvian Lithuanian Luxembourgish Macedonian Malay Maltese Marathi Norwegian Norwegian Persian Polish Portuguese Romanian Romansh Russian Scottish Serbian Slovak Slovene Spanish Swahili Swedish Tagalog Tamil Telugu Thai Turkish Turkmen Ukrainian Urdu Uzbek Vietnamese Volapük Walloon Welsh Western Frisian Yiddish For more information and additional sentiment lexicons please visit the project’s website.   Acknowledgements This dataset was collected by Yanqing Chen and Steven Skiena. If you use it in your work please cite the following paper Chen Y. & Skiena S. (2014). Building Sentiment Lexicons for All Major Languages. In ACL (2) (pp. 383-389). It is distributed here under the GNU General Public License. Note that this is the full GPL which allows many free uses but does not allow its incorporation into any type of distributed proprietary software even in part or in translation. For commercial applications please contact the dataset creators. Inspiration  These word lists contain many words with similar meanings. Can you automatically detect which words are cognates? Can you use these sentiment lexicons to reverse-engineer the knowledge graphs that generated them? ,CSV,,"[languages, linguistics]",Other,,,556,4540,2,Sentiment Polarity Lexicons (Positive vs. Negative),Sentiment Lexicons for 81 Languages,https://www.kaggle.com/rtatman/sentiment-lexicons-for-81-languages,Thu Sep 14 2017
,Jacob Boysen,[],[],Context Crime in growing cities such as Austin changes with the population. This data covers individual crimes reported in Austin primarily 2014-2015. Content 159k rows of data on type of crime reported location by various attributes (lat/lon council district census tract) and time are included. Clearance status by Austin PD is also recorded where available. Acknowledgements Data was prepared from a txt file accessed via Google Cloud BigQuery Public Datasets. Image by Tobias Zils. Inspiration Are there any clear seasonal or hourly trends in certain crimes? Which crimes are most often cleared by Austin PD and which remain open? How long do clearances take?,CSV,,[crime],CC0,,,154,1342,19,"159k Crime Reports, 2014-2016",Austin Crime Statistics,https://www.kaggle.com/jboysen/austin-crime,Thu Aug 03 2017
,Institute for Public Policy and Social Research,[],[],Context The Correlates of State Policy Project aims to compile disseminate and encourage the use of data relevant to U.S. state policy research tracking policy differences across and change over time in the 50 states. We have gathered more than nine-hundred variables from various sources and assembled them into one large useful dataset. We hope this Project will become a “one-stop shop” for academics policy analysts students and researchers looking for variables germane to the study of state policies and politics.  Content The Correlates of State Policy Project includes more than nine-hundred variables with observations across the U.S. 50 states and time (1900 – 2016). These variables represent policy outputs or political social or economic factors that may influence policy differences across the states. The codebook includes the variable name a short description of the variable the variable time frame a longer description of the variable and the variable source(s) and notes. Take a look at the codebook PDF to get more information about each column Acknowledgements This aggregated data set is only possible because many scholars and students have spent tireless hours creating collecting cleaning and making data publicly available. Thus if you use the dataset please cite the original data sources.  Jordan Marty P. and Matt Grossmann. 2016. The Correlates of State Policy Project v.1.10. East Lansing MI Institute for Public Policy and Social Research (IPPSR).  This dataset was originally downloaded from  http//ippsr.msu.edu/public-policy/correlates-state-policy,Other,,[],CC0,,,77,602,15,One-stop shop for anyone studying state policies and politics,The Correlates of State Policy Project,https://www.kaggle.com/ippsr/correlates-state-policy,Thu Jul 27 2017
,The Wall Street Journal,"[Undergraduate Major, Starting Median Salary, Mid-Career Median Salary, Percent change from Starting to Mid-Career Salary, Mid-Career 10th Percentile Salary, Mid-Career 25th Percentile Salary, Mid-Career 75th Percentile Salary, Mid-Career 90th Percentile Salary]","[string, string, string, numeric, string, string, string, string]",Salary Increase By Type of College Party school? Liberal Arts college? State School? You already know your starting salary will be different depending on what type of school you attend. But increased earning power shows less disparity. Ten years out graduates of Ivy League schools earned 99% more than they did at graduation. Party school graduates saw an 85% increase. Engineering school graduates fared worst earning 76% more 10 years out of school. See where your school ranks.  Salaries By Region Attending college in the Midwest leads to the lowest salary both at graduation and at mid-career according to the PayScale Inc. survey. Graduates of schools in the Northeast and California fared best.  Salary Increase By Major Your parents might have worried when you chose Philosophy or International Relations as a major. But a year-long survey of 1.2 million people with only a bachelor's degree by PayScale Inc. shows that graduates in these subjects earned 103.5% and 97.8% more respectively about 10 years post-commencement. Majors that didn't show as much salary growth include Nursing and Information Technology.  All data was obtained from the Wall Street Journal based on data from Payscale Inc  Salaries for Colleges by Type Salaries for Colleges by Region Degrees that Pay you Back ,CSV,,"[news agencies, universities and colleges, employment, education]",Other,,,9522,50767,0.0703125,"Salaries by college, region, and academic major",Where it Pays to Attend College,https://www.kaggle.com/wsj/college-salaries,Sat Apr 29 2017
,Asma BELHAOUA,"[, X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15, X16, X17, X18, X19, X20, X21, X22, X23, X24, X25, X26, X27, X28, X29, X30, X31, X32, X33, X34, X35, X36, X37, X38, X39, X40, X41, X42, X43, X44, X45, X46, X47, X48, X49, X50, X51, X52, X53, X54, X55, X56, X57, X58, X59, X60, X61, X62, X63, X64, X65, X66, X67, X68, X69, X70, X71, X72, X73, X74, X75, X76, X77, X78, X79, X80, X81, X82, X83, X84, X85, X86, X87, X88, X89, X90, X91, X92, X93, X94, X95, X96, X97, X98, X99]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Epilepsy is a group of neurological disorders characterized by epileptic seizures. Epileptic seizures are episodes that can vary from brief and nearly undetectable to long periods of vigorous shaking. Content The original dataset consists of 5 different folders each with 100 files with each file representing a single subject/person. Each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points. Each data point is the value of the EEG recording at a different point in time.  The column y contains the category of the 178-dimensional input vector. Specifically y in {1 2 3 4 5}  5- eyes open means when they were recording the EEG signal of the brain the patient had their eyes open  4- eyes closed means when they were recording the EEG signal the patient had their eyes closed  3- Yes they identify where the region of the tumor was in the brain and recording the EEG activity from the healthy brain area  2- They recorder the EEG from the area where the tumor was located  All subjects falling in classes 2 3 4 and 5 are subjects who did not have epileptic seizure  1- Recording of seizure activity  The Explanatory variables X1 X2 ... X178  Each 178-dimensional vector contained in a row represents a randomly selected 1-second long sample picked from the single file. Recall that  each file is a recording of brain activity for 23.6 seconds. The corresponding time-series is sampled into 4097 data points.  Acknowledgements Andrzejak RG Lehnertz K Rieke C Mormann F David P Elger CE (2001) Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity Dependence on recording region and brain state Phys. Rev. E 64 061907 Inspiration Can you help epileptic people live a better life ? ,CSV,,"[neuroscience, health]",CC0,,,282,2570,7,Can you recognize if it is a seizure or not ?,Epileptic Seizure Recognition,https://www.kaggle.com/younasm/epileptic-seizure-recognition,Sun Oct 29 2017
, U.S. Government Publishing Office,[],[],"The United States Code (""Code"") contains the general and permanent laws of the United States arranged into 54 broad titles according to subject matter. The organization of the Code was originally established by Congress in 1926 with the enactment of the act of June 30 1926 chapter 712. Since then 27 of the titles referred to as positive law titles have been restated and enacted into law by Congress as titles of the Code. The remaining titles referred to as non-positive law titles are made up of sections from many acts of Congress that were either included in the original Code or subsequently added by the editors of the Code i.e. the Office of the Law Revision Counsel and its predecessors in the House of Representatives. Positive law titles are identified by an asterisk on the Search & Browse page. For an explanation of the meaning of positive law see the Positive Law Codification page. Each title of the Code is subdivided into a combination of smaller units such as subtitles chapters subchapters parts subparts and sections not necessarily in that order. Sections are often subdivided into a combination of smaller units such as subsections paragraphs subparagraphs clauses subclauses and items. In the case of a positive law title the units are determined by Congress in the laws that enact and later amend the title. In the case of a non-positive law title the organization of the title since 1926 has been determined by the editors of the Code and has generally followed the organization of the underlying acts 1 as much as possible. For example chapter 7 of title 42 sets out the titles parts and sections of the Social Security Act as corresponding subchapters parts and sections of the chapter. In addition to the sections themselves the Code includes statutory provisions set out as statutory notes the Constitution several sets of Federal court rules and certain Presidential documents such as Executive orders determinations notices and proclamations that implement or relate to statutory provisions in the Code. The Code does not include treaties agency regulations State or District of Columbia laws or most acts that are temporary or special such as those that appropriate money for specific years or that apply to only a limited number of people or a specific place. For an explanation of the process of determining which new acts are included in the Code see the About Classification page. The Code also contains editorially created source credits notes and tables that provide information about the source of Code sections their arrangement the references they contain and their history. The law contained in the Code is the product of over 200 years of legislating. Drafting styles have changed over the years and the resulting differences in laws are reflected in the Code. Similarly Code editorial styles and policies have evolved over the 80-plus years since the Code was first adopted. As a result not all acts have been handled in a consistent manner in the Code over time. This guide explains the editorial styles and policies currently used to produce the Code but the reader should be aware that some things may have been done differently in the past. However despite the evolution of style over the years the accuracy of the information presented in the Code has always been and will always remain a top priority. Content This dataset is a snapshot of the XML version of the United States Code. It is not a suitable for any form of legal work and is intended for research purposes only.  The data are stored in a large json dictionary indexed by the title of the code. Acknowledgements This dataset was released by the United States Government Publishing Office. You can find the original dataset here. ",Other,,"[government, law]",CC0,,,53,736,591,The general and permanent laws of the United States,United States Code,https://www.kaggle.com/us-gpo/united-states-code,Tue Sep 05 2017
,Fernando O. Gallego,"[sentence_id, condition_id, begin_connective, end_connective, begin_condition, end_condition, language, domain]","[numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context This dataset was created during my PhD (http//www.tdg-seville.info/fogallego/Personal%20Info) at the University of Seville. We didn't found any datasets with labelled conditions so we decided to build one since our main goal for the PhD was to be able to identify conditions without relying on user-defined patterns or requiring any specific-purpose dictionaries taxonomies or heuristics. Content The reviews in English and Spanish were randomly gathered from ciao.com between April 2017 and May 2017. The sentences were classified into 15 domains according to their sources namely adults baby care beauty books cameras computers films headsets hotels music ovens pets phones TV sets and video games. Our dataset consist of two files sentences.csv and conditions.csv. The first one contains the whole set of sentences and the second one the manually labelled conditions. In order to better understand the meaning of each column I'll explain them in detail sentence.csv  sentence_uuid the unique identifier of the sentence sentence_text the text of the sentence language the language of the sentence domain the domain of the sentence labelled whether the sentence was labelled or not  conditions.csv  sentence_uuid the unique identifier of the corresponding labelled sentence condition_uuid the unique identifier of the condition begin_connective the character position where the connective of the condition starts end_connective the character position where the connective of the condition ends begin_condition the character position where the rest of the condition starts end_condition the character position where the rest of the condition ends language the language of the corresponding labelled sentence domain the domain of the corresponding labelled sentence  Acknowledgements My PhD and this dataset were supported by Opileak.com and the Spanish R&D programme (grants TIN2013- 40848-R and TIN2013-40848-R).,CSV,,"[nlp, text data]",GPL,,,52,761,179,A dataset with labelled and unlabelled sentences from reviews with conditions.,Reviews with conditions,https://www.kaggle.com/fogallego/reviews-with-conditions,Thu Jan 25 2018
,PyTorch,[],[],VGG-13  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,1,159,471,VGG-13 Pre-trained model with batch normalization for PyTorch,VGG-13 with batch normalization,https://www.kaggle.com/pytorch/vgg13bn,Sat Dec 16 2017
,Google Brain,"[CategoryId, CategoryName]","[numeric, string]",Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases these modifications can be so subtle that a human observer does not even notice the modification at all yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems even if the adversary has no access to the underlying model. To accelerate research on adversarial examples Google Brain is organizing Competition on Adversarial Examples and Defenses within the NIPS 2017 competition track. This dataset contains the development images for this competition. The competition on Adversarial Examples and Defenses consist of three sub-competitions  Non-targeted Adversarial Attack. The goal of the non-targeted attack is to slightly modify source image in a way that image will be classified incorrectly by generally unknown machine learning classifier. Targeted Adversarial Attack. The goal of the targeted attack is to slightly modify source image in a way that image will be classified as specified target class by generally unknown machine learning classifier. Defense Against Adversarial Attack. The goal of the defense is to build machine learning classifier which is robust to adversarial example i.e. can classify adversarial images correctly.  In each of the sub-competitions you're invited to make and submit a program which solves the corresponding task. In the end of the competition we will run all attacks against all defenses to evaluate how each of the attacks performs against each of the defenses.,CSV,,[artificial intelligence],Other,,,884,5488,146,Development images used in the NIPS 2017 Adversarial Learning challenges,NIPS 2017: Adversarial Learning Development Set,https://www.kaggle.com/google-brain/nips-2017-adversarial-learning-development-set,Sun Jul 02 2017
,Airly,"[UTC time, 3_temperature, 3_humidity, 3_pressure, 3_pm1, 3_pm25, 3_pm10, 140_temperature, 140_humidity, 140_pressure, 140_pm1, 140_pm25, 140_pm10, 142_temperature, 142_humidity, 142_pressure, 142_pm1, 142_pm25, 142_pm10, 147_temperature, 147_humidity, 147_pressure, 147_pm1, 147_pm25, 147_pm10, 169_temperature, 169_humidity, 169_pressure, 169_pm1, 169_pm25, 169_pm10, 170_temperature, 170_humidity, 170_pressure, 170_pm1, 170_pm25, 170_pm10, 171_temperature, 171_humidity, 171_pressure, 171_pm1, 171_pm25, 171_pm10, 172_temperature, 172_humidity, 172_pressure, 172_pm1, 172_pm25, 172_pm10, 173_temperature, 173_humidity, 173_pressure, 173_pm1, 173_pm25, 173_pm10, 174_temperature, 174_humidity, 174_pressure, 174_pm1, 174_pm25, 174_pm10, 176_temperature, 176_humidity, 176_pressure, 176_pm1, 176_pm25, 176_pm10, 177_temperature, 177_humidity, 177_pressure, 177_pm1, 177_pm25, 177_pm10, 178_temperature, 178_humidity, 178_pressure, 178_pm1, 178_pm25, 178_pm10, 179_temperature, 179_humidity, 179_pressure, 179_pm1, 179_pm25, 179_pm10, 180_temperature, 180_humidity, 180_pressure, 180_pm1, 180_pm25, 180_pm10, 181_temperature, 181_humidity, 181_pressure, 181_pm1, 181_pm25, 181_pm10, 182_temperature, 182_humidity, 182_pressure]","[dateTime, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string]",Context In the past the ancient city of Krakow was known as the capital of Poland. In 2000 it became known as the official European Capital of Culture. Now it is known for having some of the most polluted air in Europe... In a World Health Organiation (WHO) study Krakow has been rated amongst the most polluted in the world. In the report city was ranked 8th among 575 cities for levels of PM2.5 and 145th among 1100 cities for levels of PM10. Hazardous air quality is a common problem particularly during the colder months when many residents use solid fuels (mostly coal) for household heating. Air pollution in Krakow poses a significant danger to human health and life. Krakow's poisoned air includes amongst other things particulate matter benzo(a)pyrene and nitrogen dioxide. The state-run network of monitoring stations consists of 8 monitoring stations in Krakow. We decided to go step further - to build network of low-cost air quality sensors that can be deployed across entire city. The first step in the fight against smog is to identify areas of problem and to raise awareness among residents and the authorities. It is very important to create a network of sensors – only then you can check the actual conditions in various areas of the city. The technology enables real-time monitoring of air quality via map.airly.eu so the information about the air in a specific location is easily accessible and always up to date. Content This dataset consists air quality data (the concentrations of particulate matter PM1 PM2.5 and PM10 temperature air pressure and humidity) from 2017 generated by network of 56 low-cost sensors located in Krakow Poland. Each had its own location (6 of them where replaced during this time period and have almost the same latitude and longitude). Measurements are grouped in 12 files one for each month. Resolution of data is 1 hour. Known issues - PM1 is not calibrated and therefore can be bigger than PM2.5 - PM2.5 can be bigger than PM10 within the limits of measurement error - for the first two months humidity and temperature were not calibrated and therefore can show inaccurate values Acknowledgements The data was generated by Airly network - the project is still in its beginning stage but over 1000 sensors have already been implemented in Poland. Airly is a startup definitely worth watching especially for citizens of the most polluted cities. After all it’s clean air we all want to breathe. Inspiration I think that this dataset offers some great opportunities for predictive models and data visualization. Airly's goal is to develop an effective forecast and monitoring of air quality employ Artificial Intelligence and utilise data from extensive sensor network. If anyone has any ideas breakthroughs or other interesting models please post them. Some questions worth exploring - What are the best prediction models based on extensive sensor network - statistical or numerical forecast? - How weather affects air quality? - How much pollution comes from cars factories and coal-fired power plants?,CSV,,"[environment, pollution, health]",CC4,,,260,2220,8,"PM1, PM2.5, PM10, temp, pres and hum data for 2017 year from Krakow, Poland",Air quality data from extensive network of sensors,https://www.kaggle.com/datascienceairly/air-quality-data-from-extensive-network-of-sensors,Thu Dec 28 2017
,US Census Bureau,"[country_code, country_name, year, fertility_rate_15_19, fertility_rate_20_24, fertility_rate_25_29, fertility_rate_30_34, fertility_rate_35_39, fertility_rate_40_44, fertility_rate_45_49, total_fertility_rate, gross_reproduction_rate, sex_ratio_at_birth]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The United States Census Bureau’s International Dataset provides estimates of country populations since 1950 and projections through 2050. Specifically the data set includes midyear population figures broken down by age and gender assignment at birth. Additionally they provide time-series data for attributes including fertility rates birth rates death rates and migration rates. The full documentation is available here. For basic field details please see the data dictionary.  Note The U.S. Census Bureau provides estimates and projections for countries and areas that are recognized by the U.S. Department of State that have a population of at least 5000. Acknowledgements This dataset was created by the United States Census Bureau. Inspiration Which countries have made the largest improvements in life expectancy? Based on current trends how long will it take each country to catch up to today’s best performers? Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/international-census.,CSV,,"[demographics, international relations]",Other,,,1188,7204,2048,International health and population metrics,International Datasets,https://www.kaggle.com/census/international-data,Tue Jun 27 2017
,Hugo Mathien,[],[],"The ultimate Soccer database for data analysis and machine learning What you get  +25000 matches +10000 players 11 European Countries with their lead championship Seasons 2008 to 2016 Players and Teams' attributes* sourced from EA Sports' FIFA video game series including the weekly updates Team line up with squad formation (X Y coordinates) Betting odds from up to 10 providers Detailed match events (goal types possession corner cross fouls cards etc...) for +10000 matches  *16th Oct 2016 New table containing teams' attributes from FIFA !  Original Data Source  You can easily find data about soccer matches but they are usually scattered across different websites. A thorough data collection and processing has been done to make your life easier. I must insist that you do not make any commercial use of the data. The data was sourced from  http//football-data.mx-api.enetscores.com/  scores lineup team formation and events http//www.football-data.co.uk/  betting odds. Click here to understand the column naming system for betting odds  http//sofifa.com/  players and teams attributes from EA Sports FIFA games. FIFA series and all FIFA assets property of EA Sports.   When you have a look at the database you will notice foreign keys for   players and matches are the same as the original data sources. I have   called those foreign keys ""api_id"".   Improving the dataset  You will notice that some players are missing from the lineup (NULL values). This is because I have not been able to source their attributes from FIFA. This will be fixed overtime as the crawling algorithm is being improved. The dataset will also be expanded to include international games national cups Champion's League and Europa League. Please ask me if you're after a specific tournament.  Please get in touch with me if you want to help improve this dataset.   CLICK HERE TO ACCESS THE PROJECT GITHUB Important note for people interested in using the crawlers since I first wrote the crawling scripts (in python) it appears sofifa.com has changed its design and with it comes new requirements for the scripts. The existing script to crawl players ('Player Spider') will not work until i've updated it.  Exploring the data Now that's the fun part there is a lot you can do with this dataset. I will be adding visuals and insights to this overview page but please have a look at the kernels and give it a try yourself ! Here are some ideas for you The Holy Grail... ... is obviously to predict the outcome of the game. The bookies use 3 classes (Home Win Draw Away Win). They get it right about 53% of the time. This is also what I've achieved so far using my own SVM. Though it may sound high for such a random sport game you've got to know  that the home team wins about 46% of the time. So the base case (constantly predicting Home Win) has indeed 46% precision.  Probabilities vs Odds When running a multi-class classifier like SVM you could also output a probability estimate and compare it to the betting odds. Have a look at your variance vs odds and see for what games you had very different predictions. Explore and visualize features With access to players and teams attributes team formations and in-game events you should be able to produce some interesting insights into The Beautiful Game . Who knows Guardiola himself may hire one of you some day!",SQLite,,"[association football, europe]",ODbL,,,46463,396995,299,"25k+ matches, players & teams attributes for European Professional Football",European Soccer Database,https://www.kaggle.com/hugomathien/soccer,Mon Oct 24 2016
,Jay Ravaliya,"[Year, Month, State, County, Rate]","[numeric, string, string, string, numeric]",Context This is a dataset that I built by scraping the United States Department of Labor's Bureau of Labor Statistics. I was looking for county-level unemployment data and realized that there was a data source for this but the data set itself hadn't existed yet so I decided to write a scraper and build it out myself. Content This data represents the Local Area Unemployment Statistics from 1990-2016 broken down by state and month. The data itself is pulled from this mapping site https//data.bls.gov/map/MapToolServlet?survey=la&map=county&seasonal=u Further the ever-evolving and ever-improving codebase that pulled this data is available here https//github.com/jayrav13/bls_local_area_unemployment Acknowledgements Of course a huge shoutout to bls.gov and their open and transparent data. I've certainly been inspired to dive into US-related data recently and having this data open further enables my curiosities. Inspiration I was excited about building this data set out because I was pretty sure something similar didn't exist - curious to see what folks can do with it once they run with it! A curious question I had was surrounding Unemployment vs 2016 Presidential Election outcome down to the county level. A comparison can probably lead to interesting questions and discoveries such as trends in local elections that led to their most recent election outcome etc. Next Steps Version 1 of this is as a massive JSON blob normalized by year / month / state. I intend to transform this into a CSV in the future as well.,CSV,,[employment],CC0,,,798,6021,77,Thanks to the US Department of Labor's Bureau of Labor Statistics,"US Unemployment Rate by County, 1990-2016",https://www.kaggle.com/jayrav13/unemployment-by-county-us,Tue May 23 2017
,Electronic Frontier Foundation,[],[],Context In 2016 California investigators used state wiretapping laws 563 times to capture 7.8 million communications from 181000 people and only 19% of these communications were incriminating.  The year's wiretaps cost nearly $30 million.  We know this and much more now that the California Department of Justice (CADOJ) for the first time has released to EFF the dataset underlying its annual wiretap report to the state legislature.  Content The yearly “Electronic Interceptions Report” includes county-by-county granular data on wiretaps on landlines cell phones computers pagers1 and other devices.  Each interception is accompanied by information on the number of communications captured and the number of people those communications involved as well as what percentage of the messages were incriminating. The report also discloses the criminal justice outcomes of the wiretaps (e.g. drugs seized arrests made) and the costs to the public for running each surveillance operation.  Under California’s sunshine law government agencies must provide public records to requesters in whatever electronic format they may exist. And yet for the last three years CADOJ officials resisted releasing the data in a machine-readable format. In fact in 2015 CADOJ initially attempted to only release the “locked” version of a PDF of the report until EFF publicly called out the agency for ignoring these provisions of the California Public Records Act. EFF sought the dataset because the formatting of the paper version of the report was extremely difficult to scrape or export in a way that would result in reliable and accurate data. Tables in the reports have sometimes spanned more than 70 pages.   This year EFF has scored a major victory for open data in response to our latest request CADOJ has released not only an unlocked PDF but a spreadsheet containing all the data.  What’s especially interesting about the data is that it includes data not previously disclosed in the formal report including information on when wiretaps targeted multiple locations devices and websites such as Facebook. At the same time the data does not include some information included in the official report such as the narrative summary of the outcome of each wiretap. Inspiration Some of the highlights contained in the data.  Wiretap application in Riverside County dropped from 620 wiretap applications in 2015 to 106 in 2016. This is likely due to reforms in the Riverside County District Attorney’s office following a series of investigative reports from USA Today that showed many wiretaps were likely illegal. As in previous years many of the wiretaps captured voluminous amounts of communications from large groups of people. The largest in terms of communications was a wiretap in a Los Angeles narcotics case in which 559000 communications were captured from cell phones over 30 days. The largest in terms of number of people were caught up in a wiretap was a Riverside narcotics case in which 91000 people each had a single piece of communication captured over 120 days. The most expensive wiretap cost $1 million mostly in personnel costs to target a single person’s text message in a Los Angeles murder case. The most expensive wiretap in terms of non-personnel resources (i.e. equipment) cost $193000. Two arrests were made in the associated narcotics case.   Explore the 2016 data (reproduced here as a CSV file) and the full report. Previous years’ reports are also made available here in a ZIP archive (PDFs). Let EFF know if you discover something interesting in the data by emailing dm@eff.org. Acknowledgments This description is reproduced with slight changes from the original blog post introducing the dataset published by Dave Maass on June 9 2017. The dataset and contents from EFF are released under a CC-BY license and redistributed here in accordance with EFF's Copyright Policy. Start an Analysis,Other,,[crime],Other,,,94,1274,16,Investigate ~$30M dollars of wiretapping in the Electronic Interceptions Reports,California Wire Tapping,https://www.kaggle.com/eff/california-wire-tapping,Tue Jun 13 2017
,FiveThirtyEight,"[competitorname, chocolate, fruity, caramel, peanutyalmondy, nougat, crispedricewafer, hard, bar, pluribus, sugarpercent, pricepercent, winpercent]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context What’s the best (or at least the most popular) Halloween candy? That was the question this dataset was collected to answer. Data was collected by creating a website where participants were shown presenting two fun-sized candies and asked to click on the one they would prefer to receive. In total more than 269 thousand votes were collected from 8371 different IP addresses. Content candy-data.csv includes attributes for each candy along with its ranking. For binary variables 1 means yes 0 means no. The data contains the following fields   chocolate Does it contain chocolate? fruity Is it fruit flavored? caramel Is there caramel in the candy? peanutalmondy Does it contain peanuts peanut butter or almonds? nougat Does it contain nougat? crispedricewafer Does it contain crisped rice wafers or a cookie component? hard Is it a hard candy? bar Is it a candy bar? pluribus Is it one of many candies in a bag or box? sugarpercent The percentile of sugar it falls under within the data set. pricepercent The unit price percentile compared to the rest of the set. winpercent The overall win percentage according to 269000 matchups.  Acknowledgements This dataset is Copyright (c) 2014 ESPN Internet Ventures and distributed under an MIT license. Check out the analysis and write-up here The Ultimate Halloween Candy Power Ranking. Thanks to Walt Hickey for making the data available. Inspiration  Which qualities are associated with higher rankings? What’s the most popular candy? Least popular? Can you recreate the 538 analysis of this dataset? ,CSV,,"[food and drink, popular culture]",Other,,,545,3059,0.0048828125,What’s the best Halloween candy?,The Ultimate Halloween Candy Power Ranking,https://www.kaggle.com/fivethirtyeight/the-ultimate-halloween-candy-power-ranking,Tue Oct 31 2017
,Olga Belitskaya,[],[],"Context SVHN is a real-world image dataset.  Fragments of this dataset were preprocessed   fields of photos that do not contain digits were cut off; the photos were formatted to the standard 32X32 size; three color channels were converted into one channel (grayscaled); each of the resulting images was represented as an array of numbers; the data were converted into .csv files.   Content  64000 32x32 greyscaled images of number photos with 1-5 digits (represented as arrays).  11 categories of labels (ten ""digit"" categories and one ""empty character"" category).  Information about file names in the original dataset.  Acknowledgements The original data contains a notice ""for non-commercial use only"". Inspiration Image recognition and classification is a huge part of machine learning practice. In addition this data is based on real photos.  ",Other,,"[numbers, classification, deep learning]",Other,,,101,1136,1024,Image Classification (Digit Recognition),SVHN Preprocessed Fragments,https://www.kaggle.com/olgabelitskaya/svhn-preproccessed-fragments,Tue Oct 24 2017
,Luis Moneda,[],[],Context These datasets were created for a college course work. It was an opportunity to test Deep Learning capabilities for computer vision in a very restricted problem. The idea is to explore a classification problem for a single coin and a regression problem for a group of coins trying to count how much money they sum.  You can see my initial approach here. Content There are two datasets. One for classification and another for regression. The first contains 3000 images with just a single coin in it. The second contains the first one and another 3000 images with two or more coins present in each example. In the classification problem there are five classes 5 10 25 50 and 100 cents. For regression there are examples from 5 to 175 cents. Every file contains its value in money as its filename. For example 5_1477146780.jpg contains a single 5 cents coin. If there's only one coin it's the coin value itself. In the 80_1477851090.jpg you're going to find enough coins to sum 80 cents. An example (90_1477854720.jpg)  Different coins from each type were used to make it more interesting. My fingers appear in some images! I tried to keep the distance illumination and background constant for all of them but some differences will be noticed specially in the illumination. Changing the coin position has an great impact in how the light reflects over it. The structure used to take the pictures (in fact a second light source was added)  Inspiration A model that can sum coins and tell how much money we have in a group of coins could be used for people with vision disabilities. Can Deep Learning count classify and sum in a single model? Should we split the problem into segmentation classification and then sum it? What can be done with this amount of data? Can we achieve a good generalization and predict sums beyond the dataset greatest value? Citation If you want to use this dataset for any purpose contemplated by its license add the reference MONEDA L. (2016) Brazilian Coins Dataset. Retrieved from http//lgmoneda.github.io/ Acknowledgment I'd like to thanks Luciana Harada and Rafael de Souza my group in the college course that generated these datasets.,Other,,"[money, image data, multiclass classification]",CC4,,,568,5994,391,Classification and regression with Brazilian coin images,Brazilian Coins,https://www.kaggle.com/lgmoneda/br-coins,Sun Apr 23 2017
,Sustainable Development Solutions Network,"[Country, Region, Happiness Rank, Happiness Score, Standard Error, Economy (GDP per Capita), Family, Health (Life Expectancy), Freedom, Trust (Government Corruption), Generosity, Dystopia Residual]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012 the second in 2013 the third in 2015 and the fourth in the 2016 Update. The World Happiness 2017 which ranks 155 countries by their happiness levels was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields – economics psychology survey analysis national statistics health public policy and more – describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.  Content The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question known as the Cantril ladder asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors – economic production social support life expectancy freedom absence of corruption and generosity – contribute to making life evaluations higher in each country than they are in Dystopia a hypothetical country that has values equal to the world’s lowest national averages for each of the six factors. They have no impact on the total score reported for each country but they do explain why some countries rank higher than others. Inspiration What countries or regions rank the highest in overall happiness and each of the six factors contributing to happiness? How did country ranks or scores change between the 2015 and 2016 as well as the 2016 and 2017 reports? Did any country experience a significant increase or decrease in happiness? What is Dystopia? Dystopia is an imaginary country that has the world’s least-happy people. The purpose in establishing Dystopia is to have a benchmark against which all countries can be favorably compared (no country performs more poorly than Dystopia) in terms of each of the six key variables thus allowing each sub-bar to be of positive width. The lowest scores observed for the six key variables therefore characterize Dystopia. Since life would be very unpleasant in a country with the world’s lowest incomes lowest life expectancy lowest generosity most corruption least freedom and least social support it is referred to as “Dystopia” in contrast to Utopia. What are the residuals? The residuals or unexplained components differ for each country reflecting the extent to which the six variables either over- or under-explain average 2014-2016 life evaluations. These residuals have an average value of approximately zero over the whole set of countries. Figure 2.2 shows the average residual for each country when the equation in Table 2.1 is applied to average 2014- 2016 data for the six variables in that country. We combine these residuals with the estimate for life evaluations in Dystopia so that the combined bar will always have positive values. As can be seen in Figure 2.2 although some life evaluation residuals are quite large occasionally exceeding one point on the scale from 0 to 10 they are always much smaller than the calculated value in Dystopia where the average life is rated at 1.85 on the 0 to 10 scale. What do the columns succeeding the Happiness Score(like Family Generosity etc.) describe? The following columns GDP per Capita Family Life Expectancy Freedom Generosity Trust Government Corruption describe the extent to which these factors contribute in evaluating the happiness in each country.  The Dystopia Residual metric actually is the Dystopia Happiness Score(1.85) +  the Residual value or the unexplained value for each country as stated in the previous answer. If you add all these factors up you get the happiness score so it might be un-reliable to model them to predict Happiness Scores. Start a new kernel,CSV,,"[emotion, social sciences, economics]",CC0,,,19382,90912,0.060546875,"Happiness scored according to economic production, social support, etc.",World Happiness Report,https://www.kaggle.com/unsdsn/world-happiness,Thu Jun 15 2017
,Liling Tan,"[cs-CZ, 1, 2015-02-19, Chovatelství]","[string, numeric, dateTime, string]",Google Taxonomy This product taxonomy lists seemed to be used by merchants in tagging their products on Google Shopping see https//support.google.com/merchants/answer/6324436?hl=en These lists were also used in the SemEval Taxonomy Evaluation tasks  http//alt.qcri.org/semeval2015/task17/ http//alt.qcri.org/semeval2016/task13/  Content Contains the product category lists for  Czech Danish German (Swiss /  Germany) English (Australian /  British / American) Spanish (Spanish) French (Swiss / France) Italian (Swiss / Italy) Japanese  Dutch Norwegian Polish Portuguese (Brazillian) Russian Swedish Turkish Chinese  This post might be helpful for others too https//www.en.advertisercommunity.com/t5/Google-Shopping-and-Merchant/Taxonomy-List-Countries-Some-missing/td-p/599656 Acknowledgements The individual Google product taxonomy files came from  http//www.google.com/basepages/producttype/taxonomy-with-ids.<language_code>-<country_code>.txt  Dataset image comes from Edho Pratama Disclaimer I am not affiliated to Google or own these product categories lists. If this offends any copyrights/licenses please request for me to remove it.,Other,,[],Other,,,162,1808,24,Product categories for Google Shopping,Google Product Taxonomy,https://www.kaggle.com/alvations/google-product-taxonomy,Fri Aug 18 2017
,Jack Cosgrove,[],[],Context The LabelMe project has been run out of MIT for many years and allows users to upload and annotate images. Since the labels are crowdsourced they can be of poor quality. I have been proofreading these labels for several months correcting spelling mistakes and coalescing similar labels into a single label when possible. I have also rejected many labels that did not seem to make sense. Content The images in the LabelMe project as well as the raw metadata were downloaded from MIT servers. All data is in the public domain. Images within LabelMe may have been taken as far back as the early 2000s and run up to the present day. I have worked through 5% of the LabelMe dataset thus far. I decided to create a dataset pertaining to meals (labels such as plate glass napkins fork etc.) since there were a fair number of those in the 5% I have curated thus far. Most of the images in this dataset are of table settings. This dataset contains 596 unique images 2734 labeled shapes outlining objects in these images 1782 labeled image grids with a single number representing which portion of a grid cell is filled with a labeled object Acknowledgements Many thanks to the people of the LabelMe project! Inspiration I want to see how valuable my curation efforts have been for the LabelMe dataset. I would like to see others build object recognition models using this dataset.,CSV,,"[food and drink, image data, multiclass classification]",CC4,,,44,876,2,A curated subset of the LabelMe project with labeled images of table settings,LabelMe - Let's Eat! Labeled images of meals,https://www.kaggle.com/jackcosgrove/labelme-lets-eat,Wed Nov 29 2017
,JohnM,[],[],Context This dataset focuses on public assistance in the United States with initial coverage of the WIC Program. The program is formally known as the Special Supplemental Nutrition Program for Women Infants and Children (WIC). The program allocates Federal and State funds to help low-income women and children up to age five who are at nutritional risk. Funds are used to provide supplemental foods baby formula health care and nutrition education. Content Files include participation data and spending for state WIC programs and poverty data for each state.  Data is for fiscal years 2013-2016 which is actually October 2012 through September 2016. Motivation My original purpose here is two-fold  Explore various  aspects of US Public Assistance. Show trends over recent years and better understand differences across state agencies.  Although the federal government sponsors the program and provides funding program are administered at the state level and can widely vary. Indian nations (native Americans) also administer their own programs. Share with the Kaggle Community the joy - and pain -  of working with government data. Data is often spread across numerous agency sites and comes in a variety of formats. Often the data is provided in Excel with the files consisting of multiple tabs. Also files are formatted as reports and contain aggregated data (sums averages etc.) along with base data.  Additional Content Ideas The dataset can benefit greatly from additional content. Economics additional demographics administrative costs and more. I'd like to eventually explore the money trail from taxes and corporate subsidies through the government agencies and on to program participants. All community ideas are welcome!,Other,,"[politics, demographics, economics]",Other,,,92,678,0.7236328125,"Where does it come from, who spends it, who gets it.",US Public Assistance for Women and Children,https://www.kaggle.com/jpmiller/publicassistance,Fri Jan 19 2018
,City of New York,"[, BOROUGH, NEIGHBORHOOD, BUILDING CLASS CATEGORY, TAX CLASS AT PRESENT, BLOCK, LOT, EASE-MENT, BUILDING CLASS AT PRESENT, ADDRESS, APARTMENT NUMBER, ZIP CODE, RESIDENTIAL UNITS, COMMERCIAL UNITS, TOTAL UNITS, LAND SQUARE FEET, GROSS SQUARE FEET, YEAR BUILT, TAX CLASS AT TIME OF SALE, BUILDING CLASS AT TIME OF SALE, SALE PRICE, SALE DATE]","[numeric, numeric, string, string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, dateTime]",Context This dataset is a record of every building or building unit (apartment etc.) sold in the New York City property market over a 12-month period. Content This dataset contains the location address type sale price and sale date of building units sold. A reference on the trickier fields  BOROUGH A digit code for the borough the property is located in; in order these are Manhattan (1) Bronx (2) Brooklyn (3) Queens (4) and Staten Island (5). BLOCK; LOT The combination of borough block and lot forms a unique key for property in New York City. Commonly called a BBL. BUILDING CLASS AT PRESENT and BUILDING CLASS AT TIME OF SALE The type of building at various points in time. See the glossary linked to below.  For further reference on individual fields see the Glossary of Terms. For the building classification codes see the Building Classifications Glossary. Note that because this is a financial transaction dataset there are some points that need to be kept in mind  Many sales occur with a nonsensically small dollar amount $0 most commonly. These sales are actually transfers of deeds between parties for example parents transferring ownership to their home to a child after moving out for retirement. This dataset uses the financial definition of a building/building unit for tax purposes. In case a single entity owns the building in question a sale covers the value of the entire building. In case a building is owned piecemeal by its residents (a condominium) a sale refers to a single apartment (or group of apartments) owned by some individual.  Acknowledgements This dataset is a concatenated and slightly cleaned-up version of the New York City Department of Finance's Rolling Sales dataset. Inspiration What can you discover about New York City real estate by looking at a year's worth of raw transaction records? Can you spot trends in the market or build a model that predicts sale value in the future?,CSV,,"[cities, real estate]",CC0,,,1369,9413,13,A year's worth of properties sold on the NYC real estate market,NYC Property Sales,https://www.kaggle.com/new-york-city/nyc-property-sales,Sat Sep 23 2017
,Documenting the American South (DocSouth),[],[],"""North American Slave Narratives"" collects books and articles that document the individual and collective story of African Americans struggling for freedom and human rights in the eighteenth nineteenth and early twentieth centuries. This collection includes all the existing autobiographical narratives of fugitive and former slaves published as broadsides pamphlets or books in English up to 1920. Also included are many of the biographies of fugitive and former slaves and some significant fictionalized slave narratives published in English before 1920. Context The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world. The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives. More information about the collection and access to individual page images can be be found here http//docsouth.unc.edu/neh The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives. The .csv file acts as a table of contents for the collection and includes Title Author Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant http//voyant-tools.org/).  Copyright Statement and Acknowledgements With the exception of ""Fields's Observation The Slave Narrative of a Nineteenth-Century Virginian"" which has no known rights the texts encoding and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0http//creativecommons.org/licenses/by/4.0/). Users are free to copy share adapt and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available. If you make use of this data considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu. About the DocSouth Data Project Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools. Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured corrected and machine readable text. Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http//voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling named-entity recognition or sentiment analysis.",CSV,,"[united states, north america]",Other,,,49,766,52,First-hand Accounts of Slaves from the United States,North American Slave Narratives,https://www.kaggle.com/docsouth-data/north-american-slave-narratives,Tue Aug 15 2017
,UCI Machine Learning,"[placeID, Rpayment]","[numeric, string]",Context This dataset was used for a study where the task was to generate a top-n list of restaurants according to the consumer preferences and finding the significant features. Two approaches were tested a collaborative filter technique and a contextual approach (i) The collaborative filter technique used only one file i.e. rating_final.csv that comprises the user item and rating attributes.  (ii) The contextual approach generated the recommendations using the remaining eight data files. Content There are 9 data files and a README  and are grouped like this Restaurants  1 chefmozaccepts.csv 2 chefmozcuisine.csv 3 chefmozhours4.csv 4 chefmozparking.csv 5 geoplaces2.csv  Consumers  6 usercuisine.csv 7 userpayment.csv 8 userprofile.csv  User-Item-Rating  9 rating_final.csv  More detailed file descriptions can also be found in the README  1 chefmozaccepts.csv Instances 1314 Attributes 2 placeID Nominal Rpayment Nominal 12 2 chefmozcuisine.csv Instances 916 Attributes 2 placeID Nominal Rcuisine Nominal 59 3 chefmozhours4.csv Instances 2339 Attributes 3 placeID Nominal hours Nominal Range0000-2330 days Nominal 7  4 chefmozparking.csv Instances 702 Attributes 2 placeID Nominal parking_lot Nominal 7 5 geoplaces2.csv Instances 130 Attributes 21 placeID Nominal latitude Numeric longitude Numeric the_geom_meter Nominal (Geospatial) name Nominal address NominalMissing 27 city Nominal Missing 18 state Nominal Missing 18 country Nominal Missing 28 fax Numeric Missing 130 zip NominalMissing 74 alcohol Nominal Values 3  smoking_area Nominal 5  dress_code Nominal 3  accessibility Nominal 3  price Nominal 3  url Nominal Missing 116 Rambience Nominal 2 franchise Nominal 2  area Nominal 2 other_services Nominal 3 6 rating_final.csv Instances 1161 Attributes 5 userID Nominal placeID Nominal rating Numeric 3 food_rating Numeric 3  service_rating Numeric 3 7 usercuisine.csv Instances 330 Attributes 2 userID Nominal Rcuisine Nominal 103  8 userpayment.csv Instances 177 Attributes 2 userID Nominal Upayment Nominal 5  9 userprofile Instances 138 Attributes 19 userID Nominal latitude Numeric longitude Numeric the_geom_meter Nominal (Geospatial) smoker Nominal drink_level Nominal 3 dress_preferenceNominal 4 ambience Nominal 3 transport Nominal 3 marital_status Nominal 3 hijos Nominal 3 birth_year Nominal interest Nominal 5 personality Nominal 4 religion Nominal 5  activity Nominal 4 color Nominal 8  weight Numeric budget Nominal 3 height Numeric  Acknowledgements This dataset was originally downloaded from the UCI ML Repository UCI ML Creators  Rafael Ponce Medellín and Juan Gabriel González Serna rafaponce@cenidet.edu.mx gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET México Donors of database Blanca Vargas-Govea and Juan Gabriel González Serna blanca.vargas@cenidet.edu.mx/blanca.vg@gmail.com gabriel@cenidet.edu.mx Department of Computer Science. National Center for Research and Technological Development CENIDET México Inspiration Use this data to create a restaurant recommender or determine which restaurants a person is most likely to visit.,CSV,,[business],CC0,,,1741,11206,0.1982421875,Data from a restaurant recommender prototype,Restaurant Data with Consumer Ratings,https://www.kaggle.com/uciml/restaurant-data-with-consumer-ratings,Thu Sep 28 2017
,williamnowak,"[STATEFIPS, STATE, zipcode, agi_stub, N1, mars1, MARS2, MARS4, PREP, N2, NUMDEP, TOTAL_VITA, VITA, TCE, A00100, N02650, A02650, N00200, A00200, N00300, A00300, N00600, A00600, N00650, A00650, N00700, A00700, N00900, A00900, N01000, A01000, N01400, A01400, N01700, A01700, SCHF, N02300, A02300, N02500, A02500, N26270, A26270, N02900, A02900, N03220, A03220, N03300, A03300, N03270, A03270, N03150, A03150, N03210, A03210, N03230, A03230, N03240, A03240, N04470, A04470, A00101, N18425, A18425, N18450, A18450, N18500, A18500, N18300, A18300, N19300, A19300, N19700, A19700, N04800, A04800, N05800, A05800, N09600, A09600, N05780, A05780, N07100, A07100, N07300, A07300, N07180, A07180, N07230, A07230, N07240, A07240, N07220, A07220, N07260, A07260, N09400, A09400, N85770, A85770, N85775]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The Statistics of Income (SOI) division bases its ZIP code data on administrative records of individual income tax returns (Forms 1040) from the Internal Revenue Service (IRS) Individual Master File (IMF) system. Included in these data are returns filed during the 12-month period January 1 2015 to December 31 2015. While the bulk of returns filed during the 12-month period are primarily for Tax Year 2014 the IRS received a limited number of returns for tax years before 2014 and these have been included within the ZIP code data. There is data for more years here https//www.irs.gov/uac/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi See documentation file attached. Crucially ZIPCODE - 5-digit Zip code   AGI_STUB - Size of adjusted gross income 1 = $1 under $25000 2 = $25000 under $50000 3 = $50000 under $75000 4 = $75000 under $100000 5 = $100000 under $200000 6 = $200000 or more,CSV,,"[geography, income, demographics]",CC0,,,1293,6803,160,Distribution of income for 6 earning brackets,"USA Income Tax Data by ZIP Code, 2014",https://www.kaggle.com/wpncrh/zip-code-income-tax-data-2014,Tue Nov 15 2016
,Stanford Open Policing Project,"[id, state, stop_date, stop_time, location_raw, county_name, county_fips, fine_grained_location, police_department, driver_gender, driver_age_raw, driver_age, driver_race_raw, driver_race, violation_raw, violation, search_conducted, search_type_raw, search_type, contraband_found, stop_outcome, is_arrested, officer_id, stop_duration, road_number, milepost, consent_search, vehicle_type, ethnicity]","[string, string, dateTime, string, string, string, numeric, string, string, string, string, string, string, string, string, string, boolean, string, string, boolean, string, boolean, numeric, string, string, numeric, boolean, string, string]",Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes stop data from AZ CO CT IA MA MD MI and MO. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,CSV,,"[government agencies, crime, law]",Other,,,161,1099,2048,Data on Traffic and Pedestrian Stops by Police in many states,Stanford Open Policing Project - Bundle 1,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-bundle-1,Fri Jul 28 2017
,Jacob Boysen,[],[],Context Anticipating public nuisances and allocating proper resources is a critical part of public duties. Content This dataset contains 5 years (2008-2011 2016) worth of public incidents both criminal and non-criminal. Data includes time location description and unique key. Acknowledgements This dataset was compiled by the City of Austin and published on Google Cloud Public Data. Dataset Description Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too. Inspiration  Are there notable time variations in incidences? Can you predict incidence patterns for 2016 based on the 2008-2011 training data? Do weather patterns or other changes related to incidence changes? ,Other,,[],CC0,,,89,1058,108,5 years of Austin Incidents Data,"Incidents Around Austin, TX",https://www.kaggle.com/jboysen/austin-incidents,Fri Aug 18 2017
,PromptCloud,"[address, area, city, cleartrip_seller_rating, country, crawl_date, hotel_description, hotel_facilities, hotel_star_rating, image_count, image_urls, landmark, latitude, locality, longitude, pageurl, property_id, property_name, property_type, province, qts, room_area, room_count, room_facilities, room_type, similar_hotel, sitename, state, tad_review_count, tad_review_rating, tad_stay_review_rating, tripadvisor_seller_rating, uniq_id]","[string, string, string, numeric, string, dateTime, string, string, string, numeric, string, string, numeric, string, numeric, string, numeric, string, string, string, dateTime, string, numeric, string, string, string, string, string, numeric, numeric, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 42000 hotels) that was created by extracting data from Cleartrip.com a leading travel portal in India.  Content Analyses can be performed on the hotel description facilities and various ratings to name a few. Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service.,CSV,,"[hotels, internet]",CC0,,,137,1019,15,This dataset contains Indian hotel (5000) present on Cleartrip.com,Indian Hotels on Cleartrip,https://www.kaggle.com/PromptCloudHQ/indian-hotels-on-cleartrip,Fri Sep 15 2017
,SrinivasRao,"[Crop, State, Cost of Cultivation (`/Hectare) A2+FL, Cost of Cultivation (`/Hectare) C2, Cost of Production (`/Quintal) C2, Yield (Quintal/ Hectare) ]","[string, string, numeric, numeric, numeric, numeric]",Context Agricuture Production in India from 2001-2014 Content This Dataset Describes the Agricuture Crops Cultivation/Production in india. This is from https//data.gov.in/ fully Licensed Acknowledgements This Dataset can solves the problems of various crops Cultivation/production in india. Columns cropstring crop name Varietystringcrop subsidary name state stringCrops Cultivation/production Place QuantityIntegerno of Quintals/Hectars productionIntegerno of years Production SeasonDateTimemedium(no of days)long(no of days) UnitString  Tons CostInteger cost of cutivation and Production Recommended ZoneString place(StateMandalVillage) Inspiration Across The Globe India Is The Second Largest Country having People more than 1.3 Billion. Many People Are Dependent On The Agricuture And it is the Main Resource. In Agricuturce Cultivation/Production Having More Problems.  I want to solve the Big problem in india and usefull to  many more people,CSV,,"[india, business, agriculture]",Other,,,906,4208,0.1005859375,Various Crops Cultivation/Production,Agricuture  Crops Production In india,https://www.kaggle.com/srinivas1/agricuture-crops-production-in-india,Mon Aug 14 2017
,Rachael Tatman,"[bite_date, SpeciesIDDesc, BreedIDDesc, GenderIDDesc, color, vaccination_yrs, vaccination_date, victim_zip, AdvIssuedYNDesc, WhereBittenIDDesc, quarantine_date, DispositionIDDesc, head_sent_date, release_date, ResultsIDDesc]","[dateTime, string, string, string, string, numeric, dateTime, numeric, string, string, dateTime, string, string, string, string]",Context In the United States animal bites are often reported to law enforcement (such as animal control). The main concern with an animal bite is that the animal may be rabid. This dataset includes information on over 9000 animal bites which occurred near Louisville Kentucky from 1985 to 2017 and includes information on whether the animal was quarantined after the bite occurred and whether that animal was rabid. Content Attributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness.  Personal/identifying data has been removed. This dataset is a single .csv with the following fields.  bite_date The date the bite occurred SpeciesIDDesc The species of animal that did the biting BreedIDDesc Breed (if known) GenderIDDesc Gender (of the animal) color color of the animal vaccination_yrs how many years had passed since the last vaccination vaccination_date the date of the last vaccination victim_zip the zipcode of the victim AdvIssuedYNDesc whether advice was issued WhereBittenIDDesc Where on the body the victim was bitten quarantine_date whether the animal was quarantined DispositionIDDesc whether the animal was released from quarantine head_sent_date the date the animal’s head was sent to the lab release_date the date the animal was released     ResultsIDDesc results from lab tests (for rabies)  Acknowledgements Attributes of animal bite incidents reported to and investigated by Louisville Metro Department of Public Health and Wellness. This data is in the public domain. Inspiration  Which animals are most likely to bite humans? Are some dog breeds more likely to bite? What factors are most strongly associated with a positive rabies ID?  ,CSV,,"[healthcare, animals, crime, violence]",CC0,,,593,4471,0.6591796875,"Data on over 9,000 bites, including rabies tests",Animal Bites,https://www.kaggle.com/rtatman/animal-bites,Fri Sep 15 2017
,EveryPolitician,[],[],Context EveryPolitician is a project with the goal of providing data about every politician in the world. They collect open data on as many politicians as they can find and these datasets are just a small sample of the data available at http//www.everypolitician.org.  Content Each country has their own governmental structure and EveryPolitician provides data for as many countries as possible. At the time of publishing there was information on politicians from 233 countries. I chose to publish JSON files for these 10 countries  Australia Brazil China France India Nigeria Russia South_Africa UK US  These JSON files follow the POPOLO format  Acknowledgements These data were collected from http//everypolitician.org/. Their website has more data than I have published here - this is a small sample.,{}JSON,,[],ODbL,,,176,1203,42,Open data on politicians from 10 countries,EveryPolitician,https://www.kaggle.com/everypolitician/everypoliticiansample,Mon Aug 14 2017
,New York Public Library,"[id, name, description, menus_appeared, times_appeared, first_appeared, last_appeared, lowest_price, highest_price]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The New York Public Library is digitizing and transcribing its collection of historical menus. The collection includes about 45000 menus from the 1840s to the present and the goal of the digitization project is to transcribe each page of each menu creating an enormous database of dishes prices locations and so on. As of early November 2016 the transcribed database contains 1332279 dishes from 17545 menus. The Data This dataset is split into four files to minimize the amount of redundant information contained in each (and thus the size of each file). The four data files are Menu MenuPage MenuItem and Dish. These four files are described briefly here and in detail in their individual file descriptions below. Menu The core element of the dataset. Each Menu has a unique identifier and associated data including data on the venue and/or event that the menu was created for; the location that the menu was used; the currency in use on the menu; and various other fields. Each menu is associated with some number of MenuPage values. MenuPage Each MenuPage refers to the Menu it comes from via the menu_id variable (corresponding to Menuid). Each MenuPage also has a unique identifier of its own. Associated MenuPage data includes the page number of this MenuPage an identifier for the scanned image of the page and the dimensions of the page. Each MenuPage is associated with some number of MenuItem values. MenuItem Each MenuItem refers to both the MenuPage it is found on -- via the menu_page_id variable -- and the Dish that it represents -- via the dish_id variable. Each MenuItem also has a unique identifier of its own. Other associated data includes the price of the item and the dates when the item was created or modified in the database. Dish A Dish is a broad category that covers some number of MenuItems. Each dish has a unique id to which it is referred by its affiliated MenuItems. Each dish also has a name a description a number of menus it appears on and both date and price ranges. Inspiration What are some things we can look at with this dataset?  How has the median price of restaurant dishes changed over time? Are there particular types of dishes (alcoholic beverages seafood breakfast food) whose price changes have been greater than or less than the average change over time? Can we predict anything about a dish's price based on its name or description?  -- There's been some work on how the words used in advertisements for potato chips are reflective of their price; is that also true of the words used in the name of the food?  -- Are for example French or Italian words more likely to predict a more expensive dish?  Acknowledgments This dataset was downloaded from the New York Public Library's What's on the menu? page. The What's on the menu? data files are updated twice monthly so expect this dataset to go through multiple versions.,CSV,,[food and drink],CC0,,,999,8342,146,"Dataset on historical menus, dishes, and dish prices",What's On The Menu?,https://www.kaggle.com/nypl/whats-on-the-menu,Sun Nov 06 2016
,NLTK Data,[],[],Context NLTK redistributes the Moses machine translation models to test the nltk.translate functionalities originally from http//www.statmt.org/moses/?n=Development.GetStarted Content The Moses sample contains the following subdirectories  lm pre-trained N-gram language models using europarl and SRILM phrase-model  pre-trained Moses phrase-based model string-to-tree pre-trained Moses String-to-Tree model tree-to-tree pre-trained Moses Tree-to-Tree model  Acknowledgements Credit goes to the Moses developers who distribute this as a regression test to check that Moses Statistical Machine Translation tool is successfully installed.,Other,,[],Other,,,12,534,10,Moses sample MT models,Moses Sample,https://www.kaggle.com/nltkdata/moses-sample,Sun Aug 20 2017
,Giovanni Gonzalez,"[, Year, Month, DayofMonth, DayOfWeek, DepTime, CRSDepTime, ArrTime, CRSArrTime, UniqueCarrier, FlightNum, TailNum, ActualElapsedTime, CRSElapsedTime, AirTime, ArrDelay, DepDelay, Origin, Dest, Distance, TaxiIn, TaxiOut, Cancelled, CancellationCode, Diverted, CarrierDelay, WeatherDelay, NASDelay, SecurityDelay, LateAircraftDelay]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric]",The U.S. Department of Transportation's (DOT) Bureau of Transportation Statistics (BTS) tracks the on-time performance of domestic flights operated by large air carriers. Summary information on the number of on-time delayed canceled and diverted flights appears in DOT's monthly Air Travel Consumer Report published about 30 days after the month's end as well as in summary tables posted on this website. BTS began collecting details on the causes of flight delays in June 2003. Summary statistics and raw data are made available to the public at the time the Air Travel Consumer Report is released. This version of the dataset was compiled from the Statistical Computing Statistical Graphics 2009 Data Expo and is also available here.,Other,,[aviation],Other,,,6065,42055,239,Airline on-time statistics and delay causes,Airlines Delay,https://www.kaggle.com/giovamata/airlinedelaycauses,Sat Nov 12 2016
,Golden Oak Research Group,[],[],What you get Upvote! The database contains +40000 records on US Gross Rent & Geo Locations. The field description of the database is documented in the attached pdf file. To access all 325272 records on a scale roughly equivalent to a neighborhood (census tract) see link below and make sure to upvote. Upvote right now please. Enjoy! Get the full free database with coupon code FreeDatabase See directions at the bottom of the description... And make sure to upvote )  coupon ends at 200 pm 8-23-2017 Gross Rent & Geographic Statistics  Mean Gross Rent (double) Median Gross Rent (double) Standard Deviation of Gross Rent (double) Number of Samples (double)  Square area of land at location (double)  Square area of water at location (double)   Geographic Location  Longitude (double) Latitude (double) State Name (character) State abbreviated (character)  State_Code (character)  County Name (character) City Name (character) Name of city town village or CPD  (character) Primary Defines if the location is a track and block group. Zip Code (character) Area Code (character)                        Abstract The data set originally developed for real estate and business investment research. Income is a vital element when determining both quality and socioeconomic features of a given geographic location. The following data was derived from over +36000 files and covers 348893 location records. License Only proper citing is required please see the documentation for details. Have Fun!!! Golden Oak Research Group LLC. “U.S. Income Database Kaggle”. Publication 5 August 2017. Accessed day month year. For any questions you may reach us at  research_development@goldenoakresearch.com. For immediate assistance you may reach me on at 585-626-2965 please note it is my personal number and email is preferred Check our data's accuracy Census Fact Checker Access all 325272 location for Free Database Coupon Code Don't settle. Go big and win big.  Optimize your potential**. Access all gross rent records and more on a scale roughly equivalent to a neighborhood see link below  Website Golden Oak Research make sure to upvote  A small startup with big dreams giving the every day up and coming data scientist professional grade data at affordable prices It's what we do. ,Other,,"[finance, real estate, demographics]",Other,,,447,2648,5,"+40,000 Samples: Real Estate Application (Mean, Median, Standard Deviation)",US Gross Rent ACS Statistics,https://www.kaggle.com/goldenoakresearch/acs-gross-rent-us-statistics,Wed Aug 23 2017
,Megan Risdal,[],[],Context This dataset was originally published by the University of Zurich Robotics and Perception Group here. A sample of the data along with accompanying descriptions is provided here for research uses. This presents the world's first dataset recorded on-board a camera equipped Micro Aerial Vehicle (MAV) flying within urban streets at low altitudes (i.e. 5-15 meters above the ground). The 2 km dataset consists of time synchronized aerial high-resolution images GPS and IMU sensor data ground-level street view images and ground truth data. The dataset is ideal to evaluate and benchmark appearance-based topological localization monocular visual odometry simultaneous localization and mapping (SLAM) and online 3D reconstruction algorithms for MAV in urban environments. Content The entire dataset is roughly 28 gigabyte. We also provide a sample subset less than 200 megabyte representing the first part of the dataset. You can download the entire dataset from this page. The dataset contains time-synchronized high-resolution images (1920 x 1080 x 24 bits) GPS IMU and ground level Google-Street-View images. The high-resolution aerial images were captured with a rolling shutter GoPro Hero 4 camera that records each image frame line by line from top to bottom with a readout time of 30 millisecond. A summary of the enclosed files is given below. The data from the on-board barometric pressure sensor BarometricPressure.csv accelerometer RawAccel.csv gyroscope RawGyro.csv GPS receiver OnbordGPS.csv and pose estimation OnboardPose.csv is logged and timesynchronized using the clock of the PX4 autopilot board. The on-board sensor data was spatially and temporally aligned with the aerial images. The first column of every file contains the timestamp when the data was recorded expressed in microseconds. In the next columns the sensor readings are stored. The second column in OnbordGPS.csv encodes the identification number (ID) of every aerial image stored in the /MAV Images/ folder. The first column in GroundTruthAGL.csv is the ID of the aerial image followed by the ground truth camera position of the MAV and the raw GPS data. The second column in GroundTruthAGM.csv is the ID of of the aerial image followed by the ID of the first second and third best match ground-level street view image in the /Street View Img/ folder. Ground Truth Two types of ground truth data are provided in order to evaluate and benchmark different vision-based localization algorithms. Firstly appearance-based topological localization algorithms that match aerial images to street level ones can be evaluated in terms of precision rate and recall rate. Secondly metric localization algorithms that computed the ego-motion of the MAV using monocular visual SLAM tools can be evaluated in terms of standard deviations from the ground truth path of the vehicle. See more details here. Past Research The work listed below inspired the recording of this dataset. In these papers a much smaller dataset was used that did not contain time synchronized GPS (except a small street segment ) IMU data and accurate metric ground truth.  If you used this dataset please send your paper to majdik (at) ifi (dot) uzh (dot) ch.  A.L. Majdik D. Verda Y. Albers-Schoenberg D. Scaramuzza. Air-ground Matching Appearance-based GPS-denied Urban Localization of Micro Aerial Vehicles Journal of Field Robotics 2015. A. L. Majdik D. Verda Y. Albers-Schoenberg D. Scaramuzza Micro Air Vehicle Localization and Position Tracking from Textured 3D Cadastral Models IEEE International Conference on Robotics and Automation (ICRA) Hong Kong 2014. A. Majdik Y. Albers-Schoenberg D. Scaramuzza. MAV Urban Localization from Google Street View Data IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) Tokyo 2013. License These datasets are released under the Creative Commons license (CC BY-NC-SA 3.0) which is free for non-commercial use (including research).  Acknowledgements This dataset was recorded with the help of Karl Schwabe Mathieu Noirot-Cosson and Yves Albers-Schoenberg. To record the dataset we used a Fotokite MAV offered to our disposal by Perspective Robotics AG—http//fotokite.com.  This work was supported by the National Centre of Competence in Research Robotics (NCCR) through the Swiss National Science Foundation and by the Hungarian Scientific Research Fund (No. OTKA/NKFIH 120499).,Other,,"[cities, vehicles]",CC3,,,199,2618,383,"For appearance-based localization, visual odometry, and SLAM",The Zurich Urban Micro Aerial Vehicle Dataset,https://www.kaggle.com/mrisdal/zurich-urban-micro-aerial-vehicle,Thu Apr 13 2017
,OpenFlights,"[airline, airline ID,  source airport,  source airport id,  destination apirport,  destination airport id,  codeshare,  stops,  equipment]","[string, numeric, string, numeric, string, numeric, string, numeric, string]","Routes database As of January 2012 the OpenFlights/Airline Route Mapper Route Database contains 59036 routes between 3209 airports on 531 airlines spanning the globe. Content The data is ISO 8859-1 (Latin-1) encoded. Each entry contains the following information  Airline   2-letter (IATA) or 3-letter (ICAO) code of the airline. Airline ID    Unique OpenFlights identifier for airline (see Airline). Source airport    3-letter (IATA) or 4-letter (ICAO) code of the source airport. Source airport ID Unique OpenFlights identifier for source airport (see Airport) Destination airport   3-letter (IATA) or 4-letter (ICAO) code of the destination airport. Destination airport ID    Unique OpenFlights identifier for destination airport (see Airport) Codeshare ""Y"" if this flight is a codeshare (that is not operated by Airline but another carrier) empty otherwise. Stops Number of stops on this flight (""0"" for direct) Equipment 3-letter codes for plane type(s) generally used on this flight separated by spaces  The special value \N is used for ""NULL"" to indicate that no value is available.  Notes  Routes are directional if an airline operates services from A to B and from B to A both A-B and B-A are listed separately. Routes where one carrier operates both its own and codeshare flights are listed only once.  Acknowledgements This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website so check them out!",CSV,,[],ODbL,,,645,5629,2,"A database of 59,036 flight routes",Flight Route Database,https://www.kaggle.com/open-flights/flight-route-database,Tue Aug 29 2017
,Aubert Sigouin,[],[],Context « BIXI Montréal is a public bicycle sharing system serving Montréal Quebec Canada. Launched in May 2009 it is North America's first large-scale bike sharing system and the original BIXI brand of systems.  The location of a BIXI bike station is determined by several parameters including population density points of interest and activities (universities bike paths other transportation networks and data on travel patterns of the general public. In 2009 5000 bikes were deployed in Montreal through a network of pay stations located mainly in the boroughs of Rosemont–La Petite-Patrie the Plateau-Mont-Royal and Ville-Marie spilling over into parts of Outremont and the South West. As of 2011 the system has spread to Hochelaga-Maisonneuve Villeray–Saint-Michel–Parc-Extension Ahuntsic Côte-des-Neiges–Notre-Dame-de-Grâce Westmount and Verdun. » [1]  Content 1. BIXI - Movements history   Datasets containing the details of the travels made via the BIXI Montréal self-service bike network. Each year is a .zip file (ex. BixiMontrealRentals2014.zip) containing several .CSV files (ex. OD_2014-04.csv) for each months.  Version 2  All .csv are merged per year (OD-year.csv).  The data is extracted from the BIXI Montréal station and bike management system. Trips of less than 1 minute or more than 2 hours are excluded. The station identifiers used correspond to those of the station status data set  Data Sructure  start_date Date and time of the start of the trip ( AAAA-MM-JJ hhmm ) start_station_code Start station ID end_date Date and time of the start of the trip ( AAAA-MM-JJ hhmm ) end_station_code  End station ID is_member  Type users. (1  Suscriber 0  Non-suscriber) duration_sec Total travel time in seconds    2. BIXI - The condition of stations  This dataset presents the list of stations in the BIXI Montréal self-service bicycle network including the geographic position the number of bicycles available and the number of terminals available. It is a .json file which corresponds to a dictionary.  The data is produced by the BIXI Montréal station management system with a refresh rate of 5 minutes. Station locations are generally stable over time but may be subject to change during the season particularly when the City of Montreal is carrying out work or as part of special events. Temporary storage stations are not included in station status. The data is automatically generated on the BIXI servers so the date of last update of this dataset does not represent the actual date of update. Information about bikes and bad terminals is available in the JSON format file.  Data Sructure  id Unique station ID s Name of the station n Station terminal ID st Station status b Boolean value (true or false) specifying whether the station is blocked su Boolean value (true or false) specifying whether the station is suspended m Boolean value (true or false) specifying whether the station is displayed as out of service read timestamp of the last update of the data in milliseconds since January 1 1970. lc timestamp of the last communication with the server in milliseconds since January 1 1970. bk (For future use) bl (For future use) la latitude of the station according to the geodesic datum WGS84 lo longitude of the station according to the WGS84 datum da Number of available terminals at this station dx Number of unavailable terminals at this station ba Number of available bikes at this station bx Number of unavailable bicycles at this station   3. Geographical boundaries of Montreal (Borough and related city)  This dataset is optional and will be useful mostly for ploting data and doing some choropleth maps.   Acknowledgements Creative Commons Attribution 4.0 International For more details  http//donnees.ville.montreal.qc.ca/dataset/bixi-historique-des-deplacements http//donnees.ville.montreal.qc.ca/dataset/bixi-etat-des-stations http//donnees.ville.montreal.qc.ca/dataset/polygones-arrondissements Inspiration Can you find pattern in the behavior of Bixi users?  Are there any inefficient stations ?  What insights can we use from this data for decision making ?   [1] https//en.wikipedia.org/wiki/BIXI_Montr%C3%A9al,CSV,,"[cycling, road transport]",CC4,,,149,1655,166,Data on North America's first large-scale bike sharing system,BIXI Montreal (public bicycle sharing system),https://www.kaggle.com/aubertsigouin/biximtl,Sat Dec 02 2017
,Kingsley Samuel,"[ID_capitation, Hospital_ID, CapitationAmount, ValueDate, CompanyID, PatientUniqueID, CapitationType]","[numeric, numeric, numeric, dateTime, numeric, string, string]",Context This dataset is a capitation list containing the list of staff of a company eligible for treatment for the specified period of time. With this dataset One can study how much a company spends on her staff on a monthly basis how often staffs are added to the health insurance scheme and how often staffs are withdrawn from the scheme. Content The datasets contains the following column with about 103385 rows which is the data of two months 2017-09-01 and 2017-10-01  ID_capitation Hospital_ID CapitationAmount ValueDate CompanyID PatientUniqueID CapitationType  Inspiration How can we eradicate fraud from this system because capitation Fee are still being paid on some ghost staffs.,CSV,,[],CC0,,,48,580,11,Capitation Data of staff patients of a company between 2017-09-01 and 2017-10-01,HMO Capitation DataSet,https://www.kaggle.com/kelvinkins/hmo-capitation-dataset,Fri Oct 06 2017
,US Environmental Protection Agency,"[state_code, county_code, site_num, parameter_code, poc, latitude, longitude, datum, parameter_name, sample_duration, pollutant_standard, metric_used, method_name, year, units_of_measure, event_type, observation_count, observation_percent, completeness_indicator, valid_day_count, required_day_count, exceptional_data_count, null_data_count, primary_exceedance_count, secondary_exceedance_count, certification_indicator, num_obs_below_mdl, arithmetic_mean, arithmetic_standard_dev, first_max_value, first_max_datetime, second_max_value, second_max_datetime, third_max_value, third_max_datetime, fourth_max_value, fourth_max_datetime, first_max_non_overlapping_value, first_no_max_datetime, second_max_non_overlapping_value, second_no_max_datetime, ninety_nine_percentile, ninety_eight_percentile, ninety_five_percentile, ninety_percentile, seventy_five_percentile, fifty_percentile, ten_percentile, local_site_name, address, state_name, county_name, city_name, cbsa_name, date_of_last_change]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, numeric, string, string, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, dateTime, numeric, dateTime, numeric, dateTime, numeric, dateTime, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, dateTime]",Context The Environmental Protection Agency (EPA) creates air quality trends using measurements from monitors located across the country. All of this data comes from EPA’s Air Quality System (AQS). Data collection agencies report their data to the EPA via this system and it calculates several types of aggregate (summary) data for EPA internal use.  Content Field descriptions  State Code The FIPS code of the state in which the monitor resides. County Code The FIPS code of the county in which the monitor resides. Site NumA unique number within the county identifying the site. Parameter Code The AQS code corresponding to the parameter measured by the monitor. POC This is the “Parameter Occurrence Code” used to distinguish different instruments that measure the same parameter at the same site. Latitude The monitoring site’s angular distance north of the equator measured in decimal degrees. Longitude The monitoring site’s angular distance east of the prime meridian measured in decimal degrees. Datum The Datum associated with the Latitude and Longitude measures. Parameter Name The name or description assigned in AQS to the parameter measured by the monitor. Parameters may be pollutants or non-pollutants. Sample Duration The length of time that air passes through the monitoring device before it is analyzed (measured). So it represents an averaging period in the atmosphere (for example a 24-hour sample duration draws ambient air over a collection filter for 24 straight hours). For continuous monitors it can represent an averaging time of many samples (for example a 1-hour value may be the average of four one-minute samples collected during each quarter of the hour). Pollutant StandardA description of the ambient air quality standard rules used to aggregate statistics. (See description at beginning of document.) Metric Used The base metric used in the calculation of the aggregate statistics presented in the remainder of the row. For example if this is Daily Maximum then the value in the Mean column is the mean of the daily maximums. Method Name A short description of the processes equipment and protocols used in gathering and measuring the sample. Year The year the annual summary data represents. Units of Measure The unit of measure for the parameter. QAD always returns data in the standard units for the parameter. Submitters are allowed to report data in any unit and EPA converts to a standard unit so that we may use the data in calculations. Event Type Indicates whether data measured during exceptional events are included in the summary. A wildfire is an example of an exceptional event; it is something that affects air quality but the local agency has no control over. No Events means no events occurred. Events Included means events occurred and the data from them is included in the summary. Events Excluded means that events occurred but data form them is excluded from the summary. Concurred Events Excluded means that events occurred but only EPA concurred exclusions are removed from the summary. If an event occurred for the parameter in question the data will have multiple records for each monitor. Observation Count The number of observations (samples) taken during the year. Observation Percent The percent representing the number of observations taken with respect to the number scheduled to be taken during the year. This is only calculated for monitors where measurements are required (e.g. only certain parameters). Completeness Indicator An indication of whether the regulatory data completeness criteria for valid summary data have been met by the monitor for the year. Y means yes N means no or that there are no regulatory completeness criteria for the parameter. Valid Day Count The number of days during the year where the daily monitoring criteria were met if the calculation of the summaries is based on valid days. Required Day Count  The number of days during the year which the monitor was scheduled to take samples if measurements are required. Exceptional Data Count The number of data points in the annual data set affected by exceptional air quality events (things outside the norm that affect air quality). Null Data Count The count of scheduled samples when no data was collected and the reason for no data was reported. Primary Exceedance Count The number of samples during the year that exceeded the primary air quality standard. Secondary Exceedance Count The number of samples during the year that exceeded the secondary air quality standard. Certification Indicator An indication whether the completeness and accuracy of the information on the annual summary record has been certified by the submitter. Certified means the submitter has certified the data (due May 01 the year after collection). Certification not required means that the parameter does not require certification or the deadline has not yet passed. Uncertified (past due) means that certification is required but is overdue. Requested but not yet concurred means the submitter has completed the process but EPA has not yet acted to certify the data. Requested but denied means the submitter has completed the process but EPA has denied the request for cause. Was certified but data changed means the data was certified but data was replaced and the process has not been repeated. Num Obs Below MDL The number of samples reported during the year that were below the method detection limit (MDL) for the monitoring instrument. Sometimes these values are replaced by 1/2 the MDL in summary calculations. Arithmetic Mean The average (arithmetic mean) value for the year. Arithmetic Standard Dev The standard deviation about the mean of the values for the year. 1st Max Value The highest value for the year. 1st Max DateTime The date and time (on a 24-hour clock) when the highest value for the year (the previous field) was taken. 2nd Max Value The second highest value for the year. 2nd Max DateTime The date and time (on a 24-hour clock) when the second highest value for the year (the previous field) was taken. 3rd Max Value The third highest value for the year. 3rd Max DateTime The date and time (on a 24-hour clock) when the third highest value for the year (the previous field) was taken. 4th Max Value The fourth highest value for the year. 4th Max DateTime The date and time (on a 24-hour clock) when the fourth highest value for the year (the previous field) was taken. 1st Max Non Overlapping Value For 8-hour CO averages the highest value of the year. 1st NO Max DateTime The date and time (on a 24-hour clock) when the first maximum non overlapping value for the year (the previous field) was taken. 2nd Max Non Overlapping Value For 8-hour CO averages the second highest value of the year that does not share any hours with the 8-hour period of the first max non overlapping value. 2nd NO Max DateTime The date and time (on a 24-hour clock) when the second maximum non overlapping value for the year (the previous field) was taken. 99th Percentile The value from this monitor for which 99 per cent of the rest of the measured values for the year are equal to or less than. 98th Percentile The value from this monitor for which 98 per cent of the rest of the measured values for the year are equal to or less than. 95th Percentile The value from this monitor for which 95 per cent of the rest of the measured values for the year are equal to or less than. 90th Percentile The value from this monitor for which 90 per cent of the rest of the measured values for the year are equal to or less than. 75th Percentile The value from this monitor for which 75 per cent of the rest of the measured values for the year are equal to or less than. 50th Percentile The value from this monitor for which 50 per cent of the rest of the measured values for the year are equal to or less than (i.e. the median). 10th Percentile The value from this monitor for which 10 per cent of the rest of the measured values for the year are equal to or less than. Local Site Name The name of the site (if any) given by the State local or tribal air pollution control agency that operates it. Address The approximate street address of the monitoring site. State Name The name of the state where the monitoring site is located. County Name The name of the county where the monitoring site is located. City Name The name of the city where the monitoring site is located. This represents the legal incorporated boundaries of cities and not urban areas. CBSA Name The name of the core bases statistical area (metropolitan area) where the monitoring site is located. Date of Last Change The date the last time any numeric values in this record were updated in the AQS data system.  Acknowledgements These data come from the EPA. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on Google BigQuery too https//cloud.google.com/bigquery/public-data/epa Inspiration Within these data are tons of ways for you to learn about air pollution and how it can affect our health and environment. You can also compare key air emissions to gross domestic product vehicle miles traveled population and energy consumption back to 1970. Best of all you can check out air trends where you live!,CSV,,"[environment, climate]",CC0,,,602,4929,948,A summary of air quality from 1987 to 2017,Air Quality Annual Summary,https://www.kaggle.com/epa/air-quality,Fri Jun 30 2017
,Government of France,"[Index, Libellé index, 2017_08, 2017_07, 2017_06, 2017_05, 2017_04, 2017_03, 2017_02, 2017_01, 2016_12, 2016_11, 2016_10, 2016_09, 2016_08, 2016_07, 2016_06, 2016_05, 2016_04, 2016_03, 2016_02, 2016_01, 2015_12, 2015_11, 2015_10, 2015_09, 2015_08, 2015_07, 2015_06, 2015_05, 2015_04, 2015_03, 2015_02, 2015_01, 2014_12, 2014_11, 2014_10, 2014_09, 2014_08, 2014_07, 2014_06, 2014_05, 2014_04, 2014_03, 2014_02, 2014_01, 2013_12, 2013_11, 2013_10, 2013_09, 2013_08, 2013_07, 2013_06, 2013_05, 2013_04, 2013_03, 2013_02, 2013_01, 2012_12, 2012_11, 2012_10, 2012_09, 2012_08, 2012_07, 2012_06, 2012_05, 2012_04, 2012_03, 2012_02, 2012_01, 2011_12, 2011_11, 2011_10, 2011_09, 2011_08, 2011_07, 2011_06, 2011_05, 2011_04, 2011_03, 2011_02, 2011_01, 2010_12, 2010_11, 2010_10, 2010_09, 2010_08, 2010_07, 2010_06, 2010_05, 2010_04, 2010_03, 2010_02, 2010_01, 2009_12, 2009_11, 2009_10, 2009_09, 2009_08, 2009_07]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset is an aggregated count of all crimes committed in France broken down by month and category. Content This data was aggregated by the French national government and published online on the French Open Data Portal. It is a combination of records kept by both local and national police forces. It's important to note that the name of the categories of crime are in French! Acknowledgements This data is a part of a larger group of Excel files published by the French Goverment on the French Open Data Portal. It has been converted to a single CSV file before uploading here. Inspiration This is a simple time series dataset that can be probed for trends in the underlying types of crimes committed. Is petty theft more or less popular today than it was ten years ago? How much variation is there in the amount of robberies year-to-year? Can you normalize the growth in the number of crimes against the growth in the number of people? How do crimes committed here differ from those committed in say Los Angeles?,CSV,,"[europe, crime]",ODbL,,,375,2477,0.09375,Monthly counts of crimes committed since 2000,Crimes Committed in France,https://www.kaggle.com/government-of-france/crimes-in-france,Fri Sep 29 2017
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, numeric, string, string, string, string, string, numeric, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one datafile for each state in the U.S. Northeast region. States included in this dataset  Connecticut - ct.csv Massachusetts - ma.csv Maine - me.csv New Hampshite - nh.csv New Jersey - nj.csv New York - ny.csv Pennsylvania - pa.csv Rhode Island - ri.csv Vermont - vt.csv  Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets for crime or weather,CSV,,[],ODbL,,,138,835,2048,Addresses and geo locations for the U.S. Northeast,OpenAddresses - U.S. Northeast,https://www.kaggle.com/openaddresses/openaddresses-us-northeast,Fri Jul 28 2017
,PromptCloud,"[product_name, mrp, price, pdp_url, brand_name, product_category, retailer, description, rating, review_count, style_attributes, total_sizes, available_size, color]","[string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string]",Context These datasets provides an opportunity to perform analyses on the fashion trend of innerwear and swimwear products. Content They were created by extracting data from from the popular retail sites via PromptCloud's data extraction solutions.  Sites covered  Amazon Victoria's Secret Btemptd Calvin Klein Hanky Panky American Eagle Macy's Nordstrom Topshop USA  Time period June 2017 to July 2017 Inspiration Some of the most common questions that can be answered are  How does the pricing differ depending on the brand? Topic modelling on the product description What are the most common color used by different brands? Analyses on the product ratings (wherever applicable) Common style attributes (wherever applicable) ,CSV,,"[business, internet]",CC4,,,814,6267,506,"600,000+ innerwear product data extracted from popular retail sites",Innerwear Data from Victoria's Secret and Others,https://www.kaggle.com/PromptCloudHQ/innerwear-data-from-victorias-secret-and-others,Wed Aug 09 2017
,Joe Philleo,"[mathematicians, occupation, country of citizenship, place of birth, date of death, educated at, employer, place of death, member of, employer, doctoral advisor, languages spoken, written or signed, academic degree, doctoral student, manner of death, position held, field of work, award received, Erdős number, instance of, sex or gender, approx. date of birth, day of birth, month of birth, year of birth, approx. date of death, day of death, month of death, year of death]","[string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, boolean, numeric, string, numeric, boolean, numeric, string, numeric]",Context What distinguishes the great from the good the remembered from the accomplished and the genius from the merely brilliant? Scrapping English Wikipedia Joseph Philleo has cleaned and compiled a database of more than 8500 famous mathematicians for the Kaggle data science community to analyze and better understand. Inspiration  What are the common characteristics of famous mathematicians? How old do they live which fields do they work in where are they born and where do they live? Can you predict which mathematicians will win a Fields Medal join the Royal Society or secure tenure at Harvard? ,CSV,,"[mathematics, people, demographics, internet]",CC0,,,239,3010,10,A Dataset of the World's Most Famous Mathematicians,Mathematicians of Wikipedia,https://www.kaggle.com/joephilleo/mathematicians-on-wikipedia,Sun Sep 17 2017
,Ms Brown,"[URL, Voter_Number, Constituency, Precinct_Number, Name, Address, Year_of_Birth, error]","[string, numeric, numeric, numeric, string, string, numeric, string]",Context Azerbaijan Voter List in three files (as scraped from the Central Election Commission website in 2006) Content Includes the sequence column (meaningless) the district code (Dairənin kodu) the polling station code (Məntəqənin kodu) voters names (Seçicinin soyadı adı atasının adı) Permanent residence address (street house and apartment number) (Daimi yaşayış yerinin ünvanı (küçə ev və mənzilin nömrəsi) and year of birth (Təvəllüd).  Here is an example page with the table https//www.infocenter.gov.az/page/voters/?s=1 In total 125 districts; 5415 polling stations; 5139414 voter records  Inspiration Working on getting another earlier voter list for comparison. For now just some summary statistics and duplicate counts. ,CSV,,[politics],CC0,,,47,673,705,Three files containing the list of voters scraped from site in 2016,"Azerbaijan Voter List, 2016",https://www.kaggle.com/msbrown/azerbaijanvoterlist,Sat May 06 2017
,Jacob Boysen,[],[],Context Free Law Project seeks to provide free access to primary legal materials develop legal research tools and support academic research on legal corpora. We work diligently with volunteers to expand our efforts at building an open source open access legal research ecosystem. Currently Free Law Project sponsors the development of CourtListener Juriscraper and RECAP. CourtListener is a free legal research website containing millions of legal opinions from federal and state courts. With CourtListener lawyers journalists academics and the public can research an important case stay up to date with new opinions as they are filed or do deep analysis using our raw data. Content This dataset contains the corpus of all Supreme Court opinions and some additional supporting information. Corpus was initially collated as individual JSON objects here; these JSON objects were joined into a single csv. Note that the actual opinions column is rendered in HTML. Citations are links to additional Courtlistener API calls that are not included in this corpus. Acknowledgements CourtListener scraped and assembled this and other similar legal datasets for public use. Inspiration  Can you join this data with other Supreme Court data like here and here? Sentiment analysis would be particularly illuminating. ,Other,,[law],CC0,,,66,1131,558,"Lots of Big, Important Words",SCOTUS Opinions Corpus,https://www.kaggle.com/jboysen/scotus-corpus,Sat Aug 26 2017
,Ggzet,"[rank, dead, online, name, level, class, id, experience, account, challenges, twitch, ladder]","[numeric, boolean, boolean, string, numeric, string, string, numeric, string, numeric, string, string]",Path of Exile League statistic Data contains stats of 59000 players from 4th August of 2017 and before now. Content One file with 12 data sections. One league - Harbinger but 4 different types of divisions   Harbinger Hardcore Harbinger SSF Harbinger SSF Harbinger HC  Each division has own ladder with leaders Acknowledgements I found this data at the pathofstats.com as a JSON format and exported at CVS.  Data have been collecting by this API and free to use for interested people. If GGG or pathofstats.com don't want to share it here please contact me and it will be removed.  As I could understand it is simple data for people who are new at data science and want to have practice.  Questions for participants  A total number of players in each division usage of each class in descending order. Some of the players streaming their game (twitch colum). Do they play better than people who does not?  Predict chance to be at top 30 in each division if we are Necromancer. With and without stream. Average number of finished challanges for each division show division with highest and lowest average challanges. Show dependency between level and class of died characters. Only for HC divisions. ,CSV,,[video games],Other,,,106,1623,9,"Statistic of 59000 players, lets analyze it!",Path of exile game statistic,https://www.kaggle.com/gagazet/path-of-exile-league-statistic,Fri Oct 27 2017
,Irio Musskopf,"[ending_time, first_president, congresspeople_present, term, body, legislative_schedule, legislative_session_number, schedule, session_number]","[dateTime, string, numeric, numeric, string, string, numeric, string, numeric]","Brazilian? You can read a Portuguese version of this article here. Context Last year while I was attending a data science course in Germany my country was impeaching its president. My colleagues asked me to explain what was happening in Brazil and the possible political outcomes in South America. Although I was able to give a general context and tell multiple arguments in favor and against the impeachment deep inside my answer was ""I really don't know"". Understanding what happens in Politics is something that takes a lot of effort and research. When I decided I had to use my tech skills to make myself a better citizen I dived into government data and started Operation Serenata de Amor. After reporting hundreds of politicians for small acts of corruption and learning how to encourage the population to engage in the democratic processes my studies drove me to understand the legislative activity. Brazilians elect 594 citizens to be their representatives in the National Congress. How can we be sure that they are not defending their own interests or those who paid for their campaigns? My way as a data scientist is to ask the data. Content The National Congress of Brazil is composed of a Lower (Chamber of Deputies) and an Upper House (Federal Senate). In the first version of this dataset you are going to find data only from the Chamber of Deputies. With 513 representatives 86% of the congresspeople I hope you have enough data to explore for some time. Would be impossible for me a citizen without government ties to collect this data without the help of public servants. I processed 9717 fixed-width files and 73 XML's made officially available by the Chamber of Deputies and created 5 CSV's containing the same information. Multiple fields of the same file telling the same thing (e.g. body_id body_name and body_abbreviation) were removed. Data on session attendance votes and propositions since past century were collected and scripted in a reproducible manner. The data collection and pre-processing scripts are available in a GitHub repository under an open source license. Everything was collected from the Chamber of Deputies website at December 27 2017 containing the whole legislative activity of the year. Presence and votes date from 1999 propositions go as far as 1946. When in question about the legislative process and how the sessions work in real world the Internal Regulation of the Chamber of Deputies is the best Portuguese documentation for research. It's free! Acknowledgements Since the data was collected from a government website and the Brazilian law states that access to this information is free to any citizen I am placing my own work published here in Public Domain. I'd like to thank the hundreds of people financially supporting the work of Operation Serenata de Amor and those responsible for passing the Information Access bill in 2011. Inspiration The legislative activity should tell the history while it's happening. How much has the Congress changed over the past decades? Do the congresspeople maintain the same political views or they vary on a weekly basis? Do people vote together with their state or party peers? How often? Can you model an algorithm to tell us the real parties inside Brazilian Congress?",CSV,,"[journalism, brazil, politics]",CC0,,,51,1001,53,"Datasets of congresspeople attendance, votes and propositions since past century",Brazilian Federal Legislative activity,https://www.kaggle.com/iriomk/brazilian-federal-legislative-activity,Wed Dec 27 2017
,Jacob Boysen,"[Field, Description]","[string, string]",Context As the price of installing solar has gotten less expensive more homeowners are turning to it as a possible option for decreasing their energy bill. We want to make installing solar panels easy and understandable for anyone. Project Sunroof puts Google's expansive data in mapping and computing resources to use helping calculate the best solar plan for you. Content See metadata for indepth description. Data is at census-tract level. Project Sunroof computes how much sunlight hits your roof in a year. It takes into account Google's database of imagery and maps 3D modeling of your roof Shadows cast by nearby structures and trees All possible sun positions over the course of a year Historical cloud and temperature patterns that might affect solar energy production Acknowledgements Data was compiled by Google Project Sunroof. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too. Inspiration  Which tracts have the highest potential possible coverage? Carbon offsets? Which tracts have the highest estimated solar panel utilization? As a percent of carbon offsets?  If you want more energy data check out 30 Years of European Wind Generation and 30 Years of European Solar Generation.,CSV,,[],Other,,,526,4869,44,Solar Panel Power Consumption Offset Estimates,Google Project Sunroof,https://www.kaggle.com/jboysen/google-project-sunroof,Mon Sep 11 2017
,NLTK Data,[],[],Context This corpus contains 5001 female names and 2943 male names sorted alphabetically one per line created by Mark Kantrowitz and redistributed in NLTK.  The names.zip file includes   README The readme file. female.txt  A line-delimited list of words. male.txt A line-delimited list of words.  License/Usage Names Corpus Version 1.3 (1994-03-29) Copyright (C) 1991 Mark Kantrowitz Additions by Bill Ross  This corpus contains 5001 female names and 2943 male names sorted alphabetically one per line.  You may use the lists of names for any purpose so long as credit is given in any published work. You may also redistribute the list if you provide the recipients with a copy of this README file. The lists are not in the public domain (I retain the copyright on the lists) but are freely redistributable.  If you have any additions to the lists of names I would appreciate receiving them.  Mark Kantrowitz <mkant+@cs.cmu.edu> http//www-2.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/nlp/corpora/names/  Inspiration This corpus is used for the text classification chapter in the NLTK book.,Other,,[linguistics],Other,,,252,1576,0.0537109375,5001 female names and 2943 male names,Names Corpus,https://www.kaggle.com/nltkdata/names,Wed Aug 16 2017
,Eran Machlev,"[grade, takers, studyunits, year, subject, city, school, semel]","[numeric, numeric, numeric, numeric, string, string, string, numeric]",Info Te'udat Bagrut (Hebrew תעודת בגרות‬) is a certificate which attests that a student has successfully passed Israel's high school matriculation examination. Bagrut is a prerequisite for higher education in Israel. A Bagrut certificate is awarded to students who pass the required written (and in some cases oral) subject-matter examinations with a passing mark (56% or higher) in each exam. The Bagrut certificate however should not be confused with a high school diploma (te'udat g'mar tichon Hebrew תעודת גמר תיכון‬) which is a certificate awarded by the Ministry of Education attesting that a student has completed 12 years of study. (Source Wikipedia) Content This file contains over 60000 average Bagrut grades that were taken between 2013 and 2016 by over 1800 schools in various subjects. This data was posted under the Israeli Freedom of Information law and was formatted by me,CSV,,[education],Other,,,47,612,0.58203125,"Over 60,000 average Bagrut grades in Israel",Bagrut grades in Israeli high schools (2013-2016),https://www.kaggle.com/emachlev/bagrut-israel,Wed Jan 03 2018
,IlknurIcke,[],[],Context Data from the Coursera Course Neurohacking In R taught by  Dr. Elizabeth Sweeney  Rice Academy Postdoctoral Fellow  Ciprian M. Crainiceanu Professor and John Muschelli III  Assistant Scientist Please see https//www.coursera.org/learn/neurohacking for the lecture notes and example code from the instructors. Content Structural MRI images for visualization and image processing From the instructors About this course Neurohacking describes how to use the R programming language (https//cran.r-project.org/) and its associated packages to perform manipulation processing and analysis of neuroimaging data. We focus on publicly-available structural magnetic resonance imaging (MRI). We discuss concepts such as inhomogeneity correction image registration and image visualization. By the end of this course you will be able to Read/write images of the brain in the NIfTI (Neuroimaging Informatics Technology Initiative) format Visualize and explore these images perform inhomogeneity correction brain extraction and image registration (within a subject and to a template). Acknowledgements Dataset is public domain and was originally posted for the Coursera online course NeuroHacking in R. Notes A. When you download the zip archive double clicking might try to compress the file instead of extracting it. Unzipping on terminal (mac) correctly decompresses the archive. B. The zip file contains a directory structure         BRAINIX             -----DICOM                 ----FLAIR                 ----ROI                 ----T1                 ----T2             -----NIFTI       kirby21             -----visit 1                 ----113             -----visit 2                 ----113       Template  However when it is unzipped here on Kaggle environment somehow the directory structure is not maintained therefore files with the same names are being overwritten. As a workaround I added the directory names to the files ie. BRAINIX_DICOM_T1_IM_0001_0011.dcm instead of just IM_0001_0011.dcm.  Check out script https//www.kaggle.com/ilknuricke/d/ilknuricke/neurohackinginrimages/structural-mri-visualization/code for example use.,Other,,[healthcare],Other,,,1552,11927,212,Coursera NeuroHacking in R course datasets,"Structural MRI Datasets (T1, T2, FLAIR etc.)",https://www.kaggle.com/ilknuricke/neurohackinginrimages,Thu Jan 05 2017
,Gabriel Moreira,[],[],Context Deskdrop is an internal communications platform developed by CI&T focused in companies using Google G Suite. Among other features this platform allows companies employees to share relevant articles with their peers and collaborate around them.   Content This rich and rare dataset contains a real sample of 12 months logs (Mar. 2016 - Feb. 2017) from CI&T's Internal Communication platform (DeskDrop).  I contains about 73k logged users interactions on more than 3k public articles shared in the platform.    This dataset features some distinctive characteristics  Item attributes Articles' original URL title and content plain text are available in two languages (English and Portuguese).    Contextual information Context of the users visits like date/time client (mobile native app / browser) and geolocation.   Logged users All users are required to login in the platform providing a long-term tracking of users preferences (not depending on cookies in devices).   Rich implicit feedback Different interaction types were logged making it possible to infer the user's level of interest in the articles (eg. comments > likes > views). Multi-platform Users interactions were tracked in different platforms (web browsers and mobile native apps)  If you like it please upvote! Take a look in these featured Python kernels   - Deskdrop datasets EDA Exploratory analysis of the articles and interactions in the dataset   - DeskDrop Articles Topic Modeling A statistical analysis of the main articles topics using LDA   - Recommender Systems in Python 101 A practical introduction of the main Recommender Systems approaches Popularity model Collaborative Filtering Content-Based Filtering and Hybrid Filtering.    Acknowledgements We thank CI&T for the support and permission to share a sample of real usage data from its internal communication platform Deskdrop. Inspiration The two main approaches for Recommender Systems are Collaborative Filtering and Content-Based Filtering.   In the RecSys community there are some popular datasets available with users ratings on items (explicit feedback) like MovieLens and Netflix Prize which are useful for Collaborative Filtering techniques.    Therefore it is very difficult to find open datasets with additional item attributes which would allow the application of Content-Based filtering techniques or Hybrid approaches specially in the domain of ephemeral textual items (eg. articles and news).   News datasets are also reported in academic literature as very sparse in the sense that as users are usually not required to log in in news portals IDs are based on device cookies making it hard to track the users page visits in different portals browsing sessions and devices.   This difficult scenario for research and experiments on Content Recommender Systems was the main motivation for the sharing of this dataset.,CSV,,"[web sites, human-computer interaction, internet]",ODbL,,,343,2540,29,Logs of users interactions on shared articles for content Recommender Systems,Articles sharing and reading from CI&T DeskDrop,https://www.kaggle.com/gspmoreira/articles-sharing-reading-from-cit-deskdrop,Mon Aug 28 2017
,ilias sekkaf,"[Architecture, Best_Resolution, Boost_Clock, Core_Speed, DVI_Connection, Dedicated, Direct_X, DisplayPort_Connection, HDMI_Connection, Integrated, L2_Cache, Manufacturer, Max_Power, Memory, Memory_Bandwidth, Memory_Bus, Memory_Speed, Memory_Type, Name, Notebook_GPU, Open_GL, PSU, Pixel_Rate, Power_Connector, Process, ROPs, Release_Date, Release_Price, Resolution_WxH, SLI_Crossfire, Shader, TMUs, Texture_Rate, VGA_Connection]","[string, string, string, string, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, numeric, dateTime, string, string, string, numeric, numeric, string, numeric]","Contents This dataset contains detailed specifications release dates and release prices of computer parts. The dataset contains two CSV files gpus.csv for Graphics Processing Units (GPUs) and cpus.csv for Central Processing Units (CPUs). Each table has its own list of unique entries but the list of features includes clock speeds maximum temperatures display resolutions power draws number of threads release dates release prices die size virtualization support and many other similar fields. For more specific column-level metadata refer to the Column Metadata. Looking for inspiration? Try starting by reading ""Using regression to predict the GPUs of the future"". Inspiration  How did performance over price ratio evolve over time?  How about general computing power?   Are there any manufacturers that are known for some specific range of performance & price?   Acknowledgements The data given here belongs mainly to Intel Game-Debate and the companies involved in producing the part. I do not own the data I uploaded it solely for informative purposes under their original license.",CSV,,"[computers, internet]",Other,,,1536,10014,1,How did computer specifications and performance evolve over time?,Computer Parts (CPUs and GPUs),https://www.kaggle.com/iliassekkaf/computerparts,Sat Sep 30 2017
,Rachael Tatman,[],[],"Context “Russian is an East Slavic language and an official language in Russia Belarus Kazakhstan Kyrgyzstan and many minor or unrecognised territories. It is an unofficial but widely spoken language in Ukraine and Latvia and to a lesser extent the other countries that were once constituent republics of the Soviet Union and former participants of the Eastern Bloc.” -- “Russian Language” on Wikipedia Russian has around 150 million native speakers and 110 million non-native speakers. Russian in written in Cyrillic script. This dataset is a morphologically syntactically and semantically annotated corpus of texts in Russian fully accessible to researchers and edited by users. Content This dataset is encoded in UTF-8. There are two files included in this dataset the corpus and the dictionary. The corpus is in .json format while the dictionary is in plain text.   Dictionary  In the dictionary each entry is a lemma presented with all of its tagged derivations. The tags depend on the part of speech of the lemma. Some examples are  Nouns part of speech animacy gender & number case Verbs Part of speech aspect transitivity gender & number person tense mood Adjectives part of speech (ADJF) gender number case  A Python script to convert the tags in this corpus to this set more commonly used in English-language linguistics can be found here. Sample dictionary entries  1 ЁЖ  NOUNanimmasc singnomn ЕЖА NOUNanimmasc singgent ЕЖУ NOUNanimmasc singdatv ЕЖА NOUNanimmasc singaccs ЕЖОМ    NOUNanimmasc singablt ЕЖЕ NOUNanimmasc singloct ЕЖИ NOUNanimmasc plurnomn ЕЖЕЙ    NOUNanimmasc plurgent ЕЖАМ    NOUNanimmasc plurdatv  41 ЁРНИЧАЮ VERBimpfintr sing1perpresindc ЁРНИЧАЕМ    VERBimpfintr plur1perpresindc ЁРНИЧАЕШЬ   VERBimpfintr sing2perpresindc ЁРНИЧАЕТЕ   VERBimpfintr plur2perpresindc ЁРНИЧАЕТ    VERBimpfintr sing3perpresindc ЁРНИЧАЮТ    VERBimpfintr plur3perpresindc ЁРНИЧАЛ VERBimpfintr mascsingpastindc ЁРНИЧАЛА    VERBimpfintr femnsingpastindc ЁРНИЧАЛО    VERBimpfintr neutsingpastindc ЁРНИЧАЛИ    VERBimpfintr plurpastindc ЁРНИЧАЙ VERBimpfintr singimprexcl ЁРНИЧАЙТЕ   VERBimpfintr plurimprexcl   Corpous  In this corpus each word has been grammatically tagged. You can access individual tokens using the following general path  JSON > text > paragraphs > paragraph > [paragraph number] > sentence > [sentence number] > tokens > [token number]  Each token has  * A unique id  number (@id) * The text of the token (@text) * information on the lemma (under “l”) including the id number of the lemma as found in the dictionary You can see an example of the token portion of the .json structure below              {                 ""@id"" 1714292                 ""@text"" ""сват""                 ""tfr"" {                   ""@t"" ""сват""                   ""@rev_id"" 3754311                   ""v"" {                     ""l"" {                       ""@id"" 314741                       ""@t"" ""сват""                       ""g"" [                         {                           ""@v"" ""NOUN""                         }                         {                           ""@v"" ""anim""                         }                         {                           ""@v"" ""masc""                         }                         {                           ""@v"" ""sing""                         }                         {                           ""@v"" ""nomn""                         }                       ]                     }                   }             }           }  Acknowledgements This dataset was collected and annotated by among others Svetlana Alekseeva Anastasia Bodrova Victor Bocharov Dmitry Granovsky Irina Krylova Maria Nikolaeva Catherine Protopopova Alexander Chuchunkov Anastasia Shimorina Vasily Alekseev Natalia Ostapuk Maria Stepanova and Alexey Surikov. The code used to collect and clean this data is available online. It is reproduced here under a CC-BY-SA license. More information on this corpus and its most recent version can be found here (in Russian.)",{}JSON,,"[languages, russia, linguistics]",CC4,,,100,1592,270,A Tagged 1.5 Million Word Corpus of Russian,OpenCorpora: Russian,https://www.kaggle.com/rtatman/opencorpora-russian,Tue Sep 12 2017
,Properati Data,"[id, created_on, operation, property_type, place_name, place_with_parent_names, country_name, state_name, geonames_id, lat-lon, lat, lon, price, currency, price_aprox_local_currency, price_aprox_usd, surface_total_in_m2, surface_covered_in_m2, price_usd_per_m2, price_per_m2, floor, rooms, expenses, properati_url, description, title, image_thumbnail]","[string, dateTime, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string]",Properati is a competitive marketplace for Latin American real estate that strives to assist consumers in purchasing and renting homes while simultaneously providing location-specific property databases for the public.  Properati currently lists over 1.5 million properties through hubs in Argentina Mexico and Brazil. Listings include property name type price listing date surface area coordinates description and photos. Most of these premises are located in urban settings with few straying into suburban or rural areas. Innovators at Properati understand that a house is usually the most important and costly purchase an individual makes in his or her lifetime. Properati has developed tools such as Preciómetro to help people make well-educated property investments.  Properati also creates data analyses based on over 1.5 million properties to better understand the real estate market. Through these reports Properati uncovers social trends and urban processes that help characterize current and future listings. Visit Properati’s blog and website to join them in their advanced market knowledge.  Properati invites you to utilize their location based datasets from current and past listings in Big Query  https//bigquery.cloud.google.com/dataset/properati-data-publicproperties_ar https//bigquery.cloud.google.com/dataset/properati-data-publicproperties_br https//bigquery.cloud.google.com/dataset/properati-data-publicproperties_mx,CSV,,[home],Other,,,86,2680,464,Property attributes of 1.5 million Latin American listings,Current Properati Listing Information,https://www.kaggle.com/properati-data/properties,Tue Jul 04 2017
,Selfish Gene,"[videoID, personName, imageHeight, imageWidth, videoDuration, averageFaceSize, numVideosForPerson]","[string, string, numeric, numeric, numeric, numeric, numeric]",YouTube Faces Dataset with Facial Keypoints This dataset is a processed version of the YouTube Faces Dataset that basically contained short videos of celebrities that are publicly available and were downloaded from YouTube. There are multiple videos of each celebrity (up to 6 videos per celebrity). I've cropped the original videos around the faces plus kept only consecutive frames of up to 240 frames for each original video. This is done also for reasons of disk space but mainly to make the dataset easier to use. Additionally for this kaggle version of the dataset I've extracted facial keypoints for each frame of each video using this amazing 2D and 3D Face alignment library that was recently published. please check out this video demonstrating the library. It's performance is really amazing and I feel I'm quite qualified to say that after manually curating many thousands of individual frames and their corresponding keypoints. I removed all videos with extremely bad keypoints labeling. The end result of my curation process is approximately 2800 videos. Right now only 1293 of those videos are uploaded due to dataset size limitations (10GB) but since overall this totals into 155560 single image frames I think this is more than enough to do a lot of interesting kernels as well as potentially very interesting research. Context Kaggle datasets platform and its integration with kernels is really amazing but it's yet to have a videos dataset (at least that I'm aware of). Videos are special in the fact that they contain rich spatial patterns (in this case images of human faces) and rich temporal patterns (in this case how the faces move in time). I was also inspired by the Face Images with Marked Landmark Points dataset uploaded by DrGuillermo and decided to create and share a dataset that would be similar but would also add something extra.  Acknowledgements If you use The YouTube Faces Dataset or refer to its results please cite the following paper  Lior Wolf Tal Hassner and Itay Maoz  Face Recognition in Unconstrained Videos with Matched Background Similarity.  IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) 2011. (pdf) if you use the 2D or 3D keypoints or refer to its results please cite the following paper  Adrian Bulat and Georgios Tzimiropoulos.  How far are we from solving the 2D & 3D Face Alignment problem?  (and a dataset of 230000 3D facial landmarks) arxiv 2017. (pdf) Also I would like to thank Gil Levi for pointing out YouTube Faces to me a few years back. Inspiration The YouTube Faces Dataset was originally intended to be used for face recognition across videos i.e. given two videos are those videos of the same person or not?   I think it can be used to serve many additional goals especially when combined with the keypoints information. For example can we build a face movement model and predict what facial expression will come next?   This dataset can also be used to test transfer learning between other face datasets (like Face Images with Marked Landmark Points that I mentioned earlier) or even other types of faces like cat or dog faces (like here or here). Also using the pre-trained Keras models might be useful (example kernel). Have Fun!,Other,,"[popular culture, celebrity, humans, internet]",CC0,,,1134,16945,10240,Videos of Celebrity Faces with Facial Keypoints for each Image Frame,YouTube Faces With Facial Keypoints,https://www.kaggle.com/selfishgene/youtube-faces-with-facial-keypoints,Tue Nov 14 2017
,Gustavo Bonesso,"[ID_ENEM_ESCOLA, NU_INSCRICAO, NU_ANO, CO_ESCOLA, CO_UF_ESC, TP_DEPENDENCIA_ADM_ESC, TP_LOCALIZACAO_ESC]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset was downloaded from INEP a department from the Brazilian Education Ministry. It contains data from the applicants for the 2015 National High School Exam. Content Inside this dataset there are not only the exam results but the social and economic context of the applicants. Acknowledgements The original dataset is provided by INEP (http//portal.inep.gov.br/microdados). I removed some information from original files to fit the file size into the Kaggle constraints. Inspiration The objective is to explore the dataset to achieve a better understanding of the social and economic context of the applicants in the exams results.,CSV,,[education],CC0,,,107,1207,2048,"Data from ENEM 2015, the Brazilian High School National Exam.",ENEM 2015,https://www.kaggle.com/gbonesso/enem2015,Thu Jul 06 2017
,Rachael Tatman,[],[],Context Spanish is the second most widely-spoken language on Earth; over one in 20 humans alive today is a native speaker of Spanish. This medium-sized corpus contains 120 million words of modern Spanish taken from the Spanish-Language Wikipedia in 2010.  Content This dataset is made up of 57 text files. Each contains multiple Wikipedia articles in an XML format. The text of each article is surrounded by  tags. The initial  tag also contains metadata about the article including the article’s id and the title of the article. The text “ENDOFARTICLE.” appears at the end of each article before the closing  tag. Acknowledgements This dataset was collected by Samuel Reese Gemma Boleda Montse Cuadros Lluís Padró and German Rigau. If you use it in your work please cite the following paper Samuel Reese Gemma Boleda Montse Cuadros Lluís Padró German Rigau. Wikicorpus A Word-Sense Disambiguated Multilingual Wikipedia Corpus. In Proceedings of 7th Language Resources and Evaluation Conference (LREC'10) La Valleta Malta. May 2010. Inspiration  Can you create a stop-word list for Spanish based on this corpus? How does it compare to the one in this dataset? Can you build a topic model to cluster together articles on similar topics?  You may also like  Brazilian Portuguese Literature Corpus 3.7 million word corpus of Brazilian literature published between 1840-1908 Colonia Corpus of Historical Portuguese A 5.1 million word corpus of historical Portuguese The National University of Singapore SMS Corpus A corpus of more than 67000 SMS messages in Singapore English & Mandarin ,Other,,"[languages, europe, linguistics]",CC3,,,130,1520,646,The Spanish Language portion of the Wikicorpus (v 1.0),120 Million Word Spanish Corpus,https://www.kaggle.com/rtatman/120-million-word-spanish-corpus,Wed Aug 09 2017
,PromptCloud,"[, average_rate_per_night, bedrooms_count, city, date_of_listing, description, latitude, longitude, title, url]","[numeric, string, numeric, string, dateTime, string, numeric, numeric, string, string]",Context Sharing economy and vacation rentals are among the hottest topics that has touched millions of lives across the globe. Airbnb has been instrumental in this space and currently operating in more than 191 countries. Hence it'd be good idea to analyze this data and uncover insights. Content Dataset contains more than 18000 property listings from Texas United Staes. Given below are the data fields Rate per night Number of bedrooms City Joining month and year Longitude Latitude Property description Property title Property URL The Airbnb data was extracted by PromptCloud’s Data-as-a-Service solution. Initial Analysis The following article covers spatial data visualization and topic modelling of the description text http//www.kdnuggets.com/2017/08/insights-data-mining-airbnb.html Inspiration Some of the interesting analysis are related to spatial mapping and text mining of the description text apart from the exploratory analysis.,CSV,,"[united states, hotels]",CC4,,,480,4828,9,"Dataset of 18,000+ properties",Airbnb Property Data from Texas,https://www.kaggle.com/PromptCloudHQ/airbnb-property-data-from-texas,Tue Jul 25 2017
,HM Land Registry,"[Account Customer, FR, DFL, TP, DLG, OS(W), OS(NPW), OS(P), OS(NPP), SIMS, OC1, OC2, Total, date]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime]",Transaction data gives numbers of applications for first registrations leases transfers of part dealings official copies and searches lodged with HM Land Registry by account holders in the preceding month. The information is divided into data showing all applications lodged transactions for value by region and local authority district. Transactions for value include freehold and leasehold sales. The data published on this page gives you information about the number and types of applications. The data reflects the volume of applications lodged by customers using an HM Land Registry account number on their application form. The data does not include applications that are not yet completed or were withdrawn. Content This dataset has been altered from its original format. Specifically the monthly files have been aggregated and columns whose names changed over time have been merged to use the current title.  Some acronyms that will be helpful to know while reading the column names per the documentation Acronym Title   Description DFL Dispositionary first lease  An application for the registration of a new lease granted by the proprietor of registered land DLG Dealing An application in respect of registered land. This includes transfers of title charges and notices FR  First registration  An application for a first registration of land both freehold and leasehold. For leasehold this applies when  the landlord’s title is not registered TP  Transfer of part    An application to register the transfer of part of a registered title OS(W)   Search of whole An application to protect a transaction for value such as purchase lease or charge for the whole of a title OS(P)   Search of part  An application to protect a transaction for value such as purchase lease or charge for part of a title OS(NPW) Non-priority search of whole    An application to search the whole of the register without getting priority OS(NPP) Non-priority search of part An application to search a part of the register without getting priority OC1 Official copy   An application to obtain an official copy of a register or title plan represents a true record of entries in the register and extent of the registered title at a specific date and time. The data includes historical editions of the register and title plan where they are kept by the registrar in electronic form OC2 Official copy of a deed or document An application to obtain a copy of a document referred to in the register or relates to an application. This includes correspondence surveys application forms and emails relating to applications that are pending cancelled or completed SIM Search of the index map An application to find out whether or not land is registered and if so to obtain the title number Acknowledgements This data was kindly released by HM Land Registry under the Open Government License 3.0. You can find their current release here. Inspiration -What does this dataset tell us about the HM Land Registry's records of housing Prices Paid? Are searches a leading indicator of price changes?,CSV,,"[housing, finance, government]",Other,,,102,1134,32,"Applications for first registrations, leases, dealings, searches, etc",UK Land Registry Transactions,https://www.kaggle.com/hm-land-registry/uk-land-registry-transactions,Thu Aug 17 2017
,rojour,"[, Bib, Name, Age, M/F, City, State, Country, Citizen, , 5K, 10K, 15K, 20K, Half, 25K, 30K, 35K, 40K, Pace, Proj Time, Official Time, Overall, Gender, Division]","[numeric, numeric, string, numeric, string, string, string, string, string, string, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, dateTime, string, dateTime, numeric, numeric, numeric]",Context This is a list of the finishers of the Boston Marathon of 2015 2016 and 2017. It's important to highlight that the Boston Marathon is the oldest marathon run in the US as it is the only marathon (other than olympic trails) that most of the participants have to qualify to participate. For the professional runners it's a big accomplishment to win the marathon. For most of the other participants it's an honor to be part of it. Content It contains the name age gender country city and state (where available) times at 9 different stages of the race expected time finish time and pace overall place gender place and division place. Decided to keep every year as a separate file making it more manageable and easier to deal with it. Acknowledgements Data was scrapped from the official marathon website - http//registration.baa.org/2017/cf/Public/iframe_ResultsSearch.cfm I have found that other people have done this kind of scraping so  some of those ideas together with things I have learned in my quest to become a data scientist created the set. You can actually find the scraping notebooks at - https//github.com/rojour/boston_results . Notebook it's not very clean yet but I will get to it soon... Inspiration I was a participant in the marathon 2016 and 2017 edition as well as a data science student so it was a natural curiosity. I have done a preliminary study of some fun facts. You can see the kernel here as well as in the github page listed above. Already some people have created some fun analysis of the results (mostly of the first part - 2016) that was the first upload but I am curious of what people may come up with... now that three years are available it may spark the creative juices of some. I believe it's a simple fun dataset that can be used by the new to play with and by some veterans to get creative.,CSV,,"[running, walking]",Other,,,1242,8711,12,"This data has the names, times and general demographics of the finishers","Finishers Boston Marathon 2015, 2016 & 2017",https://www.kaggle.com/rojour/boston-results,Sun Apr 30 2017
,Oswin Rahadiyan Hartono,[],[],Context Bible (or Biblia in Greek) is a collection of sacred texts or scriptures that Jews and Christians consider to be a product of divine inspiration and a record of the relationship between God and humans (Wiki). And for data mining purpose we could do many things using Bible scriptures as for NLP Classification Sentiment Analysis and other particular topics between Data Science and Theology perspective. Content Here you will find the following bible versions in sql sqlite xml csv and json format  American Standard-ASV1901 (ASV) Bible in Basic English (BBE) Darby English Bible (DARBY) King James Version (KJV) Webster's Bible (WBT) World English Bible (WEB) Young's Literal Translation (YLT)  Each verse is accessed by a unique key the combination of the BOOK+CHAPTER+VERSE id. Example  Genesis 11 (Genesis chapter 1 verse 1) = 01001001 (01 001 001) Exodus 23 (Exodus chapter 2 verse 3) = 02002003 (02 002 003) The verse-id system is used for faster simplified queries. For instance 01001001 - 02001005 would capture all verses between Genesis 11 through Exodus 15.  Written simply SELECT * FROM bible.t_asv WHERE id BETWEEN 01001001 AND 02001005 Coordinating Tables There is also a number-to-book key (key_english table) a cross-reference list (cross_reference table) and a bible key containing meta information about the included translations (bible_version_key table). See below SQL table layout. These tables work together providing you a great basis for a bible-reading and cross-referencing app. In addition each book is marked with a particular genre mapping in the number-to-genre key (key_genre_english table) and common abbreviations for each book can be looked up in the abbreviations list (key_abbreviations_english table). While its expected that your programs would use the verse-id system book # chapter # and verse # columns have been included in the bible versions tables. A Valuable Cross-Reference Table A very special and valuable addition to these databases is the extensive cross-reference table. It was created from the project at http//www.openbible.info/labs/cross-references/. See .txt version included from http//www.openbible.info website. Its extremely useful in bible study for discovering related scriptures. For any given verse you simply query vid (verse id) and a list of rows will be returned. Each of those rows has a rank (r) for relevance start-verse (sv) and end verse (ev) if there is one. Basic Web Interaction The web folder contains two php files. Edit the first few lines of index.php to match your server's settings. Place these in a folder on your webserver. The references search box can be multiple comma separated values. (i.e. John 316 Rom 323 1 Jn 19 Romans 109-10) You can also directly link to a verse by altering the URI [http//localhost/index.php?b=John 316 Rom 323 1 Jn 19 Romans 109-10](http//localhost/index.php?b=John 316 Rom 323 1 Jn 19 Romans 109-10)  bible-mysql.sql (MySQL) is the main database and most feature-oriented due to contributions from developers. It is suggested you use that for most things or at least convert the information from it. cross_references-mysql.sql (MySQL) is the cross-reference table. It has been separated to become an optional feature. This is converted from the project at http//www.openbible.info/labs/cross-references/. bible-sqlite.db (SQLite) is a basic simplified database for simpler applications (includes cross-references too). cross_references.txt is the source cross-reference file obtained from http//www.openbible.info/labs/cross-references/  In CSV folder you will find (same list order with the other formats)  bible_version_key.csv    key_abbreviations_english.csv    key_english.csv    key_genre_english.csv    t_asv.csv t_bbe.csv t_dby.csv t_wbt.csv t_web.csv t_ylt.csv   Acknowledgements In behalf of the original contributors (Github) Inspirations WordNet as an additional semantic resource for NLP,CSV,,"[faith and traditions, linguistics]",CC0,,,332,4160,427,English Bible Translations Dataset for Text Mining and NLP,Bible Corpus,https://www.kaggle.com/oswinrh/bible,Fri Jun 16 2017
,Daniel Esteves,"[CISP;mes;vano;mes_ano;area_cisp;AISP;RISP;munic;mcirc;Regiao;hom_doloso;lesao_corp_morte;latrocinio;tentat_hom;lesao_corp_dolosa;estupro;hom_culposo;lesao_corp_culposa;encontro_cadaver;encontro_ossada;roubo_comercio;roubo_residencia;roubo_veiculo;roubo_ca, mes, vano, mes_ano, area_cisp, AISP, RISP, munic, mcirc, Regiao, hom_doloso, lesao_corp_morte, latrocinio, tentat_hom, lesao_corp_dolosa, estupro, hom_culposo, lesao_corp_culposa, encontro_cadaver, encontro_ossada, roubo_comercio, roubo_residencia, roubo_veiculo, roubo_carga, roubo_transeunte, roubo_em_coletivo, roubo_banco, roubo_cx_eletronico, roubo_celular, roubo_conducao_saque, roubo_bicicleta, outros_roubos, total_roubos, furto_veiculos, furto_bicicleta, outros_furtos, total_furtos, sequestro, extorsao, sequestro_relampago, estelionato, apreensao_drogas, recuperacao_veiculos, cump_mandado_prisao, ameaca, pessoas_desaparecidas, hom_por_interv_policial, armas_apreendidas, prisoes, grp, apf_cmp, apreensoes, gaai, aaapai_cmba, registro_ocorrencias, pol_militares_mortos_serv, pol_civis_mortos_serv, indicador_letalidade, indicador_roubo_rua, indicador_roubo_veic, fase]","[string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Rio de Janeiro is one of the most beautiful and famous city in the world. Unfortunately it's also one of the most dangerous. For the last years in a scenario of economical and political crisis in Brazil the State of Rio de Janeiro was one of the most affected. Since 2006 the Instituto de Segurança Pública do Rio de Janeiro (Institue of Public Security of Rio de Janeiro State) publishes reports of each police station.  Content Three datasets are available BaseDPEvolucaoMensalCisp - Monthly evolution of statistics by police station PopulacaoEvolucaoMensalCisp - Monthly evolution of population covered by police station delegacias - Info about each police station Most of the data are in Brazilian Portuguese because it was extracted directly from government sites. Acknowledgements This dataset is provided by the Instituto de Segurança Pública. delegacias.csv was compiled by myself. Inspiration  What is the most unsafe city in Rio de Janeiro State? And the safest?   Which events can be correlated with the numbers in dataset? (Elections crisis...)   How crime correlates with population? ,CSV,,[],CC0,,,192,1467,4,Crime records from Rio de Janeiro State,Rio de Janeiro Crime Records,https://www.kaggle.com/danielesteves/rio-police-records,Tue Aug 15 2017
,Sohier Dane,"[Cst_Cnt, Btl_Cnt, Sta_ID, Depth_ID, Depthm, T_degC, Salnty, O2ml_L, STheta, O2Sat, Oxy_µmol/Kg, BtlNum, RecInd, T_prec, T_qual, S_prec, S_qual, P_qual, O_qual, SThtaq, O2Satq, ChlorA, Chlqua, Phaeop, Phaqua, PO4uM, PO4q, SiO3uM, SiO3qu, NO2uM, NO2q, NO3uM, NO3q, NH3uM, NH3q, C14As1, C14A1p, C14A1q, C14As2, C14A2p, C14A2q, DarkAs, DarkAp, DarkAq, MeanAs, MeanAp, MeanAq, IncTim, LightP, R_Depth, R_TEMP, R_POTEMP, R_SALINITY, R_SIGMA, R_SVA, R_DYNHT, R_O2, R_O2Sat, R_SIO3, R_PO4, R_NO3, R_NO2, R_NH4, R_CHLA, R_PHAEO, R_PRES, R_SAMP, DIC1, DIC2, TA1, TA2, pH2, pH1, DIC Quality Comment]","[numeric, numeric, string, string, numeric, numeric, numeric, string, numeric, string, string, string, numeric, numeric, string, numeric, string, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, string, numeric, string, string, numeric, string, string, numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string]",Context The CalCOFI data set represents the longest (1949-present) and most complete (more than 50000 sampling stations) time series of oceanographic and larval fish data in the world. It includes abundance data on the larvae of over 250 species of fish; larval length frequency data and egg abundance data on key commercial species; and oceanographic and plankton data. The physical chemical and biological data collected at regular time and space intervals quickly became valuable for documenting climatic cycles in the California Current and a range of biological responses to them. CalCOFI research drew world attention to the biological response to the dramatic Pacific-warming event in 1957-58 and introduced the term “El Niño” into the scientific literature.  The California Cooperative Oceanic Fisheries Investigations (CalCOFI) are a unique partnership of the California Department of Fish & Wildlife NOAA Fisheries Service and Scripps Institution of Oceanography. The organization was formed in 1949 to study the ecological aspects of the sardine population collapse off California. Today our focus has shifted to the study of the marine environment off the coast of California the management of its living resources and monitoring the indicators of El Nino and climate change. CalCOFI conducts quarterly cruises off southern & central California collecting a suite of hydrographic and biological data on station and underway.  Data collected at depths down to 500 m include temperature salinity oxygen phosphate silicate nitrate and nitrite chlorophyll transmissometer PAR C14 primary productivity phytoplankton biodiversity zooplankton biomass and zooplankton biodiversity.  Content Each table has several dozen columns. Please see this page for the table details.,CSV,,"[ecology, oceanography]",Other,,,143,1275,257,Over 60 years of oceanographic data,CalCOFI,https://www.kaggle.com/sohier/calcofi,Thu Aug 24 2017
,City of New York,"[Row_Labels, Count_AnimalName]","[numeric, numeric]",Context The NYC Department of Health requires all dog owners to license their dogs. The resulting names data was released on GitHub with a nice interactive D3 word cloud. Additional data (including type and color) is available from WNYC here. Content This data covers dog names and the counts of each name.,CSV,,"[cities, animals]",CC0,,,209,1786,0.1396484375,Popularity of 16k dog names,NYC Dog Names,https://www.kaggle.com/new-york-city/nyc-dog-names,Thu Aug 31 2017
,Nigel Dalziel,[],[],Data Sources This dataset compiles data from the following  Massachusetts Department of Education reports  Enrollment by Grade Enrollment by Selected Population Enrollment by Race/Gender Class Size by Gender and Selected Populations Teacher Salaries Per Pupil Expenditure Graduation Rates Graduates Attending Higher Ed Advanced Placement Participation Advanced Placement Performance SAT Performance MCAS Achievement Results Accountability Report  In each case the data is the latest available data as of August 2017.  Data Dictionary The data dictionary lists the report from which each field is sourced. It also includes the original field names - minor changes have been made to make the field names easier to understand. Data definitions can be found on the About the Data section of the MA DOE website. Questions  What contributes to differences in schools outcomes? Are there meaningful regional differences within MA? Which schools do well despite limited resources? ,CSV,,[education],CC0,,,629,3291,2,"Student body, funding levels, and outcomes (SAT, MCAS, APs, college attendance)",Massachusetts Public Schools Data,https://www.kaggle.com/ndalziel/massachusetts-public-schools-data,Tue Aug 22 2017
,verginer,[],[],"Content and Context This dataset is a collection of  biomedicine and life science bibliometric data obtained from MEDLINE. The data covers 26759425 papers available thought MEDLINE from 1946 through 2016. This dataset has been created by processing the  publicly available data dump at nih.gov. The data dump consists of ca 23 Million xml articles. From these xml files I have extracted some meta data into more accessible csv files. The processed csvs contain both very basic metadata (i.e. creation date number of authors and ids) and the ""Medical Subject Headings"" MeSH classification. The dataset is divided in two files   paper_details.csv mesh.csv  The paper_details.csv file contains the following columns  pmid this is the unique article identified within the dataset it is also the official identifier used on PubMed doi digital object identifier this id allows to uniquely identify a paper through the widely used DOI scheme num_authors number of authors on the paper year year the document has been created month month the document has been created day  day the document has been created title title of the document issn the ISSN number of the journal the document has been published in issn_type printed or electronic volume volume of the journal the document has been published in issue issue of the journal the document has been published in journal_title Name of Journal the document has been published in journal_title_iso ISO abbreviation of the journal title.  5 GB and 26759425 rows In the mesh.csv file the following fields are available  pmid this is the unique article identified within the dataset it is also the official identifier used on PubMed descriptor_id the id of the MeSH descriptor for the given document descriptor_name name of the MeSH descriptor descriptor_major_topic Y/N indicate if the descriptor is a major topic in MeSH qualifiers a list of MeSH qualifiers (aka subheadings). Subheadings are attached to MeSH headings to describe a specific aspect of a concept.  NB a document can and has more often then not more then one MeSH descriptor associated to it. 11 GB and 247415857 rows Other Resources An introduction to MeSH can be found here and short tutorials from the U.S. National Library of Medicine on how to use MeSH can be found here. Factsheets describing MeSH and MEDLINE  https//www.nlm.nih.gov/pubs/factsheets/mesh.html https//www.nlm.nih.gov/pubs/factsheets/medline.html  Further datasets which might be useful to work with MeSH data  https//mbr.nlm.nih.gov/Downloads.shtml  Acknowledgements The data is freely available for download from MEDLINE. Specifically through their ftp service at ftp//ftp.ncbi.nlm.nih.gov/pub/. Read the disclaimer from the U.S. National Library of Medicine regarding public use and redistribution of the data here The data extraction would not have been possible without access to the computing facilities of IMT School for Advanced Studies Lucca. Inspiration Questions which one might want to look into using this dataset could be  Prediction of Mesh Headings Descriptive statistics on the rise and fall of MeSH descriptors over time. Which Journals ˙have had the most meteoric rise? Compute the co-occurrence probability of a given MeSH descriptor pair over time  How to open the files Due to the limit on Kaggle for files to be at most 500MB in size the files have been split. More specifically the two files have been compressed with zip and split. To recombine them do the following. If you are on Linux/MacOS enter the following commands in the terminal cat paper_details.csv.zip.part-* > papers_details.csv.zip unzip papers_details.csv.zip  For Windows machines this Stock Overflow Answer will help. Image Credit The beautiful cover image has been made by olsztyn-poland over at unsplash.com. Link https//unsplash.com/collections/610433/medical?photo=nss2eRzQwgw",CSV,,"[research, healthcare]",Other,,,325,2799,4096,Biomedical bibliometric data and paper classification,MEDLINE  and MeSH,https://www.kaggle.com/alucaria/medline,Fri May 26 2017
,Mozilla,"[filename, text, up_votes, down_votes, age, gender, accent, duration]","[string, string, numeric, numeric, string, string, string, string]",General Information Common Voice is a corpus of speech data read by users on the Common Voice website (http//voice.mozilla.org/) and based upon text from a number of public domain sources like user submitted blog posts old books movies and other public speech corpora. Its primary purpose is to enable the training and testing of automatic speech recognition (ASR) systems. Structure The corpus is split into several parts for your convenience. The subsets with “valid” in their name are audio clips that have had at least 2 people listen to them and the majority of those listeners say the audio matches the text. The subsets with “invalid” in their name are clips that have had at least 2 listeners and the majority say the audio does not match the clip. All other clips ie. those with fewer than 2 votes or those that have equal valid and invalid votes have “other” in their name. The “valid” and “other” subsets are further divided into 3 groups  dev - for development and experimentation train - for use in speech recognition training test - for testing word error rate  Organization and Conventions Each subset of data has a corresponding csv file with the following naming convention “cv-{type}-{group}.csv” Here “type” can be one of {valid invalid other} and “group” can be one of {dev train test}. Note the invalid set is not divided into groups. Each row of a csv file represents a single audio clip and contains the following information  filename - relative path of the audio file text - supposed transcription of the audio up_votes - number of people who said audio matches the text down_votes - number of people who said audio does not match text age - age of the speaker if the speaker reported it teens '< 19' twenties '19 - 29' thirties '30 - 39' fourties '40 - 49' fifties '50 - 59' sixties '60 - 69' seventies '70 - 79' eighties '80 - 89' nineties '> 89' gender - gender of the speaker if the speaker reported it male female other accent - accent of the speaker if the speaker reported it us 'United States English' australia 'Australian English' england 'England English' canada 'Canadian English' philippines 'Filipino' hongkong 'Hong Kong English' indian 'India and South Asia (India Pakistan Sri Lanka)' ireland 'Irish English' malaysia 'Malaysian English' newzealand 'New Zealand English' scotland 'Scottish English' singapore 'Singaporean English' southatlandtic 'South Atlantic (Falkland Islands Saint Helena)' african 'Southern African (South Africa Zimbabwe Namibia)' wales 'Welsh English' bermuda 'West Indies and Bermuda (Bahamas Bermuda Jamaica Trinidad)'  The audio clips for each subset are stored as mp3 files in folders with the same naming conventions as it’s corresponding csv file. So for instance all audio data from the valid train set will be kept in the folder “cv-valid-train” alongside the “cv-valid-train.csv” metadata file. Acknowledgments This dataset was compiled by Michael Henretty Tilman Kamp Kelly Davis & The Common Voice Team who included the following acknowledgments We sincerely  thank all of the people who donated their voice on the Common Voice website and app. You are the backbone of this project and we thank you for making this possible! We also thank our community on Discourse (https//discourse.mozilla-community.org/c/voice) and Github (https//github.com/mozilla/voice-web) you have made this project better every step of the way. And special thanks to Mycroft SNIPS.ai Mythic Tatoeba.org Bangor University and SAP for joining us on this journey. We look forward to working more with each of you.,Other,,"[languages, acoustics, linguistics]",CC0,,,232,2323,12288,"500 hours of speech recordings, with speaker demographics",Common Voice,https://www.kaggle.com/mozillaorg/common-voice,Tue Dec 12 2017
,Rachael Tatman,[],[],Content EmojiNet is the largest machine-readable emoji sense inventory that links Unicode emoji representations to their English meanings extracted from the Web. EmojiNet is a dataset consisting of   12904 sense labels over 2389 emoji which were extracted from the web and linked to machine-readable sense definitions seen in BabelNet context words associated with each emoji sense which are inferred through word embedding models trained over Google News corpus and a Twitter message corpus for each emoji sense definition specification of the most likely platform-based emoji sense for a selected set of emoji (since emoji presentation is different on different platforms)  Acknowledgements EmojiNet was developed by Sanjaya Wijeratne Lakshika Balasuriya Amit Sheth and Derek Doran. EmojiNet is licensed under aCreative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) license.. Please cite the following paper when using EmojiNet dataset(s) Sanjaya Wijeratne Lakshika Balasuriya Amit Sheth Derek Doran. EmojiNet An Open Service and API for Emoji Sense Discovery. In 11th International AAAI Conference on Web and Social Media (ICWSM 2017). Montreal Canada; 2017. You can also find more information about the dataset on the project website. The banner photo is by Frank Behrens and is licensed under a CC BY-SA 2.0 license. Inspiration  Can you use these senses to create a sentiment lexicon for emoji? Can you cluster emoji based on their sense? Which emoji are the most different across platforms? ,{}JSON,,"[popular culture, linguistics, internet]",CC4,,,235,3612,7,A machine-readable dictionary of emoji meanings,EmojiNet,https://www.kaggle.com/rtatman/emojinet,Fri Nov 03 2017
,ramamet,"[index, date, time, open, high, low, close]","[string, numeric, dateTime, numeric, numeric, numeric, numeric]",Context nifty50.csv The NIFTY 50 index is National Stock Exchange of India's benchmark stock market index for Indian equity market. It is a well diversified 50 stock index accounting for 22 sectors of the economy. It is used for a variety of purposes such as bench-marking fund portfolios index based derivatives and index funds. banknifty.csv Bank Nifty represents the 12 most liquid and large capitalized stocks from the banking sector which trade on the National Stock Exchange (NSE). It provides investors and market intermediaries a benchmark that captures the capital market performance of Indian banking sector. Content A data frame with 8 variables index date time open high low close and id. For each year from 2013 to 2016 the number of trading data of each minute of given each date. The currency of the price is Indian Rupee (INR).  index  market id date numerical value (Ex. 20121203- to be converted to 2012/12/03) time factor (Ex. 0916) open numeric (opening price) high numeric (high price) low numeric (low price) close numeric (closing price)  Inspiration Initial raw data sets are very complex and mixed datatypes. These are processed properly using R libraries like dplyr stringr and other data munging packages. The desired outputs are then converted into a CSV format to use for further analysis.,CSV,,[finance],ODbL,,,452,3284,40,1 minute intraday datasets,NSE India stocks (Indices),https://www.kaggle.com/ramamet4/nse-stocks-database,Thu May 11 2017
,Niyamat Ullah,[],[],Context There is a question in our mind that which language skills and experience should we add to our toolbox for getting a job in Google. Well I think why not we find out the answer by analyzing the Google Jobs Site. Google published all of their jobs at https//careers.google.com/. So I scraped all of the job data from that site by going every job page using Selenium. I only take Job Title Job Location Job responsibilities minimum and preferred qualifications for this dataset.  Content This dataset is collected using Selenium by scraping all of the jobs text for Google Career site.  About the column Title The title of the job Category Category of the job Location Location of the job Responsibilities Responsibilities for the job Minimum Qualifications Minimum Qualifications for the job Preferred Qualifications Preferred Qualifications for the job Acknowledgements This dataset is collected using Selenium. This product uses the Google Career site but is not endorsed or certified by Google Career site.  Inspiration  You can find most popular skills for Google Jobs Create identical job posts Most popular languages etc ,CSV,,"[databases, learning, internet]",CC4,,,613,5186,0.3974609375,Find what you need to get a job at Google ,Google Job Skills,https://www.kaggle.com/niyamatalmass/google-job-skills,Sun Jan 07 2018
,Rachael Tatman,"[id, name, rgb, is_trans]","[numeric, string, numeric, string]",Context LEGO is a popular brand of toy building bricks. They are often sold in sets with in order to build a specific object. Each set contains a number of parts in different shapes sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had. Content This dataset contains the LEGO Parts/Sets/Colors and Inventories of every official LEGO set in the Rebrickable database. These files are current as of July 2017. If you need it to be more recent data you can use Rebrickable’s API which provides up to date data and additional features. Acknowledgements This dataset was compiled by Rebrickable which is a website to help identify what  LEGO sets can be built given bricks and pieces from other LEGO sets. You can use these files for any purpose.  Inspiration This is a very rich dataset that offers lots of rooms for exploration especially since the “sets” file includes the year in which a set was first released.   How have the size of sets changed over time? What colors are associated with witch themes? Could you predict which theme a set is from just by the bricks it contains? What sets have the most-used pieces in them? What sets have the rarest pieces in them? Have the colors of LEGOs included in sets changed over time? ,CSV,,[games and toys],CC0,,,3019,21110,12,The LEGO Parts/Sets/Colors and Inventories of every official LEGO set,LEGO Database,https://www.kaggle.com/rtatman/lego-database,Fri Jul 14 2017
,Rachael Tatman,"[id, welsh, word_w]","[numeric, string, numeric]","Content Kwici is a 4m-word corpus drawn from the Welsh Wikipedia as it was on 30 December 2013.  The final pages and articles dump for 2013 was downloaded from the Wikimedia dump page. The WikiExtractor tool written by Giuseppe Attardi and Antonio Fuschetto was then used to extract plain text (discarding markup etc) from the 165Mb dump resulting in a 33Mb output file. This was tidied by removing remaining XML blank lines and blocks of English text. The text was then split to give into a total of 360477 sentences and these were imported into a PostgreSQL database table. The sentences were pruned by removing all items  less than 50 characters long all items containing numbers only (eg timelines) and all duplicates to give a final total of 204789 sentences in the corpus.  The file contains the following fields  id unique identifier for the sentence; welsh the sentence in Welsh; word_w the number of words in the Welsh sentence.  Acknowledgements This dictionary was created by  Kevin Donnell. If using Kwici in research please use the following citation Kevin Donnelly (2014). ""Kwici a 4m-word corpus drawn from the Welsh Wikipedia."" http//cymraeg.org.uk/kwici. (BibTeX) Inspiration  Can you use this corpus to add frequency information to this Welsh dictionary? Can you use this corpus to create a stemmer for Welsh? ",CSV,,"[languages, europe, linguistics]",CC4,,,24,482,26,A 4 million word corpus of contemporary Welsh,Kwici Welsh Wikipedia Corpus,https://www.kaggle.com/rtatman/kwici-welsh-wikipedia-corpus,Tue Aug 29 2017
,Chuck Ephron,"[ColumnName, ColumnDescription]","[string, string]",League of Legends competitive matches between 2015-2017. The matches include the NALCS EULCS LCK LMS and CBLoL leagues as well as the World Championship and Mid-Season Invitational tournaments.,CSV,,[video games],CC0,,,2884,21226,30,"Competitive matches, 2015 to 2018",League of Legends,https://www.kaggle.com/chuckephron/leagueoflegends,Tue Jan 30 2018
,PyTorch,[],[],DenseNet-169  Densely Connected Convolutional Networks Recent work has shown that convolutional networks can be substantially deeper more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet) which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer the feature-maps of all preceding layers are used as inputs and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages they alleviate the vanishing-gradient problem strengthen feature propagation encourage feature reuse and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10 CIFAR-100 SVHN and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL. Authors Gao Huang Zhuang Liu Kilian Q. Weinberger Laurens van der Maaten https//arxiv.org/abs/1608.06993   DenseNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,3,279,52,DenseNet-169 Pre-trained Model for Pytorch,DenseNet-169,https://www.kaggle.com/pytorch/densenet169,Wed Dec 13 2017
,Federal Aviation Administration,"[Record ID, Incident Year, Incident Month, Incident Day, Operator ID, Operator, Aircraft, Aircraft Type, Aircraft Make, Aircraft Model, Aircraft Mass, Engine Make, Engine Model, Engines, Engine Type, Engine1 Position, Engine2 Position, Engine3 Position, Engine4 Position, Airport ID, Airport, State, FAA Region, Warning Issued, Flight Phase, Visibility, Precipitation, Height, Speed, Distance, Species ID, Species Name, Species Quantity, Flight Impact, Fatalities, Injuries, Aircraft Damage, Radome Strike, Radome Damage, Windshield Strike, Windshield Damage, Nose Strike, Nose Damage, Engine1 Strike, Engine1 Damage, Engine2 Strike, Engine2 Damage, Engine3 Strike, Engine3 Damage, Engine4 Strike, Engine4 Damage, Engine Ingested, Propeller Strike, Propeller Damage, Wing or Rotor Strike, Wing or Rotor Damage, Fuselage Strike, Fuselage Damage, Landing Gear Strike, Landing Gear Damage, Tail Strike, Tail Damage, Lights Strike, Lights Damage, Other Strike, Other Damage]","[numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The dataset contains a record of each reported wildlife strike of a military commercial or civil aircraft between 1990 and 2015. Each row contains the incident date aircraft operator aircraft make and model engine make and model airport name and location species name and quantity and aircraft damage. Acknowledgements The wildlife strike database was compiled from reports received from airports airlines and pilots and published by the Federal Aviation Association.,CSV,,"[animals, aviation]",CC0,,,573,3844,35,What bird species has caused the most damage to airplanes?,"Aircraft Wildlife Strikes, 1990-2015",https://www.kaggle.com/faa/wildlife-strikes,Wed Feb 08 2017
,LiamLarsen,[],[],Context This is all of Shakespeare's plays.  Content This is a dataset comprised of all of Shakespeare's plays. It includes the following  The first column is the Data-Line it just keeps track of all the rows there are. The second column is the play that the lines are from. The third column is the actual line being spoken at any given time. The fourth column is the Act-Scene-Line from which any given line is from. The fifth column is the player who is saying any given line. The sixth column is the line being spoken.  Inspiration I've been doing Shakespeare for a while and I wanted to make a Shakespearean chatbot.,Other,,"[writing, languages, literature]",Other,,,642,4870,14,"All of shakespeares plays, characters, lines, and acts in one CSV",Shakespeare plays,https://www.kaggle.com/kingburrito666/shakespeare-plays,Thu Apr 27 2017
,Chris Crawford,[],[],"Context Jester is a joke recommender system developed at UC Berkeley to study social information filtering. Users of the system are presented a joke and then they rate them. This dataset is a collection of those ratings. http//eigentaste.berkeley.edu/ Eigentaste A Constant Time Collaborative Filtering Algorithm. Ken Goldberg Theresa Roeder Dhruv Gupta and Chris Perkins. Information Retrieval 4(2) 133-151. July 2001. Content Notes from the source  Each row is a user (Row 1 = User #1) Each column is a joke (Column 1 = Joke #1) Ratings are given as real values from -10.00 to +10.00 99 corresponds to a null rating As of May 2009 the jokes 7 8 13 15 16 17 18 19 are the ""gauge set"" (as discussed in the Eigentaste paper)   Acknowledgements Thanks go to Dr. Ken Golberg's group for putting this super cool data together and for permission to share it with the Kaggle community!  Eigentaste A Constant Time Collaborative Filtering Algorithm. Ken Goldberg Theresa Roeder Dhruv Gupta and Chris Perkins. Information Retrieval 4(2) 133-151. July 2001.  The original data file was converted to a CSV format before uploading.  The original list of jokes was converted from DAT to TSV. Original files can be found here http//eigentaste.berkeley.edu/dataset/  Inspiration Take a look at the Eigentaste paper to learn more about how the data was used. See if you can recreate the study or glean some new insight!",Other,,[humor],Other,,,128,1196,29,"Over 11 million ratings of 150 jokes from 79,681 users",Jester Online Joke Recommender,https://www.kaggle.com/crawford/jester-online-joke-recommender,Sat Jul 15 2017
,United States Department of Agriculture,"[Annotate Code, Annotated Information]","[string, string]",Context This dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately if EPA determines a pesticide is not safe for human consumption it is removed from the market. The PDP tests a wide variety of domestic and imported foods with a strong focus on foods that are consumed by infants and children. EPA relies on PDP data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. USDA uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance USDA’s Integrated Pest Management objectives. USDA also works with U.S. growers to improve agricultural practices. Content While the original 2013 MS Access database can be found here the data has been transferred to a SQLite database for easier more open use. The database contains two tables Sample Data and Results Data. Each sampling includes attributes such as extraction method the laboratory responsible for the test and EPA tolerances among others. These attributes are labeled with codes which can be referenced in PDF format here or integrated into the database using the included csv files.  Inspiration  What are the most common types of pesticides tested in this study? Do certain states tend to use one particular pesticide type over another? Does pesticide type correspond more with crop type or location (state)? Are any produce types found to have higher pesticide levels than assumed safe by EPA standards? By combining databases from several years of PDP tests can you see any trends in pesticide use?  Acknowledgement This dataset is part of the USDA PDP yearly database and the original source can be found here.,CSV,,"[food and drink, agriculture]",Other,,,315,2390,106,Study of pesticide residues in food,Pesticide Data Program (2013),https://www.kaggle.com/usdeptofag/pesticide-data-program-2013,Thu Nov 17 2016
,Sergey Kuznetsov,"[artist, song, link, text]","[string, string, string, string]",Context These are the lyrics for 57650 songs. They can be used for Natural Language Processing purposes such as clustering of the words with similar meanings or predicting artist by the song. The dataset can be expanded with some more features for more advanced research like sentiment analysis. The data is not modified only slightly cleaned which gives a lot of freedom to devise your own applications. Mining I have mined this dataset as a corpus for my NLP studies. However before performing any transformation to bag-of-words or bag-of-N-grams I decided to share the data. The data has been acquired from LyricsFreak through scraping. Then I did some very basic work on removing inconvenient data non-English lyrics extremely short and extremely long lyrics lyrics with non-ASCII symbols. However there's still work to be done in terms of data preparation. Content The dataset contains 4 columns  Artist Song Name Link to a webpage with the song (for reference). This is to be concatenated with http//www.lyricsfreak.com to form a real URL. Lyrics of the song unmodified.  Acknowledgements I would like to acknowledge LyricsFreak which is the direct source of the data.,CSV,,"[languages, music, linguistics]",CC0,,,2908,22877,69,Lyrics for 55000+ songs in English from LyricsFreak,55000+ Song Lyrics,https://www.kaggle.com/mousehead/songlyrics,Thu Jan 05 2017
,Sohier Dane,"[CAD CDW ID, CAD Event Number, General Offense Number, Event Clearance Code, Event Clearance Description, Event Clearance SubGroup, Event Clearance Group, Event Clearance Date, Hundred Block Location, District/Sector, Zone/Beat, Census Tract, Longitude, Latitude, Incident Location, Initial Type Description, Initial Type Subgroup, Initial Type Group, At Scene Time]","[numeric, numeric, numeric, numeric, string, string, string, dateTime, string, string, string, numeric, numeric, numeric, string, string, string, string, string]",This dataset records police responses to 911 calls in the city of Seattle. Acknowledgements This dataset was kindly made available by the City of Seattle. They update the data daily; you can find the original version here. Inspiration  The study discussed in this Atlantic article reviewing 911 calls in Milwaukuee found that that incidents of police violence lead to large drops in the number of 911 calls. Does this hold true for Seattle as well? This dataset technically only contains the responses to 911 calls rather than the calls themselves but it should be feasible to use the responses as a decent proxy for calls. ,CSV,,[crime],CC0,,,142,1479,362,1.4 million responses from 2009 onwards,Seattle Police Department 911 Incident Response,https://www.kaggle.com/sohier/seattle-police-department-911-incident-response,Wed Aug 30 2017
,Dan Ofer,[],[],"Context This dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The dataset was generated by scraping comments from Reddit (not by me )) containing the \s ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously and is generally a reliable indicator of sarcastic comment content. Content Data has balanced and imbalanced (i.e true distribution) versions. (True ratio is about 1100). The corpus has 1.3 million sarcastic statements along with what they responded to as well as many non-sarcastic comments from the same source. Labelled comments are in the train-balanced-sarcasm.csv file. Acknowledgements The data was gathered by Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article ""A Large Self-Annotated Corpus for Sarcasm"". The data is hosted here. Citation @unpublished{SARC   authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli}   title={A Large Self-Annotated Corpus for Sarcasm}   url={https//arxiv.org/abs/1704.05579}   year=2017 }  Annotation of files in the original dataset readme.txt. Inspiration  Predicting sarcasm and relevant NLP features (e.g. subjective determinant racism conditionals sentiment heavy words ""Internet Slang"" and specific phrases).  Sarcasm vs Sentiment Unusual linguistic features such as caps italics or elongated words. e.g. ""Yeahhh I'm sure THAT is the right answer"". Topics that people tend to react to sarcastically ",CSV,,"[humor, reddit, internet]",Other,,,196,2123,209,1.3 million labelled comments from Reddit,Sarcasm on Reddit,https://www.kaggle.com/danofer/sarcasm,Tue Feb 13 2018
,US Patent and Trademark Office,[],[],Context A trademark is a brand name. A trademark or service mark includes any word name symbol device or any combination used or intended to be used to identify and distinguish the goods/services of one seller or provider from those of others and to indicate the source of the goods/services. Content The Trademark Case Files Dataset contains detailed information on 8.6 million trademark applications filed with or registrations issued by the USPTO between January 1870 and January 2017. It is derived from the USPTO main database for administering trademarks and includes data on mark characteristics prosecution events ownership classification third-party oppositions and renewal history. This dataset is a partial version of the full dataset made up of only the case files and owner information. For the full dataset and additional information please see the USPTO website.  Inspiration  Which owner has filed for multiple trademarks with the longest break in between? Who is the most prolific trademarker? How has the volume of trademarks changed since 1870? Which US city has produced the most trademark owners? ,Other,,"[government agencies, united states, product]",CC0,,,112,1033,4096,Over 8 million Trademark case files and their owners,"US Trademark Case Files, 1870-2016",https://www.kaggle.com/uspto/us-trademark-case-files-18702016,Fri Nov 10 2017
,Food and Drug Administration,[],[],Context Identification of adverse drug reactions (ADRs) during the post-marketing phase is one of the most important goals of drug safety surveillance. Spontaneous reporting systems (SRS) data which are the mainstay of traditional drug safety surveillance are used for hypothesis generation and to validate the newer approaches. The publicly available US Food and Drug Administration (FDA) Adverse Event Reporting System (FAERS) data requires substantial curation before they can be used appropriately and applying different strategies for data cleaning and normalization can have material impact on analysis results.  Content We provide a curated and standardized version of FAERS removing duplicate case records applying standardized vocabularies with drug names mapped to RxNorm concepts and outcomes mapped to SNOMED-CT concepts and pre-computed summary statistics about drug-outcome relationships for general consumption. This publicly available resource along with the source code will accelerate drug safety research by reducing the amount of time spent performing data management on the source FAERS reports improving the quality of the underlying data and enabling standardized analyses using common vocabularies. Acknowledgements Data available from this source. When using this data please cite the original publication Banda JM Evans L Vanguri RS Tatonetti NP Ryan PB Shah NH (2016) A curated and standardized adverse drug event resource to accelerate drug safety research. Scientific Data 3 160026. http//dx.doi.org/10.1038/sdata.2016.26 Additionally please cite the Dryad data package Banda JM Evans L Vanguri RS Tatonetti NP Ryan PB Shah NH (2016) Data from A curated and standardized adverse drug event resource to accelerate drug safety research. Dryad Digital Repository. http//dx.doi.org/10.5061/dryad.8q0s4 Inspiration  This is a large-ish dataset (~4.5 gb uncompressed) so try out your batch processing skills in a Kernel What groups of drugs are most risky? What medical conditions are most at risk to drug-associated risks? ,Other,,[government agencies],CC0,,,246,3368,4096,FDA Adverse Event Reporting System (FAERS) Data,Adverse Pharmaceuticals Events,https://www.kaggle.com/fda/adverse-pharmaceuticals-events,Fri Sep 08 2017
,Sohier Dane,[],[],This dataset is a single large shapefile of the buildings in southeast England. You can use it to make gorgeous maps or join it with other datasets for some really nice visualizations. Acknowledgements This dataset was kindly made available by Alasdair Rae with the underlying raw data from the British Ordnance Survey. You can find the original shapefiles here plus shapefiles for the rest of the UK.,Other,,"[cities, geography]",CC0,,,25,608,934,A large shapefile building outlines ,The Buildings of South East England,https://www.kaggle.com/sohier/buildings-of-south-east-england,Fri Sep 22 2017
,UCI Machine Learning,"[X_Minimum, X_Maximum, Y_Minimum, Y_Maximum, Pixels_Areas, X_Perimeter, Y_Perimeter, Sum_of_Luminosity, Minimum_of_Luminosity, Maximum_of_Luminosity, Length_of_Conveyer, TypeOfSteel_A300, TypeOfSteel_A400, Steel_Plate_Thickness, Edges_Index, Empty_Index, Square_Index, Outside_X_Index, Edges_X_Index, Edges_Y_Index, Outside_Global_Index, LogOfAreas, Log_X_Index, Log_Y_Index, Orientation_Index, Luminosity_Index, SigmoidOfAreas, Pastry, Z_Scratch, K_Scatch, Stains, Dirtiness, Bumps, Other_Faults]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset comes from research by Semeion Research Center of Sciences of Communication. The original aim of the research was to correctly classify the type of surface defects in stainless steel plates with six types of possible defects (plus ""other"").  The  Input  vector  was  made  up  of  27  indicators  that  approximately [describe] the geometric shape of the defect and its outline. According to the research paper Semeion was commissioned by the Centro Sviluppo Materiali (Italy) for this task and therefore it is not possible to provide details on the nature of the 27 indicators used as Input vectors or the types of the 6 classes of defects.  Content There are 34 fields. The first 27 fields describe some kind of steel plate faults seen in images.  Unfortunately there is no other information that I know of to describe these columns.   X_Minimum X_Maximum Y_Minimum Y_Maximum Pixels_Areas X_Perimeter Y_Perimeter Sum_of_Luminosity Minimum_of_Luminosity Maximum_of_Luminosity Length_of_Conveyer TypeOfSteel_A300 TypeOfSteel_A400 Steel_Plate_Thickness Edges_Index Empty_Index Square_Index Outside_X_Index Edges_X_Index Edges_Y_Index Outside_Global_Index LogOfAreas Log_X_Index Log_Y_Index Orientation_Index Luminosity_Index SigmoidOfAreas  The last seven columns are one hot encoded classes i.e. if the plate fault is classified as ""Stains"" there will be a 1 in that column and 0's in the other columns. If you are unfamiliar with one hot encoding just know that the last seven columns are your class labels.   Pastry Z_Scratch K_Scatch Stains Dirtiness Bumps Other_Faults  Acknowledgements MetaNet The Theory of Independent Judges (PDF Download Available). Available from https//www.researchgate.net/publication/13731626_MetaNet_The_Theory_of_Independent_Judges [accessed Sep 6 2017]. Dataset provided by Semeion Research Center of Sciences of Communication Via Sersale 117 00128 Rome Italy.  www.semeion.it Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science.",CSV,,"[business, civil engineering]",Other,,,208,2243,0.2841796875,Steel plate faults classified into seven types,Faulty Steel Plates,https://www.kaggle.com/uciml/faulty-steel-plates,Wed Sep 06 2017
,dish,[],[],Context These 19th century works on Nepal were downloaded from Project Gutenberg for a quick viewing of how the line graphs of their page-by-page sentiment compared with one another in applying the the text mining analysis and visualization capabilities of R inspired by the work on janeaustenr or gutenbergr.     Content The books and collection of journals on Nepal of about 200 years ago are in text files.  Acknowledgements These works have been available in machine readable format thanks to Project Gutenberg. Inspiration Although the volumes of books and journals are growing in the online repositories of Project Gutenberg only a few works in English are about Nepal. How are their portrayals of Nepal similar or different in word-clouds and sentiments? Which R packages can be useful to make these comparisons?,Other,,"[languages, linguistics]",CC0,,,12,299,3,For a quick viewing of how they portrayed the country in words and sentiments,Nineteenth Century Works On Nepal,https://www.kaggle.com/blogdish/nineteenth-century-works-on-nepal,Thu Nov 02 2017
,PyTorch,[],[],SqueezeNet 1.0  SqueezeNet AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy smaller DNN architectures offer at least three advantages (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet).   Authors Forrest N. Iandola Song Han Matthew W. Moskewicz Khalid Ashraf William J. Dally Kurt Keutzer https//arxiv.org/abs/1602.07360  SqueezeNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,2,243,4,SqueezeNet 1.0 Pre-trained Model for PyTorch,SqueezeNet 1.0,https://www.kaggle.com/pytorch/squeezenet1,Fri Dec 15 2017
,Benjamin Visser,"[2017-05-08 21:46:36+00:00, c3.8xlarge, Windows, ap-northeast-1a, 1.6503]","[dateTime, string, string, string, numeric]",Context AWS spot Instances allow users to bid on spare server capacity. You set a bid threshold for an instance that is usually upwards of 30% cheaper than standard on-demand AWS instances. You can save a lot of money with AWS spot instances. Data Content I pulled this data from the AWS CLI with the describe-spot-price-history command. I took a lot of time to acquire and transform which is why I decided to provide it here.  There are various time periods per region (I acquired all that I could). The columns are all fairly self-evident. Please comment if you have any questions about the data or columns. The data includes the following column fields  price  the current Spot price  datetime the date and time instance_type the Spot instance type  os the Spot instance operating system  region the region and availability zone (AZ) for the Spot instance  Inspiration While AWS spot instances are significantly cheaper than on-demand instances there is only one problem with spot instances once the spot market price of an instance exceeds the bid threshold you purchased an instance for the instance is terminated and given to others with higher bids. So while hourly server costs are cheaper your server is liable to terminate without notice. But there is a difference between regions and spot pricing. Sometimes there is an arbitrage between regions and some regions have more stable prices than others (fewer price spikes). If you can find which region/AZ is most stable you can worry less about your instance terminating without notice.  I started collecting this data because I wanted answers to two questions  Which region/AZ is historically cheapest for instance X  Which region/AZ is historically most stable for instance X  We could also use this data to predict which regions are likely to stay under a certain $ Spot price which would allow you to say with some amount of certainty whether a SPOT instance lasts the next [61218]+ hours.,CSV,,[business],Other,,,296,4525,2048,"This includes price, region, instance size, and OS for AWS Spot Instances",AWS Spot Pricing Market,https://www.kaggle.com/noqcks/aws-spot-pricing-market,Tue May 16 2017
,Sasan Jafarnejad,"[NUM, DATE, ACC_X, ACC_Y, ACC_Z, GYRO_X, GYRO_Y, GYRO_Z, EOG_L, EOG_R, EOG_H, EOG_V]","[numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The study of human mobility and activities has opened up to an incredible number of studies in the past most of which included the use of sensors distributed on the body of the subject. More recently the use of smart devices has been particularly relevant because they are already everywhere and they come with accurate miniaturized sensors. Whether it is smartphones smartwatches or smartglasses each device can be used to describe complementary information such as emotions precise movements or environmental conditions. Content First of all a smartphone is used to capture mainly contextual data. Two applications are used a simple data collection application based on the SWIPE open-source sensing system (SWIPE) and a logbook application for obtaining real data on user activity (TimeLogger). SWIPE is a platform for sensing recording and processing human dynamics using smartwatches and smartphones. Then a smartwatch is used primarily to capture the user's heart rate. Motion data is also collected without being at the heart of the dataset due to its need to be configured with a low sampling frequency which would drastically increase the dataset and drain the battery as well. An application based on SWIPE is used. Finally JINS MEME smartglasses are used. This model has the advantage of being compact and simple to carry. It does not have a camera or a screen; it simply has three types of sensors an accelerometer (for detecting steps or activities) a gyroscope (for head movements) and an occulographic sensor (eye blinking eye orientation). The official DataLogger application from JINS MEME is used. For more information on the dataset please refer to the corresponding publication available at An Open Dataset for Human Activity Analysis using Smart Devices. The current dataset on Kaggle contains smartglasses data with 20ms interval (due to storage limitations) same data with 10ms interval is also available on demand. Contact sasan.jafarnejad [at] uni [dot] lu to receive the 10ms version. Acknowledgements This work was performed within the eGLASSES project which is partially funded by NCBiR FWF SNSF ANR and FNR under the ERA-NET CHIST-ERAII framework.,CSV,,[],ODbL,,,665,7408,434,"Data collected using Smartphone, Smartwatch and Smartglasses",An Open Dataset for Human Activity Analysis,https://www.kaggle.com/sasanj/human-activity-smart-devices,Fri Sep 01 2017
,Ariel Cedola,"[utc_timestamp, IT_load_new, IT_solar_generation]","[dateTime, string, numeric]","Context This dataset summarizes valuable data about recent total solar power generation and total electricity demand in a specific european country like Italy. Data are time series with hourly resolution and values represent average of real-time power (generated and used) per market time unit (*).  Data correspond to years 2015 and 2016. They are useful to analyze for instance the variation of solar generation with time in the four seasons of the year the change of electricity demand depending on the day of the week or in summer/winter holidays. Use of historical weather data could help to visualize the variation of solar power generation with climate conditions extremely useful excercise for solar power generation forecasting. Studies of load forecasting could be also conducted by making use of the present dataset. (*) Detailed data descriptions Content The two files include time series data of solar generation and total electricity consumption in Italy during the years 2015 and 2016 with hourly resolution. CSV files are structured in three columns   1. Date and Time  2. Load  3. Solar Generation The time is expressed in Coordinated Universal Time (UTC) and the format of Date and Time is ""%Y-%m-%dT%H%M%SZ"". Solar generation and load are floating point numbers which represent power expressed in MW (Mega Watts) units.  Solar generation is the total solar power generated in Italy in 2015 and 2016 calculated by adding the generation in the different italian bidding zones (6 geographical regions Nord Centro Nord Centro Sud Sud Sardegna and Sicilia and 4 poles Brindisi Foggia Priolo and Rossano). Load represents the total demand of power in the same periods. Note the 2015 file presents a few missing data. Acknowledgements Data has been extracted from ""Open Power System Data. 2017. Data Package Time series. Version 2017-07-09 (https//data.open-power-system-data.org/time_series/2017-07-09)."" The primary data source is ENTSO-E Transparency the central data platform of the European transmission system operators (https//transparency.entsoe.eu). Inspiration Do you think we could improve the Day-ahead load forecasting? Navigate for instance the ENTSO-E Transparency website it shows up-to-date comparisons between Day ahead total load forecast and Actual total load by bidding zones or countries. As you will see sometimes the differences may be significant. Important The ENTSO-E Platform is a great repository of energy data. Measured data and forecasts are provided to the Platform by the Primary Data Owners (see Terms and Conditions at https//transparency.entsoe.eu).",CSV,,[time series],Other,,,129,1028,0.5244140625,Two-years hourly-recorded data of solar power generation and electricity demand,Solar generation and demand Italy 2015-2016,https://www.kaggle.com/arielcedola/solar-generation-and-demand-italy-20152016,Wed Jan 17 2018
,Gabriel Preda,"[unique_code, sex, pupil_env, citizenship, ethnic, app_type, admission_stage, educational_alternative, teaching_language, SIRUES, school_env, handicap, orphan_institution, single_parent, after_school]","[numeric, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string]",Context data.gov.ro hosts datasets with public administrative data from Romania government much like data.gov for US relevant data.  The main file from this datasource http//data.gov.ro/dataset/inscrierea-in-invatamantul-primar-2014 originates from data.gov.ro and contains the anonymized information for the pupils registering in elementary school 1st grade in Romania in 2014. Starting from this data I wanted to represent the registration data geographically to have a sense of the geographical distribution of pupils registration in 1st grade. I therefore added few other files to this dataset from different sources    school information (to be able to connect pupils information with geographical data) - 1 file;   census information (in order to show the percent of the entire regional population registering in 1st grade in 2014) - 2 files.  Content The dataset contains 4 sources of data elementary_school_registration_2014.csv The original source of this data is to be found here (Romanian page) http//data.gov.ro/dataset/inscrierea-in-invatamantul-primar-2014 This represents the registration information for pupils in 1st grade for elementary school in Romania. The data is anonymized and shows   an unique code for each child; sex; social environment ('U' stands for Urban and 'R' for country-side);  the citizenship; the ethnic group (similar with mother tongue); the type of the registration application; the admission stage (I-1 I-2 I-3); the educational alternative (traditional or different schooling options available in Romania like special education for challenged pupils or Montessori progressive etc.); teaching language an unique code for identifying every school (SIRUES); disability (handicap) flag; orphan or institutionalized child flag; single parent flag; attendance of after-school option.  school_network.csv Origin of this data source is http//eprofu.ro/docs/tehnic/institutii/retea-scolara.xls. This file is used to connect SIRUES code from the main file in the datasource with the geographical information. The file contains  the 'judet' information (is an administrative unit in Romania larger than a municipality and smaller than a region much like a county in US); the name of the school; the unique code SIRUES (this can be used to merge with the pupils registration file); the type of school; the school category; the education form; the teaching language.  ro_judete_poligon.geojson This file original source is http//www.geo-spatial.org - shows geospatial information for Romanian counties (judet) in geojson format. It also includes the census information starting from 1948 until 2011 (last Romanian census). The detail of county geographical information is very high and therefore this geojson will be used only to extract the census information. romania.geojson This geojson file source is https//github.com/codeforamerica/click_that_hood/blob/master/public/data/romania.geojson It is used to display the county borders (contains less points than ro_judete_poligon.geojson),CSV,,"[europe, education]",CC0,,,70,930,44,Show elementary school admission patterns at county level for Romania in 2014,Elementary school admission Romania 2014,https://www.kaggle.com/gpreda/elementary-school-admission-romania-2014,Sun Jul 23 2017
,Kevin Mader,[],[],Context The dataset available for download on this webpage represents a 5x5x5µm section taken from the CA1 hippocampus region of the brain corresponding to a 1065x2048x1536 volume. The resolution of each voxel is approximately 5x5x5nm.  Content Two image datasets in 3D of Electron Microscopy data with accompanying labels. The data is provided as multipage TIF files that can be loaded in Fiji R KNIME or Python Acknowledgements The dataset was copied from http//cvlab.epfl.ch/data/em directly and only placed here to utilize the Kaggle's kernel and forum capabilities.  Please acknowledge the CV group dataset for publication or any other uses Data Citations  A. Lucchi Y. Li and P. Fua Learning for Structured Prediction Using Approximate Subgradient Descent with Working Sets Conference on Computer Vision and Pattern Recognition 2013.  A. Lucchi K.Smith R. Achanta G. Knott P. Fua Supervoxel-Based Segmentation of Mitochondria in EM Image Stacks with Learned Shape Features IEEE Transactions on Medical Imaging Vol. 30 Nr. 11 October 2011.  Challenges  How accurately can the segmentation be performed with neural networks? Is 3D more accurate than 2D for segmentation? How can mistakes critical to structure or connectivity be penalized more heavily how would a standard ROC penalize them? ,Other,,[neuroscience],Other,,,177,1850,496,A copy of the EPFL  CVLab dataset,Electron Microscopy 3D Segmentation,https://www.kaggle.com/kmader/electron-microscopy-3d-segmentation,Thu Mar 30 2017
,Ricardo Moya,"[id, season, division, round, localTeam, visitorTeam, localGoals, visitorGoals, date, timestamp]","[numeric, string, numeric, numeric, string, string, numeric, numeric, string, numeric]",Context Data Set with the football matches of the Spanish league of the 1st and 2nd division from the 1970-71 to 2016-17 season has been created with the aim of opening a line of research in the Machine Learning for the prediction of results (1X2) of football matches. Content This file contains information about a football matches with the follow features  48081977-7818Rayo VallecanoReal Madrid3230/10/1977247014000   id (4808) Unique identifier of football match season (1977-78) Season in which the match was played division (1) División in which the match was played (1st '1' 2nd '2') round (8) round in which the match was played localTeam (Rayo Vallecano) Local Team name visitorTeam (Real Madrid) Visitor Team name localGoals (3) Goals scored by the local team visitorGoals (2) Goals scored by the visitor team fecha (30/10/1977) Date in which the match was played date (247014000) Timestamp in which the match was played  Acknowledgements Scraping made from  http//www.bdfutbol.com http//www.resultados-futbol.com ,CSV,,[association football],CC4,,,469,3623,0.3662109375,Soccer matches of 1st and 2nd division from season 1970-71 to 2016-17,Football Matches of Spanish League,https://www.kaggle.com/ricardomoya/football-matches-of-spanish-league,Fri Dec 15 2017
,limi44,[],[],Context The data includes 2D human pose estimates of Parkinson's patients performing a variety of tasks (e.g. communication drinking from a cup leg agility). Pose estimates were produced using Convolutional Pose Machines (CPM https//arxiv.org/abs/1602.00134).  The goal of this project was to use features derived from videos of Parkinson's assessment to predict the severity of parkinsonism and dyskinesia based on clinical rating scales. Content Data was acquired as part of a study to measure the minimally clinically important difference in Parkinson's rating scales. Participants received a two hour infusion of levodopa followed by up to two hours of observation. During this time they were assessed at regular intervals and assessments were video recorded for post-hoc ratings by neurologists. There were between 120-130 videos per task. The data includes all movement trajectories (extracted frame-by-frame) from the videos of Parkinson's assessments using CPM as well as confidence values produced by CPM. Ground truth ratings of parkinsonism and dyskinesia severity are included using the UDysRS UPDRS and CAPSIT rating scales.  Camera shake has been removed from trajectories (see paper for more details). No other preprocessing has been performed. Files are saved in JSON format. For information on how to deal with files see data_import_demo.ipynb or view online at https//github.com/limi44/Parkinson-s-Pose-Estimation-Dataset. Acknowledgements We would like to acknowledge the staff and patients at Toronto Western Hospital for their time and assistance in this study. For the papers accompanying these results and more details on data collection and processing please see  [1] M.H. Li T.A. Mestre S.H. Fox B. Taati Vision-Based Assessment of Parkinsonism and Levodopa-Induced Dyskinesia with Deep Learning Pose Estimation arXiv1707.09416 [Cs]. (2017). http//arxiv.org/abs/1707.09416.  [2] M.H. Li T.A. Mestre S.H. Fox B. Taati Automated Vision-Based Analysis of Levodopa-Induced Dyskinesia with Deep Learning in 2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) Jeju Island South Korea 2017.  [3] T.A. Mestre I. Beaulieu-Boire C.C. Aquino N. Phielipp Y.Y. Poon J.P. Lui J. So S.H. Fox What is a clinically important change in the Unified Dyskinesia Rating Scale in Parkinson’s disease? Parkinsonism & Related Disorders. 21 (2015) 1349–1354.   doi10.1016/j.parkreldis.2015.09.044. Inspiration In our study we aimed to evaluate the readiness of off-the-shelf human pose estimation and deep learning for clinical applications in Parkinson's disease. We hope that others may find this dataset useful for furthering progress in technology-based monitoring of neurological disorders.  Banner Photo by jesse orrico on Unsplash.,Other,,[],Other,,,114,1884,132,Pose estimates of Parkinson's patients using deep learning,Parkinson's Vision-Based Pose Estimation Dataset,https://www.kaggle.com/limi44/parkinsons-visionbased-pose-estimation-dataset,Fri Sep 01 2017
,National Library of Medicine,"[file, column name, data type, description]","[string, string, string, string]",Context RxNorm was created by the U.S. National Library of Medicine (NLM) to provide a normalized naming system for clinical drugs defined as the combination of {ingredient + strength + dose form}. In addition to the naming system the RxNorm dataset also provides structured information such as brand names ingredients drug classes and so on for each clinical drug. Typical uses of RxNorm include navigating between names and codes among different drug vocabularies and using information in RxNorm to assist with health information exchange/medication reconciliation e-prescribing drug analytics formulary development and other functions. Content The full technical documentation is available here. Please note that the NLM updates RxNorm on a regular basis; you should assume that this version is out of date. Acknowledgements This dataset uses publicly available data from the U.S. National Library of Medicine (NLM) National Institutes of Health Department of Health and Human Services. Please cite this dataset as RxNorm META2016AB Full Update 2017_03_06 Bethesda MD National Library of Medicine Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/rxnorm.,CSV,,"[linguistics, medicine]",Other,,,92,1153,1016,A normalized naming system for clinical drugs,RxNorm Drug Name Conventions,https://www.kaggle.com/nlm-nih/rxnorm-drug-name-conventions,Thu Jul 06 2017
,Mathew Savage,[],[],Context The Journal of the American Chemical Society is the premier journal published by the American Chemical Society and one of the highest ranking journals in all of chemistry. With almost 60000 papers and over 120000 authors this collections of papers published between 1996 and 2016 represents the current state of chemistry research. Content This dataset is presented in 3 database tables one of published articles and one of all authors with a further table relating authors to the journal articles they have published.  Update 21-12-2017 The previous data was collected by a top level scrape of the table of contents pages from the journal. A couple of months ago I performed a page-level scrape and then forgot about it but I had some positive reactions to the data-set this week so I have processed some of the data (although more remains the raw output from the scrape is an 18 GB csv file). This new data updates the Articles table to contain many more data fields including the paper abstract number of citations and page views (page views are a relatively new feature so is probably not for the lifetime of some of the older papers). One interesting project for this data would be to look at the term frequencies in the abstracts of the papers and use that to see how the focus of chemistry research has changed over the years. If it is interesting to people I have the following unprocessed data - Articles citing papers in this database inc Title Journal Year and Author list - Institutions of authors for each papers (this data is very complicated and requires some difficult parsing) Acknowledgements This data was scraped from the Table of Contents section of the JACS website and is available online publicly. Inspiration This data could be used to determine the average number of authors per paper or the connections between authors to determine if specific research fields can be grouped by the associated authors Also see if you can find the two papers published by me in this year and see who my co-authors were!,SQLite,,"[research, chemistry]",Other,,,206,2810,39,All papers published in J. Am. Chem. Soc. between 1996 and 2016,JACS Papers 1996 - 2016,https://www.kaggle.com/mathewsavage/jacs,Fri Dec 22 2017
,William Cukierski,[],[],Kaggle’s March Machine Learning Mania competition challenged data scientists to predict winners and losers of the men's 2017 NCAA basketball tournament. This dataset contains the selected predictions of all Kaggle participants. These predictions were collected and locked in prior to the start of the tournament. The NCAA tournament is a single-elimination tournament that begins with 68 teams. There are four games usually called the “play-in round” before the traditional bracket action starts. Due to competition timing these games are included in the prediction files but should not be used in analysis as it’s possible that the prediction was submitted after the play-in round games were over. Data Description Each Kaggle team could submit up to two prediction files. The prediction files in the dataset are in the 'predictions' folder. You can map the files to the teams by team_submission_key.csv. The submission format contains a probability prediction for every possible game between the 68 teams. Refer to the competition documentation for data details. For convenience we have included the data files from the competition dataset in the dataset (you may find TourneySlots.csv and TourneySeeds.csv useful for determining matchups). However the focus of this dataset is on Kagglers' predictions.,CSV,,"[basketball, artificial intelligence]",CC4,,,393,2722,26,Forecasting the 2017 NCAA Basketball Tournament,2017 March ML Mania Predictions,https://www.kaggle.com/wcukierski/2017-march-ml-mania-predictions,Fri Mar 17 2017
,Sohier Dane,[],[],This dataset includes the applicable tariff rates and statistical categories for all merchandise imported into the United States. It is based on the international Harmonized System the global system of nomenclature that is used to describe most world trade in goods.  Although the USITC publishes and maintains the HTSA in its various forms Customs and Border Protection is the only agency that can provide legally binding advice or rulings on classification of imports. Contact your nearest Customs office with questions about how potential imports should be classified. For a binding ruling on classification contact the Bureau of Customs and Border Protection. Content The csv is a somewhat condensed version of a series of pdf documents. The row by row contents are generally comprehensive but the pdf chapters often contain general information that is not included here. Acknowledgements This dataset was made available by the United States International Trade Commission. You can find the original dataset updated regularly here.,Other,,[economics],CC0,,,79,945,9,Harmonized Tariff Rates as of July 2017,US Tariff Rates,https://www.kaggle.com/sohier/us-tariff-rates,Fri Sep 15 2017
,PyTorch,[],[],VGG-13  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,1,201,471,VGG-13 Pre-trained Model for PyTorch,VGG-13,https://www.kaggle.com/pytorch/vgg13,Thu Dec 14 2017
,Federal Election Commission,[],[],"This file contains ""24-hour"" and ""48-hour"" reports of independent expenditures filed during the current election cycle and for election cycles through 2010. The file contains detailed information about independent expenditures including who was paid the purpose of the disbursement date and amount of the expenditure and the candidate for or against whom the expenditure was made. Independent expenditures represent spending by individual people groups political committees corporations or unions expressly advocating the election or defeat of clearly identified federal candidates. These expenditures may not be made in concert or cooperation with or at the request or suggestion of a candidate the candidate's campaign or a political party. Any time up to 20 days before an election if these independent expenditures by a person or organization aggregate more than $10000 in a race they must be reported to the Commission before the end of the second day after the communication is publicly distributed. If the communications are distributed within the last 19 days before the election the expenditure must be reported within one day if they aggregate more than $1000 in any race. Acknowledgements This data comes from the US Federal Election Commission. You can find the original dataset here. If you like... If you enjoyed this dataset you might also like the Congressional Election Disbursements dataset.",CSV,,[politics],CC0,,,36,522,77,Spending by groups other than the candidates themselves,Independent Election Expenditures,https://www.kaggle.com/fec/independent-campaign-expenditures,Thu Sep 07 2017
,NLTK Data,[],[],Context This was the original pre-trained POS tagger that nltk.pos_tag used.  This is the infamous maximum entropy POS tagger that gained a lot of heat when no one knew where exactly the model came from. Acknowledge We would like to know who to acknowledge too ;P,Other,,[],Other,,,15,502,17,Maximum Entropy POS Tagger,MaxEnt Treebank POS Tagger,https://www.kaggle.com/nltkdata/maxent-treebank-pos-tagger,Sat Aug 19 2017
,Nolan Conaway,[],[],Context Pitchfork is a music-centric online magazine. It was started in 1995 and grew out of independent music reviewing into a general publication format but is still famed for its variety music reviews. I scraped over 18000 Pitchfork reviews (going back to January 1999). Initially this was done to satisfy a few of my own curiosities but I bet Kagglers can come up with some really interesting analyses!  Content This dataset is provided as a sqlite database with the following tables artists content genres labels reviews years. For column-level information on specific tables refer to the Metadata tab. Inspiration  Do review scores for individual artists generally improve over time or go down? How has Pitchfork's review genre selection changed over time? Who are the most highly rated artists? The least highly rated artists?  Acknowledgements Gotta love Beautiful Soup!,SQLite,,"[critical theory, music]",Other,,,939,8619,80,"Pitchfork reviews from Jan 5, 1999 to Jan 8, 2017","18,393 Pitchfork Reviews",https://www.kaggle.com/nolanbconaway/pitchfork-data,Fri Jan 13 2017
,PromptCloud,[],[],Context PromptCloud extracted 400 thousand reviews of unlocked mobile phones sold on Amazon.com to find out insights with respect to reviews ratings price and their relationships. Content Given below are the fields  Product Title  Brand  Price  Rating  Review text  Number of people who found the review helpful  Data was acquired in December 2016 by the crawlers build to deliver our data extraction services. Initial Analysis It can be accessed here http//www.kdnuggets.com/2017/01/data-mining-amazon-mobile-phone-reviews-interesting-insights.html,CSV,,"[business, internet, telecommunications]",CC0,,,2361,19676,126,"More than 400,000 reviews from Amazon's unlocked mobile phone category",Amazon Reviews: Unlocked Mobile Phones,https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones,Wed Jan 11 2017
,CooperUnion,"[anime_id, name, genre, type, episodes, rating, members]","[numeric, string, string, string, numeric, numeric, numeric]","Context This data set contains information on user preference data from 73516 users on 12294 anime. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings. Content Anime.csv  anime_id - myanimelist.net's unique id identifying an anime. name - full name of anime. genre - comma separated list of genres for this anime. type - movie TV OVA etc. episodes - how many episodes in this show. (1 if movie). rating - average rating out of 10 for this anime. members - number of community members that are in this anime's ""group"".  Rating.csv  user_id - non identifiable randomly generated user id. anime_id - the anime that this user has rated. rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).  Acknowledgements Thanks to myanimelist.net API for providing anime data and user ratings. Inspiration Building a better anime recommendation system based only on user viewing history.",CSV,,"[popular culture, film]",CC0,,,3443,28967,107,"Recommendation data from 76,000 users at myanimelist.net",Anime Recommendations Database,https://www.kaggle.com/CooperUnion/anime-recommendations-database,Wed Dec 21 2016
,silicon99,"[Accident_Index, Location_Easting_OSGR, Location_Northing_OSGR, Longitude, Latitude, Police_Force, Accident_Severity, Number_of_Vehicles, Number_of_Casualties, Date, Day_of_Week, Time, Local_Authority_(District), Local_Authority_(Highway), 1st_Road_Class, 1st_Road_Number, Road_Type, Speed_limit, Junction_Detail, Junction_Control, 2nd_Road_Class, 2nd_Road_Number, Pedestrian_Crossing-Human_Control, Pedestrian_Crossing-Physical_Facilities, Light_Conditions, Weather_Conditions, Road_Surface_Conditions, Special_Conditions_at_Site, Carriageway_Hazards, Urban_or_Rural_Area, Did_Police_Officer_Attend_Scene_of_Accident, LSOA_of_Accident_Location]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context UK police forces collect data on every vehicle collision in the uk on a form called Stats19. Data from this form ends up at the DfT and is published at https//data.gov.uk/dataset/road-accidents-safety-data Content There are 3 CSVs in this set. Accidents is the primary one and has references by Accident_Index to the casualties and vehicles tables. This might be better done as a database. Inspiration Questions to ask of this data -  combined with population data how do different areas compare? what trends are there for accidents involving different road users eg motorcycles peds cyclists are road safety campaigns effective? likelihood of accidents for different groups / vehicles many more..  Manifest dft05-15.tgz - tar of Accidents0515.csv Casualties0515.csv and Vehicles0515.csv tidydata.sh - script to get and tidy data.,CSV,,[road transport],CC0,,,3325,19421,534,Data from the UK Department for Transport,UK Car Accidents 2005-2015,https://www.kaggle.com/silicon99/dft-accident-data,Tue Feb 21 2017
,PyTorch,[],[],DenseNet-161  Densely Connected Convolutional Networks Recent work has shown that convolutional networks can be substantially deeper more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet) which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer the feature-maps of all preceding layers are used as inputs and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages they alleviate the vanishing-gradient problem strengthen feature propagation encourage feature reuse and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10 CIFAR-100 SVHN and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL. Authors Gao Huang Zhuang Liu Kilian Q. Weinberger Laurens van der Maaten https//arxiv.org/abs/1608.06993   DenseNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,2,212,106,DenseNet-161 Pre-trained Model for PyTorch,DenseNet-161,https://www.kaggle.com/pytorch/densenet161,Wed Dec 13 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes 1.7 gb of stop data from South Carolina covering all of 2010 onwards. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[government agencies, crime, law, violence]",Other,,,42,678,2048,Data on Traffic and Pedestrian Stops by Police in South Carolina,Stanford Open Policing Project - South Carolina,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-south-carolina,Tue Jul 11 2017
,Kevin Mader,"[Image.No., Frame.No, Time.hrs.]","[numeric, numeric, numeric]",Overview The data are a time-series of fluorescence images measured of  at OHSU Lab of Charles Allen (https//www.ohsu.edu/xd/research/centers-institutes/oregon-institute-occupational-health-sciences/research/allen/). Introduction We use a fluorescent protein as a reporter for the circadian clock gene Period1. We are able to follow the expression of this gene in many neurons for several days to understand how the neural network in the suprachiasmatic nucleus synchronizes the circadian clock of individual neurons to produce a precise circadian rhythm. We analyze each image to determine the fluorescence intensity of each neuron over multiple circadian cycles.  FAQ How where the images obtained which animal and what staining? The images were taken from a transgenic mouse in which expression of the fluorescent protein Venus is driven by the promoter for the circadian clock gene Period 1. What is the anatomy of the images and how are they oriented? The bright line is the third ventricle which resides on the midline of the brain. The two bright regions on either side of the ventricle are the two portions of the Suprachiasmatic nucleus (SCN). Below the ventricle and the SCN is a dark horizontal band that represents the optic chasm.  What is the bright vertical line in the top middle? The bright line is the third ventricle. Pericytes that line the ventricle express the Venus at very high levels. We don't know the function of the circadian clock in these cells.   Challenge Currently we have to analyze each experiment by hand to follow an individual through a couple hundred images. This takes several days. This problem is going to get worse because we have just purchased a new automated microscope stage that will allow us to simultaneously image from four suprachiasmatic nuclei.  Preview  Ideas for Analysis  Wavelets (pywavelets) following https//www.ncbi.nlm.nih.gov/pubmed/18931366   Questions  Do the cells move during the experiment?  How regular is their signal? Is the period 24 hours? Do nearby cells oscillate together? Do they form chunks or groups over what range do they work? Are there networks formed from time-precedence? ,Other,,[neuroscience],Other,,,293,4735,1024,Fluorescence signal from the circadian regulation region of the brain,Circadian Rhythm in the Brain,https://www.kaggle.com/kmader/circadian-rhythm-in-the-brain,Tue Mar 21 2017
,TESTIMON @ NTNU,"[step, type, amount, nameOrig, oldbalanceOrg, newbalanceOrig, nameDest, oldbalanceDest, newbalanceDest, isFraud, isFlaggedFraud]","[numeric, string, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric]","Context There is a lack of public available datasets on financial services and specially in the emerging mobile money transactions domain. Financial datasets are important to many researchers and in particular to us performing research in the domain of fraud detection. Part of the problem is the intrinsically private nature of financial transactions that leads to no publicly available datasets. We present a synthetic dataset generated using the simulator called PaySim as an approach to such a problem. PaySim uses aggregated data from the private dataset to generate a synthetic dataset that resembles the normal operation of transactions and injects malicious behaviour to later evaluate the performance of fraud detection methods. Content PaySim simulates mobile money transactions based on a sample of real transactions extracted from one month of financial logs from a mobile money service implemented in an African country. The original logs were provided by a multinational company who is  the provider of the mobile financial service which is currently running in more than 14 countries all around the world. This synthetic dataset is scaled down 1/4 of the original dataset and it is created just for Kaggle. Headers This is a sample of 1 row with headers explanation 1PAYMENT1060.31C4292141171089.028.69M15916544620.00.000 step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation). type - CASH-IN CASH-OUT DEBIT PAYMENT  and TRANSFER. amount -  amount of the transaction in local currency. nameOrig - customer who started the transaction oldbalanceOrg - initial balance before the transaction newbalanceOrig - new balance after the transaction nameDest - customer who is the recipient of the transaction oldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants). newbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants). isFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system. isFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction. Past Research There are 5 similar files that contain the run of 5 different scenarios. These files are better explained at my PhD thesis chapter 7 (PhD Thesis Available here http//urn.kb.se/resolve?urn=urnnbnsebth-12932). We ran PaySim several times using random seeds for 744 steps representing each hour of one month of real time which matches the original logs. Each run took around 45 minutes on an i7 intel processor with 16GB of RAM. The final result of a run contains approximately 24 million of financial records divided into the 5 types of categories CASH-IN CASH-OUT DEBIT PAYMENT  and TRANSFER. Acknowledgements This work is part of the research project ”Scalable resource-efficient systems for big data analytics” funded by the Knowledge Foundation (grant 20140032) in Sweden. Please refer to this dataset using the following citations  PaySim first paper of the simulator E. A. Lopez-Rojas  A. Elmir and S. Axelsson. ""PaySim A financial mobile money simulator for fraud detection"". In The 28th European Modeling and Simulation Symposium-EMSS Larnaca Cyprus. 2016",CSV,,"[crime, finance]",CC4,,,5268,45915,471,Synthetic datasets generated by the PaySim mobile money simulator,Synthetic Financial Datasets For Fraud Detection,https://www.kaggle.com/ntnu-testimon/paysim1,Mon Apr 03 2017
,Kevin Mader,[],[],"About This selection of images are controls selected from a screen to find novel anti-infectives using the roundworm C.elegans . The animals were exposed to the pathogen Enterococcus faecalis and either untreated or treated with ampicillin a known antibiotic against the pathogen. The untreated (negative control) worms display predominantly the ""dead"" phenotype worms appear rod-like in shape and slightly uneven in texture. The treated (ampicillin positive control) worms display predominantly the ""live"" phenotype worms appear curved in shape and smooth in texture. For more information please see Moy et al. (ACS Chem Biol 2009) [http//dx.doi.org/10.1021/cb900084v] Images One image per channel (Channel 1 = brightfield; channel 2 = GFP) was acquired at MGH on a Discovery-1 automated microscope (Molecular Devices). Original image size is 696 x 520 pixels. Images are available in 16-bit TIF. Ground Truth The 384 images are from a plate of positive and negative controls. The images are named using this format   <plate>_<wellrow>_<wellcolumn>_<wavelength>_<fileid>.tif   Columns 1-12 are positive controls treated with ampicillin. Columns 13-24 are untreated negative controls. We also provide human-corrected binary images of foreground/background segmentation. To address the problem of correctly segmenting individual worms also when they overlap or cluster we provide one binary foreground/background segmentation ground truth image for each worm Acknowledgements The data have been reposted from the original data taken from the Broad Institute. Please acknowledge the original source if this is used in other works. The original data can be found and downloaded here https//data.broadinstitute.org/bbbc/BBBC010/  These images were originally acquired for a screen in Fred Ausubel's lab at MGH. Please contact aconery AT molbio.mgh.harvard.edu for more information. Original Publication http//dx.doi.org/10.1038/nmeth.1984 Inspiration",Other,,"[healthcare, diseases]",Other,,,102,1649,137,"A small, fully annotated dataset for getting starting with HCS analysis",High-Content Screening with C.Elegans,https://www.kaggle.com/kmader/high-content-screening-celegans,Wed Apr 19 2017
,Rachael Tatman,[],[],Context It is now a common practice to compare models of human language processing by predicting participant reactions (such as reading times) to corpora consisting of rich naturalistic linguistic materials. However many of the corpora used in these studies are based on naturalistic text and thus do not contain many of the low-frequency syntactic constructions that are often required to distinguish processing theories. The corpus includes self-paced reading time data for ten naturalistic stories. Content This is a corpus of naturalistic stories meant to contain varied low-frequency syntactic constructions. There are a variety of annotations and psycholinguistic measures available for the stories. The stories in with their various annotations are coordinated around the file words.tsv which specifies a unique code for each token in the story under a variety of different tokenization schemes. For example the following lines in words.tsv cover the phrase the long-bearded mill owners. 1.54.whole      the 1.54.word       the 1.54.1  the 1.55.whole      long - bearded 1.55.word       long - bearded 1.55.1  long 1.55.2  - 1.55.3  bearded 1.56.whole      mill 1.56.word       mill 1.56.1  mill 1.57.whole      owners . 1.57.word       owners 1.57.1  owners 1.57.2  . The first column is the token code; the second is the token itself. For example 1.57.whole represents the token owners.and 1.57.word represents the token owners. The token code consists of three fields  The id of the story the token is found in The number of the token in the story An additional field whose value is whole for the entire token including punctuation word for the token stripped of punctuation to the left and right and then 1 through n for each sub-token in whole as segmented by NLTK's TreebankWordTokenizer.  The various annotations (frequencies parses RTs etc.) should reference these codes so that we can track tokens uniformly. This dataset contains reading time data collected for 10 naturalistic stories. Participants typically read 5 stories each. The data is contained in batch1_pro.csv and batch2_pro.csv all_stories.tok contains the 10 stories with one word per row. Item is the story number zone is the region where the word falls within the story. Note that some wordforms in all_stories.tok differ from those in words.tsv reflecting typos in the SPR experiment as run. Acknowledgements If you use this dataset in your work please cite the following paper Futrell R. Gibson E. Tily H. Blank I. Vishnevetsky A. Piantadosi S. T. & Fedorenko E. (2017). The Natural Stories Corpus. arXiv preprint arXiv1708.05763. A more complete version of this dataset with additional supporting files can be found in this GitHub repository maintained by  by Richard Futrell at Massachusetts Institute of Technology Titus von der Malsburg at the University of Potsdam and Cory Shain at The Ohio State University. Inspiration  What words do participants tend to read more slowly or quickly? Are certain parts of speech read more quickly or slowly? How much variation in reading speed is there between individuals? What’s the relationship between word length & reading speed? ,CSV,,"[languages, literature, linguistics]",CC4,,,67,807,31,A corpus of stories with human reading times (by word),Natural Stories Corpus,https://www.kaggle.com/rtatman/natural-stories-corpus,Tue Aug 29 2017
,Binks,[],[],"Context Youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Donald Trump's uploaded speeches. It serves as database for an introduction to algorithmic analysis of spoken language. Content Mr Donald Trump speeches dataset consists of 836 subtitles (sets of words) retrieved from Youtube playlists ""Donald Trump Speeches & Events"" ""DONALD TRUMP SPEECHES & PRESS CONFERENCE"" ""President Donald Trump Weekly Address 2017"" ""President Donald Trump's First 100 Days | NBC News"" ""Donald Trump Rally Speech Events Press Conference Rallies Playlist"". This dataset consists of a single CSV file MrTrumpSpeeches.csv. The columns are 'id' 'playlist' 'upload_date' 'title' 'view_count' 'average_rating' 'like_count' 'dislike_count' 'subtitles' which are delimited with tilde character '~'. Text data in columns 'subtitles' is not sentence based there are not commas or dots. It is only stream of words being translated from speech into text by GoogleVoice (more here https//googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html). Acknowledgements The data was downloaded using youtube-dl package.  Inspiration I'm interested in psychological profiles of people speaking based on language used. (For example see https//medium.com/@TSchnoebelen/trump-does-not-talk-like-a-woman-breaking-news-gender-continues-to-be-complicated-and-confusing-4c0d28b41d7)",CSV,,"[languages, presidents, politics]",Other,,,243,2696,13,Psychological profile of Donald Trump based on his spoken language,Mr Donald Trump Speeches,https://www.kaggle.com/binksbiz/mrtrump,Sun Aug 13 2017
,BoraPajo,[],[],Food choices and preferences of college students  This dataset includes information on food choices nutrition preferences childhood favorites and other information from college students. There are 126 responses from students. Data is raw and uncleaned.  Cleaning is in the process and as soon as that is done additional versions of the data will be posted.  Acknowledgements  Thank you to all the students of Mercyhurst University who agreed to participate in this survey.  Inspiration How important is nutrition information for today's college kids? Is their taste in food defined by their food preferences when they were children? Are kids of parents who cook more likely to make better food choices than others? Are these kids likely to have a different taste compared to others? There a number of open ended questions included in this dataset such as What is your favorite comfort food? What is your favorite cuisine? that could work well for natural language processing,Other,,"[food and drink, health]",CC0,,,5151,23273,5,College students' food and cooking preferences,Food choices,https://www.kaggle.com/borapajo/food-choices,Sun Apr 23 2017
,DMPierre,[],[],Context How good of an arbitrageur would you be?  Find it out in the  World Tennis Database which gathers more than 139K matches with odds from 15 different bookies (49 MB).  If you are looking to predict the outcome of a tennis match to find arbitrage opportunities  inspecting variations in a particular player odds or simply searching to improve your Machine Learning or visualisation skills then this dataset might be looking for you too.  Content Data is packed in CSV format ready to spit out some interesting statistics. It is composed of the following 72 columns  Url string Country string Date (yyyy-mm-dd hhmm) to ease date-time transformations. Day string Tournament name string Doubles either 0 or 1 when it is not a single player match Player 1(2) name string Player 1(2) score int number of sets won Player 1(2) set 0 score int up until set 4 - indexing of sets starts at 0 ask why to python ;)  No set info either 0 or 1 when there is no informations about the final set scores Missing bookies either 0 or 1  when there is no informations about any bookies odds Retired player either 0 or 1 when one player retired Cancelled game either 0 or 1 when the game got cancelled Comments string used to insert any comments during the scraping process Walkover either 0 or 1 when one player chose to walkover Awarded player either 0 or 1 when one player got awarded  Fifteen bookies were then taken into account each having three type of infos Player 1 odd Player 2 odd Payout. This result in adding to the preceding 27 columns 45 others.  Bookies were sorted alphabetically   10Bet 18Bet 5Dimes Bet At Home Bet365 BetHard BetOlimp BetRally BWin JetBull MarathonBet Pinnacle TempoBet TonyBet Unibet  Acknowledgements Huge kudos to the OddsPortal Website for their wonderful archiving job.  Cover photo by Jeremy Galliani on Unsplash. Inspiration Various interesting infos and predictions can be made out of this dataset.  Individual players trajectories and their respective odds movements.  Bookies respective strategies. Who sets the pace?  Detecting patterns in arbitrage situations (arbitrageur perspective). And of course predicting the winner of a game as draws are not allowed.  Of course I got inspired by the European Soccer Database. Finally for details about the scraping process visit https//dmpierre.github.io/. ,CSV,,"[tennis, sports]",CC4,,,370,4281,49,"+139K tennis matches, 15 bookies, worldwide.",World Tennis Odds Database,https://www.kaggle.com/dmpierre/world-tennis-odds-database,Sun Sep 10 2017
,NLTK Data,[],[],"Context This dataset is a subset of the Snowball data that is used in NLTK version of the Snowball stemmer. NLTK Snowball stemmer supports the following languages   Danish Dutch English  Finnish  French  German Hungarian  Italian Norwegian  Portuguese  Romanian Russian Spanish Swedish  Turkish  The original Snowball stemmer is hosted on http//snowballstem.org/  Acknowledgements License Copyright (c) 2001 Dr Martin Porter Copyright (c) 20042005 Richard Boulton All rights reserved.  Redistribution and use in source and binary forms with or without modification are permitted provided that the following conditions are met    1. Redistributions of source code must retain the above copyright notice      this list of conditions and the following disclaimer.   2. Redistributions in binary form must reproduce the above copyright notice      this list of conditions and the following disclaimer in the documentation      and/or other materials provided with the distribution.   3. Neither the name of the Snowball project nor the names of its contributors      may be used to endorse or promote products derived from this software      without specific prior written permission.  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS"" AND ANY EXPRESS OR IMPLIED WARRANTIES INCLUDING BUT NOT LIMITED TO THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT INDIRECT INCIDENTAL SPECIAL EXEMPLARY OR CONSEQUENTIAL DAMAGES (INCLUDING BUT NOT LIMITED TO PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE DATA OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY WHETHER IN CONTRACT STRICT LIABILITY OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. ",Other,,[],Other,,,26,580,35,Data for the Snowball Stemmer,Snowball Data,https://www.kaggle.com/nltkdata/snowball-data,Sat Aug 19 2017
,Kevin Mader,"[agency_id, agency_name, agency_url, agency_timezone, agency_lang, agency_phone]","[numeric, string, string, string, string, numeric]",Introduction The basic inspiration was the inability to search through lots of alternative routes while traveling over Easter weekend and having to manually to point-to-point searches. The hope is by using a bit of R/Python the search for the best routes can be made a lot easier. Data Structure The data is organized in a format called GTFS which is explained in detail here but only available in German. Kernels should make it clear how to work with most of the data Source / Attribution The data all comes from opentransportdata.swiss and can be downloaded in the original format by following this link,CSV,,[rail transport],Other,,,85,1534,315,"The locations, timetables, and fare information for the SBB/CFF/FFS Rail Network",Swiss Rail Plan,https://www.kaggle.com/kmader/swiss-rail-plan,Tue Apr 18 2017
,Department of Defense,[],[],Context This dataset is a snapshot of all of the country profiles provided in the World Factbook as of early 2017. The World Factbook is a reference almanac published by the United States Central Intelligence Agency on a continual basis. It is often used as a reference text in other academic works. Content This dataset includes high-level textual information on the economy politics demography culture military and society of every country in the world. Acknowledgements This data was scraped here then concatenated into a single entity before upload to Kaggle. Inspiration This dataset is an ideal basis of comparison for various world countries. Analyzing international data? This dataset is a rich mix-in dataset for contextualizing such analyses.,{}JSON,,"[military, politics, demographics, economics]",CC0,,,411,2596,7,Textual profiles describing every country in the world,World Factbook Country Profiles,https://www.kaggle.com/usdod/world-factbook-country-profiles,Fri Sep 15 2017
,UCI Machine Learning,"[label, TimbreAvg1, TimbreAvg2, TimbreAvg3, TimbreAvg4, TimbreAvg5, TimbreAvg6, TimbreAvg7, TimbreAvg8, TimbreAvg9, TimbreAvg10, TimbreAvg11, TimbreAvg12, TimbreCovariance1, TimbreCovariance2, TimbreCovariance3, TimbreCovariance4, TimbreCovariance5, TimbreCovariance6, TimbreCovariance7, TimbreCovariance8, TimbreCovariance9, TimbreCovariance10, TimbreCovariance11, TimbreCovariance12, TimbreCovariance13, TimbreCovariance14, TimbreCovariance15, TimbreCovariance16, TimbreCovariance17, TimbreCovariance18, TimbreCovariance19, TimbreCovariance20, TimbreCovariance21, TimbreCovariance22, TimbreCovariance23, TimbreCovariance24, TimbreCovariance25, TimbreCovariance26, TimbreCovariance27, TimbreCovariance28, TimbreCovariance29, TimbreCovariance30, TimbreCovariance31, TimbreCovariance32, TimbreCovariance33, TimbreCovariance34, TimbreCovariance35, TimbreCovariance36, TimbreCovariance37, TimbreCovariance38, TimbreCovariance39, TimbreCovariance40, TimbreCovariance41, TimbreCovariance42, TimbreCovariance43, TimbreCovariance44, TimbreCovariance45, TimbreCovariance46, TimbreCovariance47, TimbreCovariance48, TimbreCovariance49, TimbreCovariance50, TimbreCovariance51, TimbreCovariance52, TimbreCovariance53, TimbreCovariance54, TimbreCovariance55, TimbreCovariance56, TimbreCovariance57, TimbreCovariance58, TimbreCovariance59, TimbreCovariance60, TimbreCovariance61, TimbreCovariance62, TimbreCovariance63, TimbreCovariance64, TimbreCovariance65, TimbreCovariance66, TimbreCovariance67, TimbreCovariance68, TimbreCovariance69, TimbreCovariance70, TimbreCovariance71, TimbreCovariance72, TimbreCovariance73, TimbreCovariance74, TimbreCovariance75, TimbreCovariance76, TimbreCovariance77, TimbreCovariance78]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The Million Song Dataset (MSD) is a freely-available collection of audio features and metadata for a million contemporary popular music tracks. This is a subset of the MSD and contains audio features of songs with the year of the song. The purpose being to predict the release year of a song from audio features.  Content The owners recommend that you split the data like this to avoid the 'producer effect' by making sure no song from a given artist ends up in both the train and test set.  train first 463715 examples  test last 51630 examples   Field descriptions  The first value is the year (target) ranging from 1922 to 2011.  Then there are 90 attributes  TimbreAverage[1-12] TimbreCovariance[1-78]  These features were extracted from the 'timbre' features from The Echo Nest API.  The authors took the average and covariance over all 'segments' and each segment was described by a 12-dimensional timbre vector. Acknowledgements Original dataset Thierry Bertin-Mahieux Daniel P.W. Ellis Brian Whitman and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Society for Music Information Retrieval Conference (ISMIR 2011) 2 Subset downloaded from https//archive.ics.uci.edu/ml/datasets/yearpredictionmsd Inspiration Use this dataset to predict the years that each song was released based on it's audio features,CSV,,"[music, sound technology]",Other,,,190,3746,423,A subset of the Million Song Database,Audio features of songs ranging from 1922 to 2011,https://www.kaggle.com/uciml/msd-audio-features,Thu Sep 07 2017
,DaveRosenman,"[X, Year, Team, Game, Win, Home, MP, FG, FGA, FGP, TP, TPA, TPP, FT, FTA, FTP, ORB, DRB, TRB, AST, STL, BLK, TOV, PF, PTS]","[numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",'champsdata.csv' and runnerupsdata.csv' 'champs.csv' contains game-by-game team totals for the championship team from every finals game between 1980 and 2017. 'runnerups.csv' contains game-by-game team totals for the runner-up team from every finals game between 1980 and 2017. The 1980 NBA Finals was the first Finals series since the NBA added the three point line.  Content The data was scrapped from basketball-reference.com.  Variables in 'champs.csv' and 'runnerups.csv'  Year The year the series was played Team The name of the team. Win 1 = Win. 0 = Loss Home 1 = Home team. 0 = Away team. Game Game # MP - Total minutes played. Equals 240 (48x5=240) if game did not go to overtime. MP>240 if game went to overtime. FG - Field goals made FGA - Field goal attempts FGP - Field Goal Percentage TP - 3 Point Field Goals Made TPA - Three point attempts TPP - three point percentage FT - Free throws made FTA - Free throws attempted FTP - Free throw percentage ORB - Offensive rebounds DRB - Defensive rebounds TRB - Total rebounds AST - Assists STL - Steals BLK - Blocks TOV - Turnovers PF - Personal fouls PTS - points scored  Datasets created from 'champsionsdata.csv' and 'runnerupsdata.csv' The R code that I used to make  the three files listed below can be found here 'champs_and_runner_ups_series_averages.csv'  This data frame contains series averages for the champion and runnerup each year.  'champs_series_averages.csv'  This data frame contains series averages for just the champion each year.  'runner_ups_series_averages.csv' This data frame contains decade-by-decade averages for champions and runners up. ,CSV,,[],Other,,,829,4085,0.07421875,Contains team totals game by game from 1980-2017 NBA Finals,NBA Finals Team Stats,https://www.kaggle.com/daverosenman/nba-finals-team-stats,Fri Aug 25 2017
,GiorgioRoffo,[],[],Context In the last decade new ways of shopping online have increased the possibility of buying products and services more easily and faster than ever. In this new context personality is a key determinant in the decision making of the consumer when shopping. A person's buying choices are influenced by psychological factors like impulsiveness; indeed some consumers may be more susceptible to making impulse purchases than others. Since affective metadata are more closely related to the user's experience than generic parameters accurate predictions reveal important aspects of user's attitudes social life including attitude of others and social identity. This work proposes a highly innovative research that uses a personality perspective to determine the unique associations among the consumer's buying tendency and advert recommendations. In fact the lack of a publicly available benchmark for computational advertising do not allow both the exploration of this intriguing research direction and the evaluation of recent algorithms. We present the ADS Dataset a publicly available benchmark consisting of 300 real advertisements (i.e. Rich Media Ads Image Ads Text Ads) rated by 120 unacquainted individuals enriched with Big-Five users' personality factors and 1200 personal users' pictures.  Content The content of the zip files are folders. The directory tree of this disk is as follows 20 Ads folder                Ads belong to 20 product/service categories. all the ads are here. 120  Users Folders              Each folder contains data for one of the involved subjects.              300 real advertisements have been scored Ratings according to the users’ interests (1 star to 5 stars) ~1200 personal pictures (labelled as positive/negative) Big-Five personality scores (O-C-E-A-N). Data can be easily analysed in Matlab or Python Acknowledgements If you use our dataset please cite [1] Roffo G. & Vinciarelli A. (2016 August). Personality in computational advertising A benchmark. In 4 th Workshop on Emotions and Personality in Personalized Systems (EMPIRE) 2016 (p. 18). Inspiration We collected and introduced a representative benchmark for computational advertising enriched with affective-like metadata such as personality factors. The benchmark allows to (i) explore the relationship between consumer characteristics attitude toward online shopping and advert recommendation (ii) identify the underlying dimensions of consumer shopping motivations and attitudes toward online in-store conversions and (iii) have a reference benchmark for comparison of state-of-the-art advertisement recommender systems (ARSs). To the best of our knowledge the ADS dataset is the first attempt at providing a set of advertisements scored by the users according to their interest into the content.  We hope that this work motivates researchers to take into account the use of personality factors as an integral part of their future work since there is a high potential that incorporating these kind of users' characteristics into ARS could enhance recommendation quality and user experience.,Other,,[marketing],Other,,,851,7297,754,A collection of 300 real ads voted by 120 unacquainted individuals,ADS-16 Computational Advertising Dataset,https://www.kaggle.com/groffo/ads16-dataset,Sat Jan 14 2017
,Leonidas,[],[],Data Set Information This data arises from a large study to examine EEG correlates of genetic predisposition to alcoholism. It contains measurements from 64 electrodes placed on subject's scalps which were sampled at 256 Hz (3.9-msec epoch) for 1 second.  There were two groups of subjects alcoholic and control. Each subject was exposed to either a single stimulus (S1) or to two stimuli (S1 and S2) which were pictures of objects chosen from the 1980 Snodgrass and Vanderwart picture set. When two stimuli were shown they were presented in either a matched condition where S1 was identical to S2 or in a non-matched condition where S1 differed from S2.  Attribute Information Each trial is stored in its own file and will appear in the following format.  trial number    sensor position  sample num    sensor value   subject identifier   matching condition   channel   name              time 0                       FP1                      0                        -8.921                  a                  S1 obj                    0     co2a0000364    0 0                       AF8                     87                 4.14                       a                  S1 obj                  33  co2a0000364    0.33 The columns of data are  the trial number  sensor position  sample number (0-255) sensor value (in micro volts)  subject identifier(Alcoholic(a) or Control (c)) matching condition(a single object shown (S1 obj) object 2 shown in a matching condition (S2 match) and object 2 shown in non matching condition (S2 nomatch))  channel number(0-63)  name(a serial code assigned to each subject)  time(inverse of sample num measured in seconds))  Acknowledgements There are no usage restrictions on this data.  Acknowledgments for this data should made to Henri Begleiter at the Neurodynamics Laboratory at the State University of New York Health Center at Brooklyn.  You can check out more info about it on https//archive.ics.uci.edu/ml/datasets/eeg+database,Other,,"[alcohol, neuroscience]",Other,,,172,2400,885,This data contains EEG correlates of genetic predisposition to alcoholism,EEG-Alcohol,https://www.kaggle.com/nnair25/Alcoholics,Sat Aug 19 2017
,Francis Paul Flores,"[Month, Year, Region, Dengue_Cases]","[string, numeric, string, numeric]",Context Data set contains the recorded number of dengue cases per 100000 population per region of the Philippines from 2008 to 2016 Content This is a small data set that is a good starting point for beginners that wants to play around with small scale temporal and spatial data set  Acknowledgements Publisher would like to thank the Department of Health of the Philippines for providing the raw data Inspiration What is the trend of dengue cases in the Philippines? What region/s recorded the highest prevalence of dengue cases? In what specific years do we observe the highest dengue cases? When and where will a possible dengue outbreak occur?,CSV,,"[healthcare, diseases, public health, demographics]",CC0,,,279,1947,0.0498046875,"Monthly and Regional Cases of Dengue per 100,000 Population from 2008 to 2016",Dengue Cases in the Philippines,https://www.kaggle.com/grosvenpaul/dengue-cases-in-the-philippines,Mon Oct 30 2017
,Meg Shields,[],[],I wrote an article a while back about how as Tom Cruise gets older his love interests stay the same age. While Cruise is by no means exceptional in this respect his age gap seemingly mirrors and confirms the larger critique of Hollywood’s bias against older actresses (as gestured towards by the brilliant work of the folks at Time and The Pudding).  I am not a numbers person by any stretch and have no experience handling data let alone analyzing and visualizing it. But I did make this hilariously crude google doc and that's got to count for something. If you're curious about methodology it's specified at the tail end of the article. ,CSV,,"[film, gender, demographics]",CC0,,,82,1223,0.6572265625,"As Tom Cruise Gets Older, His On-Screen Love Interests Stay the Same Age",Tom Cruise's Love Interest Age Gap,https://www.kaggle.com/meghshields/tom-cruises-love-interest-age-gap,Thu Nov 02 2017
,US Patent and Trademark Office,"[case_row_id, case_number, party_row_count, party_type, attorney_row_count, name, contactinfo, position]","[numeric, string, numeric, string, numeric, string, string, string]",Context Achieving the appropriate balance of intellectual property (IP) protection through patent litigation is critical to economic growth. Examining the interplay between US patent law and economic effect is of great interest to many stakeholders. Published in March 2017 this dataset is the most comprehensive public body of information on USPTO patent litigation. Content The dataset covers over 74k cases across 52 years. Five different files (attorneys.csv cases.csv documents.csv names.csv pacer_cases.csv) detail the litigating parties their attorneys results locations and dates. The large documents.csv file covers more than 5 million relevant documents (a tool like split might be your friend here). Acknowledgements This data was collected by the Office of the Chief Economist at the USPTO. Data was collected from both the Public Access to Court Electronics Records (PACER) as well as RECAP an independent PACER repository. Further documentation available via this paper. Inspiration Patent litigation is a tug of war between patent holders competing parties using similar IP and government policy. Which industries see the most litigation? Any notable changes over time? Is there a positive (or negative) correlation between litigation and a company’s economic fortunes? License Public Domain Mark 1.0 Also see source.,CSV,,[law],Other,,,173,1417,2048,"Detailed Patent Litigation Data on 74k Cases, 1963-2015",Patent Litigations,https://www.kaggle.com/uspto/patent-litigations,Thu Jul 13 2017
,UCI Machine Learning,[],[],"Context Predict whether or not a horse can survive based upon past medical conditions.   Noted by the ""outcome"" variable in the data.   Content All of the binary representation have been converted into the words they actually represent. However a fuller description is provided by the data dictionary (datadict.txt). There are a lot of NA's in the data. This is the real struggle here. Try to find a way around it through imputation or other means. Acknowledgements This dataset was originally published by the UCI Machine Learning Database http//archive.ics.uci.edu/ml/datasets/Horse+Colic",Other,,[animals],CC0,,,720,5426,0.0576171875,Can you predict the life or death of a horse?,Horse Colic Dataset,https://www.kaggle.com/uciml/horse-colic,Wed Jun 07 2017
,Keras,[],[],VGG19  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,24,941,580,VGG-19 Pre-trained Model for Keras,VGG-19,https://www.kaggle.com/keras/vgg19,Tue Dec 12 2017
,My Khe Nguyen,[],[],Context I am fond of reading romances so I find practice text mining with novels extremely amusing. I also think this is a good way to approach literary works in quantitative light and gain some new insights. Content The dataset includes a Plain Text file and a Microsoft Word file of the book Gone With The Wind  by Margaret Mitchell. There are some problems with UTF-8 so more cleaning is needed. Acknowledgements I converted this dataset from full PDF file of the book created by Don Lainson dlainson@sympatico.ca sponsored by Project Gutenberg of Australia eBooks. http//campbellmgold.com/archive_ebooks/gone_with_the_wind_mitchell.pdf Inspiration I have started on some sentiment analysis based on chapter number and based on characters. What is the general air of this classic? Which chapter is the most depressing? Which words are most associated with Scarlett? Rhett Butler? What are the sentiments regarding the War? And the Abolition?,Other,,[linguistics],Other,,,52,779,2,Text Mining with Novels,Gone With The Wind,https://www.kaggle.com/mykhe1097/text-mining-gone-with-the-wind,Thu Dec 28 2017
,bpali26,"[Country_Rank, Website, Trustworthiness, Avg_Daily_Visitors, Child_Safety, Avg_Daily_Pageviews, Privacy, Facebook_likes, Twitter_mentions, Google_pluses, LinkedIn_mentions, Pinterest_pins, StumbleUpon_views, Status, Traffic_Rank, Reach_Day, Month_Average_Daily_Reach, Daily_Pageviews, Month_Average_Daily_Pageviews, Daily_Pageviews_per_user, Reach_Day_percentage, Month_Average_Daily_Reach_percentage, Daily_Pageviews_percentage, Month_Average_Daily_Pageviews_percentage, Daily_Pageviews_per_user_percentage, Location, Hosted_by, Subnetworks, Registrant, Registrar, country]","[numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string]",Context This dataset includes some of the basic information of the websites we daily use.  While scrapping this info I learned quite a lot in R programming system speed memory usage etc. and developed my niche in Web Scrapping. It took about 4-5 hrs for scrapping this data through my system (4GB RAM) and nearly about 4-5 days working out my idea through this project.  Content The dataset contains Top 50 ranked sites from each 191 countries along with their traffic (global) rank. Here country_rank represent the traffic rank of that site within the country and traffic_rank represent the global traffic rank of that site.  Since most of the columns meaning can be derived from their name itself its pretty much straight forward to understand this dataset. However  there are some instances of confusion which I would like to explain in here 1) most of the numeric values are in character format hence contain spaces which you might need to clean on. 2) There are multiple instances of same website. for.e.g. Yahoo. com is present in 179 rows within this dataset. This is due to their different country rank in each country.  3)The information provided in this dataset is for the top 50 websites in 191 countries as on 25th May 2017 and is subjected to change in future time due to the dynamic structure of ranking. 4) The dataset inactual contains 9540 rows instead of 9550(50*191 rows). This was due to the unavailability of information for 10 websites.  PS in case if there are anymore queries comment on this I'll add an answer to that in above list. Acknowledgements I wouldn't have done this without the help of others. I've scrapped this information from publicly available (open to all) websites namely  1) http//data.danetsoft.com/  2) http//www.alexa.com/topsites   of which i'm highly grateful. I truly appreciate and thanks the owner of these sites for providing us with the information that I included today in this dataset. Inspiration I feel that there this a lot of scope for exploring & visualization this dataset to find out the trends in the attributes of these websites across countries. Also one could try predicting the traffic(global) rank being a dependent factor on the other attributes of the website. In any case this dataset will help you find out the popular sites in your area.,CSV,,"[world, internet]",Other,,,668,4257,3,General information on some of the most viewed sites country wise,Popular websites across the globe,https://www.kaggle.com/bpali26/popular-websites-across-the-globe,Sat May 27 2017
,VarDial,[],[],This is a mirror of the data from original source of the DSLCC at http//ttg.uni-saarland.de/resources/DSLCC/  DSL Corpus Collection (DSLCC). The DSLCC is a multilingual collection of short excerpts of journalistic texts. It has been used as the main data set for the DSL shared tasks organized within the scope of the workshop on NLP for Similar languages Varieties and Dialects (VarDial). For more information please check the DSL shared task reports (links below) or the website of past editions of VarDial workshop VarDial 2017 at EACL VarDial 2016 at COLING LT4VarDial 2015 at RANLP and VarDial 2014 at COLING. So far five versions of the DSLCC have been released. Languages included in each version of the DSLCC grouped by similarity are the table below. Click on the respective version to download the dataset.  Citing the Dataset If you used the dataset we kindly ask you to refer to the corpus description paper where you can also find more information about the DSLCC Liling Tan Marcos Zampieri Nikola Ljubešić Jörg Tiedemann (2014) Merging Comparable Data Sources for the Discrimination of Similar Languages The DSL Corpus Collection. Proceedings of the 7th Workshop on Building and Using Comparable Corpora (BUCC). pp. 6-10. Reykjavik Iceland. pdf bib The DSL Reports For the results obtained by the participants of the four editions of the DSL shared task please see the shared task reports below. In 2017 the DSL shared task was part of the VarDial evaluation campaign. 2017 - Marcos Zampieri Shervin Malmasi Nikola Ljubešić Preslav Nakov Ahmed Ali Jörg Tiedemann Yves Scherrer Noëmi Aepli (2017) Findings of the VarDial Evaluation Campaign 2017. Proceedings of the Fourth Workshop on NLP for Similar Languages Varieties and Dialects (VarDial). pp. 1-15. Valencia Spain. pdf bib 2016 - Shervin Malmasi Marcos Zampieri Nikola Ljubešić Preslav Nakov Ahmed Ali Jörg Tiedemann (2016) Discriminating between Similar Languages and Arabic Dialect Identification A Report on the Third DSL Shared Task. Proceedings of the Third Workshop on NLP for Similar Languages Varieties and Dialects (VarDial). pp. 1-14. Osaka Japan. pdf bib 2015 - Marcos Zampieri Liling Tan Nikola Ljubešić Jörg Tiedemann Preslav Nakov (2015) Overview of the DSL Shared Task 2015. Proceedings of the Joint Workshop on Language Technology for Closely Related Languages Varieties and Dialects (LT4VarDial). pp. 1-9. Hissar Bulgaria. pdf bib 2014 - Marcos Zampieri Liling Tan Nikola Ljubešić Jörg Tiedemann (2014) A Report on the DSL Shared Task 2014. Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages Varieties and Dialects (VarDial). pp. 58-67. Dublin Ireland. pdf bib Additional Datasets The following datasets have been used in other shared tasks organized within the scope of the VarDial workshop. Arabic Dialect Identification (ADI) A dataset containing four Arabic dialects Egyptian Gulf Levantine North African and MSA. German Dialect Identification (GDI) The ArchiMob corpus containing Swiss German dialects from Basel Bern Lucerne and Zurich. Cross-lingual Parsing (CLP) Datasets for parsing similar languages Croatian - Slovenian Slovak - Czech Norwegian - Danish and Swedish. Acknowledgements Credits of the datasets goes to the original data creators and the VarDial workshop organizers. Credits of the banner image goest to G. Crescoli on Unsplash,Other,,"[languages, linguistics]",Other,,,18,350,55,Multilingual collection of short excerpts of journalistic texts,DSL Corpus Collection (DSLCC),https://www.kaggle.com/vardial/dslcc,Tue Nov 21 2017
,Terminal Security Agency,"[Claim Number, Date Received, Incident Date, Airport Code, Airport Name, Airline Name, Claim Type, Claim Site, Item, Claim Amount, Status, Close Amount, Disposition]","[numeric, dateTime, dateTime, string, string, string, string, string, string, string, string, string, string]",Context Did you know that claims can be filed against TSA? Sometimes US Terminal Security Agency (TSA) makes mistakes. People can get hurt and property can be damaged lost or stolen. Claims are generally filed against TSA for personal injuries and lost or damaged property during screenings and they keep records of every claim!  Content The dataset includes claims filed between 2002 through 2015.  Claim Number Date Received Incident Date Airport Code Airport Name Airline Name Claim Type Claim Site Item Claim Amount Status Close Amount Disposition  Acknowledgements File modifications - Excel format to TSV - commas to semicolons - TSV to CSV Original data can be found here https//www.dhs.gov/tsa-claims-data Inspiration I took a quick look at these data and discovered that the most claims are filed against TSA at John F. Kennedy International. I also discovered four claims against Wrongful Death!,CSV,,[government agencies],CC0,,,154,1294,34,Property and injury claims filed from 2002 - 2015,TSA Claims Database,https://www.kaggle.com/terminal-security-agency/tsa-claims-database,Tue Aug 22 2017
,Thomas,"[game_week, event_name, home_team_id, home_team_name, home_team_short_name, away_team_id, away_team_name, away_team_short_name]","[numeric, string, numeric, string, string, numeric, string, string]",Context The Fantasy Premier League has become more popular every year. In the FPL people pick fantasy teams of real-life players and every week receive points based on their picks' real-life performance. Within this dataset we have some historical data for the player performance in previous seasons as well as future match fixtures. Content The three main components currently in this dataset are  The individual players' current performance stats. The individual players' past performance stats (how much historical data depends on the player). A list of future match fixtures.  All the data was taken from the Official Fantasy Premier League website. N.B. A lot of the data was cobbled together from the output of publicly accessible JSON endpoints therefore there are a lot of duplications (as fixture data was initially from the perspective of the individual players). Also since a lot of this data is used to drive the UI of a Web Application there are a lot of redundancies all of which could do with being cleaned up. Inspiration A lot of my friends are massively into all aspects of the Premier League (fantasy or otherwise) so my main motivation in putting this dataset together was to see was it possible to gain a competitive advantage over my very domain knowledgeable friends with little to no domain knowledge myself. The obvious questions that could be answered with this data correspond to predicting the future performance of players based on historical metrics.,CSV,,[association football],CC4,,,267,2322,0.3798828125,Data for the 2017/18 season of the Fantasy Premier League,Fantasy Premier League - 2017/18,https://www.kaggle.com/thomasd9/fantasy-premier-league-201718,Mon Aug 14 2017
,LA Times Data Desk,"[Year, State Code, Producer Type, Fuel Source, Generators, Facilities, Nameplate Capacity (Megawatts), Summer Capacity (Megawatts)]","[numeric, string, string, string, string, numeric, numeric, numeric]","This data and analysis originally provided information for the February 5 2017 Los Angeles Times story ""Californians are paying billions for power they don't need"" by documenting California's glut of power and the increasing cost to consumers. It also underpins a complementary interactive graphic. The data are drawn from the Energy Information Administration a branch of the United States government. Acknowledgements Data and analysis originally published by Ryan Menezes and Ben Welsh on the LA Times Data Desk GitHub.",CSV,,[energy],Other,,,428,3165,16,Are Californians paying for power they don't need?,California Electricity Capacity,https://www.kaggle.com/la-times/california-electricity-capacity,Thu Apr 06 2017
,Rachael Tatman,"[article, text]","[numeric, string]",Context The Thai language is the primary language of Thailand and a recognized minority language in Cambodia. It has approximately twenty million native speakers in addition to 44 million second language speakers. It is written in Thai script (also called the Thai alphabet) which is notable for being the first writing system to incorporate tonal markers. Thai is written without spaces between words. Content The HSE Thai Corpus is a corpus of modern texts written in Thai language. The texts containing in whole 50 million tokens were collected from various Thai websites (mostly news websites). To make it easier for non-Thai-speakers to comprehend and use texts in the corpus the researchers decided to separate words in each sentence with spaces. The data for the corpus was collected by means of Scrapy. To tokenize texts the Pythai module was used. The text in this dataset is encoded in UTF-8. The corpus can be searched using a web interface at this site. This dataset contains text from two sources Wikipedia and thaigov.go.th. The former is licensed under a standard Wikipedia license and the latter under an Open Government License for Thailand which can be viewed here (In Thai). Acknowledgements The Thai Corpus was developed by the team of students of HSE School of Linguistics in Moscow under the guidance of professor Boris Orekhov. The team consisted of Grigory Ignatyev Alexandra Ershova Anna Kuznetsova Tatyana Shalganova Daniil Kolomeytsev and Nikolai Mikulin. The consulting help on Thai language was provided by Nadezhda Motina. Natalia Filippova Elizaveta Kuzmenko Tatyana Gavrilova Elena Krotova Elmira Mustakimova Olga Sozinova Aleksandra Martynova Maria Sheyanova Marina Kustova and Julia Badryzlova also contributed to the project. Inspiration  In this corpus unlike in most written Thai words have been separated for you with spaces. Can you remove spaces and write an algorithm to identify word boundaries? ,CSV,,"[languages, asia, linguistics, internet]",Other,,,45,753,430,A 35 Million Word Corpus of Thai,HSE Thai Corpus,https://www.kaggle.com/rtatman/hse-thai-corpus,Wed Sep 06 2017
,City of New York,[],[],Context A record of every public WiFi hotspot in New York City. Content The dataset consists of records for every public WiFi hotspot (ones provided by or in partnership with the city) in New York City. It contains over 2500 records overall and is current as of August 11 2017. Acknowledgements This dataset was published as-is by the New York City Department of Information Technology & Telecommunications. Inspiration Does free public WiFi tend to cluster around certain (more affluent) areas? Who are the free WiFi providers and where do they do it? How does NYC WiFi compare to WiFi in other cities like Buenos Aires? (https//www.kaggle.com/octaviog/buenos-aires-public-wifi-access-points),Other,,[internet],CC0,,,252,2170,0.9833984375,Every public WiFi hotspot in New York City.,New York City WiFi Hotspots,https://www.kaggle.com/new-york-city/nyc-public-wifi,Fri Sep 01 2017
,Sohier Dane,"[Item Year, Original Value, Standard Value, Original Currency, Standard Currency, Orignal Measure, Standard Measure, Sources, Notes, Location, Commodity, Variety]","[numeric, numeric, numeric, string, string, string, string, string, string, string, string, string]",Context Are you tired of hearing your elders talk about how much cheaper things were back in their day? Would you like to one-up them by talking about how much cheaper goods were a thousand years before they were born? Of course you would! You might also have an interest in an unusually comprehensive set of historic prices that cover long time spans. English port wine prices for example stretch from 1209 through 1869. Content Each commodity contains prices in the local currency and standardized silver units that allow for broader comparisons. Please note that this dataset has been consolidated into a single file from the original thousand or so csvs so the format is slightly different. Acknowledgements This dataset was kindly made available by Robert Allen and Richard Unger. You can find the original dataset here. Inspiration  Can you identify goods or locations that remained largely unaffected by the industrial revolution?  If you like If you enjoyed this dataset you might also like the millennium of macroeconomic data dataset.,CSV,,"[history, economics]",CC0,,,72,785,30,Price Time Series Between 965 and 1983,Allen-Unger Global Commodity Prices,https://www.kaggle.com/sohier/allenunger-global-commodity-prices,Fri Sep 22 2017
,chetan,"[userId, venueId, venueCategoryId, venueCategory, latitude, longitude, timezoneOffset, utcTimestamp]","[numeric, string, string, string, numeric, numeric, numeric, string]",Context This dataset contains check-ins in NYC and Tokyo collected for about 10 month (from 12 April 2012 to 16 February 2013). It contains 227428 check-ins in New York city and 573703 check-ins in Tokyo. Each check-in is associated with its time stamp its GPS coordinates and its semantic meaning (represented by fine-grained venue-categories). This dataset is originally used for studying the spatial-temporal regularity of user activity in LBSNs. Content This dataset includes long-term (about 10 months) check-in data in New York city and Tokyo collected from Foursquare from 12 April 2012 to 16 February 2013. It contains two files in tsv format. Each file contains 8 columns which are  User ID (anonymized) Venue ID (Foursquare) Venue category ID (Foursquare) Venue category name (Fousquare) Latitude Longitude Timezone offset in minutes (The offset in minutes between when this check-in occurred and the same time in UTC) UTC time  The file dataset_TSMC2014_NYC.txt contains 227428 check-ins in New York city. The file dataset_TSMC2014_TKY.txt contains 537703 check-ins in Tokyo. Acknowledgements This dataset is acquired from here Following is the citation of the dataset author's paper Dingqi Yang Daqing Zhang Vincent W. Zheng Zhiyong Yu. Modeling User Activity Preference by Leveraging User Spatial Temporal Characteristics in LBSNs. IEEE Trans. on Systems Man and Cybernetics Systems (TSMC) 45(1) 129-142 2015. PDF Inspiration One of the questions that I am trying to answer is if there is a pattern in users' checkin behaviour. For example if it's a Friday evening what all places they might be interested to visit.,CSV,,"[cities, geography, internet]",Other,,,467,4812,98,Check-ins in NYC and Tokyo collected for about 10 months,FourSquare - NYC and Tokyo Check-ins,https://www.kaggle.com/chetanism/foursquare-nyc-and-tokyo-checkin-dataset,Thu Apr 27 2017
,Chris,"[time_created, date_created, up_votes, down_votes, title, over_18, author, subreddit]","[numeric, dateTime, numeric, numeric, string, string, string, string]",Reddit is a social network which divide topics into so called 'subreddits'. In subreddit 'worldnews' news of the whole world are published. The dataset contains following columns time_created - a Unix timestamp of the submission creation date date_created - creation time in %Y-%m-%d up_votes - how often the submission was upvoted down_votes - how often the submission was downvoted title - the title of the submission over_18 - if the submission is for mature persons author - the reddit username of the author subreddit - this is always 'worldnews' With the dataset you can estimate several things in contrast to world politics and special events.,CSV,,"[news agencies, linguistics, internet]",Other,,,498,6085,78,Perfect for NLP or other tasks,Worldnews on Reddit from 2008 to Today,https://www.kaggle.com/rootuser/worldnews-on-reddit,Tue Nov 22 2016
,Rudd Fawcett,"[frequency_rank, charcter, pinyin, definition, radical, radical_code, stroke_count, hsk_levl, general_standard_num]","[numeric, string, string, string, string, numeric, numeric, numeric, numeric]",Content A ranked list (by frequency) of over 9k simplified Chinese characters.  Acknowledgements All data scraped from HanziDB.org which is based on Jun Da's Modern Chinese Character Frequency List. Inspiration Some possible questions  What is the distribution of radicals through the 100 most popular characters? 500? 1000? Does stroke count affect usage? Is there an association between the number of strokes and the HSK level of characters? ,CSV,,"[languages, linguistics]",ODbL,,,71,914,0.52734375,List of simplified Chinese characters ordered by frequency rank.,HanziDB,https://www.kaggle.com/ruddfawcett/hanzidb,Tue Oct 03 2017
,Jean-Michel D.,[],[],Context This dataset is born from a test with the twitter streaming api to filter and collect data from this flow on a specific topic in this case the French election.The script used to make this data collection is available on this Github repository. Since the 18th of March the French election enter in the final straight line until the first poll the 23 April 2017  the candidates for the position are  M. Nicolas DUPONT-AIGNAN Mme Marine LE PEN M. Emmanuel MACRON M. Benoît HAMON Mme Nathalie ARTHAUD M. Philippe POUTOU M. Jacques CHEMINADE M. Jean LASSALLE M. Jean-Luc MÉLENCHON M. François ASSELINEAU M. François FILLON  The idea was to collect the data from the Twitter API periodically. The acquisition process evolved as follows   Versions 1 2 and 3 Every hour a python script listens to the twitter api stream for 10 minutes during 3 weeks. Version 4+ The new versions will be based on a new data structure and start after the validation by the French constitutional council on 18 March 2017 of the candidates.  The data will be stored in a dbsqlite files(database_number of the week_number_block_weekday.sqlite format)  and will be updated as often as I can (at least every week). After the first round (version 18+) i had to readjust the number of files per week and the 20 files kaggle limitation push me to reduce the number of files to upload (but you can join for your local analytics the version 17 + version 18+) Example  Illustration of the number of mentions of the different candidates  I add to these databases a sqlite database that contains the informations from the google trends about the top 5 candidates.In thid database there is   A table that contains the overall interests by region A table that contains the interests by region for each candidate A table with the top25 associated queries for each candidate in top and rising ranking  Content In this dbsqlite file you will find a data table that contains for every row ===============Common===============  the index of the line the language of the tweet for each candidate mention_candidatename if the candidate or his associated account has been called (0 or 1) the tweet the timestamp in milliseconds  ===============Version 4+===============  the day the hour (London timezone) the username of the user that made the tweet the username location (that he gives with his profile) if the tweet is a retweet or a quote (0 or 1) the username that has been retweeted the original tweet (the one retweeted or quoted)  Acknowledgements This election is gonna be intense. Inspiration The first version of the dataset was just a test to collect the data and see the first pieces of work  that the community can do with this dataset.The new versions are (I think and hope) adapted to do deep text analytics. ,SQLite,,[politics],ODbL,,,909,11035,3072,Extract from Twitter about the French election (with a taste of Google Trends),French presidential election,https://www.kaggle.com/jeanmidev/french-presidential-election,Wed Jul 05 2017
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, numeric, string, string, string, string, string, string, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one datafile for each state in the U.S. Midwest region (although some are arguably not in the Midwest). States included in this dataset  Iowa - ia.csv Illinois - il.csv Indiana - in.csv Kansas - ks.csv Michigan - mi.csv Minnesota - mn.csv Missouri - mo.csv North Dakota - nd.csv Nebraska - ne.csv Ohio - oh.csv South Dakota - sd.csv Wisconsin  -wi.csv  Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets for housing prices crime or weather!,CSV,,[],Other,,,105,689,2048,Addresses and geo locations for the U.S. Midwest,OpenAddresses - U.S. Midwest,https://www.kaggle.com/openaddresses/openaddresses-us-midwest,Wed Aug 02 2017
,PyTorch,[],[],DenseNet-201  Densely Connected Convolutional Networks Recent work has shown that convolutional networks can be substantially deeper more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet) which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer the feature-maps of all preceding layers are used as inputs and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages they alleviate the vanishing-gradient problem strengthen feature propagation encourage feature reuse and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10 CIFAR-100 SVHN and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL. Authors Gao Huang Zhuang Liu Kilian Q. Weinberger Laurens van der Maaten https//arxiv.org/abs/1608.06993   DenseNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,3,349,73,DenseNet-201 Pre-trained Model for PyTorch,DenseNet-201,https://www.kaggle.com/pytorch/densenet201,Wed Dec 13 2017
,Department of Justice,"[, Lic Regn, Lic Dist, Lic Cnty, Lic Type, Lic Xprdte, Lic Seqn, License Name, Business Name, Premise Street, Premise City, Premise State, Premise Zip Code, Mail Street, Mail City, Mail State, Mail Zip Code, Voice Phone]","[numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, numeric, string, string, string, numeric, numeric]",Context Firearms sold in the United States must be licensed by the US Department of Justice Bureau of Alcohol Tobacco Firearms and Explosives. This dataset is a record of every firearm license which was still current as of July 2017. Content This dataset contains the names license types expiration dates and locations of all Federal Firearms License (FFL) holders in the United States. The possible license types are  01    Dealer in Firearms Other Than Destructive Devices (Includes Gunsmiths) 02    Pawnbroker in Firearms Other Than Destructive Devices 03    Collector of Curios and Relics 06    Manufacturer of Ammunition for Firearms 07    Manufacturer of Firearms Other Than Destructive Devices 08    Importer of Firearms Other Than Destructive Devices 09    Dealer in Destructive Devices 10    Manufacturer of Destructive Devices 11    Importer of Destructive Devices  Acknowledgements This data is published online in a tab-separated format by the Department of Justice Bureau of Alcohol Tobacco Firearms and Explosives. It has been lightly retouched into a CSV file before publication here. Inspiration  Can you geocode this data to determine where licensed gun shops are distributed? What is the distribution of gun licenses across different types? ,CSV,,[government],CC0,,,111,1201,11,Active firearm sales licenses in the United States,Federal Firearm Licences,https://www.kaggle.com/doj/federal-firearm-licensees,Fri Sep 15 2017
,Department of Transportation,[],[],The Freight Analysis Framework (FAF) integrates data from a variety of sources to create a comprehensive picture of freight movement among states and major metropolitan areas by all modes of transportation. Starting with data from the 2012 Commodity Flow Survey (CFS) and international trade data from the Census Bureau FAF incorporates data from agriculture extraction utility construction service and other sectors. FAF version 4 (FAF4) provides estimates for tonnage (in thousand tons) and value (in million dollars) by regions of origin and destination commodity type and mode. Data are available for the base year of 2012 the recent years of 2013 - 2015 and forecasts from 2020 through 2045 in 5-year intervals. Inspiration This dataset should be great for map-based visualizations.,Other,,"[industry, rail transport, shipping]",Other,,,309,2347,623,Flows of goods among US regions for all modes of transportation,Freight Analysis Framework,https://www.kaggle.com/usdot/freight-analysis-framework,Wed Aug 09 2017
,Datafiniti,"[id, asins, brand, categories, colors, dateAdded, dateUpdated, dimension, ean, keys, manufacturer, manufacturerNumber, name, prices, reviews.date, reviews.doRecommend, reviews.numHelpful, reviews.rating, reviews.sourceURLs, reviews.text, reviews.title, reviews.userCity, reviews.userProvince, reviews.username, sizes, upc, weight]","[string, string, string, string, string, dateTime, dateTime, string, string, string, string, string, string, string, dateTime, string, numeric, numeric, string, string, string, string, string, string, string, string, string]",About This Data This is a list of over 1500 consumer reviews for Amazon products like the Kindle Fire TV Stick and more provided by Datafiniti's Product Database. The dataset includes basic product information rating review text and more for each product.  What You Can Do With This Data You can use this data to analyze Amazon’s most successful consumer electronics product launches; discover insights into consumer reviews and assist with machine learning models. E.g.  What are the most reviewed Amazon products? What are the initial and current number of customer reviews for each product? How do the reviews in the first 90 days after a product launch compare to the price of the product? How do the reviews in the first 90 days after a product launch compare to the days available for sale? Map the keywords in the review text against the review ratings to help train sentiment models.  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, product]",CC4,,,918,5867,18,"A list of 1,500+ reviews of Amazon products like the Kindle, Fire TV Stick, etc.",Consumer Reviews of Amazon Products,https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products,Mon Aug 14 2017
,Rachael Tatman,[],[],Context If you've ever had to typeset mathematical expressions you might have thought  wouldn’t it be great if I could just take a picture of a handwritten expression and have it be recognized automatically? This dataset has all the data you’ll need to build a system to do just that. Description The dataset provide more than 11000 expressions handwritten by hundreds of writers from different countries merging the data sets from 4 CROHME competitions. Writers were asked to copy printed expressions from a corpus of expressions. The corpus has been designed to cover the diversity proposed by the different tasks and chosen from an existing math corpus and from expressions embedded in Wikipedia pages. Different devices have been used (different digital pen technologies white-board input device tablet with sensible screen) thus different scales and resolutions are used. The dataset provides only the on-line signal. In the last competition CROHME 2013 the test part is completely original and the train part is using 5 existing data sets  MathBrush  (University  of  Waterloo)  HAMEX  (University  of  Nantes)  MfrDB  (Czech Technical  University)  ExpressMatch  (University  of Sao Paulo) the KAIST data set.  In CROHME 2014 a new test set has been created with 987 new expressions and 2 new tasks has been added isolated symbol recognition and matrix recognition. Train and test files as the evaluation scripts for these new tasks are provided. For the isolated symbol datasets elements are extracted from full expression using the existing datasets which also includes segmentation errors. For the matrix recognition task 380 new expressions have been labelled and split into training and test sets. Furthermore 6 participants of the 2012 competition provide their recognized expressions for the 2012 test part. This data allows research on decision fusion or evaluation metrics. Technical Details The ink corresponding to each expression is stored in an InkML file. An InkML file mainly contains three kinds of information  The  ink a set of traces made of points; The  symbol level ground truth the segmentation and label information of each symbol in the expression; The  expression level ground truth the MathML structure of the expression.  The  two levels of ground truth information (at the symbol as well  as at  the expression level) are entered manually.  Furthermore some  general information is added in the file  The channels (here X and Y); The writer   information (identification handedness (left/right) age gender etc.) if available;   The LaTeX ground truth (without any reference to the ink and hence easy to render);   The unique identification code of the ink (UI) etc.  The InkML format makes references between the digital ink of the expression its segmentation into symbols and its  MathML representation. Thus the stroke segmentation of a symbol can be linked to its MathML representation. The recognized expressions are the outputs of the recognition competitors' systems. It uses the same InkML format but without the ink information (only segmentation label and MathML structure). More details available on CROHME website. Acknowledgements This dataset was compiled by Harold Mouchère and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License. If you use this dataset in your work please include the following citation Harold Mouchère ICFHR 2014 CROHME Fourth International Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME-2014) 2IDCROHME-2014_2URLhttp//tc11.cvc.uab.es/datasets/CROHME-2014_2 You might also like  Handwritten math symbols dataset Over 100 000 image samples Arabic Handwritten Digits Dataset ,Other,,"[writing, mathematics, artificial intelligence]",CC4,,,220,2478,114,Can you use computer vision to recognize handwritten mathematical expressions?,Handwritten Mathematical Expressions,https://www.kaggle.com/rtatman/handwritten-mathematical-expressions,Fri Aug 11 2017
,James Littiebrant,"[, date, id, link, retweet, text, author]","[numeric, string, numeric, string, string, string, string]","This is a dataset of tweets from various active scientists and personalities ranging from Donald Trump and Hillary Clinton to Neil deGrasse Tyson. More are forthcoming. They were obtained through javascript scraping of the browser twitter timeline rather than a Tweepy python API or the twitter timeline API. The inspiration for this twitter dataset is comparing tweets in my own twitter analysis to find who tweets like whom e.g. does Trump or Hillary tweet more like Kim Kardashian than one another? Thus this goes further back in time than anything directly available from Twitter. The data is in JSON format rather than CSV which will be forthcoming as well. Kim Kardashian Adam Savage BillNye Neil deGrasse Tyson Donald Trump and Hillary Clinton have been collected up to 2016-10-14 Richard Dawkins Commander Scott Kelly Barack Obama NASA and The Onion tweets up to 2016-10-15. For your own pleasure with special thanks to the Trump Twitter Archive for providing some of the code here is the JavaScript used to scrape tweets off of a timeline and output the results to the clipboard in JSON format 1) Construct the query with fromTWITTERHANDLE sinceDATE untilDATE 2) In the browser console set up automatic scrolling with  setInterval(function(){ scrollTo(0 document.body.scrollHeight) } 2500) 3) Scrape the resulting timeline with var allTweets = []; var tweetElements = document.querySelectorAll('li.stream-item');  for (var i = 0; i < tweetElements.length; i++) { try {     var el = tweetElements[i]; var text = el.querySelector('.tweet-text').textContent; allTweets.push({ id el.getAttribute('data-item-id') date el.querySelector('.time a').textContent text text link el.querySelector('div.tweet').getAttribute('data-permalink-path') retweet text.indexOf('\""@') == 0 && text.includes('') ? true  false }); } catch(err) {} }; copy(allTweets); Have fun!",CSV,,"[celebrity, linguistics, internet]",ODbL,,,850,5878,35,These are complete twitter timelines of various popular celebs with no retweets,Raw Twitter Timelines w/ No Retweets,https://www.kaggle.com/speckledpingu/RawTwitterFeeds,Sat Oct 15 2016
,Alexander Minushkin,"[Субъект, Пункт ФПСР, Наименование статистического показателя, Значение статистического показателя (за январь - июнь 2014 г.)]","[string, string, string, string]","Context This is official open data from The Ministry of Internal Affairs of the Russian Federation on missing and wanted people identified and unindentified corpses. Original data available here source.  Content File meta.csv - contain information about data source and contact information of original owners in Russian. File structure-20140727.csv - describe datastructure in Russian. Main things that you need to know about data columns are here  ""Subject""  - The official name of the subject of statistical reporting. That's Russian regions note that Crimean Federal District and city ​​of Sevastopol are included. ""Point FPSR"" - Item of the Federal Statistical Work Plan. You don't need to know this. ""Name of the statistical factor"" - this one speaks for itself. Available factors -- Identified persons from among those who were wanted including those who disappeared from the bodies of inquiry investigation court. -- Total cases on the identification of citizens on unidentified corpses that were on the register.  -- Total wanted persons including those who disappeared from the bodies of inquiry investigation court.  -- Identified persons from among the wanted persons including those missing.  -- Total wanted persons.  -- Number (balance) of unreturned missing persons in relation to 2011 (%)  -- Number (balance) of unresolved criminals against 2011 (%)  -- Total discontinued cases in connection with the identification of the person  -- Total wanted persons including those missing  -- Identified persons from the number of wanted persons  ""Importance of the statistical factor"" - value of correspondent statistical factor.  Files data-%Y%m%d-structure-20140727.csv contain actual data. Names of the files contain release date. Data aggregated by quarters of each year for example  data-20150127-structure-20140727.csv - data for whole 2014 year  data-20150627-structure-20140727.csv - data for Q1 and Q2 of 2015 File translate.csv is used to simplify translation from Russian to English. See usage in the kernel. Acknowledgements Thanks to newspaper Komsomolskaya Pravda for bringing up the issue of missing kids in Russia. Thanks to Liza Alert - Volunteer Search and Rescue Squad for efforts in rescue of missing people in Russia.  Photo by Alessio Lin on Unsplash Inspiration Missing people especially kids is a serious problem. However there is not much detailed information about it. Russian officials provide overall information without detalisation of victim's age. As a result many speculations appear in media on this topic   Last year about 200000 reports of missing people were filed with police in Russia. 45000 kids lost every year More than 15000 kids lost every year Radio interview - starting from minute 755 main point ""More than 15K kids lost completely i.e. was not ever found""  Some insights to official data can be found here interview year 2012 ""Annually in Russia about 20 thousand minors disappear in 90% of cases the police find children"". Still there is no information about kids in recent years. If you have any reliable sources please share.",CSV,,"[russia, crime]",CC4,,,85,1105,2,Records for 2014-2017,Missing people in Russia,https://www.kaggle.com/miniushkin/missing-people-in-russia,Thu Feb 01 2018
,Dominik Gawlik,"[, Ticker Symbol, Period Ending, Accounts Payable, Accounts Receivable, Add'l income/expense items, After Tax ROE, Capital Expenditures, Capital Surplus, Cash Ratio, Cash and Cash Equivalents, Changes in Inventories, Common Stocks, Cost of Revenue, Current Ratio, Deferred Asset Charges, Deferred Liability Charges, Depreciation, Earnings Before Interest and Tax, Earnings Before Tax, Effect of Exchange Rate, Equity Earnings/Loss Unconsolidated Subsidiary, Fixed Assets, Goodwill, Gross Margin, Gross Profit, Income Tax, Intangible Assets, Interest Expense, Inventory, Investments, Liabilities, Long-Term Debt, Long-Term Investments, Minority Interest, Misc. Stocks, Net Borrowings, Net Cash Flow, Net Cash Flow-Operating, Net Cash Flows-Financing, Net Cash Flows-Investing, Net Income, Net Income Adjustments, Net Income Applicable to Common Shareholders, Net Income-Cont. Operations, Net Receivables, Non-Recurring Items, Operating Income, Operating Margin, Other Assets, Other Current Assets, Other Current Liabilities, Other Equity, Other Financing Activities, Other Investing Activities, Other Liabilities, Other Operating Activities, Other Operating Items, Pre-Tax Margin, Pre-Tax ROE, Profit Margin, Quick Ratio, Research and Development, Retained Earnings, Sale and Purchase of Stock, Sales, General and Admin., Short-Term Debt / Current Portion of Long-Term Debt, Short-Term Investments, Total Assets, Total Current Assets, Total Current Liabilities, Total Equity, Total Liabilities, Total Liabilities & Equity, Total Revenue, Treasury Stock, For Year, Earnings Per Share, Estimated Shares Outstanding]","[numeric, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset is a playground for fundamental and  technical analysis. It is said that 30% of traffic on stocks is already generated by machines can trading be fully automated? If not there is still a lot to learn from historical data.     Content Dataset consists of following files  prices.csv raw as-is daily prices. Most of data spans from 2010 to the end 2016 for companies new on stock market date range is shorter. There have been approx. 140 stock splits in that time this set doesn't account for that. prices-split-adjusted.csv same as prices but there have been added adjustments for splits. securities.csv general description of each company with division on sectors fundamentals.csv metrics extracted from annual SEC 10K fillings (2012-2016) should be enough to derive most of popular fundamental indicators.  Acknowledgements Prices were fetched from Yahoo Finance fundamentals are from Nasdaq Financials extended by some fields from EDGAR SEC databases. Mining Here is couple of things one could try out with this data Technical  One day ahead prediction Rolling Linear Regression ARIMA Neural Networks LSTM Momentum/Mean-Reversion Strategies Security clustering portfolio construction/hedging  Fundamental Which company has biggest chance of being bankrupt? Which one is undervalued (how prices behaved afterwards) what is Return on Investment?,CSV,,[finance],CC0,,,11272,67530,101,S&P 500 companies historical prices with fundamental data,New York Stock Exchange,https://www.kaggle.com/dgawlik/nyse,Wed Feb 22 2017
,Fracking Analysis,"[tox_cas_edf_id, tox_chemical_name, tox_category, tox_cancer, tox_cardiovascular_blood, tox_developmental, tox_endocrine, tox_gastrointestinal_liver, tox_immunotoxicity, tox_kidney, tox_musculoskeletal, tox_neurotoxicity, tox_reproductive, tox_respiratory, tox_skin_sense]","[dateTime, string, string, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean]","Context These datasets are extractions provided by FrackingData.org of the SQL Server 2012 backup file obtained on a monthly basis from FracFocus.org's ""FracFocus Data Download"" web page.  As FracFocus.org's SQL Server 2012 backup file is inconvenient to ingest for most citizen-scientists or data analysts FrackingData.org ingests the database and outputs both CSV and SQLite files more readily suitable for analysis.  The files in question available herein are also available at FracFocus.org's ""FracFocus Data"" web page. Content Fracking well chemical disclosures the ""Registry"" files hierarchy as follows  RegistryUpload this is the header file. RegistryPurpose this is an intermediate file between the header and the ingredients. RegistryIngredients this is the detail file of the chemical ingredients used in each well.  Chemical health effects and toxicities by Chemical Abstract Society Registry Number (CASRN). Acknowledgements  FrackingData.org for the ingestion and conversion of the FracFocus Registry database. FracFocus.org for the collection of the fracking well chemical disclosures. Scorecard Chemical Health Effects for the compilation of the various chemicals' health effects and toxicities.  Inspiration  Start by associating each fracking well chemical disclosure to its chemical health effects using the CASNumber or CASRN as appropriate. ",CSV,,[energy],CC4,,,142,1379,547,Datasets of fracking well chemical disclosures and toxicities,Fracking Well Chemical Disclosure Datasets,https://www.kaggle.com/frackinganalysis/fracking-well-chemical-disclosure-datasets,Thu Jun 01 2017
,Allen Institute for Artificial Intelligence,[],[],"Overview The Aristo Mini corpus contains 1197377 (very loosely) science relevant  sentences drawn from public data. It provides simple science-relevant text that may be useful to help answer elementary science questions. It was  used in the Aristo Mini system (http//allenai.org/software/) and is  also available as a resource in its own right. Reference Please refer to this corpus as ""The Aristo Mini Corpus (Dec 2016 Release)"" if you mention it in a publication with a pointer to this dataset so others can obtain it. Contents The Aristo Mini corpus is primarily a ""science"" subset of Simple Wikipedia. Sentences were taken from the Simple Wikipedia pages either within the ""Science"" category or from pages whose titles also occurs in  a 4th Grade Study guide (by Barron’s). Also included are approximately 32 thousand definitions from Simple Wiktionary and around 50 thousand 4th grade science-like sentences drawn from the web. Inspiration This dataset was originally collected with the purpose of a question answering system that could answer 4th grade science questions. The simple structure and single (broad) subject makes it a useful resource for other natural language processing tasks as well.  Some interesting projects might include  domain-specific word embeddings  testing sentence-level parsers entity recognition across contexts -training a Markov chain to generate new sciency-sounding sentences ",Other,,"[science and culture, linguistics, artificial intelligence]",CC4,,,54,877,99,"1,197,377 science-relevant sentences drawn from public data",Aristo MINI Corpus,https://www.kaggle.com/allenai/aristo-mini-corpus,Fri Jul 14 2017
,Rohk,"[publish_date, headline_text]","[dateTime, string]",Context Presenting a compendium of crowdsourced journalism from the psuedo-news site The Examiner.  This dataset contains the headlines of 3 million articles written by 21000+ authors over 6 years.  While The Examiner was never praised for its quality it consistently churned out 1000s of articles per day over several years.  At their height in 2011 The Examiner was ranked highly in google search and had enormous shares on social media. At one point it was the 10th largest site on mobile and was attracting 20 M unique visitors a month. As a platform driven towards advert revenue most of their content was rushed unsourced and factually sparse.  It still manages to capture in great detail the trending topics over a long period of time. Prepared by Rohit Kulkarni Content Format CSV Rows 3089781  Column 1 publish_date Date when the article was published on the site in yyyyMMdd format Column 2 headline_text Text of the headline in English with rare utf8 chars (<1k)  Start Date 2010-01-01   End Date 2015-21-31 Another copy of the file with headlines tokenised to lowercase ascii only is included. Acknowledgements Created using Jsoup Java and Bash. Similar news datasets exploring other attributes countries and topics can be seen on my profile. This dataset is free to use with citation Rohit Kulkarni (2017) The Examiner - Spam ClickBait News 2010-2015 [CSV data files] doi10.7910/DVN/I4HKOO Retrieved from [this url] Inspiration The Examiner had emerged as an early winner in the digital content landscape of the 2000's using catchy headlines.  It changed many roles over the years from leftist citizen news to a multiuser blogging platform to a content farm. With falling views its operations were absorbed by AXS in 2014 and the website was finally shut down in June 2016. The original site and content no longer exists http//www.examiner.com This is the last surviving record of its existence.,CSV,,"[news agencies, journalism, historiography]",CC4,,,192,3475,143,6 years of crowdsourced journalism,The Examiner - Spam/Clickbait News Dataset,https://www.kaggle.com/therohk/examine-the-examiner,Mon Nov 20 2017
,Rachael Tatman,[],[],Context Pubs or public houses are popular traditional British gathering places where alcohol and food is served. Content This dataset includes information on 51566 pubs. This dataset contains the following columns  fsa_id    (int) Food Standard Agency's ID for this pub. name (string)L Name of the pub address (string) Address fields separated by commas. postcode (string) Postcode of the pub. easting (int)  northing (int)     latitude (decimal)     longitude (decimal)    local_authority    (string) Local authority this pub falls under.  Acknowledgements The data was derived from the Food Standard Agency's Food Hygiene Ratings and the ONS Postcode Directory. The data is licensed under the Open Government Licence. (See the included .html file.) Inspiration You could use this data as the basis for a real-life travelling salesman problem and plan the world’s longest pub crawl.,CSV,,"[alcohol, europe]",Other,,,259,2266,6,Every pub in the UK and its address,Every Pub in England,https://www.kaggle.com/rtatman/every-pub-in-england,Sat Aug 19 2017
,NYC Parks and Recreation,"[recordid, address, house_number, street, zip_original, cb_original, site, species, diameter, status, wires, sidewalk_condition, support_structure, borough, x, y, longitude, latitude, cb_new, zip_new, censustract_2010, censusblock_2010, nta_2010, segmentid, spc_common, spc_latin, location]","[numeric, string, numeric, string, numeric, numeric, string, string, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string]",Context New York City’s trees shade us in the summer beautify our neighborhoods help reduce noise and support urban wildlife. Beyond these priceless benefits our urban forest provides us a concrete return on the financial investment we put into it. This return includes stormwater interception energy conservation air pollutant removal and carbon dioxide storage. Our publicly owned trees are as much of an asset to us as our streets sewers bridges and public buildings. Content This dataset includes a record for every tree in New York City and includes the tree's location by borough and latitude/longitude species by Latin name and common names size health and issues with the tree's roots trunk and branches. Acknowledgements The 2015 2005 and 1995 tree censuses were conducted by NYC Parks and Recreation staff TreesCount! program staff and hundreds of volunteers.,CSV,,"[plants, forestry]",CC0,,,658,5280,475,What tree species are thriving on the streets of each NYC borough?,Tree Census in New York City,https://www.kaggle.com/nycparks/tree-census,Mon Jul 17 2017
,UCI Machine Learning,"[No, year, month, day, hour, season, PM_Dongsi, PM_Dongsihuan, PM_Nongzhanguan, PM_US Post, DEWP, HUMI, PRES, TEMP, cbwd, Iws, precipitation, Iprec]","[numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",Context PM2.5 readings are often included in air quality reports from environmental authorities and companies. PM2.5 refers to atmospheric particulate matter (PM) that have a diameter less than 2.5 micrometers. In other words it's used as a measure of pollution.  Content The time period for this data is between Jan 1st 2010 to Dec 31st 2015. Missing data are denoted as NA.   No row number  year year of data in this row  month month of data in this row  day day of data in this row  hour hour of data in this row  season season of data in this row  PM PM2.5 concentration (ug/m^3)  DEWP Dew Point (Celsius Degree)  TEMP Temperature (Celsius Degree)  HUMI Humidity (%)  PRES Pressure (hPa)  cbwd Combined wind direction  Iws Cumulated wind speed (m/s)  precipitation hourly precipitation (mm)  Iprec Cumulated precipitation (mm)  Acknowledgements Liang X. S. Li S. Zhang H. Huang and S. X. Chen (2016) PM2.5 data reliability consistency and air quality assessment in five Chinese cities J. Geophys. Res. Atmos. 121 10220â€“10236. The files were downloaded from the UCI Machine Learning Repository and have not been modified. https//archive.ics.uci.edu/ml/datasets/PM2.5+Data+of+Five+Chinese+Cities#,CSV,,"[cities, pollution]",Other,,,487,3429,15,"Measurements for Shenyang, Chengdu, Beijing, Guangzhou, and Shanghai",PM2.5 Data of Five Chinese Cities,https://www.kaggle.com/uciml/pm25-data-for-five-chinese-cities,Wed Aug 23 2017
,Chris Crawford,[],[],Context Abstract Surveys for more than 9500 households were conducted in the growing seasons 2002/2003 or 2003/2004 in eleven African countries Burkina Faso Cameroon Ghana Niger and Senegal in western Africa; Egypt in northern Africa; Ethiopia and Kenya in eastern Africa; South Africa Zambia and Zimbabwe in southern Africa. Households were chosen randomly in districts that are representative for key agro-climatic zones and farming systems. The data set specifies farming systems characteristics that can help inform about the importance of each system for a country’s agricultural production and its ability to cope with short- and long-term climate changes or extreme weather events. Further it informs about the location of smallholders and vulnerable systems and permits benchmarking agricultural systems characteristics. Content The data file contains survey data collected from different families and has 9597 rows that represent the households and 1753 columns with details about the households. The questionnaire was organized into seven sections and respondents were asked to relate the information provided to the previous 12 months’ farming season. There are too many columns to describe here however they are described in detail in this paper https//www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605  Questionnaire.pdf This file contains the questionnaire used a description for each variable name and the question ID. SurveyManual.pdf This file gives further information on the household questionnaire the research design and surveying. It was produced for the team leaders and interviewers in the World Bank/GEF project. AdaptationCoding.pdf This file describes codes for variables ‘ad711’ to ‘ad7625’ from section VII of the questionnaire on adaptation options.  There is also some description in how the data was collected in Survey.pdf. Acknowledgements Waha Katharina; Zipf Birgit; Kurukulasuriya Pradeep; Hassan Rashid (2016) An agricultural survey for more than 9500 African households. figshare. https//doi.org/10.6084/m9.figshare.c.1574094 https//www.nature.com/articles/sdata201620?WT.ec_id=SDATA-201605 The original DTA file was converted to CSV Inspiration This dataset contains a huge amount of information related to farming households in Africa. Data like these are important for studying the impact of global warming on African agriculture and farming families. ,Other,,"[africa, climate, demographics, agriculture]",CC0,,,393,1967,36,Survey of 9500+ households to study impact of climate change on agriculture,Agricultural Survey of African Farm Households,https://www.kaggle.com/crawford/agricultural-survey-of-african-farm-households,Thu Jul 20 2017
,Sohier Dane,"[Week Ending Date, Report Date, Date, Weighted Prices, Sales]","[dateTime, dateTime, dateTime, numeric, numeric]",We don't always think about industrial scale food but cheese blocks the size of a small car are important. The Mandatory Price Reporting Act of 2010 (pdf) was passed on September 27 2010 the act required USDA to release dairy product sales information on or before Wednesday at 300 pm EST (unless affected by a Federal Holiday). The act also required the establishment of an electronic mandatory price reporting system for dairy products reported under Public Law 106-532. These dairy statistics will continue to be collected on a weekly basis AMS-Dairy Programs will collect analyze aggregate and publish dairy product sales information for selected dairy commodities. Acknowledgements This data is released by the US Department of Agriculture. You can find the original dataset here.  Inspiration  Can you predict changes in moisture content for 40 pound blocks of cheese? ,CSV,,"[finance, agriculture]",CC0,,,335,2373,0.3408203125,USDA data on bulk dairy production since 2010,Weekly Dairy Product Prices,https://www.kaggle.com/sohier/weekly-dairy-product-prices,Wed Aug 30 2017
,Timo Bozsolik,"[Measure, Country, Citizenship, Year, Value]","[string, string, string, numeric, numeric]","Context *This dataset shows the migration to and from New Zealand by country and citizenship from 1979 to 2016. * Content The columns in this dataset are  Measure The signal type given in this row one of ""Arrivals"" ""Departures"" ""Net"" Country Country from where people arrived into to New Zealand (for Measure = ""Arrivals"") or to where they left (for Measure = ""Departures""). Contains special values ""Not Stated"" and ""All countries"" (grand total) Citizenship Citizenship of the migrants one of ""New Zealand Citizen"" ""Australian Citizen"" ""Total All Citizenships"" Year Year of the measurement Value Number of migrants  Permanent and long-term arrivals include overseas migrants who arrive in New Zealand intending to stay for a period of 12 months or more (or permanently) plus New Zealand residents returning after an absence of 12 months or more. Permanent and long-term departures include New Zealand residents departing for an intended period of 12 months or more (or permanently) plus overseas visitors departing New Zealand after a stay of 12 months or more. For arrival series the country of residence is the country where a person arriving in New Zealand last lived for 12 months or more (country of last permanent residence). For departure series the country of residence is the country where a person departing New Zealand intends to live for the next 12 months or more (country of next permanent residence). Acknowledgements Curated data by figure.nz original data from Stats NZ. Dataset licensed under Creative Commons 4.0 - CC BY 4.0. Inspiration A good challenge would be to explain New Zealand migration flows as a function of the economic performance of New Zealand or other countries (combine with other datasets). The data could be possibly linked up with other data sources to predict general migration to/from countries based on external factors.",CSV,,"[countries, demographics]",Other,,,258,2288,4,Migration numbers to and from New Zealand from 1979 to 2016,New Zealand Migration,https://www.kaggle.com/timoboz/migration-nz,Wed Jun 07 2017
,Connecticut Open Data,"[DOWNLOAD DATE, IDENTIFIER, LATEST ADMISSION DATE, RACE, GENDER, AGE, BOND AMOUNT, OFFENSE, FACILITY, DETAINER                                        ]","[dateTime, string, dateTime, string, string, numeric, numeric, string, string, string]",Context Since July 1 2016 Connecticut has updated this nightly dataset of every inmate held in jail while awaiting trial.  At the time of download this dataset contains just over one year of data with 1132352 rows of data where one row is one inmate. Content Field Descriptions * DOWNLOAD DATE Date in which the data were extracted and reflecting the population for that day.  IDENITIFIER Individual Inmate Identifier LATEST ADMISSION DATE Most recent date in which the inmate has been admitted. In some instances this may reflect an original date of admission to a correctional facility. Generally if a date is more than one year old an inmate should not be considered to have been held for the entire duration of that time. RACE Race of inmate AGE Age of inmate BOND AMOUNT Amount of bond for which the inmate is being held. In some instances for particularly low (less than $100) this bond amount may be considered a place holder value OFFENSE Controlling offense for which the bond amount has been set. FACILITY Department of Correction facility where the inmate is currently held. DETAINER Denotes whether inmate is being held at the request of another criminal justice agency or if another agency is to be notified upon release.  Acknowledgements Thanks to [http//dataispluralc.om] for the tip on this dataset!  This dataset was downloaded on July 26 2017 - Chekc the original source for more up-to-date data (updated nightly) [https//data.ct.gov/Public-Safety/Accused-Pre-Trial-Inmates-in-Correctional-Faciliti/b674-jy6w] Inspiration This dataset contains information about inmate's race and the nature of the crimes. The Minority Report is a sci-fi story pretty well known for predicting crimes and arresting people before they happen. Can you do the same? Would you dare do the same?,CSV,,[],CC0,,,34,598,115,Inmates being held in correcitonal facilities until trial,Connecticut inmates awaiting trial,https://www.kaggle.com/Connecticut-open-data/connecticut-inmates-awaiting-trial,Thu Jul 27 2017
,Jacob Boysen,[],[],Context The State Firearm Laws project aims to provide researchers with the data necessary to evaluate the effectiveness of various firearm laws. By carefully monitoring how gun legislation impacts firearm-related violence we can provide policymakers with the evidence they need to make gun ownership safer for everyone. Ammunition regulations establish rules for anyone in the business of buying or selling firearm ammunition. Federal regulation of firearm ammunition usually accompanies the regulation of firearms rather than existing independently. For example the federal age requirements for ammunition purchase by type of firearm and type of dealer are the same as those for the purchase of firearms and the populations that are prohibited from possessing firearms are also prohibited from possessing firearm ammunition. Content Data covers all 50 US States 1991-2017 and includes  Vendor license required to sell ammunition. Records of ammunition sales must be retained by the dealer. Permit required to purchase ammunition. Background checks required for ammunition purchases. Sale of ammunition is restricted to the same categories of those who are legally allowed to purchase firearms. Purchase of any type of ammunition restricted to those ages 18 and older. Purchase of handgun ammunition restricted to those ages 21 and older.  Acknowledgements One-hundred of the 133 provisions were coded by Michael Siegel MD MPH Boston University School of Public Health with funding from the Robert Wood Johnson Foundation Evidence for Action Investigator-Initiated Research to Build a Culture of Health program (grant #73337) using data derived from the Thomson Reuters Westlaw legislative database. The other 33 provisions were coded using a database created by Everytown for Gun Safety and Legal Science LLC. Shared in accordance with the Creative Commons Attribution-4.0 International License which is incorporated herein by this reference. No changes were made to the original coding but the data were adapted for use in this database. See the codebook for a list of which provisions were coded by which source. Additional materials include an associated report and related publications.  Inspiration  Which states have seen the most increase in regulation? Any decrease? Can you correlate gun laws and homicides from this dataset? ,CSV,,"[government agencies, government]",CC4,,,504,1785,0.4228515625,"Presence of 133 Provisions in US States, 1991-2017",Firearms Provisions in US States,https://www.kaggle.com/jboysen/state-firearms,Thu Aug 31 2017
,Allan,[],[],Context 45 episodes across 4 seasons of Monty Python's Flying Circus - all of the scripts broken down into reusable bits. Content The data attempts to create a structure around the Flying Circus scripts by breaking actions down into Dialogue (someone is speaking) and Direction (instructions for the actors). Along with each action I have tried to allocate the episode number episode name recording date air date segment name name of character and name of actor playing the character. Acknowledgements The scripts are hosted in HTML at http//www.ibras.dk/montypython/justthewords.htm  I have loaded all of the code that I wrote to scrape and process the data (warning very messy) at https//github.com/allank/monty-python Inspiration I scraped the data because I was looking at data sources for doing RNN to generate text based on an existing corpus.  While the amount of data available in the Flying Circus scripts is probably not sufficient there might be some interesting things to do with the data.  For example some Markov chain generated dialogue lines  Remember buy Whizzo butter and this dead crab.    Yeah er I I personally think this is getting too silly.    I don't like the sound of two bricks being bashed together. ,SQLite,,"[humor, mass media]",ODbL,,,1270,11229,4,"Remember, buy Whizzo butter and this dead crab.",Monty Python Flying Circus,https://www.kaggle.com/allank/monty-python-flying-circus,Sun Sep 03 2017
,PromptCloud,"[additional_info, amenities, check_in_date, check_out_date, city, country, crawl_date, description, highlight_value, hotel_star_rating, image_count, image_urls, internet, landmark, latitude, longitude, occupancy, pageurl, property_address, property_id, property_name, property_type, qts, query_time_stamp, room_price, room_types, search_term, service_value, similar_hotel, sitename, things_to_do, things_to_note, uniq_id]","[string, string, dateTime, dateTime, string, string, dateTime, string, string, string, numeric, string, string, string, numeric, numeric, string, string, string, numeric, string, string, dateTime, dateTime, string, string, string, string, string, string, string, string, string]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 61000 properties) that was created by extracting data from StayZilla.com an Indian AirBnB-like startup founded in 2005 that closed its operations in 2017.  Content This dataset has following fields  additional_info - Special considerations regarding this property. amenities - Pipe (|) delimited list of amenities offered at the property. check_in_date check_out_date city country crawl_date description - Textual description of the property as entered into the site by the lister. highlight_value - Property highlights as entered into the site by the lister. hotel_star_rating - In case the property is a hotel its out-of-five star rating. Not all hotels have ratings. image_count - Number of images posted to the site by the lister. image_urls internet - Does this property have Internet access yes/no. landmark latitude longitude occupancy - How many adults and children may book the listing. pageurl property_address property_id property_name property_type - Home? Hotel? Resort? Etc. qts - Crawler timestamp. query_time_stamp - Copy of qts. room_price room_types - Number of beds and baths for the room. search_term service_value - Whether or not the property is verified with StayZilla (plus some junk entries). similar_hotel - Some similar listings by name. sitename things_to_do - Nearby activities as entered by the lister. things_to_note - Special notes entered by the lister.  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration  What is the shape of the Indian property-sharing market and how does it differ from that of say the United States? (try comparing this dataset to say the Boston AirBnB dataset). What are the contents of textual descriptions for properties? Where are StayZilla properties located geographically? ,CSV,,"[hotels, internet]",CC4,,,103,1112,2,"6,000 Properties on StayZilla",Properties on StayZilla,https://www.kaggle.com/PromptCloudHQ/properties-on-stayzilla,Sat Sep 16 2017
,SemionKorchevskiy,[],[],"Overview PokemonGo is a mobile augmented reality game developed by Niantic inc. for iOS Android and Apple Watch devices. It was initially released in selected countries in July 2016. In the game players use a mobile device's GPS capability to locate capture battle and train virtual creatures called Pokémon who appear on the screen as if they were in the same real-world location as the player.  Dataset Dataset consists of roughly 293000 pokemon sightings (historical appearances of Pokemon) having coordinates time weather population density distance to pokestops/ gyms etc. as features. The target is to train a machine learning algorithm so that it can predict where pokemon appear in future. So can you predict'em all?) Feature description  pokemonId -  the identifier of a pokemon should be deleted to not affect predictions. (numeric; ranges between 1 and 151) latitude longitude - coordinates of a sighting (numeric) appearedLocalTime - exact time of a sighting in format yyyy-mm-dd'T'hh-mm-ss.ms'Z' (nominal) cellId 90-5850m - geographic position projected on a S2 Cell with cell sizes ranging from 90 to 5850m (numeric) appearedTimeOfDay - time of the day of a sighting (night evening afternoon morning) appearedHour/appearedMinute - local hour/minute of a sighting (numeric)  appearedDayOfWeek - week day of a sighting (Monday    Tuesday Wednesday Thursday Friday Saturday Sunday)   appearedDay/appearedMonth/appearedYear - day/month/year of a sighting (numeric) terrainType - terrain where pokemon appeared described with help of GLCF Modis Land Cover (numeric) closeToWater - did pokemon appear close (100m or less) to water (Boolean same source as above) city - the city of a sighting (nominal) continent (not always parsed right) - the continent of a sighting (nominal) weather - weather type during a sighting (Foggy Clear PartlyCloudy MostlyCloudy Overcast Rain BreezyandOvercast LightRain Drizzle BreezyandPartlyCloudy HeavyRain BreezyandMostlyCloudy Breezy Windy WindyandFoggy Humid Dry WindyandPartlyCloudy DryandMostlyCloudy DryandPartlyCloudy DrizzleandBreezy LightRainandBreezy HumidandPartlyCloudy HumidandOvercast RainandWindy)                       // Source for all weather features temperature - temperature in celsius at the location of a sighting (numeric)  windSpeed - speed of the wind in km/h at the location of a sighting (numeric) windBearing - wind direction (numeric) pressure - atmospheric pressure in bar at the location of a sighting (numeric) weatherIcon - a compact representation of the weather at the location of a sighting (fog clear-night partly-cloudy-night partly-cloudy-day cloudy clear-day rain wind) sunriseMinutesMidnight-sunsetMinutesBefore - time of appearance relatively to sunrise/sunset Source population density - what is the population density per square km of a sighting (numeric Source) urban-rural - how urban is location where pokemon appeared (Boolean built on Population density <200 for rural >=200 and <400 for midUrban >=400 and <800 for subUrban >800 for urban) gymDistanceKm pokestopDistanceKm - how far is the nearest gym/pokestop in km from a sighting? (numeric extracted from this dataset) gymIn100m-pokestopIn5000m - is there a gym/pokestop in 100/200/etc meters? (Boolean) cooc 1-cooc 151 -  co-occurrence with any other pokemon (pokemon ids range between 1 and 151) within 100m distance and within the last 24 hours (Boolean) class - says which pokemonId it is to be predicted. Data dump   All pokemon sightings (in JSON file without features) can be found in Discussion ""Datadump""",Other,,"[games and toys, video games]",Other,,,3877,32931,763,Predict where Pokemon appear in PokemonGo based on historical data,Predict'em All,https://www.kaggle.com/semioniy/predictemall,Wed Oct 12 2016
,Nick Wong,[],[],Context As I am trying to learn and build an LSTM prediction model for equity prices I have tried Gold and then want to try crops which may have strong trends in times so I prepared the dataset for the weekly corn prices. Content The file composed of simply 2 columns. One is the date (weekend) and the other is corn close price. The period is from 2015-01-04 to 2017-10-01. The original data is downloaded from Quantopian corn futures price. Acknowledgements Thanks to Jason of his tutorial about LSTM forecast https//machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/ Inspiration William Gann Time is the most important factor in determining market movements and by studying past price records you will be able to prove to yourself history does repeat and by knowing the past you can tell the future. There is a definite relation between price and time.,Other,,"[finance, agriculture]",CC0,,,331,2786,0.01953125,Weekly corn close price from 2015 to 2017 (2017-10-01),Weekly Corn Price,https://www.kaggle.com/nickwong64/corn2015-2017,Tue Oct 10 2017
,Anton Bobanev,"[car, price, body, mileage, engV, engType, registration, year, model, drive]","[string, numeric, string, numeric, numeric, string, string, numeric, string, string]",Context This dataset was collected by me from car sale advertisements for study/practice purposes in 2016. Though there is couple well known car features datasets they seems quite simple and outdated. Car topic is really interesting. But I wanted to practice with real raw data which has all inconvenient moments (as NA’s for example). This dataset contains data for more than 9.5K cars sale in Ukraine. Most of them are used cars so it opens the possibility to analyze features related to car operation. At the end of the day I look at this data as a subset from all Ukrainian car fleet. Content Dataset contains 9576 rows and 10 variables with essential meanings  car manufacturer brand  price seller’s price in advertisement (in USD) body car body type  mileage as mentioned in advertisement (‘000 Km) engV rounded engine volume (‘000 cubic cm) engType type of fuel (“Other” in this case should be treated as NA)  registration whether car registered in Ukraine or not  year year of production  model specific model name  drive drive type  Data has gaps so be careful and check for NA’s. I tried to check and drop repeated offers but theoretically duplications are possible. Inspiration Data will be handy to study and practice different models and approaches.  As a further step you can compare patters in Ukrainian market to your own domestic car market characteristics.,CSV,,[automobiles],CC0,,,993,6223,0.513671875,Data collected from private car sale advertisements in Ukraine,Car Sale Advertisements,https://www.kaggle.com/antfarol/car-sale-advertisements,Thu May 04 2017
,Melvyn Drag,[],[],"Overview We could take a music theory class to understand what a note is but why don't we just find out for ourselves? In this data set we have the notes on a guitar on open strings and on the 1st-8th frets on every string. The notes were recorded on a nice but low-end guitar called the Yamaha C-40.  The guitar is in standard tuning (from the top string to the bottom on we have E low A D G B E high). What to do with this dataset I didn't label any notes (except the open ones - an open A string is an A). If you want a challenge you can cluster the notes and see if your clustering lumps all the same notes together. If you want labels so you can do some inspection of notes that are the same you can look on google for a guitar fretboard diagram. I have done both of these experiments and learned a bit about music which I 'm hoping to verify soon when I find a good music theory book. This data set might be interesting to use to be able to write sheet music from an audio sample of some finger-picked music. Identifying chords is a difficult computational task but finger-style guitar with clear individual notes might be easier. If this is the case a simple script could be written to write sheet music from an audio sample. One draw back about the data set is that some non-plucked strings were vibrating when I played a note. I tried various techniques to muffle them but there is still some noise in the background. I don't know if this is because of my technique or something that happens to all players on all guitars. In any event this noise didn't hurt my analysis. About the data They were recorded by me Melvyn using a program called audacity. There is a directory with the name of the string. Inside the directory you will find .wav files named either open 1 2 ....8 for the fingering of the string. There is also a directory called ""scale"" I recorded some notes that make a ""do-re-mi..."" scale. You can use these for a number of things. I use the GuitarTuner app to tune the Guitar - I'm just learning so I don't have an ear for notes yet. After some initial analysis it looks like the guitar might be a bit out of tune so the resonant frequencies are a bit off from what they should be. Another thing that is interesting to think about it is how far a frequency must be from the proper on until it becomes distinguishable as a different note.",Other,,[music],CC0,,,382,5873,14,A collection of notes played on a guitar,What is a note?,https://www.kaggle.com/juliancienfuegos/what-is-a-note,Wed Nov 23 2016
,The Metropolitan Museum of Art,"[Object Number, Is Highlight, Is Public Domain, Object ID, Department, Object Name, Title, Culture, Period, Dynasty, Reign, Portfolio, Artist Role, Artist Prefix, Artist Display Name, Artist Display Bio, Artist Suffix, Artist Alpha Sort, Artist Nationality, Artist Begin Date, Artist End Date, Object Date, Object Begin Date, Object End Date, Medium, Dimensions, Credit Line, Geography Type, City, State, County, Country, Region, Subregion, Locale, Locus, Excavation, River, Classification, Rights and Reproduction, Link Resource, Metadata Date, Repository]","[string, boolean, boolean, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, dateTime, string]","Dataset source https//github.com/metmuseum/openaccess The Metropolitan Museum of Art presents over 5000 years of art from around the world for everyone to experience and enjoy. The Museum lives in three iconic sites in New York City—The Met Fifth Avenue The Met Breuer and The Met Cloisters. Millions of people also take part in The Met experience online. Since it was founded in 1870 The Met has always aspired to be more than a treasury of rare and beautiful objects. Every day art comes alive in the Museum's galleries and through its exhibitions and events revealing both new ideas and unexpected connections across time and across cultures. The Metropolitan Museum of Art provides select datasets of information on more than 420000 artworks in its Collection for unrestricted commercial and noncommercial use. To the extent possible under law The Metropolitan Museum of Art has waived all copyright and related or neighboring rights to this dataset using Creative Commons Zero. This work is published from The United States Of America. You can also find the text of the CC Zero deed in the file LICENSE in this repository. These select datasets are now available for use in any media without permission or fee; they also include identifying data for artworks under copyright. The datasets support the search use and interaction with the Museum’s collection. At this time the datasets are available in CSV format encoded in UTF-8. While UTF-8 is the standard for multilingual character encodings it is not correctly interpreted by Excel on a Mac. Users of Excel on a Mac can convert the UTF-8 to UTF-16 so the file can be imported correctly. Additional usage guidelines Images not included Images are not included and are not part of the dataset. Companion artworks listed in the dataset covered by the policy are identified in the Collection section of the Museum’s website with the Creative Commons Zero (CC0) icon. For more details on how to use images of artworks in The Metropolitan Museum of Art’s collection please visit our Open Access page. Documentation in progress This data is provided “as is” and you use this data at your own risk. The Metropolitan Museum of Art makes no representations or warranties of any kind. Documentation of the Museum’s collection is an ongoing process and parts of the datasets are incomplete. We plan to update the datasets with new and revised information on a regular basis. You are advised to regularly update your copy of the datasets to ensure you are using the best available information. Pull requests Because these datasets are generated from our internal database we do not accept pull requests. If you have identified errors or have extra information to share please email us at openaccess@metmuseum.org and we will forward to the appropriate department for review. Attribution Please consider attributing or citing The Metropolitan Museum of Art's CC0 select datasets especially with respect to research or publication. Attribution supports efforts to release other datasets in the future. It also reduces the amount of ""orphaned data"" helping to retain source links. Do not misrepresent the dataset Do not mislead others or misrepresent the datasets or their source. You must not use The Metropolitan Museum of Art’s trademarks or otherwise claim or imply that the Museum or any other third party endorses you or your use of the dataset. Whenever you transform translate or otherwise modify the dataset you must make it clear that the resulting information has been modified. If you enrich or otherwise modify the dataset consider publishing the derived dataset without reuse restrictions. The writers of these guidelines thank the The Museum of Modern Art Tate Cooper-Hewitt and Europeana.",CSV,,"[culture and humanities, museums]",CC0,,,187,2144,216,"Explore information on more than 420,000 historic artworks",The Metropolitan Museum of Art Open Access,https://www.kaggle.com/metmuseum/the-metropolitan-museum-of-art-open-access,Fri Apr 07 2017
,Mario Navas,"[id, vendor_id, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, store_and_fwd_flag, trip_duration, dist_meters, wait_sec]","[numeric, string, dateTime, dateTime, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",Context This dataset was collected using our App EC Taximeter.  An easy to use tool developed to compare fees giving the user an accurate fee based on GPS to calculate a cost of the taxi ride. Due to the ability to verify that you are charged fairly our App is very popular in several cities. We encourage our users to send us URLs with the taxi/transportation fees in their cities to keep growing our database. ★ Our App gets the available fares for your location based on your GPS perfect when traveling and not getting scammed. ★ Users can start a taximeter in their own phone and check they are charged fairly ★ Several useful information is displayed to the user during the ride Speed Wait time Distance GPS update GPS precision Range of error. ★ Each fare has information available for reference like Schedule Minimum fee Source Last update. ★ It’s possible to surf through several cities and countries which fares are available for use. If a fare is not in the app now it’s easier than ever to let us know thanks to Questbee Apps. We invite users to contribute to our project and expect this data set to be useful please don't hesitate to contact us to info@ashkadata.com to add your city or to contribute with this project. Content The data is collected from June 2016 until July 20th 2017. The data is not completely clean many users forget to turn off the taximeter when done with the route. Hence we encourage data scientist to explore it and trim the data a little bit Acknowledgements We have to acknowledge the valuable help of our users who have contributed to generate this dataset and have push our growth by mouth to mouth recommendation. Inspiration Our first inspiration for the App was after being scammed in our home city Quito. We started it as a tool for people to be fairly charged when riding a taxi. Currently with other transportation options available we also help user to compare fares in their cities or the cities which they are visiting. File descriptions mex_clean.csv - the dataset contains information of routes in Mexico City uio_clean.csv - the dataset contains information of routes in Quito Ecuador bog_clean.csv - the dataset contains information of routes in Bogota all-data_clean.csv - the dataset contains information of routes in different cities ,CSV,,"[telecommunications, road transport]",CC4,,,306,2424,20,"Data collected from Taxi, Cabify and Uber trips, using EC Taximeter","Taxi Routes of Mexico City, Quito and more",https://www.kaggle.com/mnavas/taxi-routes-for-mexico-city-and-quito,Wed Aug 02 2017
,World Bank,"[, Remittance-receiving country (across)                                                              -                                                 Remittance-sending country (down) , Afghanistan, Albania, Algeria, American Samoa, Andorra, Angola, Antigua and Barbuda, Argentina, Armenia, Aruba, Australia, Austria, Azerbaijan, Bahamas, The, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bermuda, Bhutan, Bolivia, Bosnia and Herzegovina, Botswana, Brazil, Brunei Darussalam, Bulgaria, Burkina Faso, Burundi, Cabo Verde, Cambodia, Cameroon, Canada, Cayman Islands, Central African Republic, Chad, Channel Islands, Chile, China, Colombia, Comoros, Congo, Dem. Rep., Congo, Rep., Costa Rica, Cote d'Ivoire, Croatia, Cuba, Curacao, Cyprus, Czech Republic, Denmark, Djibouti, Dominica, Dominican Republic, Ecuador, Egypt, Arab Rep., El Salvador, Equatorial Guinea, Eritrea, Estonia, Ethiopia, Faeroe Islands, Fiji, Finland, France, French Polynesia, Gabon, Gambia, The, Georgia, Germany, Ghana, Greece, Greenland, Grenada, Guam, Guatemala, Guinea, Guinea-Bissau, Guyana, Haiti, Honduras, Hong Kong SAR, China, Hungary, Iceland, India, Indonesia, Iran, Islamic Rep., Iraq, Ireland, Isle of Man, Israel, Italy, Jamaica, Japan, Jordan, Kazakhstan]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context In 2013 alone international migrants sent $413 billion home to families and friends. This money is known as ""remittance money"" and the total is more than three times that afforded by total global foreign aid ($135 billion). Remittances are traditionally associated with poor migrants moving outside of their home country to find work supporting their families back home on their foreign wages; as a result they make up a significant part of the economic picture for many developing countries in the world. This dataset published by the World Bank provides estimates of 2016 remittance movements between various countries. It also provides historical data on the flow of such money going back to 1970. For a look at how remittances play into the global economy watch ""The hidden force in global economics sending money home"". Content This dataset contains three files  bilateral-remittance.csv --- Estimated remittances between world countries in the year 2016. remittance-inflow.csv --- Historical remittance money inflow into world countries since 1970. Typically high in developing nations. remittance-outflow.csv --- Historical remittance money outflow from world countries since 1970. Typically high in more developed nations.  All monetary values are in terms of millions of US dollars. For more information on how this data was generated and calculated refer to the World Bank Remittance Data FAQ. Acknowledgements This dataset is a republished version of three of the tables published by the World Bank which has been slightly cleaned up for use on Kaggle. For the original source and other complimentary materials check out the dataset home page. Inspiration  What is the historical trend in remittance inflows and outflows for various countries? How does this relate to the developmental character of the countries in question? What countries send to most money abroad? What countries receive the most money from abroad? Try combining this dataset with a demographics dataset to see what countries are most and least reliant on income from abroad. How far do workers migrate for a job? Are they staying near home or going half the world away? Are there any surprising facts about who send money to who? ",CSV,,"[countries, demographics]",Other,,,197,1468,0.4912109375,Money sent home to family by workers abroad,Worldwide Economic Remittances,https://www.kaggle.com/theworldbank/worldwide-economic-remittances,Mon Nov 06 2017
,Rachael Tatman,[],[],Context The State of the Union is an annual address by the President of the United States before a joint session of congress. In it the President reviews the previous year and lays out his legislative agenda for the coming year. Content This dataset contains the full text of the State of the Union address from 1989 (Regan) to 2017 (Trump). Inspiration This is a nice clean set of texts perfect for exploring Natural Language Processing techniques   Topic modelling Which topics have become more popular over time? Which have become less popular? Sentiment analysis Are there differences in tone between different Presidents? Presidents from different parties? Parsing Can you train implement a parser to automatically extract the syntactic relationships between words? Authorship identification Can you correctly identify the author of a previously unseen address? ,Other,,"[presidents, politics, linguistics]",CC4,,,1244,2074,0.9716796875,Full text of the State of the  Union address between 1989 and 2017,State of the Union Corpus (1989 - 2017),https://www.kaggle.com/rtatman/state-of-the-union-corpus-1989-2017,Fri Jul 21 2017
,Mike Chirico,"[Dc_Dist, Psa, Dispatch_Date_Time, Dispatch_Date, Dispatch_Time, Hour, Dc_Key, Location_Block, UCR_General, Text_General_Code, Police_Districts, Month, Lon, Lat]","[numeric, numeric, string, dateTime, string, numeric, numeric, string, numeric, string, numeric, dateTime, numeric, numeric]",Crime Data for Philadelphia To get started quickly take a look at  Philly Data Crime Walk-through. Data was provided by OpenDataPhilly ,CSV,,[crime],Other,,,5159,42510,296,"Ten Years of Crime Data, by OpenDataPhilly",Philadelphia Crime Data,https://www.kaggle.com/mchirico/philadelphiacrimedata,Fri Mar 24 2017
,United States Air Force,"[GLOSSARY_ID, AIRCRAFT, FULL_NAME, WEBSITE_LINK, AIRCRAFT_TYPE]","[numeric, string, string, string, string]",Context THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data.  Each theater of warfare has a separate data file in addition to a THOR Overview. Content By June 1950 the U.S. Air Force had constructed a comprehensive historical program.  Over half the records in the Air Force Historical Archives consisted of World War II artifacts including unit histories and combat reports compiled by field historians as they received a steady flow of documents from operational squadrons and wings.  The archives team developed experience pouring through intelligence reports target folders bomb damage assessments and statistics to develop hard earned lessons on modern warfare.  So from the first day of combat 25 June historians embedded within operational commands in Korea knew recording events from the start would be important. In particular Albert F. Simpson the Archives' Director picked up the phone and directly called the headquarters of the Far East Air Forces (FEAF) to request they begin collecting data on all sorties generated in theater.  Their statistical services agreed and began regularly sending typed reports on 20 essential data items  Group and Squadron designations Operating base location Type and model of aircraft Aborted airborne and effective sorties Number of aircraft lost or damaged to enemy ground aircraft or other action Personnel Killed Wounded or Missing in Action Number of enemy aircraft destroyed or damaged Number of bombs rockets and bullets expended  Read more here on the Exteter database and consult the data dictionary here. Acknowledgements THOR is a dataset project initiated by  Lt Col Jenns Robertson and continued in partnership with Data.mil  an experimental project created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.  Inspiration  Which campaigns saw the heaviest bombings? Which months saw the most runs? ,CSV,,"[military, war]",CC0,,,74,889,4,Details on 12.8k Bombing Runs,Korean War Bombing Runs,https://www.kaggle.com/usaf/korean-war-bombing-runs,Thu Sep 14 2017
,PyTorch,[],[],InceptionV3  Rethinking the Inception Architecture for Computer Vision Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training) computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art 21.2% top-1 and 5.6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation we report 3.5% top-5 error on the validation set (3.6% error on the test set) and 17.3% top-1 error on the validation set.  Authors Christian Szegedy Vincent Vanhoucke Sergey Ioffe Jonathon Shlens Zbigniew Wojna https//arxiv.org/abs/1512.00567  InceptionV3 Architecture   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,8,345,96,InceptionV3 Pre-trained Model for PyTorch,InceptionV3,https://www.kaggle.com/pytorch/inceptionv3,Wed Dec 13 2017
,epattaro,"[bugged_date, receipt_date, deputy_id, political_party, state_code, deputy_name, receipt_social_security_number, receipt_description, establishment_name, receipt_value]","[numeric, dateTime, numeric, string, string, string, numeric, string, string, numeric]","Context Brazilian politicians are entitled to refunds if they spend any of their money on an activity that is enabling them to ""better serve the people"". Those expenses are public data however there is little monitoring/data analysis of it. A quick look at it shows a deputy with over 800 flights in one year. Another deputy has over 140.000R$ expenses on mailing (old fashion mail) in one year. There are a lot of very suspicious data regarding the deputies expending behavior. Can you help spot outliers and companies charging unusual amounts of money for a service? Content Data is public. It was collected from the official government website http//www2.camara.leg.br/transparencia/cota-para-exercicio-da-atividade-parlamentar/dados-abertos-cota-parlamentar It was converted from xml to csv filtered out irrelevant columns and translated a few of the features to English. The uploaded data contains u'deputy_name' u'political_party'  u'deputy_state' u'company_name' u'company_id' u'refund_date' Inspiration Brazil is currently passing through thriving times. Its political group has always been public involved in many scandals but it is just now that a few brave men and women have started doing something about it. In 2016 we have had senators former ministers and many others formally charged and arrested for their crimes.",CSV,,"[brazil, finance, politics]",CC0,,,468,5556,394,Refunds Spendings from 2009 to 2017,Brazil's House of Deputies Reimbursements,https://www.kaggle.com/epattaro/brazils-house-of-deputies-reimbursements,Sat Nov 04 2017
,Retailrocket,"[categoryid, parentid]","[numeric, numeric]","Context The dataset consists of three files a file with behaviour data (events.csv) a file with item properties (item_properties.сsv) and a file which describes category tree (category_tree.сsv). The data has been collected from a real-world ecommerce website. It is raw data i.e. without any content transformations however all values are hashed due to confidential issues. The purpose of publishing is to motivate researches in the field of recommender systems with implicit feedback. Content The behaviour data i.e. events like clicks add to carts transactions represent interactions that were collected over a period of 4.5 months. A visitor can make three types of events namely “view” “addtocart” or “transaction”. In total there are 2 756 101 events including 2 664 312 views 69 332 add to carts and 22 457 transactions produced by  1 407 580 unique visitors. For about 90% of events corresponding properties can be found in the “item_properties.csv” file.  For example  “14396940000001view100” means visitorId = 1 clicked the item with id = 100 at 1439694000000 (Unix timestamp) “14396940000002transaction1000234” means visitorId  = 2 purchased the item with id = 1000 in transaction with id = 234 at 1439694000000 (Unix timestamp)  The file with item properties (item_properties.csv) includes 20 275 902 rows i.e. different properties describing 417 053 unique items. File is divided into 2 files due to file size limitations. Since the property of an item can vary in time (e.g. price changes over time) every row in the file has corresponding timestamp. In other words the file consists of concatenated snapshots for every week in the file with the behaviour data. However if a property of an item is constant over the observed period only a single snapshot value will be present in the file. For example we have three properties for single item and 4 weekly snapshots like below timestampitemidpropertyvalue 143969400000011001000 143969500000011001000 143969600000011001000 143969700000011001000 143969400000012001000 143969500000012001100 143969600000012001200 143969700000012001300 143969400000013001000 143969500000013001000 143969600000013001100 143969700000013001100  After snapshot merge it would looks like 143969400000011001000 143969400000012001000 143969500000012001100 143969600000012001200 143969700000012001300 143969400000013001000 143969600000013001100  Because property=100 is constant over time property=200 has different values for all snapshots property=300 has been changed once. Item properties file contain timestamp column because all of them are time dependent since properties may change over time e.g. price category etc. Initially this file consisted of snapshots for every week in the events file and contained over 200 millions rows. We have merged consecutive constant  property values so it's changed from snapshot form to change log form. Thus  constant  values would appear only once in the file. This action has significantly reduced the number of rows in 10 times. All values in the “item_properties.csv” file excluding ""categoryid"" and ""available"" properties were hashed.  Value of the ""categoryid"" property contains item category identifier. Value of the ""available"" property contains availability of the item i.e. 1 means the item was available otherwise 0. All numerical values were marked with ""n"" char at the beginning and have 3 digits precision after decimal point e.g.  ""5"" will become ""n5.000"" ""-3.67584"" will become ""n-3.675"". All words in text values were normalized (stemming procedure https//en.wikipedia.org/wiki/Stemming) and hashed numbers were processed as above e.g. text ""Hello world 2017!"" will become ""24214 44214 n2017.000"" The category tree file has 1669 rows. Every row in the file specifies a child categoryId and the corresponding parent. For example  Line “100200” means that categoryid=1 has parent with categoryid=200 Line “300” means that categoryid hasn’t parent in the tree  Acknowledgements Retail Rocket (retailrocket.io) helps web shoppers make better shopping decisions by providing personalized real-time recommendations through multiple channels with over 100MM unique monthly users and 1000+ retail partners over the world. Inspiration  How to use item properties and category tree data to improve collaborative filtering model? Recurrent Neural Networks with Top-k Gains for Session-based Recommendations https//github.com/hidasib/GRU4Rec  and paper https//arxiv.org/abs/1706.03847 https//www.researchgate.net/publication/280538158_Application_of_Kullback-Leibler_divergence_for_short-term_user_interest_detection https//pdfs.semanticscholar.org/66dc/1724c4ed1e74fe6b22e636b52031a33c8ebe.pdf https//www.slideshare.net/LukasLerche/adaptation-and-evaluation-of-recommendationsfor-shortterm-shopping-goals   Adaptation and Evaluation of Recommendations for Short-term Shopping Goals  Tasks Task 1 When a customer comes to an e-commerce site he looks for a product with particular properties price range vendor product type and etc. These properties are implicit so it's hard to determine them through clicks log.  Try to create an algorithm which predicts properties of items in ""addtocart"" event by using data from ""view"" events for any visitor in the published log. Task 2 Description Process of analyzing ecommerce data include very important part of data cleaning. Researchers noticed that in some cases browsing data include up to 40% of abnormal traffic. Firstly abnormal users add a lot of noise into data and make recommendation system less effective. In order to increase efficiency of recommendation system abnormal users should be removed from the raw data. Secondly abnormal users add bias to results of split tests so this type of users should be removed also from split test data. Goals  The main goal is to find abnormal users of e-shop.  Subgoals  Generate features Build a model Create a metric that helps to evaluate quality of the model ",CSV,,"[business, internet]",CC4,,,2381,23410,942,"Ecommerce data: web events, item properties (with texts), category tree",Retailrocket recommender system dataset,https://www.kaggle.com/retailrocket/ecommerce-dataset,Fri Mar 24 2017
,gregv,[],[],"Cyrillic-oriented MNIST CoMNIST services A repository of images of hand-written Cyrillic and Latin alphabet letters for machine learning applications. The repository currently consists of 20000+ 278x278 png images representing all 33 letters of the Russian alphabet and the 26 letters of the English alphabet. Find original source on my github These images have been hand-written on touch screen through crowd-sourcing. The dataset will be regularly extended with more data as the collection progresses  An API that reads words in images CoMNIST also makes available a web service that reads drawing and identifies the word/letter you have drawn. On top of an image you can submit an expected word and get back the original image with mismtaches highlighted (for educational purposes) The API is available at this address http//35.187.34.55002/api/word It is accessible via a POST request with following input expected  {     'img' Mandatory b64 encoded image with letters in black on a white background     'word' Optional string the expected word to be read     'lang' Mandatory string either 'en' or 'ru' respectively for Latin or Cyrillic (russian) alphabets     'nb_output' Mandatory integer the ""tolerance"" of the engine }  The return information is the following  {     'img' b64 encoded image if a word was supplied as an input then modified version of that image highlighting mismatches     'word' string the word that was read by the API }  Participate The objective is to gather at least 1000 images of each class therefore your contribution is more that welcome! One minute of your time is enough and don't hesitate to ask your friends and family to participate as well. English version - Draw Latin only + common to cyrillic and latin French version - Draw Latin only + common to cyrillic and latin Russian version - Draw Cyrillic only Find out more about CoMNIST on my blog Credits and license A big thanks to all the contributors! These images have been crowd-sourced thanks to the great web-design by Anna Migushina available on her github. CoMNIST logo by Sophie Valenina",Other,,"[writing, russia, linguistics]",CC4,,,165,2253,105,Cyrillic-oriented MNIST: A dataset of Latin and Cyrillic letter images,CoMNIST,https://www.kaggle.com/gregvial/comnist,Mon Apr 10 2017
,PyTorch,[],[],VGG-19  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,1,180,508,VGG-19 Pre-trained Model for PyTorch,VGG-19,https://www.kaggle.com/pytorch/vgg19,Fri Dec 15 2017
,Dr. Rich,"[Employee Name, Employee Number, State, Zip, DOB, Age, Sex, MaritalDesc, CitizenDesc, Hispanic/Latino, RaceDesc, Date of Hire, Date of Termination, Reason For Term, Employment Status, Department, Position, Pay Rate, Manager Name, Employee Source, Performance Score]","[string, numeric, string, numeric, dateTime, numeric, string, string, string, string, string, dateTime, string, string, string, string, string, numeric, string, string, string]",Context HR data can be hard to come by and HR professionals generally lag behind with respect to analytics and data visualization competency. Thus Dr. Carla Patalano and I set out to create our own HR-related dataset which is used in one of our graduate MSHRM courses called HR Metrics and Analytics at New England College of Business. We created this data set ourselves. Content There are multiple worksheets within the Excel workbook. These include  Core data set Production staff Sales analysis Salaries Recruiting sources  The Excel workbook revolves around a fictitious company called Dental Magic and the core data set contains names DOBs age gender marital status date of hire reasons for termination department whether they are active or terminated position title pay rate manager name and performance score. Acknowledgements Dr. Carla Patalano provided many suggestions for creating this synthetic data set which has been used now by over 30 Human Resource Management students at the college. Students in the course learn data visualization techniques with Tableau Desktop and use this data set to complete a series of assignments. Inspiration  Is there any relationship between who a person works for and their performance score? What is the overall diversity profile of the organization? What are our best recruiting sources if we want to ensure a diverse organization?  There are so many other interesting questions that could be addressed through this interesting data set. Dr. Patalano and I look forward to seeing what we can come up with.,CSV,,"[employment, business]",CC4,,,2073,12939,0.171875,Dataset used for learning data visualization and basic regression,Human Resources Data Set,https://www.kaggle.com/rhuebner/human-resources-data-set,Thu Feb 15 2018
,Rachael Tatman,[],[],Context Word vectors also called word embeddings are a multi-dimensional representation of words based on which words are used in similar contexts. They can capture some elements of words’ meanings. For example documents which use a lot of words that are clustered together in a vector space representation are more likely to be on similar topics. Word vectors are very computationally intensive to train and the vectors themselves will vary based on the documents or corpora they are trained on. For these reasons it is often convenient to use word vectors which have been pre-trained rather than training them from scratch for each project. Content This dataset contains 1000653 word embeddings of dimension 300 trained on the Spanish Billion Words Corpus. These embeddings were trained using word2vec.  Parameters for Embeddings Training Word embeddings were trained using the following parameters  The selected algorithm was the skip-gram model with negative-sampling. The minimum word frequency was 5. The amount of “noise words” for the negative sampling was 20. The 273 most common words were downsampled. The dimension of the final word embedding was 300.  The original corpus had the following amount of data  A total of 1420665810 raw words. A total of 46925295 sentences. A total of 3817833 unique tokens.  After the skip-gram model was applied filtering of words with less than 5 occurrences as well as the downsample of the 273 most common words the following values were obtained  A total of 771508817 raw words. A total of 1000653 unique tokens.  Acknowledgements This dataset was created by Cristian Cardellino. If you use this dataset in your work please reference the following citation  Cristian Cardellino Spanish Billion Words Corpus and Embeddings (March 2016) http//crscardellino.me/SBWCE/ Inspiration Word vector representations are widely-used in natural language processing tasks.  Can you improve an existing part of speech tagger in Spanish by using word vectors? You might find it helpful to check out this paper. Can you use these word embeddings to improve existing parsers for Spanish? This paper outlines some approaches for this. There has been quite a bit of work recently on how word embeddings might encode implicit gender bias. For example this paper show how embeddings can capture stereyoptyes about career fields and gender. However this recent paper suggests that for languages with grammatical gender (like Spanish) grammatical gender is more influential than gender bias. Do these word embeddings support that claim?  You may also like  GloVe Global Vectors for Word Representation. Pre-trained English word vectors from Wikipedia 2014 + Gigaword 5 20 Million Word Spanish Corpus. The Spanish Language portion of the Wikicorpus (v 1.0) ,Other,,"[languages, linguistics, artificial intelligence]",CC4,,,125,1947,3072,Over 1 million 300-dimensional word vectors for Spanish,Pre-trained Word Vectors for Spanish,https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish,Wed Aug 09 2017
,Jose Berengueres,"[employee, companyAlias, numVotes, lastParticipationDate, stillExists]","[numeric, string, numeric, string, boolean]",  Can Happiness Predict Employee Turnover or is it the Other Way Around? It is the summer of 2016. I am in Barcelona and it is hot and humid. By chance I go to a talk where  Alex Rios - the ceo of myhappyforce.com explains his product. He has built an app where employees report daily happiness levels at work. This app is used by companies to track happiness of the workforce. After the talk I ask him if he would opensource the (anonymized data) so we can better understad the phenomenon of employee turnover. Here is what we did we developed a model that predicts which employees will churn. Then we looked at the features (used by the model) that are common to employees that churn. The top feautures of employees that churn are  low ratio of likes received (likeability) low posting frequency (engagement)  low relative happiness (employee happiness normalized by company mean).   Surprisingly a priori expected explanatory features such as mean happiness level and the ratio of likes (positivity) were not significant. Precision@50 = 80% out of a test set with 116 churns sample size N=2k. Another surprise was that raw happiness is a bad predictor of churn. But the question is What did we miss? Can you find more insights? Starter script R starter script https//www.kaggle.com/harriken/how-many-unlikes-it-takes-to-get-fired Content The data consists of four tables votes comments interactions and churn. A vote was obtained when an employee opened the app and answered the question How happy are you at work today? To vote the employee indicates their feeling by touching one of four icons that appeared on the screen. After the employee indicates their happiness level a second screen appears where they can input a text explanation (usually a complaint suggestion or comment) this is the comments table. Out of 4356 employees 2638 employees commented at least once. Finally in a third screen the employee can see their peers’ comments and like or dislike them this data is stored in the interactions table. 3516 employees liked or disliked at least one of their peers’ comments. The churn table contains when an employee churned (quit or was fired). Acknowledgements  Python script version with social graph features http//bit.ly/2v2sEZg More detailed R scripts https//github.com/orioli/e3 The paper which was presented at ASONAM 2017 Sydney Slides https//www.slideshare.net/harriken/ieee-happiness-an-inside-job-asoman-2017  Inspiration The cost of employee turnover has been pointed out extensively in the literature. A high turnover rate not only increases human resource costs which can reach up to 150% of the annual salary per replaced employee but it also has social costs as it is correlated with lower wages lower productivity per employee and not surprisingly a less loyal workforce 1. For reference in 2006 turnover at Walmart’s Sam’s Club was 44% with an average hourly pay of $10.11 while at Costco it was a much lower 17% with a higher $17.0 hourly wage 2. In addition a more recent study correlated companies with low turnover with a series of socially positive characteristics dubbed high-involvement work practices 3. On the other hand research on employee turnover (churn) is not a prolific topic in the engineering community. In IEEE publications one can find just over 278 publications with titles containing the keyword churn and the bulk of those focus on customer churn and specifically churn in the telecommunications industry while on the topic of employee churn there is just one title indexed 4. The goal is to clarify the characteristics of employees that will churn (or that are at risk of churning) to help companies understand the causes so they can reduce the turnover rate. ,CSV,,[economics],ODbL,,,931,8515,49,Is There a Relationship Between Employee Happiness and Job Turnover?,Daily Happiness & Employee Turnover,https://www.kaggle.com/harriken/employeeturnover,Mon Aug 07 2017
,US Census Bureau,"[minimum_age, maximum_age, gender, population, zipcode, geo_id]","[numeric, numeric, string, numeric, numeric, string]",Content The United States census count (also known as the Decennial Census of Population and Housing) is a count of every resident of the US. The census occurs every 10 years and is conducted by the United States Census Bureau. Census data is publicly available through the census website but much of the data is available in summarized data and graphs. The raw data is often difficult to obtain is typically divided by region and it must be processed and combined to provide information about the nation as a whole. The United States census dataset includes nationwide population counts from the 2000 and 2010 censuses. Data is broken out by gender age and location using zip code tabular areas (ZCTAs) and GEOIDs. ZCTAs are generalized representations of zip codes and often though not always are the same as the zip code for an area. GEOIDs are numeric codes that uniquely identify all administrative legal and statistical geographic areas for which the Census Bureau tabulates data. GEOIDs are useful for correlating census data with other censuses and surveys. Dataset Description | geo_id      | STRING  | Geo code                                                                                                                                                                       | |-------------|---------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------| | minimum_age | INTEGER | The minimum age in the age range. If null this indicates the row as a total for male female or overall population.                                                          | | maximum_age | INTEGER | The maximum age in the age range. If null this indicates the row as having no maximum (such as 85 and over) or the row is a total of the male female or overall population. | | gender      | STRING  | male or female. If empty the row is a total population summary.                                                                                                               | | population  | INTEGER | The total count of the population for this segment.                                                                                                                            | Acknowledgements This dataset was created by the United States Census Bureau. Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too https//cloud.google.com/bigquery/public-data/international-census.,CSV,,[demographics],Other,,,851,7960,112,For both 2000 and 2010,US Population By Zip Code,https://www.kaggle.com/census/us-population-by-zip-code,Tue Jun 27 2017
,sdorius,"[unid, wbid, country, year, ses, class, gdppc, yrseduc, region5, regionUN]","[numeric, string, string, numeric, numeric, string, numeric, numeric, string, string]","This dataset contains estimates of the socioeconomic status (SES) position of each of 149 countries covering the period 1880-2010. Measures of SES which are in decades allow for a 130 year time-series analysis of the changing position of countries in the global status hierarchy. SES scores are the average of each country’s income and education ranking and are reported as percentile rankings ranging from 1-99. As such they can be interpreted similarly to other percentile rankings such has high school standardized test scores. If country A has an SES score of 55 for example it indicates that 55 percent of the countries in this dataset have a lower average income and education ranking than country A. ISO alpha and numeric country codes are included to allow users to merge these data with other variables such as those found in the World Bank’s World Development Indicators Database and the United Nations Common Database. See here for a working example of how the data might be used to better understand how the world came to look the way it does at least in terms of status position of countries.  VARIABLE DESCRIPTIONS  unid ISO numeric country code (used by the United Nations)  wbid ISO alpha country code (used by the World Bank)  SES Country socioeconomic status score (percentile) based on GDP per capita and educational attainment (n=174)  country Short country name  year Survey year  gdppc GDP per capita Single time-series (imputed)  yrseduc Completed years of education in the adult (15+) population  region5 Five category regional coding schema regionUN United Nations regional coding schema DATA SOURCES  The dataset was compiled by Shawn Dorius (sdorius@iastate.edu) from a large number of data sources listed below. GDP per Capita   Maddison Angus. 2004. 'The World Economy Historical Statistics'. Organization for Economic Co-operation and Development Paris. GDP & GDP per capita data in (1990 Geary-Khamis dollars PPPs of currencies and average prices of commodities). Maddison data collected from http//www.ggdc.net/MADDISON/Historical_Statistics/horizontal-file_02-2010.xls.  World Development Indicators Database Years of Education 1. Morrisson and Murtin.2009. 'The Century of Education'. Journal of Human Capital(3)11-42. Data downloaded from http//www.fabricemurtin.com/ 2. Cohen Daniel & Marcelo Cohen. 2007. 'Growth and human capital Good data good results' Journal of economic growth 12(1)51-76. Data downloaded from http//soto.iae-csic.org/Data.htm  Barro Robert and Jong-Wha Lee 2013 ""A New Data Set of Educational Attainment in the World 1950-2010."" Journal of Development Economics vol 104 pp.184-198. Data downloaded from http//www.barrolee.com/  Maddison Angus. 2004. 'The World Economy Historical Statistics'. Organization for Economic Co-operation and Development Paris. 13.  United Nations Population Division. 2009. ",CSV,,"[demographics, economics, sociology]",ODbL,,,205,1862,0.087890625,Country-weighted measures of SES,"Country Socioeconomic Status Scores, Part II",https://www.kaggle.com/sdorius/countryses,Sat Jul 15 2017
,Mohamed Loey,[],[],Arabic Handwritten Characters Dataset Astract Handwritten Arabic character recognition systems face several challenges including the unlimited variation in human handwriting and large public databases. In this work we model a deep learning architecture that can be effectively apply to recognizing Arabic handwritten characters. A Convolutional Neural Network (CNN) is a special type of feed-forward multilayer trained in supervised mode. The CNN trained and tested our database that contain 16800 of handwritten Arabic characters. In this paper the optimization methods implemented to increase the performance of CNN. Common machine learning methods usually apply a combination of feature extractor and trainable classifier. The use of CNN leads to significant improvements across different machine-learning classification algorithms. Our proposed CNN is giving an average 5.1% misclassification error on testing data. Context The motivation of this study is to use cross knowledge learned from multiple works to enhancement the performance of Arabic handwritten character recognition. In recent years Arabic handwritten characters recognition with different handwriting styles as well making it important to find and work on a new and advanced solution for handwriting recognition. A deep learning systems needs a huge number of data (images) to be able to make a good decisions. Content The data-set is composed of 16800 characters written by 60 participants the age range is between 19 to 40 years and 90% of participants are right-hand. Each participant wrote each character (from ’alef’ to ’yeh’) ten times on two forms as shown in Fig. 7(a) & 7(b). The forms were scanned at the resolution of 300 dpi. Each block is segmented automatically using Matlab 2016a to determining the coordinates for each block. The database is partitioned into two sets a training set (13440 characters to 480 images per class) and a test set (3360 characters to 120 images per class). Writers of training set and test set are exclusive. Ordering of including writers to test set are randomized to make sure that writers of test set are not from a single institution (to ensure variability of the test set). In an experimental section we showed that the results were promising with 94.9% classification accuracy rate on testing images. In future work we plan to work on improving the performance of handwritten Arabic character recognition. Acknowledgements Ahmed El-Sawy Mohamed Loey Hazem EL-Bakry Arabic Handwritten Characters Recognition using Convolutional Neural Network WSEAS 2017 Our proposed CNN is giving an average 5.1% misclassification error on testing data. Inspiration Creating the proposed database presents more challenges because it deals with many issues such as style of writing thickness dots number and position. Some characters have different shapes while written in the same position. For example the teh character has different shapes in isolated position. Benha University http//bu.edu.eg/staff/mloey https//mloey.github.io/,CSV,,[],ODbL,,,491,4231,73,Arabic Handwritten Characters Data-set,Arabic Handwritten Characters Dataset,https://www.kaggle.com/mloey1/ahcd1,Fri Jun 23 2017
,Quora,"[id, qid1, qid2, question1, question2, is_duplicate]","[numeric, numeric, numeric, string, string, numeric]",Context Quora's first public dataset is related to the problem of identifying duplicate questions. At Quora an important product principle is that there should be a single question page for each logically distinct question. For example the queries “What is the most populous state in the USA?” and “Which state in the United States has the most people?” should not exist separately on Quora because the intent behind both is identical. Having a canonical page for each logically distinct query makes knowledge-sharing more efficient in many ways for example knowledge seekers can access all the answers to a question in a single location and writers can reach a larger readership than if that audience was divided amongst several pages. The dataset is based on actual data from Quora and will give anyone the opportunity to train and test models of semantic equivalence. Content There are over 400000 lines of potential question duplicate pairs. Each line contains IDs for each question in the pair the full text for each question and a binary value that indicates whether the line truly contains a duplicate pair. Acknowledgements For more information on this dataset check out Quora's first dataset release page. License This data is subject to Quora's Terms of Service allowing for non-commercial use.,CSV,,"[languages, linguistics, artificial intelligence]",Other,,,1475,18291,58,Can you identify duplicate questions?,Question Pairs Dataset,https://www.kaggle.com/quora/question-pairs-dataset,Thu Feb 02 2017
,PyTorch,[],[],VGG-11  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,1,169,471,VGG-11 Pre-trained model with batch normalization for PyTorch,VGG-11 with batch normalization,https://www.kaggle.com/pytorch/vgg11bn,Sat Dec 16 2017
,City of Los Angeles,"[Row ID, Year, Department Title, Payroll Department, Record Number, Job Class Title, Employment Type, Hourly or Event Rate, Projected Annual Salary, Q1 Payments, Q2 Payments, Q3 Payments, Q4 Payments, Payments Over Base Pay, % Over Base Pay, Total Payments, Base Pay, Permanent Bonus Pay, Longevity Bonus Pay, Temporary Bonus Pay, Lump Sum Pay, Overtime Pay, Other Pay & Adjustments, Other Pay (Payroll Explorer), MOU, MOU Title, FMS Department, Job Class, Pay Grade, Average Health Cost, Average Dental Cost, Average Basic Life, Average Benefit Cost, Benefits Plan, Job Class Link]","[numeric, numeric, string, numeric, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string]",Context The Los Angeles City Controller Office releases payroll information for all city employees on a quarterly basis since 2013. Content Data includes department titles job titles projected annual salaries (with breakdowns of quarterly pay) bonuses and benefits information.  Inspiration  How do benefits and salaries differ for employees across departments and titles? Are there any unusually large differences between lowest and highest employee salaries? How have salaries changed over the past three years? Have the costs of benefits changed dramatically since the passing of the Affordable Care Act? What is the most common government role in Los Angeles? ,CSV,,"[cities, income]",Other,,,568,4303,89,Payroll information for all Los Angeles city departments since 2013,City Payroll Data,https://www.kaggle.com/cityofLA/city-payroll-data,Sun Nov 27 2016
,mrpantherson,"[rank, bgg_url, game_id, names, min_players, max_players, avg_time, min_time, max_time, year, avg_rating, geek_rating, num_votes, image_url, age, mechanic, owned, category, designer, weight]","[numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, numeric]",Context Being a fan of board games I wanted to see if there was any correlation with a games rating and any particular quality the first step was to collect of this data. Content The data was collected in March of 2017 from the website https//boardgamegeek.com/ this site has an API to retrieve game information (though sadly XML not JSON). Acknowledgements Mainly I want to thank the people who run the board game geek website for maintaining such a great resource for those of us in the hobby. Inspiration I wish I had some better questions to ask of the data perhaps somebody else can think of some good ways to get some insight of this dataset.,CSV,,[board games],CC0,,,1199,11098,0.798828125,Data is a collection of board game information from Board Game Geek,Board Game Data,https://www.kaggle.com/mrpantherson/board-game-data,Fri Feb 16 2018
,PyTorch,[],[],VGG-16  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,4,259,490,VGG-16 Pre-trained Model for PyTorch,VGG-16,https://www.kaggle.com/pytorch/vgg16,Fri Dec 15 2017
,Jacob Boysen,"[address_qualifier, borough_code, borough_name, cal_year, date_of_call, easting_m, easting_rounded, first_pump_arriving_attendance_time, first_pump_arriving_deployed_from_station, frs, hour_of_call, incident_group, incident_number, incident_station_ground, northing_m, northing_rounded, num_pumps_attending, num_stations_with_pumps_attending, postcode_district, postcode_full, proper_case, property_category, property_type, second_pump_arriving_attendance_time, second_pump_arriving_deployed_from_station, special_service_type, stop_code_description, time_of_call, timestamp_of_call, ward_code, ward_name, ward_name_new]","[string, string, string, numeric, dateTime, numeric, numeric, numeric, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, dateTime, dateTime, string, string, string]",Context London's fire and rescue service is the busiest in England and one of the largest firefighting and rescue organisations in the world. In the aftermath of the Grenfell Tower fire it is critical that firefighting resources are accurately and appropriately deployed. Content This data covers Jan 01-April 30 2017 consisting of 32 columns containing information on time type and address of call as well the home station stay duration and arrival time of attending pumps. Acknowledgements This dataset was compiled by the City of London. You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data on BigQuery too. Inspiration  Which boroughs have the shortest average call response? Longest? Which boroughs have the greatest volume of calls? ,CSV,,"[government agencies, government]",CC0,,,146,1384,11,32k Calls to London Fire Brigade,London Fire Brigade Calls,https://www.kaggle.com/jboysen/london-fire,Fri Sep 01 2017
,KendallGillies,"[Age, Birth Place, Birthday, College, Current Status, Current Team, Experience, Height (inches), High School, High School Location, Name, Number, Player Id, Position, Weight (lbs), Years Played]","[numeric, string, dateTime, string, string, string, string, numeric, string, string, string, numeric, string, string, numeric, string]",NFL-Statistics-Scrape Here are the basic statistics career statistics and game logs provided by the NFL on their website (http//www.nfl.com) for all players past and present.  Summary The data was scraped using a Python code.  The code can be located at Github https//github.com/kendallgillies/NFL-Statistics-Scrape Explanation of Data  The first main group of statistics is the basic statistics provided for each player.  This data is stored in the CSV file titled Basic_Stats.csv along with the player’s name and URL identifier.  If available the data pulled for each player is as follows Number Position Current Team Height Weight Age Birthday Birth Place College Attended High School Attended High School Location Experience The second main group of statistics gathered for each player are their career statistics.  While each player has a main position they play they will have statistics in other areas; therefore the career statistics are divided into statistics types.  The statistics are then stored in CSV files based on statistic type along with the player name URL identifier and position (if available).  The following are the career statistics types and accompanying CSV file names Defensive Statistics – Career_Stats_Defensive.csv Field Goal Kickers - Career_Stats_Field_Goal_Kickers.csv Fumbles - Career_Stats_Fumbles.csv Kick Return - Career_Stats_Kick_Return.csv Kickoff - Career_Stats_Kickoff.csv Offensive Line - Career_Stats_Offensive_Line.csv Passing - Career_Stats_Passing.csv Punt Return - Career_Stats_Punt_Return.csv Punting - Career_Stats_Punting.csv Receiving - Career_Stats_Receiving.csv Rushing - Career_Stats_Rushing.csv The final group of statistics is the game logs for each player.  The game logs are stored by position and have the player name URL identifier and position (if available).  The following are the game log types and accompanying CSV file names Quarterback – Game_Logs_Quarterback.csv Running back – Game_Logs_Runningback.csv Wide Receiver and Tight End – Game_Logs_Wide_Receiver_and_Tight_End.csv Offensive Line – Game_Logs_Offensive_Line.csv Defensive Lineman – Game_Logs_Defensive_Lineman.csv Kickers – Game_Logs_Kickers.csv Punters – Game_Logs_Punters.csv  Glossary While most of the abbreviations used by the NFL have been translated in the table headers in the data files there are still a couple of abbreviations used.  FG Field Goal TD Touchdown Int Interception ,CSV,,[american football],Other,,,1597,10738,93,"Basic NFL statistics, career statistics, and game logs",NFL Statistics,https://www.kaggle.com/kendallgillies/nflstatistics,Fri Jun 09 2017
,DataSF,"[CaseID, Opened, Closed, Updated, Status, Status Notes, Responsible Agency, Category, Request Type, Request Details, Address, Supervisor District, Neighborhood, Point, Source, Media URL]","[numeric, dateTime, dateTime, dateTime, string, string, string, string, string, string, string, numeric, string, string, string, string]",Context This San Francisco 311 dataset contains all 311 cases created since 7/1/2008 (~2M).  SF311 is a way for citizens to obtain information report problems or submit service requests to the City and County of San Francisco.   Potential question(s) to get started with!  What are some effective visualizations for conveying 311 incidences and trends? How do 311 requests vary by neighborhood? or source? Over time or seasonally? What attributes have the greatest effect on how long it takes a case to close? Is there a way to identify duplicative reports (when multiple people create a 311 report for the same incidence)?  Fields Please see DataSF's 311 Case Data FAQ here  CaseID - (Numeric) - The unique ID of the service request created. Opened - (Timestamp) - The date and time when the service request was made Closed - (Timestamp) - The date and time when the service request was closed Updated - (Timestamp) - The date and time when the service request was last modified. For requests with status=closed this will be the date the request was closed Status - (Text) - The current status of the service request. Status Notes - (Text) - Explanation of why status was changed to current state or more details on current status than conveyed with status alone Responsible Agency - (Text) - The agency responsible for fulfilling or otherwise addressing the service request. Category - (Text) - The Human readable name of the specific service request type Request Type - (Text) - More specific description of the problem related to the Category Request Details - (Text) - More specific description of the problem related to the Request Type Address - (Text) - Human readable address or description of location Supervisor District - (Numeric) - Supervisor District Neighborhood - (Text) - Neighborhood Point - (Geometry Point) - latitude and longitude using the (WGS84) projection. Source - (Text) - How the service request was made Media URL - (Text) - Url to media  We have included the following commonly used geographic shapefile(s)  Supervisor Districts as of April 2012 Neighborhoods  Acknowledgements Data provided by SF311 via the San Francisco Open Data Portal at https//data.sfgov.org/d/vw6y-z8j6 PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL) Photo via Flickr Jeremy Brooks (CC BY-NC 2.0),CSV,,[crime],Other,,,131,1901,636,SF311 cases created since 7/1/2008 with location data,Case Data from San Francisco 311,https://www.kaggle.com/datasf/case-data-from-san-francisco-311,Sat Jan 14 2017
,Chris Crawford,"[nr, i1_legid, i1_rcs_p, i1_rcs_e, i1_dep_1_p, i1_dep_1_e, i1_dep_1_place, i1_rcf_1_p, i1_rcf_1_e, i1_rcf_1_place, i1_dep_2_p, i1_dep_2_e, i1_dep_2_place, i1_rcf_2_p, i1_rcf_2_e, i1_rcf_2_place, i1_dep_3_p, i1_dep_3_e, i1_dep_3_place, i1_rcf_3_p, i1_rcf_3_e, i1_rcf_3_place, i1_dlv_p, i1_dlv_e, i1_hops, i2_legid, i2_rcs_p, i2_rcs_e, i2_dep_1_p, i2_dep_1_e, i2_dep_1_place, i2_rcf_1_p, i2_rcf_1_e, i2_rcf_1_place, i2_dep_2_p, i2_dep_2_e, i2_dep_2_place, i2_rcf_2_p, i2_rcf_2_e, i2_rcf_2_place, i2_dep_3_p, i2_dep_3_e, i2_dep_3_place, i2_rcf_3_p, i2_rcf_3_e, i2_rcf_3_place, i2_dlv_p, i2_dlv_e, i2_hops, i3_legid, i3_rcs_p, i3_rcs_e, i3_dep_1_p, i3_dep_1_e, i3_dep_1_place, i3_rcf_1_p, i3_rcf_1_e, i3_rcf_1_place, i3_dep_2_p, i3_dep_2_e, i3_dep_2_place, i3_rcf_2_p, i3_rcf_2_e, i3_rcf_2_place, i3_dep_3_p, i3_dep_3_e, i3_dep_3_place, i3_rcf_3_p, i3_rcf_3_e, i3_rcf_3_place, i3_dlv_p, i3_dlv_e, i3_hops, o_legid, o_rcs_p, o_rcs_e, o_dep_1_p, o_dep_1_e, o_dep_1_place, o_rcf_1_p, o_rcf_1_e, o_rcf_1_place, o_dep_2_p, o_dep_2_e, o_dep_2_place, o_rcf_2_p, o_rcf_2_e, o_rcf_2_place, o_dep_3_p, o_dep_3_e, o_dep_3_place, o_rcf_3_p, o_rcf_3_e, o_rcf_3_place, o_dlv_p, o_dlv_e, o_hops, legs]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric]",Transport and Logistics Case Study Data Set (Cargo 2000) Cargo 2000 is an initiative of IATA the International Air Transport Association (Cargo 2000 has been re-branded as Cargo iQ in 2016). It aims at delivering a new quality management system for the air cargo industry. Cargo 2000 allows for unprecedented transparency in the supply chain. Stakeholders involved in the transport process can share agreed Cargo 2000 messages comprising transport planning replanning and service completion events. Cargo 2000 is based on the following key principles (1) Every shipment gets a plan (called a route map) describing predefined monitoring events. (2) Every service used during shipment is assigned a predefined milestone with a planned time of achievement. (3) Stakeholders receive alerts when a milestone has failed and notifications upon milestone completion which include the effective time the milestone has been achieved. Content The case study data comprises tracking and tracing events from a forwarding company’s Cargo 2000 system for a period of five months. From those Cargo 2000 messages we reconstructed execution traces of 3942 actual business process instances comprising 7932 transport legs and 56082 service invocations. Each execution trace includes planned and effective durations (in minutes) for each of the services of the business process (introduced in Section II) as well as airport codes for the DEP (“departure”) and RCF (“arrival”) services. Due to the fact that handling of transport documents along the business process differs based on whether the documents are paper-based or electronic we focus on the flow of physical goods as our data set did not allow us to discern the different document types. The reconstruction process involved data sanitation and anonymization. We filtered overlapping and incomplete Cargo 2000 messages removed canceled transports (i.e. deleted route maps) sanitized for exceptions from the C2K system (such as events occurring before route map creation) and homogenized the way information was represented in different message types. Finally due to confidentiality reasons message fields which might exhibit business critical or customer-related data (such as airway bill numbers flight numbers and airport codes) have been eliminated or masked.  Each of the transport legs involves the following physical transport services • RCS Check in freight at departure airline. Shipment is checked in and a receipt is produced at departure airport. • DEP Confirm goods on board. Aircraft has departed with shipment on board. • RCF Accept freight at arrival airline. Shipment is checked in according to the documents and stored at arrival warehouse. • DLV Deliver freight. Receipt of shipment was signed at destination airport. Acknowledgements A. Metzger P. Leitner D. Ivanovic E. Schmieders R. Franklin M. Carro S. Dustdar and K. Pohl “ Comparing and combining predictive business process monitoring techniques” IEEE Trans. on Systems Man Cybernetics Systems 2015. A. Metzger R. Franklin and Y. Engel “ Predictive monitoring of heterogeneous service-oriented business networks The transport and logistics case” in Service Research and Innovation Institute Global Conference (SRII 2012) ser. Conference Publishing Service (CPS) R. Badinelli F. Bodendorf S. Towers S. Singhal and M. Gupta Eds. IEEE Computer Society 2012. Z. Feldmann F. Fournier R. Franklin and A. Metzger “Industry article Proactive event processing in action A case study on the proactive management of transport processes” in Proceedings of the Seventh ACM International Conference on Distributed Event-Based Systems DEBS 2013 Arlington Texas USA S. Chakravarthy S. Urban P. Pietzuch E. Rundensteiner and S. Dietrich Eds. ACM 2013.,Other,,"[road transport, shipping]",CC0,,,93,734,0.31640625,A Transport and Logistics Case Study Data Set ,Cargo 2000 Dataset,https://www.kaggle.com/crawford/cargo-2000-dataset,Thu Feb 08 2018
,def love(x):,"[Year, State, CoC Number, CoC Name, Measures, Count]","[dateTime, string, string, string, string, numeric]","Context The previous New York City policies eliminated all housing resources for homeless families and single adults. I wanted to see the consequences. Content ""These raw data sets contain Point-in-Time (PIT) estimates and national PIT estimates of homelessness as well as national estimates of homelessness by state and estimates of chronic homelessness from 2007 - 2016. Estimates of homeless veterans are also included beginning in 2011. The accompanying Housing Inventory Count (HIC) data is available as well from 2007 - 2016."" (Department of Housing and Urban Development Acknowledgements I would like to thank Matthew Schnars for providing this dataset from  https//www.hudexchange.info/resource/3031/pit-and-hic-data-since-2007/ Inspiration Many of our fellow human beings go through hardships that we would never know about. But it's our obligation as a society to take care of one another. That's why I was hoping this dataset shine light on some of the challenges our cities and states are still facing in this topic.",CSV,,"[united states, sociology]",CC0,,,621,4984,7,"Homelessness in the United States, 2007 to 2016",Homelessness,https://www.kaggle.com/adamschroeder/homelessness,Mon Aug 07 2017
,Keras,[],[],Xception  Xception Deep Learning with Depthwise Separable Convolutions We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture dubbed Xception slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for) and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17000 classes. Since the Xception architecture has the same number of parameters as Inception V3 the performance gains are not due to increased capacity but rather to a more efficient use of model parameters. Author François Chollet https//arxiv.org/abs/1610.02357    What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,24,618,155,Xception Pre-trained Model for Keras,Xception,https://www.kaggle.com/keras/xception,Tue Dec 12 2017
,Federal Election Commission,"[committee_id, committee_name, report_year, report_type, image_number, line_number, file_number, payee_name, payee_first_name, payee_middle_name, payee_last_name, payee_street_1, payee_street_2, payee_city, payee_state, payee_zip, expenditure_description, expenditure_date, dissemination_date, expenditure_amount, office_total_ytd, category_code, category_code_full, support_oppose_indicator, memo_code, memo_code_full, candidate_id, candidate_name, candidate_prefix, candidate_first_name, candidate_middle_name, candidate_last_name, candidate_suffix, candidate_office, cand_office_state, cand_office_district, conduit_committee_id, conduit_committee_name, conduit_committee_street1, conduit_committee_street2, conduit_committee_city, conduit_committee_state, conduit_committee_zip, election_type, election_type_full, independent_sign_name, independent_sign_date, notary_sign_name, notary_sign_date, notary_commission_expiration_date, back_reference_transaction_id, back_reference_schedule_name, filer_first_name, filer_middle_name, filer_last_name, transaction_id, original_sub_id, action_code, action_code_full, schedule_type_full, filing_form, link_id, sub_id, payee_prefix, payee_suffix, is_notice, memo_text, filer_prefix, filer_suffix, schedule_type, pdf_url]","[string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, string, string, numeric, string, numeric, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string]","What is an Independent Expenditure? Independent expenditures are what some refer to as ""hard money"" in politics -- spending on ads that specifically mention a candidate (either supporting or opposing). The money for these ads must come from PACs that are independent of the candidate and campaign and the PACs cannot coordinate with the candidate. The Federal Election Commission (FEC) collects information on independent expenditures to ensure payers' independence from candidates. What can we look at? I'm super interested to see how much spending has increased over the years. The FEC data only goes back to 2004 and it may be the case that the older data is spotty but I don't doubt that political spending has gone up in the past few years (the 2016 Presidential campaign reportedly involved the most political money since the 1970s). What does the data look like? This dataset includes a ton of information from the independent expenditure reports  committee_id  unique id of the PAC that made the payment committee_name  name of the PAC that made the payment report_year  the year the report was file report_type  one of 24 or 48; whether this is a 24-hour report or a 48-hour report image_number  unique id of the scanned image of the report line_number  line number in the report file_number  unique id of the report payee_name  who got paid payee_first_name  if an individual payee their first name payee_middle_name  if an individual payee their middle name payee_last_name  if an individual payee their last name payee_street_1  payee street address (1 of 2) payee_street_2  payee street address (2 of 2) payee_city  payee city payee_state  payee state payee_zip  payee ZIP code expenditure_description  a string describing the expenditure expenditure_date  when was this expenditure made? dissemination_date  when was the advertisement disseminated? expenditure_amount  how much was spent? office_total_ytd  how much has this PAC spent on this office year-to-date? category_code  category of the expenditure (need to find categories!) category_code_full  category of the expenditure (need to find categories!) support_oppose_indicator  one of S or O; whether the ad is in support of or opposition to the candidate memo_code  memo_code_full  candidate_id  unique id of the candidate candidate_name  name of the candidate candidate_prefix  title or prefix of the candidate candidate_first_name  first name of the candidate candidate_middle_name  middle name of the candidate candidate_last_name  last name of the candidate candidate_suffix  suffix of the candidate's name candidate_office  office that the candidate is running for -- one of P (President) S (Senate) or H (House) cand_office_state  if House or Senate race in what state? cand_office_district  if House or Senate race in what district? conduit_committee_id  conduit_committee_name  conduit_committee_street1  conduit_committee_street2  conduit_committee_city  conduit_committee_state  conduit_committee_zip  election_type  one of P (primary) or G (general) election_type_full  an id comprising the election type and the year with no delimiter independent_sign_name  independent_sign_date  notary_sign_name  notary_sign_date  notary_commission_expiration_date  back_reference_transaction_id  back_reference_schedule_name  filer_first_name   filer_middle_name  filer_last_name  transaction_id  unique id identifying the transaction original_sub_id  action_code  action_code_full  schedule_type_full  filing_form  link_id  sub_id  payee_prefix  payee_suffix  is_notice  memo_text  filer_prefix  filer_suffix  schedule_type  pdf_url  link to the scanned form ",CSV,,"[finance, politics]",CC0,,,219,2592,172,Spending on political ads by independent (non-candidate) groups,Independent Political Ad Spending (2004-2016),https://www.kaggle.com/fec/independent-political-ad-spending,Tue Nov 01 2016
,Food and Drug Administration,[],[],The objective of FDA regulatory programs is to assure compliance with the Federal Food Drug and Cosmetic Act (the Act). Specific enforcement activities include actions to correct and prevent violations remove violative products or goods from the market and punish offenders. The type of enforcement activity FDA uses will depend on the nature of the violation. The range of enforcement activities include issuing a letter notifying the individual or firm of a violation and requesting correction to criminal prosecution of the individual or firm. Adulteration or misbranding is usually the result of an individual failing to take steps to assure compliance with the law. Such an individual may be liable for a violation of the Act and if found guilty be subject to the penalties specified by the law. Acknowledgements This dataset was kindly made available by the United States Food and Drug Administration. You can find the most current version of the dataset here. Inspiration  All but two out of every thousand drug enforcement actions were voluntary recalls. Does this hold for food and medical devices as well? Was there anything special about the non-voluntary enforcement actions that leads the industry to largely self-police? ,{}JSON,,[public health],CC0,,,103,1264,1024,"Food, drug, and medical device enforcements",FDA Enforcement Actions,https://www.kaggle.com/fda/fda-enforcement-actions,Tue Sep 12 2017
,PromptCloud,"[amtsave, brand, breadcrumbs, country, desc, discount, domain, gallery, image, insertedon, list_price, model, name, other_sellers, payment_methods_supported, productcode, selling_price, specifications, type, uniq_id, url, weight]","[numeric, string, string, string, string, string, string, string, string, dateTime, numeric, numeric, string, string, string, numeric, numeric, string, string, string, string, numeric]",Context This is a pre-crawled dataset taken as subset of a bigger dataset (more than 16000 books) that was created by extracting data from paytm.com a leading eCommerce store in India. Content This dataset has following fields  amtsave brand breadcrumbs country desc discount domain gallery image insertedon list_price model name other_sellers payment_methods_supported productcode selling_price specifications type uniq_id url weight  Acknowledgements This dataset was created by PromptCloud's in-house web-crawling service. Inspiration Analyses of pricing discount specifications and authors can be performed.,CSV,,"[books, internet]",CC4,,,274,2327,2, 1500 bestseller books on Paytm, Bestseller books on Paytm,https://www.kaggle.com/PromptCloudHQ/bestseller-books-on-paytm,Sat Sep 16 2017
,PyTorch,[],[],AlexNet  ImageNet Classification with Deep Convolutional Neural Networks We trained a large deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network which has 60 million parameters and 650000 neurons consists of five convolutional layers some of which are followed by max-pooling layers and three fully-connected layers with a final 1000-way softmax. To make training faster we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3% compared to 26.2% achieved by the second-best entry. Authors Alex Krizhevsky Ilya Sutskever Geoffrey E. Hinton https//papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks   Top of the image is cut-off even in the original paper D  What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,40,734,216,AlexNet Pre-trained Model for PyTorch,AlexNet,https://www.kaggle.com/pytorch/alexnet,Wed Dec 13 2017
,Golden Oak Research Group,[],[],Background The underlying concept behind hedging strategies is simple create a model and make money doing it. The hardest part is finding the features that matter. For a more in-depth look at hedging strategies I have attached one of my graduate papers to get you started.  Mortgage-Backed Securities Geographic Business Investment Real Estate Analysis  For any questions you may reach us at  research_development@goldenoakresearch.com. For immediate assistance you may reach me on at 585-626-2965. Please Note the number is my personal number and email is preferred Statistical Fields Note All interpolated statistical data include Mean Median and Standard Deviation Statistics. For more information view the variable definitions document.  Monthly Mortgage & Owner Costs Sum of mortgage payments home equity loans utilities property taxes Monthly Owner Costs Sum of utilities property taxes Gross Rent contract rent plus the estimated average monthly cost of utilities Household Income sum of the householder and all other individuals +15 years who reside in the household Family Income Sum of incomes of all members +15 years of age related to the householder.  Location Fields Note The location fields were derived from a variety of sources. The zip code area code and city were derived using a heuristic. All other locations used the census Geo ID to cross reference data between multiple datasets.  State Name Abbreviation Number reported by the U.S. Census Bureau County Name reported by the U.S. Census Bureau Location Type Specifies Classification of location {City Village Town CPD  ... etc.} Area Code Defined via heuristic.  Zip Code Defined via heuristic.  City Defined via heuristic.   Access All 325258 Location of Our Most Complete Database Ever Monetize Risk and Optimize your portfolio instantly at an unbeatable price. Don't settle. Go big and win big.  Access all gross rent records and more on a scale roughly equivalent to a neighborhood see link below  Full Dataset View Full Dataset Real Estate Research  View Research ,Other,,"[finance, demographics]",Other,,,609,4838,2,+30 Features To Build Models & Make Money,US ACS Financial Hedging Features,https://www.kaggle.com/goldenoakresearch/us-acs-mortgage-equity-loans-rent-statistics,Tue Feb 06 2018
,Rohk,"[publish_date, headline_text]","[dateTime, string]",Context This contains data of news headlines published over a period of 15 years. From the reputable Australian news source ABC (Australian Broadcasting Corp.) Site http//www.abc.net.au/ Prepared by Rohit Kulkarni Content Format CSV Rows 1103665 Column 1 publish_date (yyyyMMdd format) Column 2 headline_text (ascii lowercase) Start Date 2003-02-19 End Date 2017-12-31 Acknowledgements Special thanks to the java jsoup library. This dataset is free to use with citation Rohit Kulkarni (2017) A Million News Headlines [CSV Data file] doi10.7910/DVN/SYBGZL Retrieved from [this url] Inspiration I look at this news dataset as a summarised historical record of noteworthy events in the globe from early-2003 to end-2017 with a more granular focus on Australia. This includes the entire corpus of articles published by the ABC website in the given time range.  With a volume of 200 articles per day and a good focus on international news we can be fairly certain that every event of significance has been captured here. Digging into the keywords one can see all the important episodes shaping the last decade and how they evolved over time. Ex financial crisis iraq war multiple US elections ecological disasters terrorism famous people Australian crimes  etc. Similar Work Your kernals can be reused with minimal changes across all these datasets  3M Clickbait Headlines for 6 years Examine the Examiner 1.3M Global Headlines from 20K sources over 1 week Global News Week 2.6M News Headlines from India from 2001-2017 Headlines of India ,CSV,,"[news agencies, historiography, linguistics, sociology]",CC4,,,1287,14807,19,News headlines published over a period of 14 years.,A Million News Headlines,https://www.kaggle.com/therohk/million-headlines,Tue Jan 02 2018
,Centers for Disease Control and Prevention,"[SEQN, SDDSRVYR, RIDSTATR, RIAGENDR, RIDAGEYR, RIDAGEMN, RIDRETH1, RIDRETH3, RIDEXMON, RIDEXAGM, DMQMILIZ, DMQADFC, DMDBORN4, DMDCITZN, DMDYRSUS, DMDEDUC3, DMDEDUC2, DMDMARTL, RIDEXPRG, SIALANG, SIAPROXY, SIAINTRP, FIALANG, FIAPROXY, FIAINTRP, MIALANG, MIAPROXY, MIAINTRP, AIALANGA, DMDHHSIZ, DMDFMSIZ, DMDHHSZA, DMDHHSZB, DMDHHSZE, DMDHRGND, DMDHRAGE, DMDHRBR4, DMDHREDU, DMDHRMAR, DMDHSEDU, WTINT2YR, WTMEC2YR, SDMVPSU, SDMVSTRA, INDHHIN2, INDFMIN2, INDFMPIR]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The National Health and Nutrition Examination Survey (NHANES) is a program of studies designed to assess the health and nutritional status of adults and children in the United States. The survey is unique in that it combines interviews and physical examinations. NHANES is a major program of the National Center for Health Statistics (NCHS). NCHS is part of the Centers for Disease Control and Prevention (CDC) and has the responsibility for producing vital and health statistics for the Nation. The NHANES program began in the early 1960s and has been conducted as a series of surveys focusing on different population groups or health topics. In 1999 the survey became a continuous program that has a changing focus on a variety of health and nutrition measurements to meet emerging needs. The survey examines a nationally representative sample of about 5000 persons each year. These persons are located in counties across the country 15 of which are visited each year. The NHANES interview includes demographic socioeconomic dietary and health-related questions. The examination component consists of medical dental and physiological measurements as well as laboratory tests administered by highly trained medical personnel. To date thousands of research findings have been published using the NHANES data. Content The 2013-2014 NHANES datasets include the following components  Demographics dataset  A complete variable dictionary can be found here Examinations dataset which contains  Blood pressure Body measures Muscle strength - grip test Oral health - dentition Taste & smell A complete variable dictionary can be found here Dietary data - total nutrient intake first day   A complete variable dictionary can be found here Laboratory dataset which includes  Albumin & Creatinine - Urine Apolipoprotein B Blood Lead Cadmium Total Mercury Selenium and Manganese Blood mercury inorganic ethyl and methyl Cholesterol - HDL Cholesterol - LDL & Triglycerides Cholesterol - Total Complete Blood Count with 5-part Differential - Whole Blood Copper Selenium & Zinc - Serum Fasting Questionnaire Fluoride - Plasma Fluoride - Water Glycohemoglobin Hepatitis A Hepatitis B Surface Antibody Hepatitis B core antibody surface antigen and Hepatitis D antibody Hepatitis C RNA (HCV-RNA) and Hepatitis C Genotype Hepatitis E IgG & IgM Antibodies Herpes Simplex Virus Type-1 & Type-2 HIV Antibody Test Human Papillomavirus (HPV) - Oral Rinse Human Papillomavirus (HPV) DNA - Vaginal Swab Roche Cobas & Roche Linear Array Human Papillomavirus (HPV) DNA Results from Penile Swab Samples Roche Linear Array Insulin Iodine - Urine Perchlorate Nitrate & Thiocyanate - Urine Perfluoroalkyl and Polyfluoroalkyl Substances (formerly Polyfluoroalkyl Chemicals - PFC) Personal Care and Consumer Product Chemicals and Metabolites Phthalates and Plasticizers Metabolites - Urine Plasma Fasting Glucose Polycyclic Aromatic Hydrocarbons (PAH) - Urine Standard Biochemistry Profile Tissue Transglutaminase Assay (IgA-TTG) & IgA Endomyseal Antibody Assay (IgA EMA) Trichomonas - Urine Two-hour Oral Glucose Tolerance Test Urinary Chlamydia Urinary Mercury Urinary Speciated Arsenics Urinary Total Arsenic Urine Flow Rate Urine Metals Urine Pregnancy Test Vitamin B12  A complete data dictionary can be found here Questionnaire dataset which includes information on Acculturation Alcohol Use Blood Pressure & Cholesterol Cardiovascular Health Consumer Behavior Current Health Status Dermatology Diabetes Diet Behavior & Nutrition Disability Drug Use Early Childhood Food Security Health Insurance Hepatitis Hospital Utilization & Access to Care Housing Characteristics Immunization Income Medical Conditions Mental Health - Depression Screener Occupation Oral Health Osteoporosis Pesticide Use Physical Activity Physical Functioning Preventive Aspirin Use Reproductive Health Sexual Behavior Sleep Disorders Smoking - Cigarette Use Smoking - Household Smokers Smoking - Recent Tobacco Use Smoking - Secondhand Smoke Exposure Taste & Smell Weight History Weight History - Youth A complete variable dictionary can be found here Medication dataset which includes prescription medications A complete variable dictionary can be found here  Acknowledgements Original data and additional documents related to the datasets or NHANES can be found here.,CSV,,"[healthcare, health]",Other,,,2155,14718,31,NHANES datasets from 2013-2014,National Health and Nutrition Examination Survey,https://www.kaggle.com/cdc/national-health-and-nutrition-examination-survey,Fri Jan 27 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[Sector, 2000-01, 2001-02, 2002-03, 2003-04, 2004-05, 2005-06, 2006-07, 2007-08, 2008-09, 2009-10, 2010-11, 2011-12, 2012-13, 2013-14, 2014-15, 2015-16, 2016-17]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context To understand the Foreign direct investment in India for the last 17 years from 2000-01 to 2016-17. Content This dataset contains sector and financial year wise data of FDI in India. Acknowledgements Ministry of Commerce and Industry has published Financial Year wise FDI Equity Inflows from 2000-01 to 2016-17 dataset in Open Government Data Platform India under Govt. Open Data License - India. Inspiration  How much FDI has changed over the year? How much has varied since 2014 after Narendra Modi become PM of India? ,CSV,,"[india, finance]",CC4,,,254,1779,0.0078125,Sector & Financial year wise time series data from 2000-2016.,Foreign Direct Investment in India,https://www.kaggle.com/rajanand/fdi-in-india,Fri Aug 18 2017
,JoniHoppen,"[PatientId, AppointmentID, Gender, ScheduledDay, AppointmentDay, Age, Neighbourhood, Scholarship, Hipertension, Diabetes, Alcoholism, Handcap, SMS_received, No-show]","[numeric, numeric, string, dateTime, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context A person makes a doctor appointment receives all the instructions and no-show. Who to blame?   If this is help don´t forget to upvote ) Greatings!  Content 300k medical appointments and its 15 variables (characteristics) of each. The most important one if the patient show-up or no-show the appointment.  Variable names are self-explanatory if you have doubts just let me know!  scholarship variable means this concept = https//en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia Data Dictionary PatientId - Identification of a patient AppointmentID - Identification of each appointment Gender = Male or Female  . Female is the greater proportion woman takes way more care of they health in comparison to man. DataMarcacaoConsulta = The day of the actuall appointment when they have to visit the doctor. DataAgendamento = The day someone called or registered the appointment this is before appointment of course. Age = How old is the patient. Neighbourhood = Where the appointment takes place.  Scholarship = Ture of False . Observation this is a broad topic consider reading this article https//en.wikipedia.org/wiki/Bolsa_Fam%C3%ADlia  Hipertension = True or False Diabetes = True or False Alcoholism = True or False Handcap = True or False SMS_received = 1 or more messages sent to the patient. No-show = True or False.  Inspiration What if that possible to predict someone to no-show an appointment?,CSV,,"[brazil, healthcare, public health]",CC4,,,10013,78678,10,Why do 30% of patients miss their scheduled appointments?,Medical Appointment No Shows,https://www.kaggle.com/joniarroba/noshowappointments,Mon Aug 21 2017
,Open Knowledge International,"[name, country, subcountry, geonameid]","[string, string, string, numeric]",Utility Data The data is extracted from geonames a very exhaustive list of worldwide toponyms. It can be joined with datasets containing geographic fields to facilitate geospatial analysis including mapping. This datapackage only lists cities above 15000 inhabitants. Each city is associated with its country and subcountry to reduce the number of ambiguities. Subcountry can be the name of a state (e.g. in United Kingdom or the United States of America) or the major administrative section (e.g. ''region'' in France''). See admin1 field on geonames website for further info about subcountry. Notice that  Some cities like Vatican City or Singapore are a whole state so they don't belong to any subcountry. Therefore subcountry is N/A. There is no guaranty that a city has a unique name in a country and subcountry (At the time of writing there are about 60 ambiguities). But for each city the source data primary key geonameid is provided.  Preparation You can run the script yourself to update the data and publish them to GitHub/Kaggle see scripts README Acknowledgments and License All data is licensed under the Creative Common Attribution License as is the original data from geonames. This means you have to credit geonames when using the data. And while no credit is formally required a link back or credit to Lexman and the Open Knowledge Foundation is much appreciated. This dataset description is reproduced here from its original source with slight modifications.,CSV,,[cities],Other,,,522,4090,0.83203125,"All of the world's major cities above 15,000 inhabitants",World Cities,https://www.kaggle.com/okfn/world-cities,Wed Jun 14 2017
,Rachael Tatman,[],[],Context/background Discourse acts are the different types of things you can do in a conversation like agreeing disagreeing or elaborating. This dataset contains annotations of the discourse acts of different Twitter comments. The discourse acts labeled here are “coarse” in the sense that they’re labelled broadly (for the whole Reddit comment) rather than for individual sentences or phrases not in the sense of being vulgar. The discourse act of each post has been annotated by multiple annotators. Content A large corpus of discourse annotations and relations on ~10K forum threads. Please refer to the following paper for an in depth analysis and explanation of the data Characterizing Online Discussion Using Coarse Discourse Sequences (ICWSM '17).  Explanation of fields Thread fields  URL - reddit URL of the thread title - title of the thread as written by the first poster is_self_post - True if the first post in the thread is a self-post (text addressed to the reddit community as opposed to an external link) subreddit - the subreddit of the thread posts - a list of all posts in the thread  Post fields  id - post ID reddit ID of the current post in_reply_to - parent ID reddit ID of the parent post or the post that the current post is in reply to post_depth - the number of replies the current post is from the initial post is_first_post - True if the current post is the initial post annotations - a list of all annotations made to this post (see below) majority_type - the majority annotated type if there is a majority type between the annotators when considering only the main_type field majority_link - the majority annotated link if there is a majority link between the annotators  Annotation fields  annotator - an unique ID for the annotator main_type - the main discourse act that describes this post secondary_type - if a post contains more than one discourse act in sequence this is the second discourse act in the post link_to_post - the post that this post is linked to  Data sampling and pre-processing Selecting Reddit threads This data was randomly sampled from the full Reddit dataset starting from its inception to the end of May 2016 which is made available publicly as a dump on Google BigQuery.  This dataset was subsampled from the larger dataset and does not include posts with fewer than two comments not in English which contain pornographic material or from Subreddits focused on trading. Further the number of replies to a single thread was limited to 40. Annotation Three annotators were assigned to each thread and were instructed to annotate each comment in the thread with its discourse act (main_type) as well as the relation of each comment to a prior comment (link_to_post) if it existed. Annotators were instructed to consider the content at the comment level as opposed to sentence or paragraph level to make the task simpler. Authors Amy X. Zhang MIT CSAIL Cambridge MA USA. axz@mit.edu Ka Wong Google Mountain View CA USA. kawong@google.com Bryan Culbertson Calthorpe Analytics Berkeley CA USA. bryan.culbertson@gmail.com Praveen Paritosh Google Mountain View CA USA. pkp@google.com Citation Guidelines If you are using this data towards a research publication please cite the following paper. Amy X. Zhang Bryan Culbertson Praveen Paritosh. Characterizing Online Discussion Using Coarse Discourse Sequences. In Proceedings of the International AAAI Conference on Weblogs and Social Media (ICWSM '17). Montreal Canada. 2017.  Bibtex @inproceedings{coarsediscourse   title={Characterizing Online Discussion Using Coarse Discourse Sequences}   author={Zhang Amy X. and Culbertson Bryan and Paritosh Praveen}   booktitle={Proceedings of the 11th International AAAI Conference on Weblogs and Social Media}   series={ICWSM '17}   year={2017}   location = {Montreal Canada} } License CC-by Inspiration  Can you visualize which discourse acts are used to in replies to each kind of discourse act? Are threads more likely to be made up of a single type of discourse act or multiple discourse acts? Are certain discourse acts more closely associated with specific subreddits? ,{}JSON,,"[languages, linguistics]",Other,,,62,941,52,"Annotated discourse acts fom 10,000 posts and replies",Discourse Acts on Reddit,https://www.kaggle.com/rtatman/discourse-acts-on-reddit,Tue Jul 25 2017
,Mike Chirico,"[incident_id, case_number, incident_datetime, incident_type_primary, incident_description, clearance_type, address_1, address_2, city, state, zip, country, latitude, longitude, created_at, updated_at, location, hour_of_day, day_of_week, parent_incident_type]","[numeric, dateTime, dateTime, string, string, string, string, string, string, string, string, string, numeric, numeric, dateTime, dateTime, string, numeric, string, string]",Cheltenham PA Crime Data Cheltenham is a home rule township bordering North Philadelphia in Montgomery County.  It has a population of about 37000 people.  You can find out more about Cheltenham on wikipedia. Cheltenham's Facebook Groups. contains postings on crime and other events in the community. Getting Started Reading Data is a simple python script for getting started. If you prefer to use R there is an example Kernel  here. Proximity to Philadelphia This township borders on Philadelphia which may or may not influence crime in the community.  For Philadelphia crime patterns see the Philadelphia Crime Dataset.  Reference Data was obtained from socrata.com,CSV,,[crime],ODbL,,,248,3017,1,Cheltenham Township Police Department incident dataset,Cheltenham Crime Data,https://www.kaggle.com/mchirico/chtpd,Sat Dec 16 2017
,OpenAddresses,"[LON, LAT, NUMBER, STREET, UNIT, CITY, DISTRICT, REGION, POSTCODE, ID, HASH]","[numeric, numeric, numeric, string, string, string, string, string, string, string, string]",Context OpenAddresses's goal is to connect the digital and physical worlds by sharing geographic coordinates street names house numbers and postal codes.  Content This dataset contains one datafile for each state in the U.S. South region (although some are arguably not in the South). States included in this dataset  Alabama - al.csv Arkansas - ar.csv Washington D.C. - dc.csv Delaware - de.csv Florida - fl.csv Georgia - ga.csv Kentucky - ky.csv Louisiana - la.csv Maryland - md.csv Mississippi - ms.csv North Carolina - nc.csv Oklahoma - ok.csv South carolina - sc.csv Tennessee - tn.csv Texas - tx.csv Virginia - va.csv West Virginia - wv.csv  Field descriptions  LON - Longitude LAT - Latitude NUMBER - Street number STREET - Street name UNIT - Unit or apartment number CITY - City name DISTRICT - ? REGION - ? POSTCODE - Postcode or zipcode ID - ? HASH - ?  Acknowledgements Data collected around 2017-07-25 by OpenAddresses (http//openaddresses.io). Address data is essential infrastructure. Street names house numbers and postal codes when combined with geographic coordinates are the hub that connects digital to physical places. Data licenses can be found in LICENSE.txt. Data source information can be found at https//github.com/openaddresses/openaddresses/tree/9ea72b079aaff7d322349e4b812eb43eb94d6d93/sources Inspiration Use this dataset to create maps in conjunction with other datasets for crime or weather,CSV,,[],CC0,,,131,740,3072,Addresses and geo locations for the U.S. South,OpenAddresses - U.S. South,https://www.kaggle.com/openaddresses/openaddresses-us-south,Wed Aug 02 2017
,Google Brain,[],[],"Inception-v3 is trained for the ImageNet Large Visual Recognition Challenge using the data from 2012. This is a standard task in computer vision where models try to classify entire images into 1000 classes like ""Zebra"" ""Dalmatian"" and ""Dishwasher"". Here's code on GitHub to train Inception-v3",Other,,[artificial intelligence],Other,,,670,19222,104,Inception V3 Tensorflow Model,Inception V3 Model,https://www.kaggle.com/google-brain/inception-v3,Fri Jun 30 2017
,City of New York,"[ProjectTitle, EventName, EventType, EventStartDate, EventEndDate, Location, Boro, ProjectId, CategoryName, SubCategoryName, CompanyName]","[string, string, string, dateTime, dateTime, string, string, numeric, string, string, string]","Context Dataset is a list of film and television permits received from the Mayor's Office of Media and Entertainment in response to a series of FOIL requests in 2015. The permits stretch from October 2011 through September 2015. Content  ProjectTitle The title of the film/television project. EventName A shorthand name for the specific shoot/event being permitted e.g. SunsetPark-010815. EventType One of the following Scouting Permit Rigging Permit Shooting Permit Film Shoot / Production DCAS Prep/Shoot/Wrap Permit Grid Request or Red Carpet Premiere.  According to the MOME Shooting Permit and Film Shoot / Production are interchangeable. EventStartDate and EventEndDate The start and end date and time of the permit. Location One or more locations covered by the permit. Boro What borough the listed locations are in. ProjectId An internal identifier. CategoryName Film or Television. SubCategoryName Includes values such as Pilot Student Film Variety Reality etc. This probably isn't a reliable classification for TV shows it's chosen by the permit applicant on the online form and is not vetted by the Mayor's Office. It also includes overlapping subcategories. For example a show could be both a Morning Show and a Talk Show but would have to choose one or the other. CompanyName The supplied production company name which can be useful in connecting a working title to an actual film/show. The project title is sometimes a variation on the actual title (e.g. Mozart in the Jungle S1 or The Wolf of Wall Street ReShoots) or a working title (e.g. Untitled Female Buddy Cop Movie instead of The Heat St James Place instead of Bridge of Spies). In some cases the locations listed actually span multiple boroughs and the Boro field only represents the primary borough or the borough of the first listed location.  In some cases the Boro field is blank. A given shooting permit can have any number of locations listed for a single day.  According to the guidelines the locations are supposed to be listed in the order they're used on that day.  Most locations are either an address or a range of blocks in the format of STREET 1 between STREET 2 and STREET 3. Permits are generally required when asserting the exclusive use of city property like a sidewalk a street or a park. A shooting permit on a street doesn't necessarily mean there is exterior shooting on the street.  It may just mean for example that something is being shot indoors and the crew needs special parking privileges for trucks. See ""When a Permit is Required"". Shooting on Department of Citywide Administrative Services (DCAS) property like in a city courthouse involves an additional permitting process. Shooting on MTA property or on state/federal property is subject to a different permitting process. A shooting permit is typically but not always for a single day or a single overnight period.  Acknowledgements Data was FOIL’d by WNYC Data Journalism team and hosted originally on GitHub here. Check out these great related resources  General MOME Permit Info The Made in NY Location Library DCAS Managed Public Buildings Metrocosm's NYC Film Permits Map 2015 BCG Report on Media and Entertainment in NYC  Inspiration  Where do most films occur in the city? When is the most common filming time? Who films the most in the city? ",CSV,,"[government agencies, visual arts]",CC0,,,49,687,13,Information on ~40k Filming Locations,NYC Filming Permits,https://www.kaggle.com/new-york-city/nyc-filming-permits,Tue Sep 19 2017
,Centers for Medicare & Medicaid Services,"[Change_Type, Program_Year, Payment_Type, Record_ID]","[string, numeric, string, numeric]",Context Open Payments is a national disclosure program created by the Affordable Care Act (ACA) and managed by Centers for Medicare & Medicaid Services (CMS). The purpose of the program is to promote transparency into the financial relationships between pharmaceutical and medical device industries and physicians and teaching hospitals. The financial relationships may include consulting fees research grants travel reimbursements and payments from industry to medical practitioners.  Content There are 3 datasets that represent 3 different payment types   General Payments Payments not made in connection with a research agreement. This dataset contains 65 variables. Research Payments Payments made in connection with a research agreement. This dataset contains 166 variables. Physician Ownership or Investment Interest Information about physicians who hold ownership or investment interest in the manufacturer/GPO or who have an immediate family member holding such interest. This dataset contains 29 variables. Deleted/Removed Records Contains any deleted/removed records.  A comprehensive methodology overview and data dictionary for each dataset can be found here.  Acknowledgements The original datasets can be found here. Inspiration  Using the General Payments dataset can you determine any trends in the total amount of payment to hospitals and physicians across the medical specialties or by the form/nature of the payments? According to the Research Payments dataset which area(s) of research or the type of drug/medical device receive the most amount of payment? ,CSV,,"[healthcare, finance, health]",Other,,,311,3961,2048,Creating Public Transparency into Industry-Physician Financial Relationship,CMS Open Payments Dataset 2013,https://www.kaggle.com/cms/cms-open-payments-dataset-2013,Mon Nov 07 2016
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes over 2 gb of stop data from Texas covering all of 2010 onwards. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[government agencies, crime, law]",Other,,,109,1265,3072,Data on Traffic and Pedestrian Stops by Police in Texas,Stanford Open Policing Project - Texas,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-texas,Tue Jul 11 2017
,PyTorch,[],[],ResNet-18  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,14,884,41,ResNet-18 Pre-trained Model for PyTorch,ResNet-18,https://www.kaggle.com/pytorch/resnet18,Wed Dec 13 2017
,RakanNimer,"[Rank, Song, Artist, Year, Lyrics, Source]","[numeric, string, string, numeric, string, numeric]","Original Dataset Author  https//github.com/walkerkq From https//github.com/walkerkq/musiclyrics  50 Years of Pop Music Lyrics Billboard has published a Year-End Hot 100 every December since 1958. The chart measures the performance of singles in the U.S. throughout the year. Using R I’ve combined the lyrics from 50 years of Billboard Year-End Hot 100 (1965-2015) into one dataset for analysis. You can download that dataset here. The songs used for analysis were scraped from Wikipedia’s entry for each Billboard Year-End Hot 100 Songs (e.g. 2014). This is the year-end chart not weekly rankings. Many artists have made the weekly chart but not the final year end chart. The final chart is calculated using an inverse point system based on the weekly Billboard charts (100 points for a week at number one 1 point for a week at number 100 etc). I used the xml and RCurl packages to scrape song and artist names from each Wikipedia entry. I then used that list to scrape lyrics from sites that had predictable URL strings (for example metrolyrics.com uses metrolyrics.com/SONG-NAME-lyrics-ARTIST-NAME.html). If the first site scrape failed I moved onto the second and so on. About 78.9% of the lyrics were scraped from metrolyics.com 15.7% from songlyrics.com 1.8% from lyricsmode.com. About 3.6% (187/5100) were unavailable. The dataset features 5100 observations with the features rank (1-100) song artist year lyrics and source. The artist feature is fairly standardized thanks to Wikipedia but there is still quite a bit of noise when it comes to artist collaborations (Justin Timberlake featuring Timbaland for example). If there were any errors in the lyrics that were scraped such as spelling errors or derivatives like ""nite"" instead of ""night"" they haven't been corrected.   Full analysis can be found here.  walkerkq  Acknowledgements Dataset is a mirror of  https//github.com/walkerkq/musiclyrics All credits to gathering it goes to https//github.com/walkerkq Inspiration What makes a song's lyrics popular ?",CSV,,"[music, linguistics]",Other,,,861,7299,8,50 years of pop music lyrics,Billboard 1964-2015 Songs + Lyrics,https://www.kaggle.com/rakannimer/billboard-lyrics,Sun Apr 16 2017
,GregKondla,"[Id, Age, Job, Marital, Education, Default, Balance, HHInsurance, CarLoan, Communication, LastContactDay, LastContactMonth, NoOfContacts, DaysPassed, PrevAttempts, Outcome, CallStart, CallEnd, CarInsurance]","[numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, string, dateTime, dateTime, string]","Introduction Here you find a very simple beginner-friendly data set. No sparse matrices no fancy tools needed to understand what's going on. Just a couple of rows and columns. Super simple stuff. As explained below this data set is used for a competition. As it turns out this competition tends to reveal a common truth in data science KISS - Keep It Simple Stupid What is so special about this data set is given it's simplicity it pays off to use ""simple"" classifiers as well. This year's competition was won by a C5.0 . Can you do better? Description We are looking at cold call results. Turns out same salespeople called existing insurance customers up and tried to sell car insurance. What you have are details about the called customers. Their age job marital status whether the have home insurance a car loan etc. As I said super simple. What I would love to see is some of you applying some crazy XGBoost classifiers which we can square off against some logistic regressions. It would be curious to see what comes out on top. Thank you for your time I hope you enjoy using the data set. Acknowledgements Thanks goes to the Decision Science and Systems Chair of Technical University of Munich (TUM) for getting the data set from a real world company and making it available to be shared publicly. Also Vladimir Fux who oversees the challenge associated with this data set. Inspiration This is a data set used for teaching entry level data mining skills at the TUM. Every year there is a competition as part of the curriculum of a particular course. This Data Mining Cup teaches some of the very fundamentals that are always worthy to be revisited especially by pros abundant at Kaggle. For some of my thoughts see the verbose comments in the Kernel.",CSV,,[business],Other,,,946,5979,0.9287109375,We help the guys and girls at the front to get out of Cold Call Hell,Car Insurance Cold Calls,https://www.kaggle.com/kondla/carinsurance,Fri Jun 16 2017
,US Census Bureau,"[HRHHID, HRMONTH, HRYEAR4, HURESPLI, HUFINAL, HUSPNISH, HETENURE, HEHOUSUT, HETELHHD, HETELAVL, HEPHONEO, HEFAMINC, HUTYPEA, HUTYPB, HUTYPC, HWHHWGT, HRINTSTA, HRNUMHOU, HRHTYPE, HRMIS, HUINTTYP, HUPRSCNT, HRLONGLK, HRHHID2, HWHHWTLN, HUBUS, HUBUSL1, HUBUSL2, HUBUSL3, HUBUSL4, GEREG, GEDIV, GESTFIPS, GTCBSA, GTCO, GTCBSAST, GTMETSTA, GTINDVPC, GTCBSASZ, GTCSA, FILLER, PERRP, PEPARENT, PRTAGE, PRTFAGE, PEMARITL, PESPOUSE, PESEX, PEAFEVER, FILLER, PEAFNOW, PEEDUCA, PTDTRACE, PRDTHSP, PUCHINHH, PULINENO, PRFAMNUM, PRFAMREL, PRFAMTYP, PEHSPNON, PRMARSTA, PRPERTYP, PENATVTY, PEMNTVTY, PEFNTVTY, PRCITSHP, PRCITFLG, PRINUSYR, PUSLFPRX, PEMLR, PUWK, PUBUS1, PUBUS2OT, PUBUSCK1, PUBUSCK2, PUBUSCK3, PUBUSCK4, PURETOT, PUDIS, PERET1, PUDIS1, PUDIS2, PUABSOT, PULAY, PEABSRSN, PEABSPDO, PEMJOT, PEMJNUM, PEHRUSL1, PEHRUSL2, PEHRFTPT, PEHRUSLT, PEHRWANT, PEHRRSN1, PEHRRSN2, PEHRRSN3, PUHROFF1, PUHROFF2, PUHROT1, PUHROT2]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Current Population Survey - August 2016 Context The Current Population Survey (CPS) is one of the oldest largest and most well-recognized surveys in the United States.  It is immensely important providing information on many of the things that define us as individuals and as a society – our work our earnings and our education.  Frequency Monthly Period August 2016  Content In addition to being the primary source of monthly labor force statistics the CPS is used to collect data for a variety of other studies that keep the nation informed of the economic and social well-being of its people.  This is done by adding a set of supplemental questions to the monthly basic CPS questions.  Supplemental inquiries vary month to month and cover a wide variety of topics such as child support volunteerism health insurance coverage and school enrollment.  Supplements are usually conducted annually or biannually but the frequency and recurrence of a supplement depend completely on what best meets the needs of the supplement’s sponsor.  Data Dictionary http//thedataweb.rm.census.gov/pub/cps/basic/201501-/January_2015_Record_Layout.txt Acknowledgements The Current Population Survey (CPS) is administered processed researched and disseminated by the U.S. Census Bureau on behalf of the Bureau of Labor Statistics (BLS).,CSV,,"[employment, demographics]",CC0,,,910,8365,300,The primary source of labor force statistics for the US population,Current Population Survey,https://www.kaggle.com/census/current-population-survey,Mon Oct 24 2016
,PyTorch,[],[],ResNet-152  Deep Residual Learning for Image Recognition Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity.  An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.  The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions where we also won the 1st places on the tasks of ImageNet detection ImageNet localization COCO detection and COCO segmentation. Authors Kaiming He Xiangyu Zhang Shaoqing Ren Jian Sun https//arxiv.org/abs/1512.03385  Architecture visualization http//ethereon.github.io/netscope/#/gist/db945b393d40bfa26006   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,8,525,214,ResNet-152 Pre-trained Model for PyTorch,ResNet-152,https://www.kaggle.com/pytorch/resnet152,Thu Dec 14 2017
,zjf,"[id, band, title, year]","[numeric, numeric, string, numeric]","Context Metal-Archives.com (MA for short) is an encyclopedia website which includes information of nearly all heavy bands and albums on the earth. This information is collected and submitted by metalheads from all around the world. This dataset includes all ""death metal"" bands and albums on MA (by Nov. 2016). It's the search result by genre key word ""death metal""  which includes all bands which  contain phrase ""death metal"" in their genre. (e.g. ""technical death metal"" ""brutal death metal"" ""melodic death metal"" ... ) The banner of dataset is the cover art of New Jersey-based death metal band Disma's debut full-length album ""Towards the Megalith"" (2011 Profound Lore Records). It's beautiful but not quite typical for this dataset's theme. But 1900+ resolution picture about death metal is rare so I've chosen this one. Content There are three csv files included in the dataset bands.csv contains 37723 bands. Each record is consisted of 8 fields   id sequential integer id.  name the band's name which can contains non-english character punctuations numbers and other weird characters. country country the band is from. ""International""  means the members of the band are from multiple countries. status band's current activity-status 'Unknown' 'Split-up' 'Active' 'Changed name' 'On hold' and 'Disputed'. from_in the year in which the band formed. genre the description of the band's genre. It's irregular so you'd better not deem it as category but short text. theme the description of the band's lyric theme.  active the time-span in which the band is active.  albums.csv contains 28069 albums. Each record is consisted of 4 fields  id sequential integer id.  band foreign key to band's id in bands.csv. title album title. year the album's release year.  reviews.csv  contains 21510 reviews. Each record is consisted of 5 fields  id sequential integer id.  album foreign key to album's id in albums.csv. title the review's title. score the score for that album. Float number from 0.0 to 1.0 (from negative to positive).  content the review's text.  Notice  This dataset only contains full-length studio albums (excluding EPs singles live albums split albums and others). All commas in dataset are replaced by ""|"" to make comma available for fields separator.  NA value is ""N/A"".  All text is in utf-8 encoding.  Acknowledgements Metal-Archives! \m/ Inspiration  Statistical analysis Emotional analysis (reviews) Genre classification (by NLP of title and/or theme) Score prediction (by NLP of review's content) ",CSV,,[music],Other,,,456,6071,71,Death metal bands and albums,Death Metal,https://www.kaggle.com/zhangjuefei/death-metal,Fri Jan 13 2017
,"Jihye Sofia Seo, Ph.D.","[측정일시, 측정소명, 이산화질소농도(ppm), 오존농도(ppm), 일산화탄소농도(ppm), 아황산가스(ppm), 미세먼지(㎍/㎥), 초미세먼지(㎍/㎥)]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context Sadly Seoul South Korea has some of the most polluted air in the world. Since Seoul also represents 25-50% of the South Korean population the air quality is a concern to many.  It used to be that in Korea we have bad air quality in spring (yellow wind blowing from the Chinese Yellow River) and clear air in autumn. Now with more industries in China the air is getting worse in Korea in a different seasonality pattern. This is known as Asian Dust. Content Hourly measurement on several air pollutants in dozens of districts in Seoul. Acknowledgements Data downloaded from here. http//data.seoul.go.kr/openinf/sheetview.jsp?infId=OA-2275&tMenu=11 We thank Seoul Open Data Plaza for making the datasets available. http//english.seoul.go.kr/policy-information/key-policies/informatization/seoul-open-data-plaza/ The banner photos are via JEONGUK HA on Unsplash Inspiration Recently fine dusts are posing a big problem in Korea.  https//www.ft.com/content/b49a9878-141b-11e7-80f4-13e067d5072c,CSV,,"[cities, pollution]",CC4,,,222,1352,0.2685546875,"Yellow dust,  fine dust, where and when to avoid?",Air pollutants measured in Seoul,https://www.kaggle.com/jihyeseo/seoulairreport,Fri Nov 24 2017
,DrGuillermo,"[, Player, height, weight, collage, born, birth_city, birth_state]","[numeric, string, numeric, numeric, string, numeric, string, string]",Content The data-set contains aggregate individual statistics for 67 NBA seasons. from basic box-score attributes such as points assists rebounds etc. to more advanced money-ball like features such as Value Over Replacement. Acknowledgements The data was scraped from Basketball-reference Take a look in their glossary for a detailed column description Glossary,CSV,,[basketball],Other,,,3502,20649,5,"3000+ Players over 60+ Seasons, and 50+ features per player",NBA Players stats since 1950,https://www.kaggle.com/drgilermo/nba-players-stats,Tue Jun 06 2017
,Wendy Kan,[],[],"These files contain complete loan data for all loans issued through the 2007-2015 including the current loan status (Current Late Fully Paid etc.) and latest payment information. The file containing loan data through the ""present"" contains complete loan data for all loans issued through the previous completed calendar quarter. Additional features include credit scores number of finance inquiries address including zip codes and state and collections among others. The file is a matrix of about 890 thousand observations and 75 variables. A data dictionary is provided in a separate file. k",SQLite,,[finance],Other,,,26819,152077,913,Analyze Lending Club's issued loans,Lending Club Loan Data,https://www.kaggle.com/wendykan/lending-club-loan-data,Tue May 03 2016
,United States Air Force,[],[],Context THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordinance in Southeast Asia and improving Air Force combat tactics. Our goal is to see where public discourse and innovation takes this data.  Each theater of warfare has a separate data file in addition to a THOR Overview. Content The THOR data contains data for Allied aircraft carrying a combined bomb load of more than a million pounds over 1437 recorded missions. This Theater History of Operations (THOR) dataset combines digitized paper mission reports from WWI. It can be searched by date conflict geographic location and more than 30 additional data attributes forming a live-action sequence of events. See the data dictionary here and additonal background here. Acknowledgements THOR is a dataset project initiated by  Lt Col Jenns Robertson and continued in partnership with Data.mil  an experimental project created by the Defense Digital Service in collaboration with the Deputy Chief Management Officer and data owners throughout the U.S. military.  Inspiration  Can you create animated maps of certain campaigns? See for example this map of the Argonne-Meuse offensive. Can you match weather data with campaigns? Where were the most campaigns? ,CSV,,"[military, war]",CC0,,,98,979,1,Details on 1441 Allied Runs,WWI Bombing Operations,https://www.kaggle.com/usaf/wwi-bombing-operations,Thu Sep 14 2017
,Chris Crawford,"[Product_Code, W0, W1, W2, W3, W4, W5, W6, W7, W8, W9, W10, W11, W12, W13, W14, W15, W16, W17, W18, W19, W20, W21, W22, W23, W24, W25, W26, W27, W28, W29, W30, W31, W32, W33, W34, W35, W36, W37, W38, W39, W40, W41, W42, W43, W44, W45, W46, W47, W48, W49, W50, W51, MIN, MAX, Normalized 0, Normalized 1, Normalized 2, Normalized 3, Normalized 4, Normalized 5, Normalized 6, Normalized 7, Normalized 8, Normalized 9, Normalized 10, Normalized 11, Normalized 12, Normalized 13, Normalized 14, Normalized 15, Normalized 16, Normalized 17, Normalized 18, Normalized 19, Normalized 20, Normalized 21, Normalized 22, Normalized 23, Normalized 24, Normalized 25, Normalized 26, Normalized 27, Normalized 28, Normalized 29, Normalized 30, Normalized 31, Normalized 32, Normalized 33, Normalized 34, Normalized 35, Normalized 36, Normalized 37, Normalized 38, Normalized 39, Normalized 40, Normalized 41, Normalized 42, Normalized 43, Normalized 44]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Contains weekly purchased quantities of 800 over products over 52 weeks. These data were used in the paper ""Time series clustering A superior alternative for market basket analysis"" by Tan Swee Chuan and San Lau Jess Pei. Content  Each row represents a different product Each column represents a week of the year (52 total weeks). The last half of the columns are normalized for you. Values represent quantity of the products sold during the week 52 weeks W0 W1 ... W51 Normalised vlaues of weekly data Normalised 0 Normalised 1 ... Normalised 51   Acknowledgements Tan Swee Chuan and San Lau Jess Pei Time series clustering A superior alternative for market basket analysis. This dataset was downloaded from the UCI Machine Learning Repository.  https//archive.ics.uci.edu/ml/datasets/Sales_Transactions_Dataset_Weekly",CSV,,"[timelines, time series, business, product]",Other,,,1122,7371,0.302734375,Weekly purchase quantities of over 800 products over 52 weeks,Weekly Sales Transactions,https://www.kaggle.com/crawford/weekly-sales-transactions,Wed Aug 23 2017
, U.S. Government Publishing Office,[],[],The Code of Federal Regulations (CFR) is the codification of the general and permanent rules and regulations (sometimes called administrative law) published in the Federal Register by the executive departments and agencies of the federal government of the United States.  The 50 subject matter titles contain one or more individual volumes which are updated once each calendar year on a staggered basis. The annual update cycle is as follows titles 1-16 are revised as of January 1; titles 17-27 are revised as of April 1; titles 28-41 are revised as of July 1; and titles 42-50 are revised as of October 1. Each title is divided into chapters which usually bear the name of the issuing agency. Each chapter is further subdivided into parts that cover specific regulatory areas. Large parts may be subdivided into subparts. All parts are organized in sections and most citations to the CFR refer to material at the section level. The CFR is published in multiple formats by the US Government Publishing Office. You can find the latest version of the XML format here http//www.gpo.gov/fdsys/bulkdata/CFR.,Other,,"[government, law]",CC0,,,51,488,336,XML annotated US regulations as of mid 2017,Code of Federal Regulations,https://www.kaggle.com/us-gpo/code-of-federal-regulations,Tue Aug 29 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[pc_code, pc_description, unit, country_code, country_name, quantity, value]","[string, string, string, numeric, string, string, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context To better understand the imports and exports by India and how it changed in 3 years. Content Import and export data available by principle commodity and country wise for 3 years from Apr'2014 to Mar'2017. Column Descriptions  pc_code Integer Principal Commodity Code pc String Principal Commodity Name unit String measurement of quantity country_code  Integer country code country_name String country name quantity Integer quantify of export or import value Integer monetary valeu of the quantity (in million USD)  Acknowledgements Ministry of Commerce and Industry Govt of India has published these datasets in Open Govt Data Platform India portal under Govt. Open Data License - India. Inspiration Some of questions I would like to be answered are  Top countries by growth percentage. Top commodity by quantity or value. YoY growth of export and import. ,CSV,,"[india, business, industry]",CC4,,,713,4904,5,Commodity & country wise annual import and export data.,Import and Export by India from 2014 to 2017,https://www.kaggle.com/rajanand/import-and-export-by-india,Sat Aug 05 2017
,Thomas De Jonghe,"[Name, Team, Pos, Height, Weight, BMI, Birth_Place, Birthdate, Age, College, Experience, Games Played, MIN, FGM, FGA, FG%, 15:00, 3PA, 3P%, FTM, FTA, FT%, OREB, DREB, REB, AST, STL, BLK, TO, PTS, DD2, TD3]","[string, string, string, numeric, numeric, numeric, string, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Scraped and copied from http//www.wnba.com/stats/player-stats/#?Season=2017&SeasonType=Regular%20Season&PerMode=Totals + http//www.wnba.com/ in general for the bio data. Content Stats from all games of season 2016-2017  G = Games Played MIN = Minutes Played FGM = Field Goals Made FGA = Field Goals Attempts FG% = Field Goals % 3PM = 3Points Made  3PA = 3Points Attempts 3P% = 3Points % FTM = Free Throws made FTA = Free Throws Attempts FT% = Free Throws % OREB = Offensive Rebounds DREB = Defensive Rebounds REB = Total Rebounds AST = Assists STL = Steals BLK = Blocks TO = Turnovers PTS = Total points DD2 = Double doubles TD3 = Triple doubles  Inspiration Compare WNBA to NBA in best players average heights ...,CSV,,[sports],Other,,,146,1054,0.01953125,"Points, Assists, Height, Weight and other personal details and stats",WNBA Player stats Season 2016-2017,https://www.kaggle.com/jinxbe/wnba-player-stats-2017,Fri Aug 25 2017
,Federal Reserve,"[Year, Month, Day, Federal Funds Target Rate, Federal Funds Upper Target, Federal Funds Lower Target, Effective Federal Funds Rate, Real GDP (Percent Change), Unemployment Rate, Inflation Rate]","[numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, string]",Context The Federal Reserve sets interest rates to promote conditions that achieve the mandate set by the Congress — high employment low and stable inflation sustainable economic growth and moderate long-term interest rates. Interest rates set by the Fed directly influence the cost of borrowing money. Lower interest rates encourage more people to obtain a mortgage for a new home or to borrow money for an automobile or for home improvement. Lower rates encourage businesses to borrow funds to invest in expansion such as purchasing new equipment updating plants or hiring more workers. Higher interest rates restrain such borrowing by consumers and businesses.  Content This dataset includes data on the economic conditions in the United States on a monthly basis since 1954. The federal funds rate is the interest rate at which depository institutions trade federal funds (balances held at Federal Reserve Banks) with each other overnight. The rate that the borrowing institution pays to the lending institution is determined between the two banks; the weighted average rate for all of these types of negotiations is called the effective federal funds rate. The effective federal funds rate is determined by the market but is influenced by the Federal Reserve through open market operations to reach the federal funds rate target. The Federal Open Market Committee (FOMC) meets eight times a year to determine the federal funds target rate; the target rate transitioned to a target range with an upper and lower limit in December 2008. The real gross domestic product is calculated as the seasonally adjusted quarterly rate of change in the gross domestic product based on chained 2009 dollars. The unemployment rate represents the number of unemployed as a seasonally adjusted percentage of the labor force. The inflation rate reflects the monthly change in the Consumer Price Index of products excluding food and energy. Acknowledgements The interest rate data was published by the Federal Reserve Bank of St. Louis' economic data portal. The gross domestic product data was provided by the US Bureau of Economic Analysis; the unemployment and consumer price index data was provided by the US Bureau of Labor Statistics. Inspiration How does economic growth unemployment and inflation impact the Federal Reserve's interest rates decisions? How has the interest rate policy changed over time? Can you predict the Federal Reserve's next decision? Will the target range set in March 2017 be increased decreased or remain the same?,CSV,,"[history, finance]",CC0,,,717,4353,0.025390625,"Interest rates, economic growth, unemployment, and inflation data","Federal Reserve Interest Rates, 1954-Present",https://www.kaggle.com/federalreserve/interest-rates,Thu Mar 16 2017
,Aleksey Bilogur,"[, state, vet_pop, overall_pop_18, vet_pop_p, vet_suicides, all_suicides, vet_suicides_p, vet_males, vet_males_p, vet_females, vet_females_p, vet_15_24, vet_18_29, vet_17_34, vet_25_34, vet_30_39, vet_25_44, vet_35_44, vet_40_49, vet_35-54, vet_45_54, vet_50_59, vet_45_64, vet_55_64, vet_60, vet_65, vet_rate, civ_rate]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric]", Context There is a well-documented phenomenon of increased suicide rates among United States military veterans. One recent analysis published in 2016 found the suicide rate amongst veterans to be around 20 per day. The widespread nature of the problem has resulted in efforts by and pressure on the United States military services to combat and address mental health issues in and after service in the country's armed forces. In 2013 News21 published a sequence of reports on the phenomenon aggregating and using data provided by individual states to typify the nationwide pattern. This dataset is the underlying data used in that report as collected by the News21 team. Content The data consists of six files one for each year between 2005 and 2011. Each year's worth of data includes the general population of each US state a count of suicides a count of state veterans and a count of veteran suicides. Acknowledgements This data was originally published by News21. It has been converted from an XLS to a CSV format for publication on Kaggle. The original data visualizations and stories can be found at the source. Inspiration What is the geospatial pattern of veterans in the United States? How much more vulnerable is the average veteran to suicide than the average citizen? Is the problem increasing or decreasing over time?,CSV,,"[mental health, military]",CC4,,,238,1689,0.056640625,2005-2011 veteran deaths outside of combat by state,US Veteran Suicides,https://www.kaggle.com/residentmario/us-veteran-suicides,Tue Nov 14 2017
,Little Boat,[],[],Introduction With multispectral images  we can capture more data per pixel and understand objects based on their chemical composition or the variation of composition that encompasses an object.  Examples of this might be is the image you see an apple or an orange? Further is the apple or the orange real?  If it is plastic was it made in Mexico or India? Real life impacts of using spectral data as part of object detection in images could one day save a life if a self driving car could not only detect faces but also the difference between skin and plastic a lone pedestrian could avoid being if it was a choice between them or a group of three manikins. This sample data contains a series of multispectral images of handwritten numbers between 0 and 9 from six different peoples using two different pens.  And here I am asking the great kagglers to explore and build models to tell each of the numbers from one another and with what ink each was written in.  Data Each csv file contains pixels for 10 grayscale images (350 * 350) that represent 10 channels for the multispectral image where X Y represent the location of the pixel and channel0 - channel9 represent channels. And we also have a labels csv that contains labels for each pixel csv file. Licence You can do whatever you want with the data.,Other,,"[writing, artificial intelligence, image data, multiclass classification]",Other,,,482,5469,5120,Handwritten numbers (0-9) from six different people and two different pens,Multispectral Image Classification,https://www.kaggle.com/xiaozhouwang/multispectralimages,Mon Mar 13 2017
,The Movie Database (TMDb),[],[],Background What can we say about the success of a movie before it is released? Are there certain companies (Pixar?) that have found a consistent formula? Given that major films costing over $100 million to produce can still flop this question is more important than ever to the industry. Film aficionados might have different interests. Can we predict which films will be highly rated whether or not they are a commercial success? This is a great place to start digging in to those questions with data on the plot cast crew budget and revenues of several thousand films. Data Source Transfer Summary We (Kaggle) have removed the original version of this dataset per a DMCA takedown request from IMDB. In order to minimize the impact we're replacing it with a similar set of films and data fields from The Movie Database (TMDb) in accordance with their terms of use. The bad news is that kernels built on the old dataset will most likely no longer work. The good news is that  You can port your existing kernels over with a bit of editing. This kernel offers functions and examples for doing so. You can also find a general introduction to the new format here. The new dataset contains full credits for both the cast and the crew rather than just the first three actors. Actor and actresses are now listed in the order they appear in the credits. It's unclear what ordering the original dataset used; for the movies I spot checked it didn't line up with either the credits order or IMDB's stars order. The revenues appear to be more current. For example IMDB's figures for Avatar seem to be from 2010 and understate the film's global revenues by over $2 billion. Some of the movies that we weren't able to port over (a couple of hundred) were just bad entries. For example this IMDB entry has basically no accurate information at all. It lists Star Wars Episode VII as a documentary.  Data Source Transfer Details  Several of the new columns contain json. You can save a bit of time by porting the load data functions from this kernel. Even in simple fields like runtime may not be consistent across versions. For example previous dataset shows the duration for Avatar's extended cut while TMDB shows the time for the original version. There's now a separate file containing the full credits for both the cast and crew. All fields are filled out by users so don't expect them to agree on keywords genres ratings or the like. Your existing kernels will continue to render normally until they are re-run. If you are curious about how this dataset was prepared the code to access TMDb's API is posted here.  New columns  homepage id original_title overview popularity production_companies production_countries release_date spoken_languages status tagline vote_average  Lost columns  actor_1_facebook_likes actor_2_facebook_likes actor_3_facebook_likes aspect_ratio cast_total_facebook_likes color content_rating director_facebook_likes facenumber_in_poster movie_facebook_likes movie_imdb_link num_critic_for_reviews num_user_for_reviews  Open Questions About the Data There are some things we haven't had a chance to confirm about the new dataset. If you have any insights please let us know in the forums!  Are the budgets and revenues all in US dollars? Do they consistently show the global revenues? This dataset hasn't yet gone through a data quality analysis. Can you find any obvious corrections? For example in the IMDb version it was necessary to treat values of zero in the budget field as missing. Similar findings would be very helpful to your fellow Kagglers! (It's probably a good idea to keep treating zeros as missing with the caveat that missing budgets much more likely to have been from small budget films in the first place).  Inspiration  Can you categorize the films by type such as animated or not? We don't have explicit labels for this but it should be possible to build them from the crew's job titles. How sharp is the divide between major film studios and the independents? Do those two groups fall naturally out of a clustering analysis or is something more complicated going on?  Acknowledgements This dataset was generated from The Movie Database API. This product uses the TMDb API but is not endorsed or certified by TMDb. Their API also provides access to data on many additional movies actors and actresses crew members and TV shows. You can try it for yourself here. ,CSV,,[film],Other,,,62096,446883,44,"Metadata on ~5,000 movies from TMDb",TMDB 5000 Movie Dataset,https://www.kaggle.com/tmdb/tmdb-movie-metadata,Thu Sep 28 2017
,Rachael Tatman,[],[],Context Some words like “the” or “and” in English are used a lot in speech and writing. For most Natural Language Processing applications you will want to remove these very frequent words. This is usually done using a list of “stopwords” which has been complied by hand. Content This dataset contains a list of stopwords for the following languages (Languages which are not from the Indo-European language family have been starred)  English French German Italian Spanish Portuguese Finnish* Swedish Arabic* Russian Hungarian Bulgarian Romanian Czech Polish Persian/Farsi Hindi Marathi Bengali  Acknowledgements This dataset is Copyright (c) 2005 Jacques Savoy and distributed under the BSD License. More information can be found here. Inspiration This dataset is mainly helpful for use during NLP analysis however there may some interesting insights to be found in the data.  What qualities do stopwords share across languages? Given a novel language could you predict what its stopwords should be? What stopwords are shared across languages? Often related languages will have words with the same meaning and similar spellings. Can you automatically identify any of these pairs of words?  You may also like  Stopword Lists for 9 African Languages ,Other,,"[languages, india, europe, linguistics]",Other,,,342,2008,0.0517578125,Lists of high-frequency words usually removed during NLP analysis,Stopword Lists for 19 Languages,https://www.kaggle.com/rtatman/stopword-lists-for-19-languages,Fri Jul 28 2017
,VoteView,"[congress, chamber, icpsr, state_icpsr, district_code, state_abbrev, party_code, occupancy, last_means, bioname, bioguide_id, born, died, dim1, dim2, log_likelihood, geo_mean_probability, number_of_votes, number_of_errors, conditional]","[numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",DW-Nominate scores of congressional voting behavior regularly appears in media such as the New York Times Washington Post and 538. This dataset contains the voting records used to generate those scores and additional features related to the DW-NOMINATE calculations. Content This dataset contains descriptive data as well as ideological data for congressional rollcalls individual member votes members of congress and parties. You can find information such the descriptions of rollcalls what proportion of voting members were correctly classified by the ideological cutting line for that rollcall the ideological position of members of congress and more. Both the rollcall data and the data on members are split into chambers and congresses. The data on parties is a dataset with some metadata about all of the different parties as well as their average ideological position and membership size broken down by congress and chamber. The full details behind the DW-NOMINATE calculations may be helpful in interpreting some of this data. The technical details of the DW-NOMINATE model can be found in Poole's Spatial Models of Parliamentary Voting. Poole and Rosenthal's Ideology and Congress explores the nature of voting in Congress and the political history of the United States through the lens of the ideological dimensions recovered by DW-NOMINATE. Acknowledgements This dataset was prepared by the team at VoteView. Please visit their site if you require up-to-date records. You may also be interested in their blog. Inspiration -Using national scores as a training set can you develop polarization scores for you own state legislature? -Can you find correlates that help explain changes in DW-NOMINATE scores?,CSV,,"[history, government, politics]",Other,,,125,1342,510,All roll call votes made by the United States Congress 1789-2017,Congressional Voting Records,https://www.kaggle.com/voteview/congressional-voting-records,Fri Aug 11 2017
,Aleksey Bilogur,"[isqno, atp_number, start_time_min, start_time_max, end_time_min, end_time_max, duration_max, duration_min, duration_only]","[numeric, numeric, dateTime, dateTime, dateTime, dateTime, numeric, numeric, numeric]","Context This dataset is a cleaned-up and modernized version of ""CAA Database of Battles Version 1990"" shortnamed ""CDB90"". It contains information on over 600 battles that were fought between 1600 AD and 1973 AD. Descriptive data include battle name date and location; the strengths and losses on each side; identification of the victor; temporal duration of the battle; and selected environmental and tactical environment descriptors (such as type of fortifications type of tactical scheme weather conditions width of front etc.). Content The data contained therein is split across several files. The most important of these is battles.csv which is lists and gives information about the battles themselves. Files marked enum describe the keys used by specific fields. Other files provide additional context. Acknowledgements The original version of this database was distributed by the U.S. Army Concepts Analysis Agency. The version of this dataset you see here is a cleaned-up version created by Jeffrey Arnold. This dataset cleanup code and source data are all available here. Inspiration  How often were battles fought in various weather conditions? How often did an attacker or defender achieve the element of surprise? Did it have a significant effect on the outcome? Did prepared fortifications have a significant effect on outcomes? ",CSV,,"[history, military]",ODbL,,,264,1998,0.642578125,Conditions and results from over 600 battles fought in 1600 - 1973 AD,Historical Military Battles,https://www.kaggle.com/residentmario/database-of-battles,Thu Sep 14 2017
,Soumitra Agarwal,"[Name, url]","[string, string]",The dataset for people who double on Fifa and Data Science Content   17000+ players 50+ attributes per player ranging from ball skills aggression etc. Player's attributes sourced from EA Sports' FIFA video game series including the weekly updates Players from all around the globe URLs to their homepage Club logos Player images male and female National and club team data  Weekly Updates would include   Real life data (Match events etc.) The fifa generated player dataset Betting odds Growth    Data Source Data was scraped from https//www.fifaindex.com/ first by getting player profile url set (as stored in PlayerNames.csv) and then scraping the individual pages for their attributes  Improvements  You may have noticed that for a lot of players their national details are absent (Team and kit number) even though the nationality is listed. This may be attributed to the missing data on fifa sites.   GITHUB PROJECT  There is much more than just 50 attributes by which fifa decides what happens to players over time how they perform under pressure how they grow etc. This data obviously would be well hidden by the organisation and thus would be tough to find  Important note for people interested in using the scraping  The site is not uniform and thus the scraping script requires considering a lot of corner cases (i.e. interchanged position of different attributes). Also the script contains proxy preferences which may be removed if not required.  Exploring the data For starters you can become a scout  Create attribute dependent or overall best teams Create the fastest/slowest teams See which areas of the world provide which attributes (like Africa  Stamina Pace) See which players are the best at each position See which outfield players can play a better role at some other position See which youngsters have attributes which can be developed  And that is just the beginning. This is the playground.. literally!  Data description  The file FullData.csv contains attributes describing the in game play style and also some of the real statistics such as Nationality etc. The file PlayerNames.csv contains URLs for different players from their profiles on fifaindex.com. Append the URLs after the base url fifaindex.com. The compressed file Pictures.zip contains pictures for top 1000 players in Fifa 17. The compressed file Pictures_f.zip contains pictures for top 139 female players in Fifa 17. The compressed file ClubPictures.zip contains pictures for emblems of some major clubs in Fifa 17.   Inspiration I am a huge FIFA fanatic. While playing career mode I realised that I picked great young players early on every single time and since a lot of digital learning relies on how our brain works I thought scouting great qualities in players would be something that can be worked on. Since then I started working on scraping the website and here is the data. I hope we can build something on it.   With access to players attributes you can become the best scout in the world. Go for it!,CSV,,"[video games, association football]",ODbL,,,5826,39621,9,"15k+ players, 50+ Attributes per player from the latest EA Sports Fifa 17",Complete FIFA 2017 Player dataset (Global),https://www.kaggle.com/artimous/complete-fifa-2017-player-dataset-global,Thu Apr 13 2017
,Philip Corr,[],[],Context This dataset was recorded as part of an investigation into machine learning algorithms for iOS. 20136 glyphs were drawn by 257 subjects on the touch screen of an iPhone 6. An iOS app was developed to record the dataset. Firstly subjects entered their age sex nationality and handedness. Each subject was then instructed to draw the digits 0 to 9 on the touchscreen using their index finger and thumb. This was repeated four times for each subject resulting in 80 glyphs drawn per subject 40 using index finger and 40 using thumb. The sequence of glyph entry was random. Instructions to the user were provided using voice synthesis to avoid suggesting a specific glyph rendering.  The index finger and thumb were both used to account for situations in which the subject may only have one hand free. The aim here was to train a model that could accurately classify the glyph drawn in as many real life scenarios as possible.  Cubic interpolation of touches during gesture input was rendered on the screen to provide visual feedback to the subject and to compute arclengths. The screen was initially blank (white) and the gestures were displayed in black.  The subject could use most of screen to draw with small areas at the top and bottom reserved for instructions/interactions/guidance. The subject was permitted to erase and repeat the entry if desired. Content  The database consists of 4 tables as seen in the schema. The tables are Subject Glyph Stroke and Touch. This is a logical structure as each subject draws 80 glyphs each glyph consists of a number of strokes and each stroke consists of a number of touches. The four tables are presented in csv format and sqlite format.  Note that in the files below all columns start with a capital Z. This is automatically prepended to column names by Core Data apples database framework. Column names which start with Z_ were automatically created by Core Data and hence do not appear in the schema above. The tables are connected through the first column in each table (Z_PK). This primary key links to the relevant column name in the next table. For example  the subject that entered any given glyph can be found by taking the value from the ZSUBJECT column in the glyph table and finding the matching Z_PK value in the subject table. Some questions to get you started...  What is the best model for classifying glyphs? What is the best model for classifying sequences of these glyphs? What is the best model to predict what number a glyph is before completion? How much of the glyph needs to be completed before a prediction can be made? What is the best method for interpolating between the touches in the dataset? How can a trained model be integrated into iOS apps?  CITATION REQUEST Please cite the following paper in any publications reporting on use of this dataset Philip J. Corr Guenole C. Silvestre Chris J. Bleakley Open Source Dataset and Deep Learning Models for Online Digit Gesture Recognition on Touchscreens Irish Machine Vision and Image Processing Conference (IMVIP) 2017 Maynooth Ireland 30 August-1 September 2017 http//arxiv.org/abs/1709.06871,CSV,,[],CC0,,,101,1510,86,The numbers 0-9 drawn by 257 people,Numeral Gestures recorded on iOS,https://www.kaggle.com/corrphilip/numeral-gestures,Thu Aug 24 2017
,Chris Crawford,[],[],"Context This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques such as text classification and text clustering. Content There is file (list.csv) that contains a reference to the document_id number and the newsgroup it is associated with. There are also 20 files that contain all of the documents one document per newsgroup. In this dataset duplicate messages have been removed and the original messages only contain ""From"" and ""Subject"" headers (18828 messages total). Each new message in the bundled file begins with these four headers Newsgroup alt.newsgroup Document_id xxxxxx From  Cat Subject  Meow Meow Meow The Newsgroup and Document_id can be referenced against list.csv Organization - Each newsgroup file in the bundle represents a single newsgroup - Each message in a file is the text of some newsgroup document that was posted to that newsgroup. This is a list of the 20 newsgroups  comp.graphics comp.os.ms-windows.misc comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x    rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey  sci.crypt sci.electronics sci.med sci.space misc.forsale  talk.politics.misc talk.politics.guns talk.politics.mideast talk.religion.misc alt.atheism soc.religion.christian  Acknowledgements Ken Lang is credited by the source for collecting this data. The source of the data files is here  http//qwone.com/~jason/20Newsgroups/ Inspiration  This dataset text can be used to classify text documents ",Other,,"[linguistics, internet]",Other,,,257,3139,69,"A collection of ~18,000 newsgroup documents from 20 different newsgroups",20 Newsgroups,https://www.kaggle.com/crawford/20-newsgroups,Thu Jul 27 2017
,Ben Hamner,"[id, name]","[numeric, string]",Neural Information Processing Systems (NIPS) is one of the top machine learning conferences in the world. It covers topics ranging from deep learning and computer vision to cognitive science and reinforcement learning.  This dataset includes the title authors abstracts and extracted text for all NIPS papers to date (ranging from the first 1987 conference to the current 2016 conference). I've extracted the paper text from the raw PDF files and are releasing that both in CSV files and as a SQLite database. The code to scrape and create this dataset is on GitHub. Here's a quick RMarkdown exploratory overview of what's in the data.  We encourage you to explore this data and share what you find through Kaggle Kernels!,CSV,,"[linguistics, artificial intelligence]",ODbL,,,1534,20832,142,"Titles, authors, abstracts, and extracted text for all NIPS papers (1987-2017)",NIPS Papers,https://www.kaggle.com/benhamner/nips-papers,Wed Dec 06 2017
,Mukarram Pasha,"[Business Name, Contact Name, Telephone, Website, Services, Address, City]","[string, string, string, string, string, string, string]",Context I created this dataset to enable everyone to explore local businesses of Pakistan. This dataset might help the local community in gathering information of local businesses. This might also help in local economic development of Pakistan by bridging traders and manufacturers. Content Geography Pakistan Time period 1990-2017 Dataset The dataset contains information of approx 67000 businesses in Pakistan (~5000 in each csv file) Features  The dataset has total 7 columns  - Business Name   - Contact Name   - Telephone   - Website  - Services (Description of types of products/services provided by the business)  - Address  - City Acknowledgements This Dataset was created by scraping this website. I wrote the script in Python using BeautifulSoup Library. Link to script https//tinyurl.com/ybb4bdky Inspiration A lot of questions can be answered and analysis can be done using this dataset. Few interesting ideas I can think of are  - Applying NLP techniques on Services column to extract business category - Clustering of categories of business according to cities,CSV,,"[information, business, product]",ODbL,,,31,810,9,Information of Local Business of Pakistan,Yellow Pages of Pakistan,https://www.kaggle.com/mpasha96/yellow-pages-of-pakistan,Wed Dec 13 2017
,United States Air Force,"[Mission ID, Mission Date, Theater of Operations, Country, Air Force, Unit ID, Aircraft Series, Callsign, Mission Type, Takeoff Base, Takeoff Location, Takeoff Latitude, Takeoff Longitude, Target ID, Target Country, Target City, Target Type, Target Industry, Target Priority, Target Latitude, Target Longitude, Altitude (Hundreds of Feet), Airborne Aircraft, Attacking Aircraft, Bombing Aircraft, Aircraft Returned, Aircraft Failed, Aircraft Damaged, Aircraft Lost, High Explosives, High Explosives Type, High Explosives Weight (Pounds), High Explosives Weight (Tons), Incendiary Devices, Incendiary Devices Type, Incendiary Devices Weight (Pounds), Incendiary Devices Weight (Tons), Fragmentation Devices, Fragmentation Devices Type, Fragmentation Devices Weight (Pounds), Fragmentation Devices Weight (Tons), Total Weight (Pounds), Total Weight (Tons), Time Over Target, Bomb Damage Assessment, Source ID]","[numeric, dateTime, string, string, string, string, string, string, numeric, string, string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, string, numeric]",Content This dataset consists of digitized paper mission reports from WWII. Each record includes the date conflict geographic location and other data elements to form a live-action sequence of air warfare from 1939 to 1945. The records include U.S. and Royal Air Force data in addition to some Australian New Zealand and South African air force missions. Acknowledgements Lt Col Jenns Robertson of the US Air Force developed the Theater History of Operations Reports (THOR) and posted them online after receiving Department of Defense approval.,CSV,,"[history, war]",CC0,,,430,3659,27,"Target, aircraft used, and bombs deployed for every mission in WWII",Aerial Bombing Operations in World War II,https://www.kaggle.com/usaf/world-war-ii,Tue Jan 31 2017
,City of Chicago,"[Name, Job Titles, Department, Full or Part-Time, Salary or Hourly, Typical Hours, Annual Salary, Hourly Rate]","[string, string, string, string, string, string, string, string]",Context This dataset contains the name job title department and salary of every employee that was on the City of Chicago payroll at the time of capture in mid-2017. It provides a transparent lens into who gets paid how much and for what. Content This dataset provides columns for employee name the city department they work for their job title and various fields describing their compensation. Most employee salaries are covered by the Annual Salary field but some employees paid hourly are covered by a combination of Typical Hours and Hourly Rate fields. Acknowledgements This dataset is published as-is by the City of Chicago (here). Inspiration  How many people do the various city agencies employ and how much does each department spend on salary in total? What are the most numerous job titles in civic government employment? How do Chicago employee salaries compare against salaries of city employees in New York City? Is the difference more or less than the difference in cost of living between the two cities? ,CSV,,"[cities, money]",CC0,,,192,1349,2,Salaries paid to Chicago employees,Chicago - Citywide Payroll Data,https://www.kaggle.com/chicago/chicago-citywide-payroll-data,Wed Sep 13 2017
,PyTorch,[],[],VGG-19  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,2,237,509,VGG-19 Pre-trained model with batch normalization for PyTorch,VGG-19 with batch normalization,https://www.kaggle.com/pytorch/vgg19bn,Sat Dec 16 2017
,Rachael Tatman,"[wals_code, iso_code, glottocode, Name, latitude, longitude, genus, family, macroarea, countrycodes, 1A Consonant Inventories, 2A Vowel Quality Inventories, 3A Consonant-Vowel Ratio, 4A Voicing in Plosives and Fricatives, 5A Voicing and Gaps in Plosive Systems, 6A Uvular Consonants, 7A Glottalized Consonants, 8A Lateral Consonants, 9A The Velar Nasal, 10A Vowel Nasalization, 11A Front Rounded Vowels, 12A Syllable Structure, 13A Tone, 14A Fixed Stress Locations, 15A Weight-Sensitive Stress, 16A Weight Factors in Weight-Sensitive Stress Systems, 17A Rhythm Types, 18A Absence of Common Consonants, 19A Presence of Uncommon Consonants, 20A Fusion of Selected Inflectional Formatives, 21A Exponence of Selected Inflectional Formatives, 22A Inflectional Synthesis of the Verb, 23A Locus of Marking in the Clause, 24A Locus of Marking in Possessive Noun Phrases, 25A Locus of Marking: Whole-language Typology, 26A Prefixing vs. Suffixing in Inflectional Morphology, 27A Reduplication, 28A Case Syncretism, 29A Syncretism in Verbal Person/Number Marking, 30A Number of Genders, 31A Sex-based and Non-sex-based Gender Systems, 32A Systems of Gender Assignment, 33A Coding of Nominal Plurality, 34A Occurrence of Nominal Plurality, 35A Plurality in Independent Personal Pronouns, 36A The Associative Plural, 37A Definite Articles, 38A Indefinite Articles, 39A Inclusive/Exclusive Distinction in Independent Pronouns, 40A Inclusive/Exclusive Distinction in Verbal Inflection, 41A Distance Contrasts in Demonstratives, 42A Pronominal and Adnominal Demonstratives, 43A Third Person Pronouns and Demonstratives, 44A Gender Distinctions in Independent Personal Pronouns, 45A Politeness Distinctions in Pronouns, 46A Indefinite Pronouns, 47A Intensifiers and Reflexive Pronouns, 48A Person Marking on Adpositions, 49A Number of Cases, 50A Asymmetrical Case-Marking, 51A Position of Case Affixes, 52A Comitatives and Instrumentals, 53A Ordinal Numerals, 54A Distributive Numerals, 55A Numeral Classifiers, 56A Conjunctions and Universal Quantifiers, 57A Position of Pronominal Possessive Affixes, 58A Obligatory Possessive Inflection, 59A Possessive Classification, 60A Genitives, Adjectives and Relative Clauses, 61A Adjectives without Nouns, 62A Action Nominal Constructions, 63A Noun Phrase Conjunction, 64A Nominal and Verbal Conjunction, 65A Perfective/Imperfective Aspect, 66A The Past Tense, 67A The Future Tense, 68A The Perfect, 69A Position of Tense-Aspect Affixes, 70A The Morphological Imperative, 71A The Prohibitive, 72A Imperative-Hortative Systems, 73A The Optative, 74A Situational Possibility, 75A Epistemic Possibility, 76A Overlap between Situational and Epistemic Modal Marking, 77A Semantic Distinctions of Evidentiality, 78A Coding of Evidentiality, 79A Suppletion According to Tense and Aspect, 80A Verbal Number and Suppletion, 81A Order of Subject, Object and Verb, 82A Order of Subject and Verb, 83A Order of Object and Verb, 84A Order of Object, Oblique, and Verb, 85A Order of Adposition and Noun Phrase, 86A Order of Genitive and Noun, 87A Order of Adjective and Noun, 88A Order of Demonstrative and Noun, 89A Order of Numeral and Noun, 90A Order of Relative Clause and Noun]","[string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context There are over 7000 human languages in the world. The World Atlas of Language Structures (WALS) contains information on the structure of 2679 of them. It also includes information about where languages are used. WALS is widely-cited and used in the linguistics research community. Content The World Atlas of Language Structures (WALS) is a large database of structural (phonological grammatical lexical) properties of languages gathered from descriptive materials (such as reference grammars) by a team of 55 authors. The atlas provides information on the location linguistic affiliation and basic typological features of a great number of the world's languages WALS Online is a publication of the (Max Planck Institute for Evolutionary Anthropology)[http//www.eva.mpg.de/]. It is a separate publication edited by Dryer Matthew S. & Haspelmath Martin (Leipzig Max Planck Institute for Evolutionary Anthropology 2013) The main programmer is Robert Forkel. This dataset includes three files  source.bib A BibTex file with all of the sources cited in the dataset in it language.csv A file with a list of all the languages included in WALS wals-data.csv A file containing information on the features associated with each individual language  Acknowledgements This dataset is licensed under a Creative Commons Attribution 4.0 International License . The World Atlas of Language Structures was edited by Matthew Dryer and Martin Haspelmath. If you use this data in your work please include the following citation  Dryer Matthew S. & Haspelmath Martin (eds.) 2013. The World Atlas of Language Structures Online. Leipzig Max Planck Institute for Evolutionary Anthropology. (Available online at http//wals.info Accessed on September 7 2017.) Inspiration  This dataset was designed to make interactive maps of language features. Can you make an interactive map that shows different linguistic features? You might find it helpful to use Leaflet (for R) or Plotly (for Python). This blog post is a great resource to help you get started. There’s a lot of discussion of “linguistic universals” in linguistics. These are specific features that every language (should) have. Can you identify any features that you think may be universals from this dataset?  You may also like  Atlas of Pidgin and Creole Language Structures Information on 76 Creole and Pidgin Languages World Language Family Map The Sign Language Analyses (SLAY) Database ,CSV,,"[languages, linguistics]",Other,,,210,2491,13,"Information on the linguistic structures in 2,679  languages",World Atlas of Language Structures,https://www.kaggle.com/rtatman/world-atlas-of-language-structures,Fri Sep 08 2017
,toby jolly,"[#_llins, location, country, when, by_whom, country_code]","[numeric, string, string, dateTime, string, string]",Context The data here is from the Global Health Observatory (GHO) who provide data on malaria incidence death and prevention from around the world. I have also included malaria net distribution data the Against Malaria Foundation (AMF). The AMF has consistently been ranked as the most cost effective charity by charity evaluators Give Well - http//www.givewell.org/charities/top-charities Content GHO data is all in narrow format with variables for a country in a given year being found on different rows.  GHO data (there are a number or superfluous columns)  GHO (CODE) GHO (DISPLAY) - this is the variable being measured GHO (URL) PUBLISHSTATE (CODE) PUBLISHSTATE (DISPLAY) PUBLISHSTATE (URL) YEAR (CODE) YEAR (DISPLAY) YEAR (URL) REGION (CODE) REGION (DISPLAY) REGION (URL) COUNTRY (CODE) - can be used to join this data with the AMF data COUNTRY (DISPLAY) COUNTRY (URL) Display Value - this is the measured value Low - lower confidence interval High - higher confidence interval Comments  AMF distribution data  #_llins - total number of malaria nets distributed location - the specific area that received the nets within the target country country - the country in which the nets were distributed when - the period the distribution  by_whom - the organisation(s) which partnered with the AMF to perform the distribution country_code - the country's GHO country code (this will allow joining with the GHO data)  For the current version all data was downloaded 20-08-17 The GHO data covers the years from 2000 to 2015 (not all files have data in all years) The AMF data runs from 2006 - the present. The GHO data is taken as is from the csv (lists) available here http//apps.who.int/gho/data/node.main.A1362?lang=en The source of the AMF's distribution data is here https//www.againstmalaria.com/distributions.aspx - it was assembled into a single csv using Excel (mea culpa) Inspiration Malaria is one of the world's most devastating diseases not least because it largely affects some of the poorest people. Over the past 15 years malaria rates and mortality have dropped (http//www.who.int/malaria/media/world-malaria-report-2016/en/) but there is still a long way to go. Understanding the data is generally one of the most important steps in solving any large problem. I'm excited to see what the Kaggle community can find out about the global trends in malaria over this period and if we can find out anything about the impact of organisations such as the AMF.,CSV,,[public health],CC0,,,362,2455,7,Who is dying and being saved from this destructive disease?,The fight against malaria,https://www.kaggle.com/teajay/the-fight-against-malaria,Tue Aug 22 2017
,criticalhits,"[EVENT CATEGORY	EVENT GROUP	EVENT SUBGROUP	EVENT TYPE	PLACE	EVENT START DATE	COMMENTS	FATALITIES	INJURED / INFECTED	EVACUATED	ESTIMATED TOTAL COST	NORMALIZED TOTAL COST	EVENT END DATE	FEDERAL DFAA PAYMENTS	PROVINCIAL DFAA PAYMENTS	PROVINCIAL DEPARTMENT PAY, EVENT GROUP, EVENT SUBGROUP, EVENT TYPE, PLACE, EVENT START DATE, COMMENTS, FATALITIES, INJURED / INFECTED, EVACUATED, ESTIMATED TOTAL COST, NORMALIZED TOTAL COST, EVENT END DATE, FEDERAL DFAA PAYMENTS, PROVINCIAL DFAA PAYMENTS, PROVINCIAL DEPARTMENT PAYMENTS, MUNICIPAL COSTS, OGD COSTS, INSURANCE PAYMENTS, NGO PAYMENTS, UTILITY - PEOPLE AFFECTED, MAGNITUDE]","[string, string, string, string, string, dateTime, string, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context The Canadian Disaster Database The Canadian Disaster Database (CDD) contains detailed disaster information on more than 1000 natural technological and conflict events (excluding war) that have happened since 1900 at home or abroad and that have directly affected Canadians.  Content Data description copied from https//www.publicsafety.gc.ca/cnt/rsrcs/cndn-dsstr-dtbs/index-en.aspx Dataset date range 1900 - present The CDD tracks ""significant disaster events"" which conform to the Emergency Management Framework for Canada definition of a ""disaster"" and meet one or more of the following criteria  10 or more people killed   100 or more people affected/injured/infected/evacuated or homeless   an appeal for national/international assistance   historical significance   significant damage/interruption of normal processes such that the community affected cannot recover on its own    The database describes where and when a disaster occurred the number of injuries evacuations and fatalities as well as a rough estimate of the costs.  As much as possible the CDD contains primary data that is valid current and supported by reliable and traceable sources including federal institutions provincial/territorial governments non-governmental organizations and media sources.   Data is updated and reviewed on a semi-annual basis. Data Field Description      Disaster Type The type of disaster (e.g. flood earthquake etc.) that occurred.  Date of Event The date a specific event took place.  Specific Location The city town or region where a specific event took place.  Description of Event A brief description of a specific event including pertinent details that may not be captured in other data fields (e.g. amount of precipitation temperatures neighbourhoods etc.)  Fatalities The number of people killed due to a specific event.  Injured/Infected The number of people injured or infected due to a specific event.  Evacuees The number of individuals evacuated by the government of Canada due to a specific event.  Latitude & Longitude The exact geographic location of a specific event.  Province/Territory The province or territory where a specific event took place.  Estimated Total Cost A roll-up of all the costs listed within the financial data fields for a specific event.  DFAA Payments The amount in dollars paid out by Disaster Financial Assistance Arrangements (Public Safety Canada) due to a specific event.  Insurance Payments The amount in dollars paid out by insurance companies due to a specific event.  Provincial/Territorial Costs/Payments The amount in dollars paid out by a Province or Territory due to a specific event.  Utility Costs/Losses The amount of people whose utility services (power water etc.) were interrupted/affected by a specific event.  Magnitude A measure of the size of an earthquake related to the amount of energy released.  Other Federal Institution Costs The amount in dollars paid out by other federal institutions.     Acknowledgements Data gathered from http//cdd.publicsafety.gc.ca Terms of use for commercial and non-comerical reproduction https//www.publicsafety.gc.ca/cnt/ntcs/trms-en.aspx Inspiration This dataset provides valuable insight to natural and non-natrual disasters which have affected Canada.  Possible explorations * Where do different types of disasters occur more frequently?  * Which Province / Location in Canada has been hit the hardest in terms of fatalities number of injuries estimated total cost etc.? Spatial-temporal correlations between natural/artifical distasters *  I think that this can be used to produce some interesting data visualizations. Some of the questions I look forward to answering include  Can any spatial-temporal correlations between disasters be found in this dataset? Which locations in Canada have been hit the hardest in terms of people injured fatalities financial impact etc. ",CSV,,"[north america, time series, geography]",Other,,,285,2140,2,Over 1000 Disasters Affecting Canadians At-Home or Abroad Since 1900,Canadian Disaster Database,https://www.kaggle.com/criticalhits/canadian-disaster-database,Sat Sep 30 2017
,Stanford Open Policing Project,[],[],Context On a typical day in the United States police officers make more than 50000 traffic stops. The Stanford Open Policing Project team is gathering analyzing and releasing records from millions of traffic stops by law enforcement agencies across the country. Their goal is to help researchers journalists and policymakers investigate and improve interactions between police and the public. If you'd like to see data regarding other states please go to https//www.kaggle.com/stanford-open-policing. Content This dataset includes over 1 gb of stop data from Ohio. Please see the data readme for the full details of the available fields. Acknowledgements This dataset was kindly made available by the Stanford Open Policing Project. If you use it for a research publication please cite their working paper E. Pierson C. Simoiu J. Overgoor S. Corbett-Davies V. Ramachandran C. Phillips S. Goel. (2017) “A large-scale analysis of racial disparities in police stops across the United States”. Inspiration  How predictable are the stop rates? Are there times and places that reliably generate stops? Concerns have been raised about jurisdictions using civil forfeiture as a funding mechanism rather than to properly fight drug trafficking. Can you identify any jurisdictions that may be exhibiting this behavior? ,Other,,"[government agencies, crime, law]",Other,,,72,754,988,Data on Traffic and Pedestrian Stops by Police in Ohio,Stanford Open Policing Project - Ohio,https://www.kaggle.com/stanford-open-policing/stanford-open-policing-project-ohio,Tue Jul 25 2017
,Sohier Dane,"[AADFYear, CP, Estimation_method, Estimation_method_detailed, ONS GOR Name, ONS LA Name, Road, RCat, iDir, S Ref E, S Ref N, S Ref Latitude, S Ref Longitude, A-Junction, B-Junction, LenNet, LenNet_miles, FdPC, Fd2WMV, FdCar, FdBUS, FdLGV, FdHGVR2, FdHGVR3, FdHGVR4, FdHGVA3, FdHGVA5, FdHGVA6, FdHGV, FdAll_MV]","[numeric, numeric, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Data are available for each junction to junction link on the major road network (motorways and A roads). Data are also available for the sample of points on the minor road network (B C and unclassified roads) that are counted each year and these counts are used to produce estimates of traffic growth on minor roads. The data are produced for every year and are in three formats a) the raw manual count data collected by trained enumerators; b) Annual Average Daily Flows (AADFs) for count points on major roads and minor roads; and c) traffic figures for major roads only. Explanatory notes (metadata) are available for each dataset and in one combined note. A description of how annual road traffic estimates are produced is available at https//www.gov.uk/government/uploads/system/uploads/attachment_data/file/270083/contents-page.pdf This dataset was kindly released by the British Department of Transportation. You can find the original dataset here.,CSV,,[road transport],Other,,,268,2012,988,Details of traffic in England and Wales,UK Traffic Counts,https://www.kaggle.com/sohier/uk-traffic-counts,Wed Aug 30 2017
,Chippy,"[key, id, lat, lon, tolerance_m]","[string, numeric, numeric, numeric, numeric]",Context This data set was created for use in the Sberbank Kaggle competition.  Content The data consists of three GIS shapefiles one for each of the 3 major Moscow ring roads; the MKAD TTK (or third ring) and Sadovoe (or garden ring). Acknowledgements The road shapefiles have been extracted from OpenStreetMap data processed in QGiS to extract only the roads of interest.    OpenStreetMap License https//www.OpenStreetMap.org/copyright  Inspiration With these files and the distances given in the Sberbank dataset it should be possible to better understand the location of properties. With a better understanding of location it may be possible to improve the quality of the overall dataset which contains material amounts of missing or poorly coded data.,CSV,,"[russia, geography]",ODbL,,,1136,3262,3,Shapefiles for use in Sberbank,Moscow Ring Roads,https://www.kaggle.com/nigelcarpenter/sberbankmoscowroads,Sat May 27 2017
,TheScientistBR,[],[],"Context The newspaper publications on the Internet increases every day. There are many news agencies newspapers and magazines with digital publications on the big network. Published documents made available to users who in turn use search engines to find them. To deliver the closest searched documents these documents must be previously indexed and classified. With the huge volume of documents published every day many researches have been carried out in order to find a way of dealing with the automatic document classification.  Content The ""Tribuna"" database is of journalistic origin with its digital publication a factor that may be important for professionals of the area also serving to understand other similar datasets. In order to carry out the experiment we adopted the ""A Tribuna"" database whose main characteristics presented previously show that the collection is a good source of research since it is already classified by specialists and has 21 classes that can be Displayed in the table below. Acknowledgements My thanks to the company ""A Tribuna"" that gave all these text files for experiment at the Federal University of Espírito Santo. To the High Desermpenho Computation Laboratory (LCAD) for all the help in the experiments. Thanks also to Prof. PhD Oliveira Elias for all the knowledge shared. Inspiration There are two issues involving this dataset  What is the best algorithm for sorting these documents? What are the elements that describe each of the 21 classes in the collection?",Other,,"[news agencies, internet]",Other,,,86,1540,134,Journalistic documents published on the Internet,A Tribuna,https://www.kaggle.com/TheScientistBR/atribuna,Thu Mar 09 2017
,ugocupcic,"[experiment_number,  robustness,  H1_F1J2_pos ,  H1_F1J2_vel ,  H1_F1J2_eff ,  H1_F1J3_pos ,  H1_F1J3_vel ,  H1_F1J3_eff ,  H1_F1J1_pos ,  H1_F1J1_vel ,  H1_F1J1_eff ,  H1_F3J1_pos ,  H1_F3J1_vel ,  H1_F3J1_eff ,  H1_F3J2_pos ,  H1_F3J2_vel ,  H1_F3J2_eff ,  H1_F3J3_pos ,  H1_F3J3_vel ,  H1_F3J3_eff ,  H1_F2J1_pos ,  H1_F2J1_vel ,  H1_F2J1_eff ,  H1_F2J3_pos ,  H1_F2J3_vel ,  H1_F2J3_eff ,  H1_F2J2_pos ,  H1_F2J2_vel ,  H1_F2J2_eff ,  measurement_number]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context At Shadow Robot we are leaders in robotic grasping and manipulation. As part of our Smart Grasping System development we're developing different algorithms using machine learning.  This first public dataset was created to investigate the use of machine learning to predict the stability of a grasp. Due to the limitations of the current simulation it is a restricted dataset - only grasping a ball. The dataset is annotated with an objective grasp quality and contains the different data gathered from the joints (position velocity effort). You can find all the explanations for this dataset over on Medium. Inspiration I'll be more than happy to discuss this dataset as well as which dataset you'd like to have to try your hands at solving real world robotic problems focused on grasping using machine learning. Let's connect on twitter (@ugocupcic)!,CSV,,[robotics],GPL,,,116,2863,485,A grasping dataset from simulation using Shadow Robot's Smart Grasping Sandbox,Grasping Dataset,https://www.kaggle.com/ugocupcic/grasping-dataset,Mon Sep 11 2017
,Albert Costas,"[Data, Tipus, Nom, Simbol, Preu (Euros), Tipus_cotitzacio]","[dateTime, string, string, string, numeric, string]",«Datasets per la comparació de moviments i patrons entre els principals índexs borsatils espanyols i les crypto-monedes» Context En aquest cas el context és detectar o preveure els diferents moviments que es produeixen per una serie factors tant de moviment interns (compra-venda) com externs (moviments polítics econòmics etc...) en els principals índexs borsatils espanyols i de les crypto-monedes. Hem seleccionat diferents fonts de dades per generar fitxers «csv» guardar diferents valors en el mateix període de temps. És important destacar que ens interessa més les tendències alcistes o baixes que podem calcular o recuperar en aquests períodes de temps. Content En aquest cas el contingut està format per diferents csv especialment tenim els fitxers de moviments de cryptomoneda els quals s’ha generat un fitxer per dia del període de temps estudiat. Pel que fa als moviments del principals índexs borsatils s’ha generat una carpeta per dia del període en cada directori un fitxer amb cadascun del noms dels índexs. Degut això s’han comprimit aquests últims abans de publicar-los en el directori de «open data» kaggle.com. Pel que fa als camps ens interessà detectar els moviments alcistes i baixistes o almenys aquelles que tenen un patró similar en les cryptomonedes i els índexs. Els camps especialment destacats són • Data Data de la observació • Nom Nom empresa o cryptomoneda per identificar de quina moneda o index estem representant. • Símbol Símbol de la moneda o del index borsatil per realitzar gràfic posteriorment d’una forma mes senzilla que el nom. • Preu Valor en euros d’una acció o una cryptomoneda (transformarem la moneda a euros en el cas de estigui en dòlars amb l'última cotització (un dollar a 08501 euro) • Tipus_cotitzacio Valor nou que agregarem per discretitzar entre la cotització baix (0 i 1) normal (1 i 100) alt (100 i 1000) molt_alt (&gt;1000)  Script R  Anàlisis de les observacions i el domini de les dades Anàlisis en especial de Bitcoin i la IOTA. Test de Levene per veure la homogeneitat Kmeans per creació de cluster per veure la homegeneitat Freqüències de les distribucions Test de contrast d'hipòtesis de variables dependents (Wilcoxon) Test de Shapiro-Wilk per veure la normalitat de les dades per normalitzar-les o no Correlació d'índexs borsatils per eliminar complexitat dels índexs amb grau més alt de correlació Iteració de Regressions lineals per obtenir el model amb més qualitat observa'n el p-valor i l'índex de correlació Validació de la qualitat del model Representació grafica  Acknowledgements En aquest cas les fonts de dades que s’han utilitzat per a la realització dels datasets corresponent a  http//www.eleconomista.es https//coinmarketcap.com  Per aquest fet les dades de borsa i crypto-moneda estan en última instància sota llicència de les webs respectivament. Pel que fa a la terminologia financera podem veure vocabulari en renta4banco.  [https//www.r4.com/que-necesitas/formacion/diccionario] Inspiration Hi ha un estudi anterior on poder tenir primícies de com han enfocat els algoritmes       https//arxiv.org/pdf/1410.1231v1.pdf  En aquest cas el «trading» en cryptomoneda és relativament nou força popular per la seva formulació com a mitja digital d’intercanvi utilitzant un protocol que garanteix la seguretat integritat i equilibri del seu estat de compte per mitjà d’un entramat d’agents. La comunitat podrà respondre entre altres preguntes a  Està afectant o hi ha patrons comuns en les cotitzacions de cryptomonedes i el mercat de valors principals del país d'Espanya? Els efectes o agents externs afecten per igual a les accions o cryptomonedes?  Hi ha relacions cause efecte entre les acciones i cryptomonedes?  Project repository https//github.com/acostasg/scraping Datasets Els fitxers csv generats que componen el dataset s’han publicat en el repositori kaggle.com  https//www.kaggle.com/acostasg/stock-index/  https//www.kaggle.com/acostasg/crypto-currencies  Per una banda els fitxers els «stock-index» estan comprimits per carpetes amb la data d’extracció i cada fitxer amb el nom dels índexs borsatil.  De forma diferent les cryptomonedes aquestes estan dividides per fitxer on són totes les monedes amb la data d’extracció.,CSV,,"[time series, money, economics]",ODbL,,,145,1645,0.6494140625,Relation and patterns between movements of stock exchange indexes and cryptocurrency,Analysis about crypto currencies and Stock Index,https://www.kaggle.com/acostasg/cryptocurrenciesvsstockindex,Thu Dec 14 2017
,Chris Crawford,"[Model, Release date, Max resolution, Low resolution, Effective pixels, Zoom wide (W), Zoom tele (T), Normal focus range, Macro focus range, Storage included, Weight (inc. batteries), Dimensions, Price]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Some camera enthusiast went and described 1000 cameras based on 13 properties!  Content Row one describes the datatype for each column and can probably be removed. The 13 properties of each camera  Model Release date Max resolution Low resolution Effective pixels Zoom wide (W) Zoom tele (T) Normal focus range Macro focus range Storage included Weight (inc. batteries) Dimensions Price  Acknowledgements These datasets have been gathered and cleaned up by Petra Isenberg Pierre Dragicevic and Yvonne Jansen.  The original source can be found here. This dataset has been converted to CSV.,CSV,,[],CC3,,,654,3679,0.0830078125,Data describing 1000 cameras in 13 properties,1000 Cameras Dataset,https://www.kaggle.com/crawford/1000-cameras-dataset,Tue Oct 24 2017
,Centers for Disease Control and Prevention,"[Year, Month, State, Location, Food, Ingredient, Species, Serotype/Genotype, Status, Illnesses, Hospitalizations, Fatalities]","[numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric]","Context Next time you take a bite consider this roughly one in six (or 48 million) people in the United States get sick from eating contaminated food per year. More than 250 pathogens and toxins have been known to cause foodborne illness and almost all of them can cause an outbreak. A foodborne disease outbreak occurs when two or more people get the same illness from the same contaminated food or drink. While most foodborne illnesses are not part of a recognized outbreak outbreaks provide important information on how germs spread which foods cause illness and how to prevent infection. Public health agencies in all 50 states the District of Columbia U.S. territories and Freely Associated States have primary responsibility for identifying and investigating outbreaks and use a standard form to report outbreaks voluntarily to CDC. During 1998–2008 reporting was made through the electronic Foodborne Outbreak Reporting System (eFORS). Content This dataset provides data on foodborne disease outbreaks reported to CDC from 1998 through 2015. Data fields include year state (outbreaks occurring in more than one state are listed as ""multistate"") location where the food was prepared reported food vehicle and contaminated ingredient etiology (the pathogen toxin or chemical that caused the illnesses) status (whether the etiology was confirmed or suspected) total illnesses hospitalizations and fatalities. In many outbreak investigations a specific food vehicle is not identified; for these outbreaks the food vehicle variable is blank. Inspiration Are foodborne disease outbreaks increasing or decreasing? What contaminant has been responsible for the most illnesses hospitalizations and deaths? What location for food preparation poses the greatest risk of foodborne illness?",CSV,,"[food and drink, public health]",CC0,,,1037,6495,1,What contaminant has caused the most hospitalizations and fatalities?,"Foodborne Disease Outbreaks, 1998-2015",https://www.kaggle.com/cdc/foodborne-diseases,Wed Feb 15 2017
,United Nations,[],[],Context The mass movement of uprooted people is a highly charged geopolitical issue. This data gathered by the UN High Commissioner for Refugees (UNHCR) covers movement of displaced persons (asylum seekers refugees internally displaced persons (IDP) stateless). Also included are destination country responses to asylum petitions. Content This dataset includes 6 csv files covering  Asylum monthly applications opened (asylum_seekers_monthly.csv) Yearly progress through the refugee system (asylum_seekers.csv)  Refugee demographics (demographics.csv) Yearly time series data on UNHCR’s populations of concern (time_series.csv) Yearly population statistics on refugees by residence and destination (persons_of_concern.csv) Yearly data on resettlement arrivals with or without UNHCR assistance (resettlement.csv)  Acknowledgements This dataset was gathered from UNHCR. Photo by Ali Tareq. Inspiration What are the most frequent destination countries for refugees? How has refugee flow changed? Any trends that could predict future refugee patterns?,CSV,,"[politics, demographics]",CC4,,,714,3778,40,Data on Uprooted Populations and Asylum Processing,UNHCR Refugee Data,https://www.kaggle.com/unitednations/refugee-data,Tue Aug 01 2017
,"AndrewMalinow, PhD",[],[],What is the world saying about Donald Trump? Find out in this dataset of over 37000 Reddit comments about the new US president. Photo credit Gage Skidmore CC BY-SA 2.0,CSV,,"[politics, linguistics, internet]",CC4,,,444,5118,22,"Over 183,000 full text comments with post metadata",Donald Trump Comments on Reddit,https://www.kaggle.com/amalinow/donald-trump-comments-on-reddit,Thu Aug 31 2017
,UCI Machine Learning,"[7VvBnz1Ngi4, 1Y-Au-tnBLs, right]","[string, string, string]",Context This dataset provides user vote data on which video from a pair of videos was funnier. YouTube Comedy Slam was a discovery experiment running on YouTube 2011 and 2012. In the experiment pairs of videos were shown to users and the users voted for the video that they found funniest.  Content The datasets includes roughly 1.7 million votes recorded chronologically. The first 80% are provided here as the training dataset and the remaining 20% as the testing dataset.  Each row in this text file represents one anonymous user vote and there are three comma-separated fields.   The first two fields are YouTube video IDs.  The third field is either 'left' or 'right'.  Left indicates the first video from the pair was voted to be funnier than the second. Right indicates the opposite preference.  Acknowledgements Sanketh Shetty 'Quantifying comedy on YouTube why the number of o's in your LOL matter' Google Research Blog https//research.googleblog.com/2012/02/quantifying-comedy-on-youtube-why.html. Dataset was downloaded from UCI ML repository https//archive.ics.uci.edu/ml/datasets/YouTube+Comedy+Slam+Preference+Data Inspiration Predict which videos are going to be funny!,CSV,,[humor],CC0,,,86,1380,32,Votes for the funniest videos,YouTube Comedy Slam,https://www.kaggle.com/uciml/youtube-comedy-slam,Wed Sep 20 2017
,David Skipper Everling,"[, product_title, ships_from_to, grams, quality, btc_price, cost_per_gram, cost_per_gram_pure, escrow, product_link, vendor_link, vendor_name, successful_transactions, rating, ships_from, ships_to, ships_to_US, ships_from_US, ships_to_NL, ships_from_NL, ships_to_FR, ships_from_FR, ships_to_GB, ships_from_GB, ships_to_CA, ships_from_CA, ships_to_DE, ships_from_DE, ships_to_AU, ships_from_AU, ships_to_EU, ships_from_EU, ships_to_ES, ships_from_ES, ships_to_N. America, ships_from_N. America, ships_to_BE, ships_from_BE, ships_to_WW, ships_from_WW, ships_to_SI, ships_from_SI, ships_to_IT, ships_from_IT, ships_to_DK, ships_from_DK, ships_to_S. America, ships_from_S. America, ships_to_CH, ships_from_CH, ships_to_BR, ships_from_BR, ships_to_CZ, ships_from_CZ, ships_to_SE, ships_from_SE, ships_to_CO, ships_from_CO, ships_to_CN, ships_from_CN, ships_to_PL, ships_from_PL, ships_to_GR, ships_from_GR]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, string, string, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean, boolean]","The dataset is approximately 1400 cleaned and standardized product listings from Dream Market's ""Cocaine"" category. It was collected with web-scraping and text extraction techniques in July 2017. Extracted features for each listing include  product_title ships_from_to quantity in grams quality btc_price vendor details shipping dummy variables (true/false columns)  For further details on the creation of this dataset and what it contains see the blog post here https//medium.com/thought-skipper/dark-market-regression-calculating-the-price-distribution-of-cocaine-from-market-listings-10aeff1e89e0",CSV,,"[crime, illegal drugs, economics, internet]",CC4,,,253,4094,0.76953125,How much does cocaine cost on the internet?,Darknet Market Cocaine Listings,https://www.kaggle.com/everling/cocaine-listings,Thu Nov 23 2017
,City of New York,"[Domain Name , Domain Registration Date , Nexus Category]","[string, dateTime, string]",Context Every website registered as a .nyc domain between the period of March 20 2014 when the domain was acquired by the City of New York and August 31 2017. Content The data includes the domain name registered the date the name was registered and the domain classification type. The .nyc domain has been a focus of New York City’s smart city branding ever since its acquisition in 2014. Customers and businesses local to the city have been encouraged to register new websites with a “.nyc” ending under the aegis of the Own It NYC marketing campaign. As a recent and very popular greenfield domain name the .nyc registrations dataset provide a lens into what constitutes a “high value domain” on today's web. Acknowledgements This dataset is published as-is by the New York City Department of Information Technology and Telecommunications. Inspiration  What is the split between individuals and organizations registering a .nyc domain? What common words or text strings were registered first? Is there evidence that these are “high-value” domains? ,CSV,,"[cities, internet]",CC0,,,26,380,3,All .nyc domain names registered with the City of New York,.nyc Domain Registrations,https://www.kaggle.com/new-york-city/dot-nyc-domain-registrations,Sat Sep 02 2017
,Stanford Network Analysis Project ,[],[],This database contains a subset of the Memetracker dataset collected by SNAP.  The full Memetracker dataset has observations broken into months. Because of size considerations however this version consists of one-half of a month the first 15 days of Memetracker observations from November 2008. About Memetracker tracks the quotes and phrases that appear most frequently over time across the entire online news spectrum. This makes it possible to see how different stories compete for news and blog coverage each day and how certain stories persist while others fade quickly. Overall Memetracker tracks more than 17 million different phrases and about 54% of the total phrase/quote mentions appear on blogs and 46% in news media. Acknowledgments This dataset was collected by the Stanford Network Analysis Project. Detailed information about the data and its analysis can be found at the website here. An analysis of this dataset was published here  J. Leskovec L. Backstrom J. Kleinberg. Meme-tracking and the Dynamics of the News Cycle. ACM SIGKDD Intl. Conf. on Knowledge Discovery and Data Mining 2009. The Data The SQLite database contains three tables articles 4542920 records with the following fields  article_id a unique id for the article (int) url the URL of the article (text) date the date of the article (text) in the strptime format '%Y-%m-%d %H%M%S'  quotes 7956125 records with the following fields  article_id unique id for the article that this quote was found in (int) phrase the high-frequency phrase found in the article (text)  links 16727125 records with the following fields  article_id unique id for the article that this link was found in (int) link_out the URL of the link out (text) link_out_id unique id for the target article (int) if it exists; else NULL ,SQLite,,"[linguistics, internet]",CC0,,,156,2880,3072,Tracking high-frequency phrases across internet news,SNAP Memetracker,https://www.kaggle.com/snap/snap-memetracker,Mon Nov 21 2016
,Federal Reserve,"[Series Description, Overnight AA Nonfinancial Commercial Paper Interest Rate, 7-Day AA Nonfinancial Commercial Paper Interest Rate, 15-Day AA Nonfinancial Commercial Paper Interest Rate, 30-Day AA Nonfinancial Commercial Paper Interest Rate, 60-Day AA Nonfinancial Commercial Paper Interest Rate, 90-Day AA Nonfinancial Commercial Paper Interest Rate, Overnight A2/P2 Nonfinancial Commercial Paper Interest Rate, 7-Day A2/P2 Nonfinancial Commercial Paper Interest Rate, 15-Day A2/P2 Nonfinancial Commercial Paper Interest Rate, 30-Day A2/P2 Nonfinancial Commercial Paper Interest Rate, 60-Day A2/P2 Nonfinancial Commercial Paper Interest Rate, 90-Day A2/P2 Nonfinancial Commercial Paper Interest Rate, Overnight AA Financial Commercial Paper Interest Rate, 7-Day AA Financial Commercial Paper Interest Rate, 15-Day AA Financial Commercial Paper Interest Rate, 30-Day AA Financial Commercial Paper Interest Rate, 60-Day AA Financial Commercial Paper Interest Rate, 90-Day AA Financial Commercial Paper Interest Rate, Overnight AA Asset-backed Commercial Paper Interest Rate, 7-Day AA Asset-backed Commercial Paper Interest Rate, 15-Day AA Asset-backed Commercial Paper Interest Rate, 30-Day AA Asset-backed Commercial Paper Interest Rate, 60-Day AA Asset-backed Commercial Paper Interest Rate, 90-Day AA Asset-backed Commercial Paper Interest Rate]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Commercial paper in the global financial market is an unsecured promissory note with a fixed maturity of not more than 270 days. Commercial paper is a money-market security issued (sold) by large corporations to obtain funds to meet short-term debt obligations (for example payroll) and is backed only by an issuing bank or company promise to pay the face amount on the maturity date specified on the note. Since it is not backed by collateral only firms with excellent credit ratings from a recognized credit rating agency will be able to sell their commercial paper at a reasonable price. Commercial paper is usually sold at a discount from face value and generally carries lower interest repayment rates than bonds due to the shorter maturities of commercial paper. Typically the longer the maturity on a note the higher the interest rate the issuing institution pays. Interest rates fluctuate with market conditions but are typically lower than banks' rates. Commercial paper – though a short-term obligation – is issued as part of a continuous rolling program which is either a number of years long (as in Europe) or open-ended (as in the U.S.) Acknowledgements This dataset was made available by the Federal Reserve. You can find the original dataset updated daily here. Inspiration  Based solely on this dataset when would you say the Great Recession financial crisis started? How does that compare with media reports? ,CSV,,"[finance, banking]",CC0,,,86,1213,1,Rates & Volumes for 1998-2017,Commercial Paper,https://www.kaggle.com/federalreserve/commercial-paper-rates,Tue Sep 19 2017
,Mike Chirico,"[lat, lng, desc, zip, title, timeStamp, twp, addr, e]","[numeric, numeric, string, string, string, dateTime, string, string, numeric]",Emergency (911) Calls Fire Traffic EMS for Montgomery County PA You can get a quick introduction to this Dataset with this kernel Dataset Walk-through Acknowledgements Data provided by montcoalert.org,CSV,,"[crime, law]",ODbL,,,6695,58251,11,"Montgomery County, PA",Emergency - 911 Calls,https://www.kaggle.com/mchirico/montcoalert,Sat Dec 30 2017
,PromptCloud,"[Rank, Title, Genre, Description, Director, Actors, Year, Runtime (Minutes), Rating, Votes, Revenue (Millions), Metascore]","[numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Here's a data set of 1000 most popular movies on IMDB in the last 10 years. The data points included are Title Genre Description Director  Actors Year Runtime Rating Votes Revenue Metascrore Feel free to tinker with it and derive interesting insights.,CSV,,[film],Other,,,2127,15173,0.2958984375,"A data set of 1,000 popular movies on IMDB in the last 10 years",IMDB data from 2006 to 2016,https://www.kaggle.com/PromptCloudHQ/imdb-data,Mon Jun 26 2017
,William Cukierski,"[id, name, normalized_name, gender]","[numeric, string, string, string]",This dataset contains the characters locations episode details and script lines for approximately 600 Simpsons episodes dating back to 1989. Inspiration and credit for gathering the data goes to Todd Schneider http//toddwschneider.com/posts/the-simpsons-by-the-data/  https//github.com/toddwschneider/flim-springfield,CSV,,[popular culture],Other,,,4183,38334,34,"27 seasons of ""Simpsons did it.""",The Simpsons by the Data,https://www.kaggle.com/wcukierski/the-simpsons-by-the-data,Thu Sep 29 2016
,City of New York,"[JOB_NUMBER, JOB_TYPE, C_O_ISSUE_DATE, BIN_NUMBER, BOROUGH, NUMBER, STREET, BLOCK, LOT, POSTCODE, PR_DWELLING_UNIT, EX_DWELLING_UNIT, APPLICATION_STATUS_RAW, FILING_STATUS_RAW, ITEM_NUMBER, ISSUE_TYPE, LATITUDE, LONGITUDE, COMMUNITY_BOARD, COUNCIL_DISTRICT, CENSUS_TRACT, BIN, BBL, NTA, LOCATION]","[numeric, string, dateTime, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context The City of New York issues Certificates of Occupancy to newly constructed (and newly reconstructed e.g. “gut renovated”) buildings in New York City. These documents assert that the city has deemed the building habitable and safe to move into. Content This dataset includes all temporary (expirable) and final (permanent) Certificates of Occupancies issues to newly habitable buildings in New York City split between new (Job Type NB) and reconstructed (Job Type A1) buildings issued between July 12 2012 and August 29 2017. Acknowledgements This data is published as-is by the New York City Department of Buildings. Inspiration  In what areas of New York City are the newly constructed buildings concentrated? What is the difference in distribution between buildings that are newly built and ones that are newly rebuilt? In combination with the New York City Buildings Database dataset what are notable differences in physical characteristics between recently constructed buildings and existing ones? ,CSV,,"[cities, civil engineering]",CC0,,,52,572,14,New and newly reconstructed buildings in New York City.,New York City - Certificates of Occupancy,https://www.kaggle.com/new-york-city/nyc-certificates-of-occupancy,Fri Sep 01 2017
,Dryad Digital Repository,"[disaster.event, user.anon, latitude, longitude.anon, time]","[string, numeric, numeric, numeric, dateTime]",This dataset contains geolocation information for thousands of Twitter users during natural disasters in their area.  Abstract (from original paper) Natural disasters pose serious threats to large urban areas therefore understanding and predicting human movements is critical for evaluating a population’s vulnerability and resilience and developing plans for disaster evacuation response and relief. However only limited research has been conducted into the effect of natural disasters on human mobility. This study examines how natural disasters influence human mobility patterns in urban populations using individuals’ movement data collected from Twitter. We selected fifteen destructive cases across five types of natural disaster and analyzed the human movement data before during and after each event comparing the perturbed and steady state movement data. The results suggest that the power-law can describe human mobility in most cases and that human mobility patterns observed in steady states are often correlated with those in perturbed states highlighting their inherent resilience. However the quantitative analysis shows that this resilience has its limits and can fail in more powerful natural disasters. The findings from this study will deepen our understanding of the interaction between urban dwellers and civil infrastructure improve our ability to predict human movement patterns during natural disasters and facilitate contingency planning by policymakers. Acknowledgments The original journal article for which this dataset was collected  Wang Q Taylor JE (2016) Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. PLoS ONE 11(1) e0147299. http//dx.doi.org/10.1371/journal.pone.0147299 The Dryad page that this dataset was downloaded from  Wang Q Taylor JE (2016) Data from Patterns and limitations of urban human mobility resilience under the influence of multiple types of natural disaster. Dryad Digital Repository. http//dx.doi.org/10.5061/dryad.88354 The Data This dataset contains the following fields  disaster.event the natural disaster during which the observation was collected. One of  -- one of  --- *01_Wipha* *02_Halong* *03_Kalmaegi* *04_Rammasun_Manila* (typhoons)  --- *11_Bohol* *12_Iquique* *13_Napa* (earthquakes)  --- *21_Norfolk* *22_Hamburg* *23_Atlanta* (winter storms)  --- *31_Phoenix* *32_Detroit* *33_Baltimore* (thunderstorms)  --- *41_AuFire1* *42_AuFire2* (wildfires)   user.anon an anonymous user id; unique within each disaster event   latitude latitude of user's tweet   longitude.anon longitude of user's tweet; shifted to preserve anonymity time the date and time of the tweet ,CSV,,"[geography, demographics, internet]",CC0,,,511,5427,285,Twitter geolocation for users during 15 natural disasters,Human Mobility During Natural Disasters,https://www.kaggle.com/dryad/human-mobility-during-natural-disasters,Thu Aug 31 2017
,Stanford University,[],[],The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment contradiction and neutral supporting the task of natural language inference (NLI) also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text especially including those induced by representation learning methods as well as a resource for developing NLP models of any kind. Acknowledgements This dataset was kindly made available bye the Stanford Natural Language Processing Group. Please cite it as Samuel R. Bowman Gabor Angeli Christopher Potts and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP) Inspiration This dataset has been used to evaluate academic work on sentence encoding-based models for 3 way classification with previous scores tabulated at https//nlp.stanford.edu/projects/snli/. Most of the entries use deep learning. How close to those scores (peak of 88.8% test accuracy) can you get with less computationally intensive methods?,CSV,,"[languages, linguistics]",CC4,,,215,2635,373,A collection of 570k labeled human-written English sentence pairs,Stanford Natural Language Inference Corpus,https://www.kaggle.com/stanfordu/stanford-natural-language-inference-corpus,Fri Jul 21 2017
,National Snow and Ice Data Center,"[Glacier ID, Political Unit, Continent, Basin Code, Location Code, Glacier Code, Glacier Name, Latitude, Longitude, Primary Class, Glacier Source, Basin Count, Glacier Form, Glacier Activity, Activity Start, Activity End, Minimum Elevation, Minimum Elevation Exposed, Mean Elevation, Mean Elevation Accumulation, Mean Elevation Ablation, Maximum Elevation, Snow Line Elevation, Snow Line Accuracy, Glacier Area, Area Accuracy, Area Exposed, Mean Width, Mean Length, Maximum Length, Maximum Length Exposed, Maximum Length Ablation, Mean Depth, Depth Accuracy, Accumulation Orientation, Ablation Orientation, Topographic Map Year, Topographic Map Scale, Photograph Year]","[string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, string, string, numeric, string, numeric, string, string, numeric, string, string, numeric, numeric, string, string, string, numeric, string, string, string, string, string, string, numeric, numeric, string]",Content The World Glacier Inventory contains information for over 130000 glaciers. Inventory parameters include geographic location area length orientation elevation and classification. The WGI is based primarily on aerial photographs and maps with most glaciers having one data entry only. The data set can be viewed as a snapshot of the glacier distribution in the second half of the twentieth century. It was founded on the original WGI from the World Glacier Monitoring Service. Acknowledgements The National Snow & Ice Data Center continues to work with the World Glacier Monitoring Service to update the glacier inventory database.,CSV,,"[environment, climate]",CC0,,,265,2643,16,"Name, location, altitude, and area of every glacier on the planet",World Glacier Inventory,https://www.kaggle.com/nsidcorg/glacier-inventory,Thu Feb 09 2017
,Rachael Tatman,"[Number, Digimon, Stage, Type, Attribute, Memory, Equip Slots, Lv 50 HP, Lv50 SP, Lv50 Atk, Lv50 Def, Lv50 Int, Lv50 Spd]","[numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Digimon short for “digital monsters” is a franchise which revolves around a core mechanic of capturing caring for and training monsters and then engaging in combat with them. It’s similar to Pokémon. This dataset contains information on digimon from “Digimon Digimon Story Cyber Sleuth” released for Playstation Vita in 2015 and Playstation 4 in 2016.  Content This database contains three files a list of all the digimon that can be captured or fought in Cyber Sleuth all the moves which Digimon can perform and all the Support Skills. (Support Skills are a passive stackable team-wide buff. Each species of Digimon is associated with a single Support Skill.) Acknowledgements This dataset was created by Mark Korsak and is used here with permission. You can find an interactive version of this database here. http//digidb.io/ Inspiration This dataset will help you theorycraft the ultimate team as well as ask interesting questions.  Which set of moves will get the best ratio of attack power to SP spent? Which team of 3 digimon have the highest attack? Defense? What’s the tradeoff between HP and SP? Are some types over- or under-represented? Both the moves and support skills have short text descriptions. Can an NLP analysis reveal underlying clusters of moves? Are different types and attributes evenly represented across stages? ,CSV,,[games and toys],CC4,,,233,2216,0.056640625,A database of Digimon and their moves from Digimon Story CyberSleuth,Digimon Database,https://www.kaggle.com/rtatman/digidb,Fri Jul 14 2017
,Rachael Tatman,[],[],Project Description The Paradis corpus consists of naturalistic language samples from 25 children learning English as a second language (English language learners or learners of English as an additional language). Transcription is in English orthography only; phonetic transcription was not included in this research. Any real names of people or places in the transcripts have been replaced with pseudonyms. The participants are identified with four letter codes. Content The data in this corpus was collected in 2002 in Edmonton Canada. Children were video-­‐taped in conversation with a student research assistant in their homes for approximately 45 minutes. During this time the research assistant had a list of “interview” questions to ask. If the child introduced his or her own topics and the conversation moved forward the questions were not asked. This dataset only includes data from the first stage of data collection in 2002. The full longituinal corpus may be found on the CHILDES website here http//childes.talkbank.org/access/Biling/Paradis.html  These data are in .cha files which are intended for use with the program CLAN (http//alpha.talkbank.org/clan/). However you may also treat these files as raw text files with one speech snippet per line. Lines starting with @ are metadata.  File format information   *EXP Experimenter speaking *CHI Child speaking %[some text] These lines contain non-linguistic information  Biographical data Participants in this study were children from newcomer (immigrant and refugee) families to Canada. The children started to learn English as a second language (L2) after their first language (L1) had been established at 4 years 11 months on average. In the table below “AOA” refers to the “age of arrival” of the child when the family immigrated. The number “1” indicates children who were Canadian born. The column “AOE” refers to the age of onset of English acquisition. All ages are in months. Each child’ s L1 and gender is also listed in the table below. For more information about the participants and procedures in this research see the following Paradis J. (2005). Grammatical morphology in children learning English as a second language Implications of similarities with Specific Language Impairment. Language Speech and Hearing Services in the Schools 36 172-187. Golberg H. Paradis J. & Crago M. (2008). Lexical acquisition over time in minority L1 children learning English as a L2. Applied Psycholinguistics 29 1-25. Inspiration  Does children’s first language affect what English words they use? How many words? Do some children pause (marked as (.) or (..)) more often than others? Do children at different ages interrupt/overlap their speech more often? (Marked by <> around text.) Does a children’s age of first exposure to English affect how often then say “um”? (Transcribed as“&-um”.)  Related datasets  When do children learn words? Diagnosing specing language impairment in children ,Other,,"[languages, children, linguistics]",CC4,,,486,1915,0.912109375,Transcribed natural speech from 25 bilingual children,Corpus of bilingual children's speech,https://www.kaggle.com/rtatman/corpus-of-bilingual-childrens-speech,Sat Jul 22 2017
,Aleksey Bilogur,[],[], Methodology This is a data dump of the top 100 products (ordered by number of mentions) from every subreddit that has posted an amazon product. The data was extracted from Google Bigquery's Reddit Comment database. It only extracts Amazon links so it is certainly a subset of all products posted to Reddit. The data is organized in a file structure that follows  reddits/<first lowercase letter of subreddit>/<subreddit>.csv  An example of where to find the top products for /r/Watches would be  reddits/w/Watches.csv  Definitions Below are the column definitions found in each <subreddit>.csv file. name The name of the product as found on Amazon. category The category of the product as found on Amazon. amazon_link The link to the product on Amazon. total_mentions The total number of times that product was found on Reddit. subreddit_mentions The total number of times that product was found on that subreddit. Want more? You can search and discover products more easily on ThingsOnReddit Acknowledgements This dataset was published by Ben Rudolph on GitHub and was republished as-is on Kaggle. ,Other,,"[business, product, reddit, internet]",Other,,,74,1352,8,The top 100 products in each subreddit from 2015 to 2017,Things on Reddit,https://www.kaggle.com/residentmario/things-on-reddit,Thu Oct 26 2017
,Andreas Klintberg,"[Inger, PER]","[string, numeric]",Context Bootstrapped and manually annotated NER Swedish web news from 2012. NER stands for Named entity recognition and its used to describe entities in a text such as organisations locations and people for instance.  Its a very common operation in general NLP pipeline and several algorithms can be used to train a model. Traditionally many NER systems were trained using some kind of CRF (conditionally random fields) approach but nowadays many people successfully uses LSTMs or other sequence based deep learning techniques.   A tutorial on how to use this dataset to train an NER for Stanford CoreNLP is available here https//medium.com/@klintcho/training-a-swedish-ner-model-for-stanford-corenlp-part-2-20a0cfd801dd Content The dataset is very simple and can easily be adapted into other formats it is specifically adapted to CoreNLP NER. Thus the first column is a word. Second column (tab separated) is either the NER category (ORG PER LOC MISC) or a 0 if it does not belong to any category (not an entity). Each word is separated by a new line and each sentence is separated by an empty new line. Sample structure (of two sentences one three word sentence and another 4 word sentence) Apple     ORG is            0 nice        0 .              0 Per         PER is            0 not         0 sad        0 Acknowledgements Text is annotated from http//spraakbanken.gu.se/eng/resource/webbnyheter2012. Thanks Norah Klintberg Sakal for helping out with the annotation and reviewing all annotations as well. Inspiration Feel free to use this for whatever you like. As most datasets it would definitely benefit from becoming larger feel free to create a pull request https//github.com/klintan/swedish-ner-corpus/ or update it here on Kaggle.,Other,,[languages],CC4,,,14,385,1,"~8000 sentences annotated for Swedish NER (PER, LOC, ORG, MISC)",Swedish NER corpus,https://www.kaggle.com/andreasklintberg/swedish-ner-corpus,Wed Dec 13 2017
,Team AI,[],[],"Background NLP is a hot topic currently!  Team AI really want's to leverage the NLP research and this an attempt for all the NLP researchers to explore exciting insights from bilingual data The Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles” aims mainly at supporting research and development relevant to high-performance multilingual machine translation information extraction and other language processing technologies.  Unique Features A precise and large-scale corpus containing about 500000 pairs of manually-translated sentences. Can be exploited for research and development of high-performance multilingual machine translation information extraction and so on. The three-step translation process (primary translation -> secondary translation to improve fluency -> final check for technical terms) has been clearly recorded. Enables observation of how translations have been elaborated so it can be applied for uses such as research and development relevant to translation aids and error analysis of human translation. Translated articles concern Kyoto and other topics such as traditional Japanese culture religion and history. Can also be utilized for tourist information translation or to create glossaries for travel guides. The Japanese-English Bilingual Kyoto Lexicon is also available. This lexicon was created by extracting the Japanese-English word pairs from this corpus. Sample One Wikipedia article is stored as one XML file in this corpus and the corpus contains 14111 files in total. The following is a short quotation from a corpus file titled “Ryoan-ji Temple”. Each tag has different implications. For example <j>Original Japanese sentence<j> <e type=""trans"" ver=""1"">Primary translation</e> <e type=""trans"" ver=""2"">Secondary translation</e> <e type=""check"" ver=""1"">Final translation</e> <cmt>Comment added by translators</cmt> Categories The files have been divided into 15 categories school railway family building Shinto person name geographical name culture road Buddhism literature title history shrines and temples and emperor (Click the link to view a sample file for each category). Github https//github.com/venali/BilingualCorpus Explains how to load the corpus Acknowledgements The National Institute of Information and Communications Technology (NICT) has created this corpus by manually translating Japanese Wikipedia articles (related to Kyoto) into English. Licence Use and/or redistribution of the Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles and the Japanese-English Bilingual Kyoto Lexicon is permitted under the conditions of Creative Commons Attribution-Share-Alike License 3.0. Details can be found at http//creativecommons.org/licenses/by-sa/3.0/. Link to web https//alaginrc.nict.go.jp/WikiCorpus/index_E.html",Other,,"[languages, linguistics, artificial intelligence]",Other,,,119,1731,357,Japanese-English Bilingual Corpus of Wikipedia's Kyoto Articles,Japanese-English Bilingual Corpus,https://www.kaggle.com/team-ai/japaneseenglish-bilingual-corpus,Fri Sep 15 2017
,Rachael Tatman,[],[],Context This dataset contains pending and registered trademark text data (no drawings/images) to include word mark serial number registration number filing date registration date goods and services classification number(s) status code(s) design search code(s) pseudo mark(s) from the April 7 1884 - Present. The file format .json converted from eXtensible Markup Language (XML) in accordance with the U.S. Trademark Applications Version 2.0 Document Type Definition (DTD).  Content This dataset is made up of one .json file with the following fields. (Note that the documentation was written for an XML file which was subsequently converted into .json). The following is a description of some of the fields; full documentation can be found in the applications-documentation.pdf file.  The trademark-applications-daily element is mandatory and will occur one time identifying the beginning of a daily application process. The trademark applications daily element will contain one occurrence of the version and data processed indicator elements and zero or more occurrences of the file segments element between the trademark-applications-daily start and trademark-applications-daily end tags. The version element is mandatory and will occur one time between the version start and version end tags identifying the version number and version date of the Trademark Applications DTD. The version element will contain one occurrence of the version number and version date elements. The version-no element is mandatory and will occur one time between the version-no start and version-no end tags containing the version number of the Trademark Applications DTD. Example the first production version will be 1.0 The version-date element is mandatory and will occur one time between the version-date start and version-date end tags containing the date of the Trademark Applications DTD an 8-position numeric in the format YYYYMMDD.  Application Information Section  The application-information element is optional and will occur zero or one times between the application-information start and application-information end tags containing the daily trademark applications data. The data-available-code element is optional and will occur zero or one times when trademark data is NOT present. The data-available-code start tag and the data-available-code end tag will be present and contain a one-position “N” indicating that trademark data is Not present. This element will not be present when data is available. The file-segments element is optional and will occur zero or more times between the file-segments start and file- segments end tags identifying the beginning of the Trademark Applications file. The file-segments element will contain zero or more occurrences of the file segment and action keys elements. The file-segment element is optional and will occur zero or more times between the file-segment start and file-segment end tags containing the File Segment text data a four-position alphabetic field identifying the type of data in the Trademark Daily XML Process. The Trademark Applications file-segment will always have a constant of “TRMK”. The”TRMK” file segment contains new trademark data and modifications made to existing Trademark data. The action-keys element is optional and will occur zero or more times between the action-keys start and action-keys end tags. The action keys element will contain zero or more occurrences of the action key and case file elements. The action-key element is optional and will occur zero or more times between the action-key start and action-key end tags containing the KEY ACTION a two-position alphanumeric field. The contents of the action key will be one of the following values for marks appearing in the Trademark Official Gazette (OG).  Case file section The Case file section and case-file end tags containing the following elements in the sequence as follows  The serial-number element is mandatory and will occur one time between the serial-number start and serial-number end tags containing the SERIAL NUMBER an eight-position numeric field consisting of the following Serial number Position 1-2 will contain a series code SERIES CODE FILING DATE 70 1881 – 03/31/05 71 04/01/05 – 12/31/55 72 01/01/56 – 08/31/73 73 09/01/73 – 11/15/89 74 11/16/89 – 09/30/95 75 10/01/95 – 03/19/2000 76 03/20/2000 – Present - (76 - Will be used for all paper filed applications.) 78 03/20/2000 – Present - (78 - Will be used for electronically filed (e-TEAS) applications.) Position 3-8 will be a six-position serial number right justified with leading zeros. NOTE When the serial-number begins with 80 81 or 82 the actual serial number is unknown and the serial number is created by placing an “8” before the seven-digit registration number. If the registration number is less than seven digits in length it is preceded by zeros. When the serial-number begins with 89 non-registration data consists of information entered in the database because of treaty obligations U.S. Statutes or other requirements. The registration-number element is optional and will occur zero or one times between the registration-number start and registration-number end tags containing the REGISTRATION NUMBER a seven-position numeric field right justified with leading zeros. NOTE If a mark does not contain a registration number the registration number element will contain zeros. The transaction-date element is optional and will occur zero or one times between the transaction-date start and transaction-date end tags containing the TRANSACTION DATE an eight position date in the format YYYYMMDD. The transaction date is the date of the Trademark Daily XML Process for Action Key entries.  Case File Header Section   The case-file-header element is optional and occurs zero or one times between the case-file-header start and case-file-header end tags identifying the first record sequence of a trademark application document which contains the equivalent of a TWTF/GENX record. Each case file header element will contain the following optional elements. NOTE Any of the following elements that do not have the required date will contain zeros. The filing-date element is optional and occurs zero or one times between the filing-date start and filing-date end tags containing the FILING DATE an eight position date in the format YYYYMMDD which is the date on which a statutorily complete trademark application is filed at the USPTO. The registration date element is optional and occurs zero or one times between the registration-date start and registration-date end tags containing the REGISTRATION DATE an eight-position date in the format YYYYMMDD that identifies the date the mark was registered. The status-code element is optional and occurs zero or one times between the status-code start and status-code end tags containing the STATUS CODE a three position numeric field which identifies the status of the mark. The status codes can be found in the trademark-status-codes file. The status-date element is optional and occurs zero or one times between the status-date start and status-date end tags containing the STATUS DATE an eight-position date in the format YYYYMMDD which is the date on which the current status was reported to the system. The mark-identification element is optional and occurs zero or one times between the mark-identification start and mark-identification end tags containing the MARK-1-LIN a variable length alphanumeric field containing the characters of the actual mark. The mark-drawing-code element is optional and occurs zero or one times between the mark-drawing-code start and mark-drawing-code end tags containing the MARK DRAWING CODE a four-position alphanumeric field. The first position identifies the physical characteristics of the mark. The published-for-opposition-date element is optional and occurs zero or one times between the published-for-opposition-date start and published-for-opposition-date end tags containing the DATE PUBLISHED FOR OPPOSITION an eight-position date in the format YYYYMMDD which is the date that the mark published for opposition in the Official Gazette. The amend-to-register-date element is optional and occurs zero or one times between the amend-to-register-date start and amend-to-register end tags containing the AMENDED TO REGISTER DATE an eight-position date in the format of YYYYMMDD. This element contains the date on which a case is entered as an amendment to a register. The abandonment-date element is optional and occurs zero or one times between the abandonment-date start and abandonment-date end tags containing the DATE ABANDONED an eight-position date in the format YYYYMMDD which is the date that the mark is abandoned. The cancellation-code element is a optional and occurs zero or one times between the cancellation-code start and cancellation-code end tags containing the CANCELLATION CODE a one-position alphanumeric which identifies the section of the statute under which an entire registration is being cancelled or under which some classes in a multiple class registration are being cancelled. CC Definition 0 No entry 1 Section 7(d) Entire Registration 2 Section 8 Entire Registration 3 Section 18 Entire Registration 4 Section 24 Entire Registration 5 Section 37 Entire Registration 6 Entire Registration inadvertently Issued 7 Inadvertently issued-entire Registration restored to pendency A Section 7 (d ) - Class(es) in multiple class Registration B Section 8 - Class(es) in multiple class Registration C Section 18 - Class(es) in multiple class Registration D Section 24 - Class(es) in multiple class Registration E Section 37 - Class(es) in multiple class Registration The cancellation-date element is optional and occurs zero or one times between the cancellation-date start and cancellation-date end tags containing the CANCELLATION DATE an eight-position date in the format YYYYMMDD which is the date that the cancellation of the entire registration was recorded. The republished 12c-date element is optional and occurs zero or one times between the republished-12c-date start and republished-12c-date end tags containing the DATE PUBLISHED UNDER SECTION 12(C) an eight-position date in the format YYYYMMDD which is the date of publication under Section 12(c). The domestic-representative-name element is optional and occurs zero or one times between the domestic-representative-name start and domestic-representative-name end tags and contains the DOMESTIC REPRESENTATIVE information for the application. The attorney-docket-number element is optional and occurs zero or one times between the attorney-docket-number start and attorney-docket-number end tags containing the ATTORNEY DOCKET NUMBER a twelve-position number containing the reference or identification number of a case as assigned and used in the office of the attorney filing the application. The attorney-name element is optional and occurs zero or one times between the attorney-name start and attorney-name end tags and contains the ATTORNEY information from the OWNX record. The principal-register-amended-indicator element is optional and occurs zero or one times between the principal-register-amended-in start and principal-register-amended-in end tags containing the FLAG AMENDED TO THE PRINCIPAL REGISTER a one-position alphabetic indicating the register has been amended for an application on the Supplemental Register. A “T” in this field indicates an amendment to the Principal Register. An “F” in this field indicates no amendment to the Principal Register. The supplemental-register-amended-indicator element is optional and occurs zero or one times between the supplemental-register-amended-in start and supplemental-register-amended-in end tags containing the FLAG AMENDED TO THE SUPPLEMENTAL REGISTER a one-position field indicating the register has been amended for an application on the Principal Register. A “T” in this field indicates an amendment to the Supplemental Register. An “F” in this field indicates no amendment to the supplemental Register.  Prior Registration Applications Section  The prior-registration-applications element is optional and occurs zero or one times between the prior-registration-applications start and prior-registration-applications end tags which contains the equivalent of all TWTF/PRUS records. Each prior registration applications element will contain the optional other related in and prior registration application elements. The other-related-indicator element is optional and occurs zero or more times between the other-related-in start and other-related-in end tags containing the AND OTHERS a one-position alphabetic field. A “T” in this field would indicate that the words “and others” appears in conjunction with a list of prior registrations that are claimed as being related to this mark. An “F” would indicate that the statement is not present.  Acknowledgements This dataset is provided by the United States Government and is in the public domain. Daily uploads of this dataset are available online here. ,CSV,,"[government agencies, united states, product]",CC0,,,52,850,210,Information on individual trademark applications,United States Trademark Applications,https://www.kaggle.com/rtatman/trademark-application,Wed Sep 20 2017
,Dan Ofer,"[GameID, LeagueIndex, Age, HoursPerWeek, TotalHours, APM, SelectByHotkeys, AssignToHotkeys, UniqueHotkeys, MinimapAttacks, MinimapRightClicks, NumberOfPACs, GapBetweenPACs, ActionLatency, ActionsInPAC, TotalMapExplored, WorkersMade, UniqueUnitsMade, ComplexUnitsMade, ComplexAbilitiesUsed]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Dataset of Starcraft 2 games played in different leagues/levels.  Content Screen movements aggregated into screen-fixations.  -- Time is recorded in terms of timestamps in the StarCraft 2 replay file. When the game is played on 'faster' 1 real-time second is equivalent to roughly 88.5 timestamps.  Attribute Information  GameID Unique ID number for each game (integer)  LeagueIndex Bronze Silver Gold Platinum Diamond Master GrandMaster and Professional leagues coded 1-8 (Ordinal)  Age Age of each player (integer)  HoursPerWeek Reported hours spent playing per week (integer)  TotalHours Reported total hours spent playing (integer)  APM Action per minute (continuous)  SelectByHotkeys Number of unit or building selections made using hotkeys per timestamp (continuous)  AssignToHotkeys Number of units or buildings assigned to hotkeys per timestamp (continuous)  UniqueHotkeys Number of unique hotkeys used per timestamp (continuous)  MinimapAttacks Number of attack actions on minimap per timestamp (continuous)  MinimapRightClicks number of right-clicks on minimap per timestamp (continuous)  NumberOfPACs Number of PACs per timestamp (continuous)  GapBetweenPACs Mean duration in milliseconds between PACs (continuous)  ActionLatency Mean latency from the onset of a PACs to their first action in milliseconds (continuous)  ActionsInPAC Mean number of actions within each PAC (continuous)  TotalMapExplored The number of 24x24 game coordinate grids viewed by the player per timestamp (continuous)  WorkersMade Number of SCVs drones and probes trained per timestamp (continuous)  UniqueUnitsMade Unique unites made per timestamp (continuous)  ComplexUnitsMade Number of ghosts infestors and high templars trained per timestamp (continuous)  ComplexAbilitiesUsed Abilities requiring specific targeting instructions used per timestamp (continuous)  Acknowledgements Source 1. Thompson JJ Blair MR Chen L Henrey AJ (2013) Video Game Telemetry as a Critical Tool in the Study of Complex Skill Learning. PLoS ONE 8(9) e75129. [Web Link]  -- Results  -- Skip league conditional inference forest classification (Bronze-Gold;Silver-Platinum;Gold-Diamond;Platinum-Masters;Diamond-Professional) showed changing patterns of variable importance with skill.  http//archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset Inspiration  Ordinal Classification / regression model to determine League Index  (""LeagueIndex"") Suggest additional features to gather and analyze for predicting leagues/performance. Are there features which do not increase/decrease linearly as we go up in the leagues?  ",CSV,,"[games and toys, video games, sports, internet]",CC4,,,77,662,0.46875,Classifying Starcraft 2 league-level performance,SkillCraft-StarCraft,https://www.kaggle.com/danofer/skillcraft,Wed Dec 27 2017
,Chris Crawford,"[building, 1, 0, 0, 0, 0, 0]","[string, numeric, numeric, numeric, numeric, numeric, numeric]",DeepSat SAT-6  Originally images were extracted from the National Agriculture Imagery Program (NAIP) dataset. The NAIP dataset consists of a total of 330000 scenes spanning the whole of the Continental United States (CONUS). The authors used the uncompressed digital Ortho quarter quad tiles (DOQQs) which are GeoTIFF images and the area corresponds to the United States Geological Survey (USGS) topographic quadrangles. The average image tiles are ~6000 pixels in width and ~7000 pixels in height measuring around 200 megabytes each. The entire NAIP dataset for CONUS is ~65 terabytes. The imagery is acquired at a 1-m ground sample distance (GSD) with a horizontal accuracy that lies within six meters of photo-identifiable ground control points. The images consist of 4 bands - red green blue and Near Infrared (NIR). In order to maintain the high variance inherent in the entire NAIP dataset we sample image patches from a multitude of scenes (a total of 1500 image tiles) covering different landscapes like rural areas urban areas densely forested mountainous terrain small to large water bodies agricultural areas etc. covering the whole state of California. An image labeling tool developed as part of this study was used to manually label uniform image patches belonging to a particular landcover class.  Once labeled 28x28 non-overlapping sliding window blocks were extracted from the uniform image patch and saved to the dataset with the corresponding label. We chose 28x28 as the window size to maintain a significantly bigger context and at the same time not to make it as big as to drop the relative statistical properties of the target class conditional distributions within the contextual window. Care was taken to avoid interclass overlaps within a selected and labeled image patch. Content  Each sample image is 28x28 pixels and consists of 4 bands - red green blue and near infrared.  The training and test labels are one-hot encoded 1x6 vectors The six classes represent the six broad land covers which include barren land trees grassland roads buildings and water bodies.  Training and test datasets belong to disjoint set of image tiles.  Each image patch is size normalized to 28x28 pixels. Once generated both the training and testing datasets were randomized using a pseudo-random number generator.   CSV files  X_train_sat6.csv 324000 training images 28x28 images each with 4 channels  y_train_sat6.csv 324000 training labels 1x6 one-hot encoded vectors  X_test_sat6.csv 81000 training images 28x28 images each with 4 channels  y_test_sat6.csv 81000 training labels 1x6 one-hot encoded vectors   The original MAT file   train_x  28x28x6x324000 uint8 (containing 400000 training samples of 28x28 images each with 4 channels) train_y  324000x6 uint8 (containing 6x1 vectors having labels for the 400000 training samples) test_x   28x28x6x18000 uint8 (containing 100000 test samples of 28x28 images each with 4 channels) test_y   81000x6 uint8 (containing 6x1 vectors having labels for the 100000 test samples)  Acknowledgements The original MATLAB file was converted to multiple CSV files  The original SAT-4 and SAT-6 airborne datasets can be found here http//csc.lsu.edu/~saikat/deepsat/ Thanks to Saikat Basu Robert DiBiano Manohar Karki and Supratik Mukhopadhyay Louisiana State University Sangram Ganguly Bay Area Environmental Research Institute/NASA Ames Research Center Ramakrishna R. Nemani NASA Advanced Supercomputing Division NASA Ames Research Center,CSV,,"[geography, machine learning, image data, multiclass classification]",CC0,,,93,7234,2048,"405,000 image patches each of size 28x28 and covering 6 landcover classes ",DeepSat (SAT-6) Airborne Dataset,https://www.kaggle.com/crawford/deepsat-sat6,Wed Jan 17 2018
,Ashok Kumar Pant,"[Numerals, , , ]","[numeric, numeric, string, string]",Context This dataset is created as a part of my dissertation work for the fulfillment of the Master's degree in Computer Science (Tribhuvan University Nepal 2012). Content The dataset contains three individual categories. Samples are collected from 40 individuals (persons) from different fields and cropped for character boundary.  Numerals (288 samples per class 10 classes)  Vowels (221 samples per class 12 classes)  Consonants (205 samples per class 36 classes)  Citation Please cite in your publications if it helps your research @inproceedings{pant2012off   title={Off-line Nepali handwritten character recognition using Multilayer Perceptron and Radial Basis Function neural networks}   author={Pant Ashok Kumar and Panday Sanjeeb Prasad and Joshi Shashidhar Ram}   booktitle={2012 Third Asian Himalayas International Conference on Internet}   pages={1--5}   year={2012}   organization={IEEE} } ,CSV,,[],ODbL,,,246,2279,9,Devanagari (Nepali) Handwritten Character Dataset,Devanagari Character Dataset,https://www.kaggle.com/ashokpant/devanagari-character-dataset,Fri Jun 23 2017
,LucasVinze,"[, X, REPORT_DAT, SHIFT, OFFENSE, METHOD, BLOCK, DISTRICT, PSA, WARD, ANC, NEIGHBORHOOD_CLUSTER, BLOCK_GROUP, CENSUS_TRACT, VOTING_PRECINCT, CCN, START_DATE, END_DATE, XBLOCK, YBLOCK, optional, date, year, month, day, hour, minute, second, EW, NS, quad, crimetype]","[numeric, numeric, dateTime, string, string, string, string, numeric, numeric, numeric, string, string, dateTime, numeric, string, numeric, dateTime, dateTime, numeric, numeric, boolean, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string]",Dataset of all of the crimes in the DC metro police system ranging from Theft Arson Assault Homicide Sex Abuse Robbery and Burglary.  Data can be easily geocoded and mapped trends can be extracted and predictions can be made.  Would be interesting to combine with other datasets i.e. changes in housing prices history of construction sites etc.  j An informal hypothesis would be If the local government invests in fixing the sidewalks in a neighborhood how much would the investment decrease crime levels on a block by block basis. Raw Data can be accessed from http//crimemap.dc.gov/CrimeMapSearch.aspx#tabs-GeoOther The data is most easily accessed by downloading 1 ward at a time for the specific data range.,CSV,,[crime],Other,,,1703,13535,109,Consolidated set of all registered crimes from crimemaps.dc.gov,DC Metro Crime Data,https://www.kaggle.com/vinchinzu/dc-metro-crime-data,Fri Nov 03 2017
,Zhijin,"[Loan_ID, loan_status, Principal, terms, effective_date, due_date, paid_off_time, past_due_days, age, education, Gender]","[string, string, numeric, numeric, dateTime, dateTime, dateTime, string, numeric, string, string]",Context This data set includes customers who have paid off their loans who have been past due and put into collection without paying back their loan and interests and who have paid off only after they were put in collection. The financial product is a bullet loan that customers should pay off all of their loan debt in just one time by the end of the term instead of an installment schedule. Of course they could pay off earlier than their pay schedule. Content Loan_id A unique loan number assigned to each loan customers Loan_status Whether a loan is paid off in collection new customer yet to payoff or paid off after the collection efforts Principal   Basic principal loan amount at the origination terms   Can be weekly (7 days) biweekly and monthly payoff schedule Effective_date  When the loan got originated and took effects Due_date    Since it’s one-time payoff schedule each loan has one single due date Paidoff_time    The actual time a customer pays off the loan Pastdue_days    How many days a loan has been past due Age education gender  A customer’s basic demographic information,CSV,,[finance],CC0,,,3476,22008,0.0419921875,This dataset includes customers who have paid off their loans or not,Loan  Data,https://www.kaggle.com/zhijinzhai/loandata,Tue Apr 11 2017
,SamiraKlaylat,[],[],Emotion expression is an essential part of human interaction. The same text can hold different meanings when expressed with different emotions. Thus understanding the text alone is not enough for getting the meaning of an utterance. Acted and natural corpora have been used to detect emotions from speech. Many speech databases for different languages including English German Chinese Japanese Russian Italian Swedish and Spanish exist for modeling emotion recognition. Since there is no reported reference of an available Arabic corpus we decided to collect the first Arabic Natural Audio Dataset (ANAD) to recognize discrete emotions. Embedding an effective emotion detection feature in speech recognition system seems a promising solution for decreasing the obstacles faced by the deaf when communicating with the outside world. There exist several applications that allow the deaf to make and receive phone calls normally as the hearing-impaired individual can type a message and the person on the other side hears the words spoken and as they speak the words are received as text by the deaf individual. However missing the emotion part still makes these systems not hundred percent reliable. Having an effective speech to text and text to speech system installed in their everyday life starting from a very young age will hopefully replace the human ear. Such systems will aid deaf people to enroll in normal schools at very young age and will help them to adapt better in classrooms and with their classmates. It will help them experience a normal childhood and hence grow up to be able to integrate within the society without external help.  Eight videos of live calls between an anchor and a human outside the studio were downloaded from online Arabic talk shows. Each video was then divided into turns callers and receivers. To label each video 18 listeners were asked to listen to each video and select whether they perceive a happy angry or surprised emotion. Silence laughs and noisy chunks were removed. Every chunk was then automatically divided into 1 sec speech units forming our final corpus composed of 1384 records. Twenty five acoustic features also known as low-level descriptors were extracted. These features are intensity zero crossing rates MFCC 1-12 (Mel-frequency cepstral coefficients) F0 (Fundamental frequency) and F0 envelope probability of voicing and LSP frequency 0-7. On every feature nineteen statistical functions were applied. The functions are maximum minimum range absolute position of maximum absolute position of minimum arithmetic of mean Linear Regression1 Linear Regression2 Linear RegressionA Linear RegressionQ standard Deviation kurtosis skewness quartiles 1 2 3 and inter-quartile ranges 1-2 2-3 1-3. The delta coefficient for every LLD is also computed as an estimate of the first derivative hence leading to a total of 950 features. I would have never reached that far without the help of my supervisors. I warmly thank and appreciate Dr. Rached Zantout Dr. Lama Hamandi and Dr. Ziad Osman for their guidance support and constant supervision.,Other,,[],CC4,,,227,2462,560,Automatic Emotion Recognition,Arabic Natural Audio Dataset,https://www.kaggle.com/suso172/arabic-natural-audio-dataset,Fri Dec 01 2017
,UCI Machine Learning,"[tBodyAcc-mean()-X, tBodyAcc-mean()-Y, tBodyAcc-mean()-Z, tBodyAcc-std()-X, tBodyAcc-std()-Y, tBodyAcc-std()-Z, tBodyAcc-mad()-X, tBodyAcc-mad()-Y, tBodyAcc-mad()-Z, tBodyAcc-max()-X, tBodyAcc-max()-Y, tBodyAcc-max()-Z, tBodyAcc-min()-X, tBodyAcc-min()-Y, tBodyAcc-min()-Z, tBodyAcc-sma(), tBodyAcc-energy()-X, tBodyAcc-energy()-Y, tBodyAcc-energy()-Z, tBodyAcc-iqr()-X, tBodyAcc-iqr()-Y, tBodyAcc-iqr()-Z, tBodyAcc-entropy()-X, tBodyAcc-entropy()-Y, tBodyAcc-entropy()-Z, tBodyAcc-arCoeff()-X,1, tBodyAcc-arCoeff()-X,2, tBodyAcc-arCoeff()-X,3, tBodyAcc-arCoeff()-X,4, tBodyAcc-arCoeff()-Y,1, tBodyAcc-arCoeff()-Y,2, tBodyAcc-arCoeff()-Y,3, tBodyAcc-arCoeff()-Y,4, tBodyAcc-arCoeff()-Z,1, tBodyAcc-arCoeff()-Z,2, tBodyAcc-arCoeff()-Z,3, tBodyAcc-arCoeff()-Z,4, tBodyAcc-correlation()-X,Y, tBodyAcc-correlation()-X,Z, tBodyAcc-correlation()-Y,Z, tGravityAcc-mean()-X, tGravityAcc-mean()-Y, tGravityAcc-mean()-Z, tGravityAcc-std()-X, tGravityAcc-std()-Y, tGravityAcc-std()-Z, tGravityAcc-mad()-X, tGravityAcc-mad()-Y, tGravityAcc-mad()-Z, tGravityAcc-max()-X, tGravityAcc-max()-Y, tGravityAcc-max()-Z, tGravityAcc-min()-X, tGravityAcc-min()-Y, tGravityAcc-min()-Z, tGravityAcc-sma(), tGravityAcc-energy()-X, tGravityAcc-energy()-Y, tGravityAcc-energy()-Z, tGravityAcc-iqr()-X, tGravityAcc-iqr()-Y, tGravityAcc-iqr()-Z, tGravityAcc-entropy()-X, tGravityAcc-entropy()-Y, tGravityAcc-entropy()-Z, tGravityAcc-arCoeff()-X,1, tGravityAcc-arCoeff()-X,2, tGravityAcc-arCoeff()-X,3, tGravityAcc-arCoeff()-X,4, tGravityAcc-arCoeff()-Y,1, tGravityAcc-arCoeff()-Y,2, tGravityAcc-arCoeff()-Y,3, tGravityAcc-arCoeff()-Y,4, tGravityAcc-arCoeff()-Z,1, tGravityAcc-arCoeff()-Z,2, tGravityAcc-arCoeff()-Z,3, tGravityAcc-arCoeff()-Z,4, tGravityAcc-correlation()-X,Y, tGravityAcc-correlation()-X,Z, tGravityAcc-correlation()-Y,Z, tBodyAccJerk-mean()-X, tBodyAccJerk-mean()-Y, tBodyAccJerk-mean()-Z, tBodyAccJerk-std()-X, tBodyAccJerk-std()-Y, tBodyAccJerk-std()-Z, tBodyAccJerk-mad()-X, tBodyAccJerk-mad()-Y, tBodyAccJerk-mad()-Z, tBodyAccJerk-max()-X, tBodyAccJerk-max()-Y, tBodyAccJerk-max()-Z, tBodyAccJerk-min()-X, tBodyAccJerk-min()-Y, tBodyAccJerk-min()-Z, tBodyAccJerk-sma(), tBodyAccJerk-energy()-X, tBodyAccJerk-energy()-Y, tBodyAccJerk-energy()-Z, tBodyAccJerk-iqr()-X]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",The Human Activity Recognition database was built from the recordings of 30 study participants performing activities of daily living (ADL) while carrying a waist-mounted smartphone with embedded inertial sensors. The objective is to classify activities into one of the six activities performed. Description of experiment The experiments have been carried out with a group of 30 volunteers within an age bracket of 19-48 years. Each person performed six activities (WALKING WALKING_UPSTAIRS WALKING_DOWNSTAIRS SITTING STANDING LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist. Using its embedded accelerometer and gyroscope we captured 3-axial linear acceleration and 3-axial angular velocity at a constant rate of 50Hz. The experiments have been video-recorded to label the data manually. The obtained dataset has been randomly partitioned into two sets where 70% of the volunteers was selected for generating the training data and 30% the test data.  The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window). The sensor acceleration signal which has gravitational and body motion components was separated using a Butterworth low-pass filter into body acceleration and gravity. The gravitational force is assumed to have only low frequency components therefore a filter with 0.3 Hz cutoff frequency was used. From each window a vector of features was obtained by calculating variables from the time and frequency domain. Attribute information For each record in the dataset the following is provided   Triaxial acceleration from the accelerometer (total acceleration) and the estimated body acceleration.  Triaxial Angular velocity from the gyroscope.  A 561-feature vector with time and frequency domain variables.  Its activity label.  An identifier of the subject who carried out the experiment.  Relevant papers Davide Anguita Alessandro Ghio Luca Oneto Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. International Workshop of Ambient Assisted Living (IWAAL 2012). Vitoria-Gasteiz Spain. Dec 2012  Davide Anguita Alessandro Ghio Luca Oneto Xavier Parra Jorge L. Reyes-Ortiz. Energy Efficient Smartphone-Based Activity Recognition using Fixed-Point Arithmetic. Journal of Universal Computer Science. Special Issue in Ambient Assisted Living Home Care. Volume 19 Issue 9. May 2013 Davide Anguita Alessandro Ghio Luca Oneto Xavier Parra and Jorge L. Reyes-Ortiz. Human Activity Recognition on Smartphones using a Multiclass Hardware-Friendly Support Vector Machine. 4th International Workshop of Ambient Assited Living IWAAL 2012 Vitoria-Gasteiz Spain December 3-5 2012. Proceedings. Lecture Notes in Computer Science 2012 pp 216-223.  Jorge Luis Reyes-Ortiz Alessandro Ghio Xavier Parra-Llanas Davide Anguita Joan Cabestany Andreu Català. Human Activity and Motion Disorder Recognition Towards Smarter Interactive Cognitive Environments. 21st European Symposium on Artificial Neural Networks Computational Intelligence and Machine Learning ESANN 2013. Bruges Belgium 24-26 April 2013. Citation Davide Anguita Alessandro Ghio Luca Oneto Xavier Parra and Jorge L. Reyes-Ortiz. A Public Domain Dataset for Human Activity Recognition Using Smartphones. 21st European Symposium on Artificial Neural Networks Computational Intelligence and Machine Learning ESANN 2013. Bruges Belgium 24-26 April 2013.,CSV,,"[sociology, human-computer interaction, telecommunications]",CC0,,,4401,46551,64,Recordings of 30 study participants performing activities of daily living,Human Activity Recognition with Smartphones,https://www.kaggle.com/uciml/human-activity-recognition-with-smartphones,Thu Oct 06 2016
,Akshay Babbar,"[accident_index, vehicle_reference, vehicle_type, towing_and_articulation, vehicle_manoeuvre, vehicle_location-restricted_lane, junction_location, skidding_and_overturning, hit_object_in_carriageway, vehicle_leaving_carriageway, hit_object_off_carriageway, 1st_point_of_impact, was_vehicle_left_hand_drive?, journey_purpose_of_driver, sex_of_driver, age_of_driver, age_band_of_driver, engine_capacity_(cc), propulsion_code, age_of_vehicle, driver_imd_decile, driver_home_area_type, vehicle_imd_decile, NUmber_of_Casualities_unique_to_accident_index, No_of_Vehicles_involved_unique_to_accident_index, location_easting_osgr, location_northing_osgr, longitude, latitude, police_force, accident_severity, number_of_vehicles, number_of_casualties, date, day_of_week, time, local_authority_(district), local_authority_(highway), 1st_road_class, 1st_road_number, road_type, speed_limit, junction_detail, junction_control, 2nd_road_class, 2nd_road_number, pedestrian_crossing-human_control, pedestrian_crossing-physical_facilities, light_conditions, weather_conditions, road_surface_conditions, special_conditions_at_site, carriageway_hazards, urban_or_rural_area, did_police_officer_attend_scene_of_accident, lsoa_of_accident_location, casualty_reference, casualty_class, sex_of_casualty, age_of_casualty, age_band_of_casualty, casualty_severity, pedestrian_location, pedestrian_movement, car_passenger, bus_or_coach_passenger, pedestrian_road_maintenance_worker, casualty_type, casualty_home_area_type, casualty_imd_decile]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Road Accidents Content Dataset has been fetched from here and the files have been merged and cleaned to reach the final data attached. Primarily Captures Road Accidents in UK between 1979 and 2015 and has 70 features/columns and about 250K rows. Also attached with it is an excel file with Multiple Tabs that can help one to understand the Data. Acknowledgements Data has been fetched from Open Data Platform UK and is being shared under Open Government Licence. For more details refer to Open Data UK,CSV,,[road transport],Other,,,1619,9643,68,Road Accidents Data Great Britain 1979-2015,Road Accidents Incidence,https://www.kaggle.com/akshay4/road-accidents-incidence,Mon Jan 23 2017
,Gokagglers ,"[Age, Number of sexual partners, First sexual intercourse, Num of pregnancies, Smokes, Smokes (years), Smokes (packs/year), Hormonal Contraceptives, Hormonal Contraceptives (years), IUD, IUD (years), STDs, STDs (number), STDs:condylomatosis, STDs:cervical condylomatosis, STDs:vaginal condylomatosis, STDs:vulvo-perineal condylomatosis, STDs:syphilis, STDs:pelvic inflammatory disease, STDs:genital herpes, STDs:molluscum contagiosum, STDs:AIDS, STDs:HIV, STDs:Hepatitis B, STDs:HPV, STDs: Number of diagnosis, STDs: Time since first diagnosis, STDs: Time since last diagnosis, Dx:Cancer, Dx:CIN, Dx:HPV, Dx, Hinselmann, Schiller, Citology, Biopsy]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Cervical Cancer Risk Factors for Biopsy This Dataset is Obtained from UCI Repository and kindly acknowledged! This file contains a List of Risk Factors for Cervical Cancer leading to a Biopsy Examination! About 11000 new cases of invasive cervical cancer are diagnosed each year in the U.S. However the number of new cervical cancer cases has been declining steadily over the past decades. Although it is the most preventable type of cancer each year cervical cancer kills about 4000 women in the U.S. and about 300000 women worldwide. In the United States cervical cancer mortality rates plunged by 74% from 1955 - 1992 thanks to increased screening and early detection with the Pap test. AGE Fifty percent of cervical cancer diagnoses occur in women ages 35 - 54 and about 20% occur in women over 65 years of age. The median age of diagnosis is 48 years. About 15% of women develop cervical cancer between the ages of 20 - 30. Cervical cancer is extremely rare in women younger than age 20. However many young women become infected with multiple types of human papilloma virus which then can increase their risk of getting cervical cancer in the future. Young women with early abnormal changes who do not have regular examinations are at high risk for localized cancer by the time they are age 40 and for invasive cancer by age 50. SOCIOECONOMIC AND ETHNIC FACTORS Although the rate of cervical cancer has declined among both Caucasian and African-American women over the past decades it remains much more prevalent in African-Americans -- whose death rates are twice as high as Caucasian women. Hispanic American women have more than twice the risk of invasive cervical cancer as Caucasian women also due to a lower rate of screening. These differences however are almost certainly due to social and economic differences. Numerous studies report that high poverty levels are linked with low screening rates. In addition lack of health insurance limited transportation and language difficulties hinder a poor woman’s access to screening services. HIGH SEXUAL ACTIVITY Human papilloma virus (HPV) is the main risk factor for cervical cancer. In adults the most important risk factor for HPV is sexual activity with an infected person. Women most at risk for cervical cancer are those with a history of multiple sexual partners sexual intercourse at age 17 years or younger or both. A woman who has never been sexually active has a very low risk for developing cervical cancer. Sexual activity with multiple partners increases the likelihood of many other sexually transmitted infections (chlamydia gonorrhea syphilis).Studies have found an association between chlamydia and cervical cancer risk including the possibility that chlamydia may prolong HPV infection. FAMILY HISTORY Women have a higher risk of cervical cancer if they have a first-degree relative (mother sister) who has had cervical cancer. USE OF ORAL CONTRACEPTIVES Studies have reported a strong association between cervical cancer and long-term use of oral contraception (OC). Women who take birth control pills for more than 5 - 10 years appear to have a much higher risk HPV infection (up to four times higher) than those who do not use OCs. (Women taking OCs for fewer than 5 years do not have a significantly higher risk.) The reasons for this risk from OC use are not entirely clear. Women who use OCs may be less likely to use a diaphragm condoms or other methods that offer some protection against sexual transmitted diseases including HPV. Some research also suggests that the hormones in OCs might help the virus enter the genetic material of cervical cells. HAVING MANY CHILDREN Studies indicate that having many children increases the risk for developing cervical cancer particularly in women infected with HPV. SMOKING Smoking is associated with a higher risk for precancerous changes (dysplasia) in the cervix and for progression to invasive cervical cancer especially for women infected with HPV. IMMUNOSUPPRESSION Women with weak immune systems (such as those with HIV / AIDS) are more susceptible to acquiring HPV. Immunocompromised patients are also at higher risk for having cervical precancer develop rapidly into invasive cancer. DIETHYLSTILBESTROL (DES) From 1938 - 1971 diethylstilbestrol (DES) an estrogen-related drug was widely prescribed to pregnant women to help prevent miscarriages. The daughters of these women face a higher risk for cervical cancer. DES is no longer prsecribed.,CSV,,"[healthcare, human genetics]",Other,,,4477,21628,0.09765625,prediction of  cancer indicators; Please download; run kernel & upvote,Cervical Cancer Risk Classification,https://www.kaggle.com/loveall/cervical-cancer-risk-classification,Thu Aug 31 2017
,Marco De Nadai,"[PROVINCIA, P1, P2, P3, P4, P5, P6, P7, P8, P9, P10, P11, P12, P13, P14, P15, P16, P17, P18, P19, P20, P21, P22, P23, P24, P25, P26, P27, P28, P29, P30, P31, P32, P33, P34, P35, P36, P37, P38, P39, P40, P41, P42, P43, P44, P45, P46, P47, P48, P49, P50, P51, P52, P53, P54, P55, P56, P57, P58, P59, P60, P61, P62, P64, P65, P66, P128, P129, P130, P131, P132, P135, P136, P137, P138, P139, P140, ST1, ST2, ST3, ST4, ST5, ST6, ST7, ST8, ST9, ST10, ST11, ST12, ST13, ST14, ST15, A2, A3, A5, A44, A46, A47, A48, PF1]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Introduction The Mobile phone activity dataset is composed by one week of Call Details Records (CDRs) from the city of Milan and the Province of Trentino (Italy).  Description of the dataset Every time a user engages a telecommunication interaction a Radio Base Station (RBS) is assigned by the operator and delivers the communication through the network. Then a new CDR is created recording the time of the interaction and the RBS which handled it. The following activities are present in the dataset   received SMS sent SMS incoming calls outgoing calls Internet activity  In particular Internet activity is generated each time a user starts an Internet connection or ends an Internet connection. Moreover during the same connection a CDR is generated if the connection lasts for more than 15 min or the user transferred more than 5 MB.  The datasets is spatially aggregated in a square cells grid. The area of Milan is composed of a grid overlay of 1000 (squares with size of about 235×235 meters. This grid is projected with the WGS84 (EPSG4326) standard. For more details we link the original paper http//go.nature.com/2fcOX5E The data provides CellID CountryCode and all the aforementioned telecommunication activities aggregated every 60 minutes. Original datasource The Mobile phone activity dataset is a part of the Telecom Italia Big Data Challenge 2014 which is a rich and open multi-source aggregation of telecommunications weather news social networks and electricity data from the city of Milan and the Province of Trentino (Italy).  The original dataset has been created by Telecom Italia in association with EIT ICT Labs SpazioDati MIT Media Lab Northeastern University Polytechnic University of Milan Fondazione Bruno Kessler University of Trento and Trento RISE. In order to make it easy-to-use here we provide a subset of telecommunications data that allows researchers to design algorithms able to exploit an enormous number of behavioral and social indicators. The complete version of the dataset is available at the following link http//go.nature.com/2fz4AFr Relevant external data sources The presented datasets can be enriched by using census data provided by the Italian National Institute of Statistics (ISTAT) (http//www.istat.it/en/) a public research organization and the main provider of official statistics in Italy. The census data have been released for 1999 2001 and 2011.  The dataset (http//www.istat.it/it/archivio/104317) released in Italian is composed of four parts Territorial Bases (Basi Territoriali) Administrative Boundaries (Confini Amministrativi) Census Variables (Variabili Censuarie) and data about Toponymy (Dati Toponomastici). Motivational video https//www.youtube.com/watch?v=_d2_RWMsUKc Relevant papers Blondel Vincent D. Adeline Decuyper and Gautier Krings. ""A survey of results on mobile phone datasets analysis."" EPJ Data Science 4 no. 1 (2015) 1. Francesco Calabrese Laura Ferrari and Vincent D. Blondel. 2014. Urban Sensing Using Mobile Phone Network Data A Survey of Research. ACM Comput. Surv. 47 2 Article 25 (November 2014) 20 pages. Eagle Nathan Michael Macy and Rob Claxton. ""Network diversity and economic development."" Science 328 no. 5981 (2010) 1029-1031. Lenormand Maxime Miguel Picornell Oliva G. Cantú-Ros Thomas Louail Ricardo Herranz Marc Barthelemy Enrique Frías-Martínez Maxi San Miguel and José J. Ramasco. ""Comparing and modelling land use organization in cities."" Royal Society open science 2 no. 12 (2015) 150449. Louail Thomas Maxime Lenormand Oliva G. Cantu Ros Miguel Picornell Ricardo Herranz Enrique Frias-Martinez José J. Ramasco and Marc Barthelemy. ""From mobile phone data to the spatial structure of cities."" Scientific reports 4 (2014). De Nadai Marco Jacopo Staiano Roberto Larcher Nicu Sebe Daniele Quercia and Bruno Lepri. ""The Death and Life of Great Italian Cities A Mobile Phone Data Perspective."" WWW 2016. Citation We kindly ask people who use this dataset to cite the following paper where this aggregation comes from Barlacchi Gianni Marco De Nadai Roberto Larcher Antonio Casella Cristiana Chitic Giovanni Torrisi Fabrizio Antonelli Alessandro Vespignani Alex Pentland and Bruno Lepri. ""A multi-source dataset of urban life in the city of Milan and the Province of Trentino."" Scientific data 2 (2015).",CSV,,"[cities, internet, telecommunications]",ODbL,,,2619,25703,1024,"Hourly phone calls, SMS and Internet communication of an entire city",Mobile phone activity in a city,https://www.kaggle.com/marcodena/mobile-phone-activity,Fri Nov 11 2016
,HarshaVardhan,"[Match_Id, Innings_Id, Over_Id, Ball_Id, Team_Batting_Id, Team_Bowling_Id, Striker_Id, Striker_Batting_Position, Non_Striker_Id, Bowler_Id, Batsman_Scored, Extra_Type, Extra_Runs, Player_dissimal_Id, Dissimal_Type, Fielder_Id]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string]",Upon request from some users I am uploading  CSV Version. Yes There is already a dataset from manas However I thought this dataset is different than that one.which includes player metadata information about all the 11 players who participated in the match. Thoughts    ■ who are the valuable players for respective teams.   ■ who are more effective bowlers to bowl in the slog overs  is it spinners ?    ■ Dhoni's strike rate against left-arm spinners in last five overs Have fun with this dataset.  Files in the dataset include   1. Ball_by_Ball  Includes ball by ball details of all the 577 matches.   2. Match  Match metadata    3. Player  Player metadata    4. Player_Match  to know  who is the captain and keeper of the match  Includes every player who take part in match even If player haven't get a chance to either bat or bowl.    5. Season  Season wise details  Orange cap  Purple cap  Man_Of_The_Series    6. Team  Team Name   Diagram ,CSV,,[cricket],Other,,,3236,15736,6,577 matches up to season 9,Indian Premier League CSV dataset,https://www.kaggle.com/harsha547/indian-premier-league-csv-dataset,Fri Dec 30 2016
,utmhikari,"[ID, Movie_Name_EN, Movie_Name_CN, Crawl_Date, Number, Username, Date, Star, Comment, Like]","[numeric, string, string, dateTime, numeric, string, dateTime, numeric, string, numeric]","Douban Movie Short Comments Dataset V2 Introduction Douban Movie is a Chinese website that allows Internet users to share their comments and viewpoints about movies. Users are able to post short or long comments on movies and give them marks. This dataset contains more than 2 million short comments of 28 movies in Douban Movie website. It can be used on text classification text clustering sentiment analysis semantic web construction and some other fields that relate to web mining or NLP (of Chinese lol). Metadata ID the ID of the comment (start from 0)  Movie_Name_EN the English name of the movie  Movie_Name_CN the Chinese name of the movie  Crawl_Date the date that the data are crawled  Number the number of the comment  Username the username of the account  Date the date that the comment posted  Star the star that users give to the movie (from 1 to 5 5 grades)  Comment the content of the comment  Like the count of ""like"" on the comment ",CSV,,"[film, internet]",CC0,,,518,7236,387,DMSCD producted by utmhikari,Douban Movie Short Comments Dataset,https://www.kaggle.com/utmhikari/doubanmovieshortcomments,Mon Mar 06 2017
,World Bank,"[Country.Code, Short.Name, Table.Name, Long.Name, 2-alpha.code, Currency.Unit, Special.Notes, Region, Income.Group, WB-2.code, National.accounts.base.year, National.accounts.reference.year, SNA.price.valuation, Lending.category, Other.groups, System.of.National.Accounts, Alternative.conversion.factor, PPP.survey.year, Balance.of.Payments.Manual.in.use, External.debt.Reporting.status, System.of.trade, Government.Accounting.concept, IMF.data.dissemination.standard, Latest.population.census, Latest.household.survey, Source.of.most.recent.Income.and.expenditure.data, Vital.registration.complete, Latest.agricultural.census, Latest.industrial.data, Latest.trade.data, Latest.water.withdrawal.data]","[string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric]",The Gender Statistics database is a comprehensive source for the latest sex-disaggregated data and gender statistics covering demography education health access to economic opportunities public life and decision-making and agency. The Data The data is split into several files with the main one being Data.csv. The Data.csv contains all the variables of interest in this dataset while the others are lists of references and general nation-by-nation information. Data.csv contains the following fields Data.csv  Country.Name the name of the country Country.Code the country's code Indicator.Name the name of the variable that this row represents Indicator.Code a unique id for the variable 1960 - 2016 one column EACH for the value of the variable in each year it was available  The other files I couldn't find any metadata for these and I'm not qualified to guess at what each of the variables mean. I'll list the variables for each file and if anyone has any suggestions (or even better actual knowledge/citations) as to what they mean please leave a note in the comments and I'll add your info to the data description. Country-Series.csv  CountryCode SeriesCode DESCRIPTION  Country.csv  Country.Code Short.Name Table.Name Long.Name 2-alpha.code Currency.Unit Special.Notes Region Income.Group WB-2.code National.accounts.base.year National.accounts.reference.year SNA.price.valuation Lending.category Other.groups System.of.National.Accounts Alternative.conversion.factor PPP.survey.year Balance.of.Payments.Manual.in.use External.debt.Reporting.status System.of.trade Government.Accounting.concept IMF.data.dissemination.standard Latest.population.census Latest.household.survey Source.of.most.recent.Income.and.expenditure.data Vital.registration.complete Latest.agricultural.census Latest.industrial.data Latest.trade.data Latest.water.withdrawal.data  FootNote.csv  CountryCode SeriesCode Year DESCRIPTION  Series-Time.csv  SeriesCode Year DESCRIPTION  Series.csv  Series.Code Topic Indicator.Name Short.definition Long.definition Unit.of.measure Periodicity Base.Period Other.notes Aggregation.method Limitations.and.exceptions Notes.from.original.source General.comments Source Statistical.concept.and.methodology Development.relevance Related.source.links Other.web.links Related.indicators License.Type  Acknowledgements This dataset was downloaded from The World Bank's Open Data project. The summary of the Terms of Use of this data is as follows  You are free to copy distribute adapt display or include the data in other products for commercial and noncommercial purposes at no cost subject to certain limitations summarized below. You must include attribution for the data you use in the manner indicated in the metadata included with the data. You must not claim or imply that The World Bank endorses your use of the data by or use The World Bank’s logo(s) or trademark(s) in conjunction with such use. Other parties may have ownership interests in some of the materials contained on The World Bank Web site. For example we maintain a list of some specific data within the Datasets that you may not redistribute or reuse without first contacting the original content provider as well as information regarding how to contact the original content provider. Before incorporating any data in other products please check the list Terms of use Restricted Data.  -- [ed. note this last is not applicable to the Gender Statistics database]  The World Bank makes no warranties with respect to the data and you agree The World Bank shall not be liable to you in connection with your use of the data. This is only a summary of the Terms of Use for Datasets Listed in The World Bank Data Catalogue. Please read the actual agreement that controls your use of the Datasets which is available here Terms of use for datasets. Also see World Bank Terms and Conditions. ,CSV,,"[gender, education, demographics]",Other,,,1029,8047,76,"Sex-disaggregated and gender-specific data on demographics, education, etc.",World Gender Statistics,https://www.kaggle.com/theworldbank/world-gender-statistics,Mon Nov 28 2016
,Sohier Dane,"[State, District, Year, Party, Incumbent, Dem Votes, GOP Votes, Other Votes]","[string, numeric, numeric, string, numeric, numeric, numeric, numeric]",Context This dataset contains results of general elections to the lower house of the state legislatures in the United States over the last fifty years up to 2012. This dataset was created by the Princeton Gerrymandering Project as part of their effort to analyze and combat partisan gerrymandering. The Supreme Court will be hearing a very important case on this issue on October 3rd 2017. Regardless of who wins this dataset will be of interest to anyone hoping to defeat (or achieve!) a gerrymandering attempt. Content Each row represents one election from the perspective of the winner. For example the first row of the data should be read as a victory for a Democrat who was not the incumbent. Acknowledgements This dataset was kindly made available by the Princeton Gerrymandering Project. You can find their copy detailed discussion of the data and their code here.,CSV,,"[politics, political science]",CC0,,,161,1404,2,"Results for 72,000 elections",State Election Results 1971 - 2012,https://www.kaggle.com/sohier/state-election-results-1971-2012,Tue Oct 03 2017
,US Patent and Trademark Office,[],[],Grant Red Book (GRB) Full Text Context Every Tuesday the US Patent and Trademark Office (USPTO) issues approximately 6000 patent grants (patents) and posts the full text of the patents online. These patent grant documents contain much of the supporting details for a given patent. From this data we can track trends in innovation across industries.  Frequency Weekly (Tuesdays) Period 10/11/2016  Content The fields include patent number series code and application number type of patent filing date title issue date applicant information inventor information assignee(s) at time of issue foreign priority information related US patent documents classification information (IPCR CPC US) US and foreign references attorney agent or firm/legal representative examiner citations Patent Cooperation Treaty (PCT) information abstract specification and claims. Inspiration How many times will you find “some assembly required”? What inventions are at the cutting edge of machine learning? To answer these questions and any others you may have about this catalog of knowledge fork the kernel Library of Inventions and Innovations which demonstrates how to work with XML files in Python.  Acknowledgements The USPTO owns the dataset. These files are a subset and concatenation of the Patent Grant Data/XML Version 4.5 ICE (Grant Red Book). Because of the concatenation of the individual XML documents these files will not parse successfully or open/display by default in Internet Explorer. They also will not import into MS Excel. Each XML document within the file should have one start tag and one end tag. Concatenation creates a file that contains 6000 plus start/end tag combinations. If you take one document out of the Patent Grant Full Text file and place it in a directory with the correct DTD and then double click that individual document Internet Explorer will parse/open the document successfully. NOTE  You may receive a warning about Active X controls. NOTE  All Patent Grant Full Text files will open successfully in MS Word; NotePad; WordPad; and TextPad. License Creative Commons - Public Domain Mark 1.0,Other,,[law],Other,,,305,5585,569,Contains the full text of each patent grant issued weekly for 10/11/2016,Patent Grant Full Text,https://www.kaggle.com/uspto/patent-grant-full-text,Thu Oct 20 2016
,Food and Drug Administration,"[RA_Report #, RA_CAERS Created Date, AEC_Event Start Date, PRI_Product Role, PRI_Reported Brand/Product Name, PRI_FDA Industry Code, PRI_FDA Industry Name, CI_Age at Adverse Event, CI_Age Unit, CI_Gender, AEC_One Row Outcomes, SYM_One Row Coded Symptoms]","[numeric, dateTime, dateTime, string, string, numeric, string, numeric, string, string, string, string]",Context The CFSAN Adverse Event Reporting System (CAERS) is a database that contains information on adverse event and product complaint reports submitted to FDA for foods dietary supplements and cosmetics. The database is designed to support CFSAN's safety surveillance program. Adverse events are coded to terms in the Medical Dictionary for Regulatory Activities (MedDRA) terminology. Content See the metadata description in the accompanying README.pdf below or here. Approximately 90k reactions are recorded from 2004-mid 2017 with 12 columns of information regarding type of reaction and related event details. Acknowledgements This dataset is collected by the US Food and Drug Administration. Inspiration  What are the most commonly reported foodstuffs? What are the most commonly reported medical reactions to foods? Where do people in the US most commonly report food-related conditions? ,CSV,,"[government agencies, food and drink, healthcare]",CC0,,,264,2466,19,~90k FDA Recorded Medical Events,Adverse Food Events,https://www.kaggle.com/fda/adverse-food-events,Fri Sep 08 2017
,PyTorch,[],[],DenseNet-121  Densely Connected Convolutional Networks Recent work has shown that convolutional networks can be substantially deeper more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet) which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer the feature-maps of all preceding layers are used as inputs and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages they alleviate the vanishing-gradient problem strengthen feature propagation encourage feature reuse and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10 CIFAR-100 SVHN and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them whilst requiring less memory and computation to achieve high performance. Code and models are available at this https URL. Authors Gao Huang Zhuang Liu Kilian Q. Weinberger Laurens van der Maaten https//arxiv.org/abs/1608.06993   DenseNet Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,1,372,29,DenseNet-121 Pre-trained Model for PyTorch,DenseNet-121,https://www.kaggle.com/pytorch/densenet121,Wed Dec 13 2017
,BurakH,[],[],Predict molecular properties Context This dataset contains molecular properties scraped from the PubChem database. Each file contains properties for thousands of molecules  made up of the elements H C N O F Si P S Cl Br and I. The dataset is related to a previous one which had fewer number of molecules where the features were preconstructed.  Instead this dataset is a challenging case for feature engineering and is subject of active research (see references below).  Data Description The utilities used to download and process the data can be accessed from my Github repo. Each JSON file contains a list of molecular data. An example molecule is given below { 'En' 37.801 'atoms' [ {'type' 'O' 'xyz' [0.3387 0.9262 0.46]} {'type' 'O' 'xyz' [3.4786 -1.7069 -0.3119]} {'type' 'O' 'xyz' [1.8428 -1.4073 1.2523]} {'type' 'O' 'xyz' [0.4166 2.5213 -1.2091]} {'type' 'N' 'xyz' [-2.2359 -0.7251 0.027]} {'type' 'C' 'xyz' [-0.7783 -1.1579 0.0914]} {'type' 'C' 'xyz' [0.1368 -0.0961 -0.5161]} {'type' 'C' 'xyz' [-3.1119 -1.7972 0.659]} {'type' 'C' 'xyz' [-2.4103 0.5837 0.784]} {'type' 'C' 'xyz' [-2.6433 -0.5289 -1.426]} {'type' 'C' 'xyz' [1.4879 -0.6438 -0.9795]} {'type' 'C' 'xyz' [2.3478 -1.3163 0.1002]} {'type' 'C' 'xyz' [0.4627 2.1935 -0.0312]} {'type' 'C' 'xyz' [0.6678 3.1549 1.1001]} {'type' 'H' 'xyz' [-0.7073 -2.1051 -0.4563]} {'type' 'H' 'xyz' [-0.5669 -1.3392 1.1503]} {'type' 'H' 'xyz' [-0.3089 0.3239 -1.4193]} {'type' 'H' 'xyz' [-2.9705 -2.7295 0.1044]} {'type' 'H' 'xyz' [-2.8083 -1.921 1.7028]} {'type' 'H' 'xyz' [-4.1563 -1.4762 0.6031]} {'type' 'H' 'xyz' [-2.0398 1.417 0.1863]} {'type' 'H' 'xyz' [-3.4837 0.7378 0.9384]} {'type' 'H' 'xyz' [-1.9129 0.5071 1.7551]} {'type' 'H' 'xyz' [-2.245 0.4089 -1.819]} {'type' 'H' 'xyz' [-2.3 -1.3879 -2.01]} {'type' 'H' 'xyz' [-3.7365 -0.4723 -1.463]} {'type' 'H' 'xyz' [1.3299 -1.3744 -1.7823]} {'type' 'H' 'xyz' [2.09 0.1756 -1.3923]} {'type' 'H' 'xyz' [-0.1953 3.128 1.7699]} {'type' 'H' 'xyz' [0.7681 4.1684 0.7012]} {'type' 'H' 'xyz' [1.5832 2.901 1.6404]} ] 'id' 1 'shapeM' [259.66  4.28  3.04  1.21  1.75  2.55  0.16  -3.13  -0.22  -2.18  -0.56  0.21  0.17  0.09] }  En This field is the molecular energy calculated using a force-field method. See references [12] for details. This is the target variable which is being predicted. atoms This field contains the name of the element and the position (xyz coordinates) and needs to be used for feature engineering. id  This field is the PubChem Id  shapeM  This field contains the shape multipoles and can be used for feature engineering. For definition of shape multipoles see reference [3].  Notice that each molecule contains different number and types of atoms so it is challenging to come up with features that can describe every molecule in  a unique way. There are several approaches taken in the literature (see the references) one of which is to use the Coulomb Matrix for a given molecule defined by $$ C_{IJ} = \frac{Z_I  Z_J}{\vert R_I - R_J \vert} \quad  ({\rm I \neq J}) \qquad C_{IJ} = Z_I^{2.4} \quad (I=J) $$ where $Z_I$ are atomic numbers (can be looked up from the periodic table for each element) and ${\vert R_I - R_J \vert}$ is the distance between two atoms I and J.  The previous dataset used these features for a subset of molecules given here where the maximum number of elements in a given molecules was limited by 50. In this case each molecule has a 50x50 Coulomb matrix where zeroes were used as padding when the molecule had smaller than 50 number of atoms.  There are around 100000000 molecules in the whole database. As more files are scraped new data will be added in time.  Note In the previous dataset the molecular energies were computed by quantum mechanical simulations. Here the given energies are computed using another method so their values are different.  Inspiration Simulations of molecular properties are computationally expensive. The purpose of this project is to use machine learning methods to come up with a model that can predict molecular properties from a database. In the PubChem database there are around 100000000 molecules. It could take years to do simulations on all  of these molecules however machine learning can be used to predict their properties much faster. As a result this could open up many possibilities in computational design and discovery of molecules compounds and new drugs. This is a regression problem so mean squared error is minimized during training. I am looking for Kagglers to find the best model and reduce mean squared error as much as possible! References [1] Halgren TA. Merck Molecular Force Field I. Basis Form Scope Parameterization and Performance of MMFF94.  J. Comp. Chem. 1996;17490-519. [2] Halgren TA. Merck Molecular Force Field VI. MMFF94s Option for Energy Minimization Studies.  J. Comp. Chem. 1999;20720-729. [3] Kim Sunghwan Evan E Bolton and Stephen H Bryant. “PubChem3D Shape Compatibility Filtering Using Molecular Shape Quadrupoles.” J. Cheminf. 3 (2011) 25. [4] Himmetoglu B. Tree based machine learning framework for predicting ground state energies of molecules J. Chem. Phys 145 134101 (2016) [5] Rupp M. Ramakrishnan R. von Lilienfeld OA. Machine Learning for Quantum Mechanical Properties of Atoms in Molecules  J. Phys. Chem. Lett.  6(16) 3309–3313 (2015) [6] Montavon G. Rupp M. Gobre V. Vazquez-Mayagoitia A. Hansen K. Tkatchenko A. Müller K-R. von Lilienfeld OA. Machine learning of molecular electronic properties in chemical compound space New J. Phys. 15(9) 095003 (2013) [7] Hansen K. Montavon G. Biegler F. Fazli S. Rupp M. Scheffler M. von Lilienfeld OA. Tkatchenko A. Müller K-P. Assessment and Validation of Machine Learning Methods for Predicting Molecular Atomization Energies J. Chem. Theory Comput.  9(8) 3543–3556 (2013),{}JSON,,"[chemistry, physics]",CC0,,,256,3298,1024,Help computational alchemy with machine learning!,Predict Molecular Properties,https://www.kaggle.com/burakhmmtgl/predict-molecular-properties,Tue Aug 15 2017
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,"[state_name, is_state, year, category, type, gender, age_16_18, age_18_30, age_30_50, age_50_above]","[string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric]",Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context This dataset contains the complete detail about the Prison and various characteristics of inmates. This will help to understand better about prison system in India. Content  Details of Jail wise population of prison inmates Details about the list of jails in India at the end of year 2015. Jail category wise population of inmates.  Capacity of jails by inmate population.  Age group nationality and gender wise population of inmates.   Religion and gender wise population of inmates. Caste and gender wise population of inmates.  Education standards of inmates.   Domicile of inmates.   Incidence of recidivism. Rehabilitation of prisoners. Distribution of sentence periods of convicts in various jails by sex and age-groups.  Details of under trial prisoners by the type of IPC (Indian Penal Code) offences. Details of convicts by the type of IPC (Indian Penal Code) offences. Details of SLL (special & local law) Crime headwise distribution of inmates who convicted Details of SLL (special & local law) Crime head wise distribution of inmates under trial Details of educational facilities provided to prisoners.  Details of Jail breaks group clashes and firing in jail (Tranquility). Details of wages per day to convicts.  Details of Prison inmates trained under different vocational training. Details of capital punishment (death sentence) and life imprisonment.  Details of prison inmates escaped.  Details of prison inmates released.  Details of Strength of officials Details of Total Budget and Actual Expenditure during the year 2015-16. Details of Budget Details of Expenditure Details of Expenditure on inmates Details of Inmates suffering from mental ilness Details of Period of detention of undertrials Details of Number of women prisoners with children Details of Details of inmates parole during the year Details of Value of goods produced by inmates Details of Number of vehicles available Details of Training of Jail Officers Details of Movements outside jail premises Details of Details of electronic equipment used in prison  Inspiration There are many questions about Indian prison with this dataset. Some of the interesting questions are  Percentage of jails over crowded. Is there any change in percentage over time? How many percentage of inmates re-arrested? Which state/u.t pay more wages to the inmates? Which state/u.t has more capital punishment/life imprisonment inmates? Inmates gender ratio per state   Acknowledgements National Crime Records Bureau (NCRB) Govt of India has shared this dataset under Govt. Open Data License - India. NCRB has also shared prison data on their website.,CSV,,"[india, crime]",CC4,,,558,3590,9,Details of Inmates are classified according to 35+ factors. (35+ csv files),Indian Prison Statistics  (2001 - 2013),https://www.kaggle.com/rajanand/prison-in-india,Tue Sep 05 2017
,Michal Januszewski,[],[],Context I've always wanted to have a proper sample Forex currency rates dataset for testing purposes so I've created one. Content The data contains Forex EURUSD currency rates in 15-minute slices (OHLC - Open High Low Close and Volume). BID price only. Spread is not provided so be careful.  (Quick reminder Bid price + Spread = Ask price) The dates are in the yyyy-mm-dd hhmm format GMT. Volume is in Units. Acknowledgements Dukascopy Bank SA https//www.dukascopy.com/swiss/english/marketwatch/historical/ Inspiration Just would like to see if there is still an way to beat the current Forex market conditions with the prop traders' advanced automatic algorithms running in the wild.,CSV,,"[finance, money]",CC4,,,483,4522,14,"FOREX currency rates data for EURUSD, 15 minute candles, BID, years 2010-2016",EURUSD - 15m - 2010-2016,https://www.kaggle.com/meehau/EURUSD,Wed Feb 22 2017
,4Quant,[],[],Summary The data is a preprocessed subset of the TCIA Study named Soft Tissue Sarcoma. The data have been converted from DICOM folders of varying resolution and data types to 3D HDF5 arrays with isotropic voxel size. This should make it easier to get started and test out various approaches (NN RF CRF etc) to improve segmentations. TCIA Summary This collection contains FDG-PET/CT and anatomical MR (T1-weighted T2-weighted with fat-suppression) imaging data from 51 patients with histologically proven soft-tissue sarcomas (STSs) of the extremities. All patients had pre-treatment FDG-PET/CT and MRI scans between November 2004 and November 2011. (Note date in the TCIA images have been changed in the interest of de-identification; the same change was applied across all images preserving the time intervals between serial scans). During the follow-up period 19 patients developed lung metastases. Imaging data and lung metastases development status were used in the following study Vallières M. et al. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology 60(14) 5471-5496. doi10.1088/0031-9155/60/14/5471. Imaging data tumor contours (RTstruct DICOM objects) clinical data and source code is available for this study. See the DOI below for more details and links to access the whole dataset. Please contact Martin Vallières (mart.vallieres@gmail.com) of the Medical Physics Unit of McGill University for any scientific inquiries about this dataset. Acknowledgements We would like to acknowledge the individuals and institutions that have provided data for this collection McGill University Montreal Canada - Special thanks to Martin Vallières of the Medical Physics Unit License This collection is freely available to browse download and use for commercial scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License.  See TCIA's Data Usage Policies and Restrictions for additional details. Questions may be directed to help@cancerimagingarchive.net. Citation Data Citation Vallières Martin Freeman Carolyn R. Skamene Sonia R. & El Naqa Issam. (2015). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. The Cancer Imaging Archive. http//doi.org/10.7937/K9/TCIA.2015.7GO2GSKS Publication Citation Vallières M. Freeman C. R. Skamene S. R. & Naqa I. El. (2015 June 29). A radiomics model from joint FDG-PET and MRI texture features for the prediction of lung metastases in soft-tissue sarcomas of the extremities. Physics in Medicine and Biology. IOP Publishing. http//doi.org/10.1088/0031-9155/60/14/5471 TCIA Citation Clark K Vendt B Smith K Freymann J Kirby J Koppel P Moore S Phillips S Maffitt D Pringle M Tarbox L Prior F. The Cancer Imaging Archive (TCIA) Maintaining and Operating a Public Information Repository Journal of Digital Imaging Volume 26 Number 6 December 2013 pp 1045-1057. (paper),Other,,[healthcare],Other,,,1114,11596,379,A challenge to automate tumor segmentation,Segmenting Soft Tissue Sarcomas,https://www.kaggle.com/4quant/soft-tissue-sarcoma,Fri Dec 09 2016
,NHTSA,"[STATE, ST_CASE, VE_TOTAL, VE_FORMS, PVH_INVL, PEDS, PERNOTMVIT, PERMVIT, PERSONS, COUNTY, CITY, DAY, MONTH, YEAR, DAY_WEEK, HOUR, MINUTE, NHS, RUR_URB, FUNC_SYS, RD_OWNER, ROUTE, TWAY_ID, TWAY_ID2, MILEPT, LATITUDE, LONGITUD, SP_JUR, HARM_EV, MAN_COLL, RELJCT1, RELJCT2, TYP_INT, WRK_ZONE, REL_ROAD, LGT_COND, WEATHER1, WEATHER2, WEATHER, SCH_BUS, RAIL, NOT_HOUR, NOT_MIN, ARR_HOUR, ARR_MIN, HOSP_HR, HOSP_MN, CF1, CF2, CF3, FATALS, DRUNK_DR]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Quick Start For a quick introduction to this Dataset take a look at the Kernel Traffic Fatalities Getting Started. See the Fatality Analysis Reporting System FARS User’s Manual for understanding the column abbreviations and possible values.  Also see the following reference Original source of this data containing all files can be obtained here  Below are the files released by the (NHTSA) National Highway Traffic Safety Administration in their original format.   Additional files can be found in the extra folder.  Reference Traffic Fatalities Getting Started. for how to access this extra folder with contents. Data Compared to 2014 A few interesting notes about this data compared to 2014  Pedalcyclist fatalities increased by 89 (12.2 percent) Motorcyclist fatalities increased by 382 (8.3-percent increase) Alcohol-impaired driving fatalities increased by 3.2 percent from 9943 in 2014 to 10265 in 2015 Vehicle miles traveled (VMT) increased by 3.5 percent from 2014 to 2015 the largest increase since 1992 nearly 25 years ago.  See TRAFFIC SAFETY FACTS for more detail on the above findings.,CSV,,[road transport],ODbL,,,3346,23972,88,National Highway Traffic Safety Administration,2015 Traffic  Fatalities,https://www.kaggle.com/nhtsa/2015-traffic-fatalities,Sat Nov 26 2016
,GetTheData,"[TR23 0PR, \N, None, \N, \N, \N, 87897, 15021, 49.953605, -6.352647]","[string, string, string, string, string, string, numeric, numeric, numeric, numeric]",Context This dataset takes the Environment Agency's Risk of Flooding from Rivers and Sea and places English postcodes in their appropriate flood risk area allowing you to look up flood risk from postcode. Content Risk of Flooding from Rivers and Sea consists of geographical areas within England which are at risk of flooding from rivers and sea. Each area is assigned a flood risk within a banding  High Medium Low Very low None  Open Flood Risk by Postcode takes postcodes as point locations (from Open Postcode Geo) and places the postcode in the appropriate flood risk area. It is important to note that actual properties within a specific postcode may have a slightly different point location and therefore be in a different flood risk area. Generally speaking the point location of a postcode is the point location of the central property in that postcode. For a full field list and explanations of values see the Open Flood Risk by Postcode documentation. Acknowledgements Open Flood Risk by Postcode is derived from two open datasets  Risk of Flooding from Rivers and Sea Open Postcode Geo  Both of these datasets are licensed under the OGL. The following attribution statements are required  Contains OS data © Crown copyright and database right 2017 Contains Royal Mail data © Royal Mail copyright and database right 2017 Contains National Statistics data © Crown copyright and database right 2017 Contains Environment Agency data licensed under the Open Government Licence v3.0.  The dataset is maintained by GetTheData. The latest version and full documentation is available here. Inspiration Example application Lookup or drill down to individual English postcodes to see a map of that postcode and its flood risk alongside surrounding postcodes and their flood risks  Application Flood Map by Postcode Example postcode RG9 2LP ,CSV,,[geography],Other,,,128,1898,85,English postcodes with Environment Agency flood risk,Open Flood Risk by Postcode,https://www.kaggle.com/getthedata/open-flood-risk-by-postcode,Tue Mar 21 2017
,William Cukierski,"[file, message]","[string, string]",The Enron email dataset contains approximately 500000 emails generated by employees of the Enron Corporation. It was obtained by the Federal Energy Regulatory Commission during its investigation of Enron's collapse. This is the May 7 2015 Version of dataset as published at https//www.cs.cmu.edu/~./enron/,CSV,,"[crime, linguistics]",Other,,,6608,62571,1024,"500,000+ emails from 150 employees of the Enron Corporation",The Enron Email Dataset,https://www.kaggle.com/wcukierski/enron-email-dataset,Fri Jun 17 2016
,Rachael Tatman,"[Country, Date of holiday, Year celebrated, Event celebrated, Name of holiday]","[string, string, numeric, string, string]",Context An Independence Day is an annual event commemorating the anniversary of a nation's independence or statehood usually after ceasing to be a group or part of another nation or state; more rarely after the end of a military occupation. Most countries observe their respective independence days as national holidays.   Content This dataset is a collection of information about 184 independence celebrations in countries around the world. Acknowledgements This dataset was taken from the Wikipedia article “List of national independence days” on July 17 2017. Inspiration  Are there seasonal clusters of independence day celebrations?  How popular is it for an independence celebration’s name to include the name of the country? Can you create an interactive visualization that shows when/where independence days are celebrated? ,CSV,,[],CC4,,,97,1176,0.021484375,Independence days around the world,Independence days,https://www.kaggle.com/rtatman/independence-days,Tue Jul 25 2017
,VishnuRaghavan,"[Date of Interview, Client name, Industry, Location, Position to be closed, Nature of Skillset, Interview Type, Name(Cand ID), Gender, Candidate Current Location, Candidate Job Location, Interview Venue, Candidate Native location, Have you obtained the necessary permission to start at the required time, Hope there will be no unscheduled meetings, Can I Call you three hours before the interview and follow up on your attendance for the interview, Can I have an alternative number/ desk number. I assure you that I will not trouble you too much, Have you taken a printout of your updated resume. Have you read the JD and understood the same, Are you clear with the venue details and the landmark., Has the call letter been shared, Expected Attendance, Observed Attendance, Marital Status, , , , , ]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context The data pertains to the recruitment industry in India for the years 2014-2016 and deals with candidate interview attendance for various clients. The details are largely self explanatory.  Content The data have been collected by me and my fellow researchers over a period of over 2 years between September 2014 and January 2017.  There are a set of questions that are asked by a recruiter while scheduling the candidate. The answers to these determine whether expected attendance is yes no or uncertain. For details on individual feeds see the Data tab. Acknowledgements A great measure of thanks goes to my colleaues in research and work.   Dr Rajendra Desai Associate Professor St Joseph's College of Management Bangalore Marcia Akshaya Leo HR recruiter Chennai Dr Rashmi Nakra Professor St Josephs College of Management Bangalore Prima Student St Josephs College of Management Bangalore Trupthi Student St Josephs College of Management Bangalore  Inspiration A few pointers  We have already attempted Naive Bayes with decent results. We would like to know if we can predict whether a candidate will attend interviews or not with other algorithms Also whether any of the present set of variables can be modified to yield better results Whether we can find additional variables from the internet and whether we can derive variables from the existing set of variables. ,Other,,"[strategy, business]",Other,,,468,3620,0.3671875,Predict which candidates will attend the intervew,The Interview Attendance Problem,https://www.kaggle.com/vishnusraghavan/the-interview-attendance-problem,Thu Dec 14 2017
,US Bureau of Labor Statistics,"[stateFips, area, areaType, period, periodYear, periodType, periodTypeDescription, cpi, title, type, source, cpiSourceDescription, percentChangeYear, percentChangeMonth, dataRegion, areaName, areaDescription]","[numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, string, numeric, numeric, string, string, string]",Context The Consumer Price Indexes (CPI) program produces monthly data on changes in the prices paid by urban consumers for a representative basket of goods and services. It is a useful way to compare changes in the economy across time. Content This data covers Jan 1913-May 2017 and is normalized to “CPI-U all items 1982-84=100 not seasonally adjusted”. Fields include time of measurement and CPI score. Acknowledgements This dataset was compiled on behalf of the Bureau of Labor Statistics (BLS) via Colorado Department of Labor & Employment (CDLE) and hosted on data.colorado.gov. Inspiration  What periods of time have seen the highest/lowest CPI?  When has inflation been the worse? Can you predict present CPI? ,CSV,,[economics],CC0,,,50,649,3,104 years of monthly CPI data,"Consumer Price Index in Denver, CO",https://www.kaggle.com/bls/denver-cpi,Tue Aug 22 2017
,Rachael Tatman,[],[],Context The fact that some languages extensively use suffixes and prefixes to convey grammatical meaning(e.g.  subject-verb agreement) poses a challenge to most current human language technology (HLT). Suffixes and prefixes in such languages can more generally be called morphemes which are defined as the meaningful subparts of words.  The rules that languages use to combine morphemes together with the actual morphemes that they use (i.e. suffixes and prefixes themselves) are both referred to as a language's morphology.  Languages which make extensive use of morphemes to build words are said to be morphologically-rich.  These include languages such as Turkish and can be contrasted with so-called analytic languages such as Mandarin Chinese which does not use suffixes or prefixes all. The  goal  of  the  Universal  Morphological  Feature  Schema  is  to  allow  an  inflected  word  from any language to be dened by its lexical meaning (typically carried in the root or stem) and by a  rendering  of  its  inflectional  morphemes  in  terms  of  features  from  the  schema  (i.e.  a  vector  of universal  morphological  features).   When  an  inflected  word  is  defined in  this  way  it  can  then  be translated  into  any  other  language  since  all  other  inflected  words  from  all  other  languages  can also  be  defined  in  terms  of  the  Universal  Morphological  Feature  Schema.   Although  building  an interlingual representation for the semantic content of human language as a whole is typically seen as prohibitively difficult the comparatively small extent of grammatical meanings that are conveyed by overt affixal inflectional morphology places a natural bound on the range of meaning that must be expressed by an interlingua for inflectional morphology. Content This dataset contains Unimorph morphological annotations for 352 languages. Each language’s annotations are in a separate file and each file has a different number of words. Many cells in each file are empty. This is because not every feature that is annotated applies to every part of speech. Nouns for example do not have a tense. In addition not every language makes use of every possible morphological marking. For instance English does not have an evidentiality inflection while other languages like Mongolian and Eastern Pomo do. Acknowledgments The Unimorph framework was developed by John Sylak-Glassman. If you use this framework in your work please cite the following paper Sylak-Glassman J. (2016). The composition and use of the universal morphological feature schema (unimorph schema). Technical report Department of Computer Science Johns Hopkins University. You may also like  Extinct Languages Number of endangered languages in the world and their likelihood of extinc Stopword Lists for 19 Languages Lists of high-frequency words usually removed during NLP analysis Atlas of Pidgin and Creole Language Structures Information on 76 Creole and Pidgin Languages ,Other,,[linguistics],CC4,,,53,773,1024,Morphological annotation for 352 languages,Unimorph,https://www.kaggle.com/rtatman/unimorph,Sat Aug 12 2017
,Dalia Research,"[question_id_mapped, question_wording, question_kind, code, value]","[string, string, string, numeric, string]","Context  The election of Donald Trump has taken the world by surprise and is fuelling populist movements in Europe e.g. in Italy Austria and France. Understanding populism and assessing the impact of the “Trump effect” on Europe is a tremendous challenge and Dalia wants to help pool brainpower to find answers.  The goal is to find out where the next wave of populism could hit in Europe by comparing and contrasting US and EU voter profiles opinions of Trump vs Clinton voters Brexit vs. Bremain voters and future expectations. Content Expanding Dalia’s quarterly ""EuroPulse"" omnibus survey to the USA Dalia has conducted a representative survey with n=11.283 respondents across all 28 EU member countries and n=1.052 respondents from the United States of America. To find out where the next wave of populism could hit Europe Dalia’s survey traces commonalities in social and political mindsets (like authoritarianism prejudice open-mindedness xenophobia etc.) voting behaviour and socio-demographic profiles on both sides of the Atlantic. Inspiration The sources of our inspirations are many but to name a few who influenced the way we asked questions we were very inspired by the 'angry voter' profile laid out by Douglas Rivers the influence of political and moral attitudes pointed out by Jonathan Haidt and the profile of ""America's forgotten working class"" by J. D. Vance.  Researchers should apply the necessary logic caution and diligence when analysing and interpreting the results. ",Other,,"[politics, international relations]",CC4,,,1033,9251,51,A post-election survey about populism in the US and the EU-28,"The ""Trump Effect"" in Europe",https://www.kaggle.com/daliaresearch/trump-effect,Tue Jan 24 2017
,GeorgeMcIntire,"[, acousticness, danceability, duration_ms, energy, instrumentalness, key, liveness, loudness, mode, speechiness, tempo, time_signature, valence, target, song_title, artist]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]","Context A dataset of 2017 songs with attributes from Spotify's API. Each song is labeled ""1"" meaning I like it and ""0"" for songs I don't like. I used this to data to see if I could build a classifier that could predict whether or not I would like a song. I wrote an article about the project I used this data for. It includes code on how to grab this data from the Spotipy API wrapper and the methods behind my modeling. https//opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/ Content Each row represents a song. There are 16 columns. 13 of which are song attributes one column for song name one for artist and a column called ""target"" which is the label for the song. Here are the 13 track attributes acousticness danceability duration_ms energy instrumentalness key liveness loudness mode speechiness tempo time_signature valence. Information on what those traits mean can be found here https//developer.spotify.com/web-api/get-audio-features/ Acknowledgements I would like to thank Spotify for providing this readily accessible data. Inspiration I'm a music lover who's curious about why I love the music that I love.",CSV,,[],Other,,,1536,13283,0.2119140625,An attempt to build a classifier that can predict whether or not I like a song,Spotify Song Attributes,https://www.kaggle.com/geomack/spotifyclassification,Sat Aug 05 2017
,Rachael Tatman,[],[],Context Short Message Service (SMS) messages are short messages sent from one person to another from their mobile phones. They represent a means of personal communication that is an important communicative artifact in our current digital era. This dataset contains SMS messages that were collected from users who knew they were participating in a research project and that their messages would be shared publicly. This dataset contains two SMS messages in two languages Singapore English and Mandarin Chinese. Content This is a corpus of SMS (Short Message Service) messages collected for research at the Department of Computer Science at the National University of Singapore. This dataset consists of 67093 SMS messages taken from the corpus on Mar 9 2015. The messages largely originate from Singaporeans and mostly from students attending the University. These messages were collected from volunteers who were made aware that their contributions were going to be made publicly available. The data collectors opportunistically collected as much metadata about the messages and their senders as possible so as to enable different types of analyses.  Acknowledgements This corpus was collected by Tao Chen and Min-Yen Kan. If you use this data please cite the following paper Tao Chen and Min-Yen Kan (2013). Creating a Live Public Short Message Service Corpus The NUS SMS Corpus. Language Resources and Evaluation 47(2)(2013) pages 299-355. URL https//link.springer.com/article/10.1007%2Fs10579-012-9197-9 Inspiration This dataset contains a lot of short informal texts and is ideal for trying your hand at various natural language processing tasks. There’s also a lot of information about the messages which might reveal interesting insights. Here are some ideas to get you started  This dataset contains Singapore English. How well do tools trained on other varieties of English like stemmers or part of speech taggers work on it? What time of day are most SMS messages sent? Is this different for the English and Mandarin datasets? Unlike English Mandarin does not have spaces between words which can be made up of several characters. Can you build or implement a system for word identification? ,{}JSON,,"[languages, linguistics, telecommunications]",Other,,,413,2666,67,"A corpus of more than 67,000 SMS messages in Singapore English & Mandarin",The National University of Singapore SMS Corpus,https://www.kaggle.com/rtatman/the-national-university-of-singapore-sms-corpus,Tue Aug 08 2017
,JoniHoppen,"[Id, Human Development Index HDI-2014, Gini coefficient 2005-2013, Adolescent birth rate 15-19 per 100k 20102015, Birth registration funder age 5 2005-2013, Carbon dioxide emissionsAverage annual growth, Carbon dioxide emissions per capita 2011 Tones, Change forest percentable 1900 to 2012, Change mobile usage 2009 2014, Consumer price index 2013, Domestic credit provided by financial sector 2013, Domestic food price level 2009 2014 index, Domestic food price level 2009-2014 volitility index, Electrification rate or population, Expected years of schooling - Years, Exports and imports percentage GPD 2013, Female Suicide Rate 100k people, Foreign direct investment net inflows percentage GDP 2013, Forest area percentage of total land area 2012, Fossil fuels percentage of total 2012, Fresh water withdrawals 2005, Gender Inequality Index 2014, General government final consumption expenditure - Annual growth 2005 2013, General government final consumption expenditure - Perce of GDP 2005-2013, Gross domestic product GDP 2013, Gross domestic product GDP percapta, Gross fixed capital formation of GDP 2005-2013, Gross national income GNI per capita - 2011  Dollars, Homeless people due to natural disaster 2005 2014 per million people, Homicide rate per 100k people 2008-2012, Infant Mortality 2013 per thousands, International inbound tourists thausands 2013, International student mobility of total tetiary enrolvemnt 2013, Internet users percentage of population 2014, Intimate or nonintimate partner violence ever experienced 2001-2011, Life expectancy at birth- years, MaleSuicide Rate 100k people, Maternal mortality ratio deaths per 100 live births 2013, Mean years of schooling - Years, Mobile phone subscriptions per 100 people 2014, Natural resource depletion, Net migration rate per 1k people 2010-2015, Physicians per 10k people, Population affected by natural desasters average annual per million people 2005-2014, Population living on degraded land Percentage 2010, Population with at least some secondary education percent 2005-2013, Pre-primary 2008-2014, Primary-2008-2014, Primary school dropout rate 2008-2014, Prison population per 100k people, Private capital flows percentage GDP 2013, Public expenditure on education Percentange GDP, Public health expenditure percentage of GDP 2013, Pupil-teacher ratio primary school pupils per teacher 2008-2014, Refugees by country of origin, Remittances inflows percentual GDP 2013, Renewable sources percentage of total 2012, Research and development expenditure  2005-2012, Secondary 2008-2014, Share of seats in parliament percentage held by womand 2014, Stock of immigrants percentage of population 2013, Taxes on income profit and capital gain 205 2013, Tertiary -2008-2014, Total tax revenue of GDP 2005-2013, Tuberculosis rate per thousands 2012, Under-five Mortality 2013 thousands]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Why some countries are so different from the others?  Feel free to upvote ) Autor Joni Hoppen - linkedin -  https//www.linkedin.com/in/joniarroba/ Content I have gathered manually most of the information at World Bank Unicef and so on. Some data were not there so I used K-nn to guess some values and have a full dataset that can be used of our data science community.   Information of each of the 65 variables were made available here http//bit.ly/2l2Hjh3 Acknowledgements Thanks www.aquare.la Advanced Analytics that came up with the idea of creating this dataset to test their VORTX tool. Also Thanks to  professionals involved in creating Indexes and collecting them this is such a great valuable work to help better see the world.  Inspiration What would be the best way to equalize the world? ,CSV,,[international relations],ODbL,,,940,5859,0.1171875,Why are some countries so different?,65 World Indexes,https://www.kaggle.com/joniarroba/65-world-indexes-gathered,Mon Feb 27 2017
,NeilS,"[Date, Revenue, Cost of Sales, Gross profit, Operating profit, Net Profit, Issue of shares, Share repurchase, Non-current assets, Current assets, Non-current liabilities, Current liabilities, Net cash inflow/outflow from operating activities]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",South African Stock Price Predictions Welcome to SA Stock Market Data ) The dataset contains information for the largest 34 companies in South Africa by market cap as well as data for the SA40 Futures.  The Price data  (P.csv) is in the format Close   Open   High   Low   Vol   Change The Financials data (F.csv) is in the format Revenue   Cost of Sales   Gross profit   Net Profit   Issue of shares   Share repurchase   Non-current assets   Current assets  Non-current liabilities   Current liabilities   Net cash inflow/outflow from operating activities Interest rate data (R.csv) has a single column rates Symbol reference BTIJ = British American Tobacco PLC BILJ = BHP Billiton PLC BGAJ = Barclays Africa Group Ltd CFRJ = Compagnie Financiere Richemont SA DRC CCOJ = Capital & Counties Properties PLC AGLJ = Anglo American PLC MTNJ = MTN Group Ltd NPNJn = Naspers Ltd SOLJ = Sasol Ltd SBKJ = Standard Bank Group Ltd VODJ = Vodacom Group Ltd KIOJ = Kumba Iron Ore Ltd FSRJ = Firstrand Ltd OMLJ = Old Mutual PLC SLMJ = Sanlam Ltd SHPJ = Shoprite Holdings Ltd REMJ = Remgro Ltd NEDJ = Nedbank Group Ltd APNJ = Aspen Pharmacare Holdings Ltd BVTJ = The Bidvest Group Ltd ANGJ = Anglogold Ashanti Ltd IMPJ = Impala Platinum Holdings Ltd WHLJ = Woolworths Holdings Ltd TBSJ = Tiger Brands Ltd EXXJ = Exxaro Resources Ltd RMHJ = RMB Holdings Ltd ITUJ = Intu Properties PLC GRTJ = Growthpoint Properties Ltd MNDJ = Mondi Ltd SNHJ = Steinhoff International Holdings Ltd INPJ = Investec PLC LHCJ = Life Healthcare REIJ = Reinet DSYJ = Discovery Holdings Ltd IPLJ = Imperial Holdings Ltd ARIJ = African Rainbow Minerals Ltd SA = FTSE/JSE Top 40 Futures If you are interested in how I put everything together to build a single model you can look at the following Github link please note this contains a lot of copy-pasta and needs to be simplified with some loops and functions before I put it on Kaggle  https//github.com/Nlabbert/SA-Stock-Market,CSV,,[finance],CC0,,,322,2708,4,"Price, financials and rates",South Africa Stock Market Data,https://www.kaggle.com/neilslab/south-africa-stock-market-data,Sun Jul 02 2017
,WUZZUF,"[id, user_id, job_id, app_date]","[numeric, string, string, dateTime]",Context One of the main challenges in any marketplace business is achieving the balance between demand and supply. At WUZZUF we optimize for demand relevance and quality while connecting employers with the matching applicants and recommending relevant jobs to the job seekers.  Content The dataset includes  Wuzzuf_Job_Posts_Sample  a sample of jobs posted on WUZZUF during 2014-2016. Wuzzuf_Applications_Sample  the corresponding applications  (Excluding some entries).   Note  The jobs are mainly in Egypt but other locations are included. Exploration Ideas There are several areas to explore including but not limited to   Correlations between different features Salaries trends Insights about supply/demand Growth opportunities Data quality ,CSV,,[employment],CC4,,,361,3229,131,Explore jobs and job seekers applications on WUZZUF (2014-2016),WUZZUF Job Posts (2014-2016),https://www.kaggle.com/WUZZUF/wuzzuf-job-posts,Tue Apr 11 2017
,Rachael Tatman,"[observation_date, IPG3113N]","[dateTime, numeric]",Context Halloween begins frenetic candy consumption that continues into the Christmas holidays and New Year’s Day when people often make (usually short-lived) resolutions to lose weight. But all this consumption first needs production. The graph shows the relevant data from the industrial production index and its stunning seasonality Content The industrial production (IP) index measures the real output of all relevant establishments located in the United States regardless of their ownership but not those located in U.S. territories. This dataset tracks industrial production every month from January 1972 to August 2017.  Acknowledgements Board of Governors of the Federal Reserve System (US) Industrial Production Nondurable Goods Sugar and confectionery product [IPG3113N] retrieved from FRED Federal Reserve Bank of St. Louis; https//fred.stlouisfed.org/series/IPG3113N October 13 2017. Inspiration  Can you correct for the seasonality in this data? Which months have the highest candy production? Can you predict production for September through December 2017? ,CSV,,"[food and drink, time series, product]",CC0,,,738,4814,0.009765625,From January 1972 to August 2017,US Candy Production by Month,https://www.kaggle.com/rtatman/us-candy-production-by-month,Sat Oct 14 2017
,GovLab,"[company_name_id, company_name, url, year_founded, city, state, country, zip_code, full_time_employees, company_type, company_category, revenue_source, business_model, social_impact, description, description_short, source_count, data_types, example_uses, data_impacts, financial_info, last_updated]","[string, string, string, numeric, string, string, string, numeric, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, dateTime]",Context The Open Data 500 funded by the John S. and James L. Knight Foundation (http//www.knightfoundation.org/) and conducted by the GovLab is the first comprehensive study of U.S. companies that use open government data to generate new business and develop new products and services. Study Goals  Provide a basis for assessing the economic value of government open data Encourage the development of new open data companies Foster a dialogue between government and business on how government data can be made more useful  The Govlab's Approach The Open Data 500 study is conducted by the GovLab at New York University with funding from the John S. and James L. Knight Foundation. The GovLab works to improve people’s lives by changing how we govern using technology-enabled solutions and a collaborative networked approach. As part of its mission the GovLab studies how institutions can publish the data they collect as open data so that businesses organizations and citizens can analyze and use this information. Company Identification The Open Data 500 team has compiled our list of companies through (1) outreach campaigns (2) advice from experts and professional organizations and (3) additional research. Outreach Campaign  Mass email to over 3000 contacts in the GovLab network Mass email to over 2000 contacts OpenDataNow.com Blog posts on TheGovLab.org and OpenDataNow.com Social media recommendations Media coverage of the Open Data 500 Attending presentations and conferences  Expert Advice  Recommendations from government and non-governmental organizations Guidance and feedback from Open Data 500 advisors  Research  Companies identified for the book Open Data Now Companies using datasets from Data.gov Directory of open data companies developed by Deloitte Online Open Data Userbase created by Socrata General research from publicly available sources  What The Study Is Not The Open Data 500 is not a rating or ranking of companies. It covers companies of different sizes and categories using various kinds of data. The Open Data 500 is not a competition but an attempt to give a broad inclusive view of the field. The Open Data 500 study also does not provide a random sample for definitive statistical analysis. Since this is the first thorough scan of companies in the field it is not yet possible to determine the exact landscape of open data companies.,CSV,,[business],CC4,,,426,4268,0.466796875,The first comprehensive study of U.S. companies using open government data,Open Data 500 Companies,https://www.kaggle.com/govlab/open-data-500-companies,Fri Jun 23 2017
,Rishi Anand,[],[],"Context This is a dataset of Devanagari Script Characters. It comprises of 92000 images [32x32 px] corresponding to 46 characters consonants ""ka"" to ""gya"" and the digits 0 to 9. The vowels are missing. Content The CSV file is of the dimension 92000 * 1025. There are 1024 input features of pixel values in grayscale (0 to 255). The column ""character"" represents the Devanagari Character Name corresponding to each image. Acknowledgements This dataset was originally created by Computer Vision Research Group Nepal. [website archive] (https//web.archive.org/web/20160105230017/http//cvresearchnepal.com/wordpress/dhcd/) Example Script https//nbviewer.jupyter.org/github/rishianand9/devanagari-character-recognition/blob/master/DCRS.ipynb",Other,,"[languages, linguistics, image data, multiclass classification]",CC0,,,434,5608,121,Over 92 thousand images of characters from devanagari script,Devanagari Character Set,https://www.kaggle.com/rishianand/devanagari-character-set,Tue Nov 21 2017
,tusha kutusha,"[earthquake.time, earthquake.latitude, earthquake.longitude, earthquake.mag, earthquake.place, MoonPhase.dynamic, MoonPhase.value, MoonPhase.total, MoonPhase.percent, MoonPhase.illumination, day.sunrise, day.zenith, day.sunset, day.duration, night.duration, twilight.civil, twilight.nautical, twilight.astronomical, Sun.longitude, Sun.latitude, Sun.rectascension, Sun.declination, Sun.azimuth, Sun.height, Sun.speed, Sun.house, Sun.housenumber, Moon.longitude, Moon.latitude, Moon.rectascension, Moon.declination, Moon.azimuth, Moon.height, Moon.speed, Moon.house, Moon.housenumber, Mercury.longitude, Mercury.latitude, Mercury.rectascension, Mercury.declination, Mercury.azimuth, Mercury.height, Mercury.speed, Mercury.house, Mercury.housenumber, Venus.longitude, Venus.latitude, Venus.rectascension, Venus.declination, Venus.azimuth, Venus.height, Venus.speed, Venus.house, Venus.housenumber, Mars.longitude, Mars.latitude, Mars.rectascension, Mars.declination, Mars.azimuth, Mars.height, Mars.speed, Mars.house, Mars.housenumber, Jupiter.longitude, Jupiter.latitude, Jupiter.rectascension, Jupiter.declination, Jupiter.azimuth, Jupiter.height, Jupiter.speed, Jupiter.house, Jupiter.housenumber, Saturn.longitude, Saturn.latitude, Saturn.rectascension, Saturn.declination, Saturn.azimuth, Saturn.height, Saturn.speed, Saturn.house, Saturn.housenumber, Uranus.longitude, Uranus.latitude, Uranus.rectascension, Uranus.declination, Uranus.azimuth, Uranus.height, Uranus.speed, Uranus.house, Uranus.housenumber, Neptune.longitude, Neptune.latitude, Neptune.rectascension, Neptune.declination, Neptune.azimuth, Neptune.height, Neptune.speed, Neptune.house, Neptune.housenumber, Pluto.longitude]","[dateTime, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, dateTime, dateTime, dateTime, dateTime, dateTime, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context We all know that there is relationship between tides and Moon. What if we try to find the match between the position of objects in our Solar System and Earthquakes? So I prepared the dataset to answer the question. Content The dataset has information about Earthquakes (magnitude 6.1+) occur between  yyyy.mm.dd 1986.05.04 to 2016.05.04 and position (and other params) of Solar System planets + Sun and Moon  at the time when the specific Earthquake occured Each row has 1) info about the specific Earthquake date and time where it occur latitude and longitude magnitude place (it is useless field since we have latitude and longitude but I left the filed just to have humanreading meaning e.g. 7km SW of Ueki Japan) 2) Planets Moon and Sun information (position and etc)  relatively  latitude and longitude  of place and time of the Earthquake. Acknowledgements Tha dataset has two sources of information. Both ones are free and available for public. 1) Earthquakes info The USGS Earthquake Hazards Program https//earthquake.usgs.gov/ 2) Solar System objects info  NGINOV Astropositions http//api.nginov note NGINOV uses a bit specific calculation of azimuth (as far as i know more often the one calculates from geographicall north). The note from NGINOV ""An astronomical azimuth is expressed in degree or schedules arcs. Taking as a reference point geographically south of the place of observation and a dextrorotatory angular progression."" note I am not quite strong in Astronomic stuff. I expressed the idea about  possible relationship and wrote a simple app to match and merge the info from two data sources into the dataset which possibly help to answer the question. Inspiration May be there is a relationship between the position and other params of objects in our Solar System and Earthquakes? Hope someone will find the match! ) Good luck!!",CSV,,"[geology, space]",CC4,,,381,4005,4,Is there any relationship between Earthquakes and Solar System objects?,Earthquakes <-?-> Solar System objects?,https://www.kaggle.com/aradzhabov/earthquakes-solar-system-objects,Sun Apr 23 2017
,Team AI,[],[],Context More than 10 million people worldwide are living with Parkinson's disease. Improving machine learning model which identifies Parkinson's disease will lead to helping patients with early dialogs and reduction of treatment cost.  Content Handwriting database consists of 62 PWP(People with Parkinson) and 15 healthy individuals.  The data was collected in 2009. Number of instances 77 Number of attributes 7 Acknowledgements Source https//archive.ics.uci.edu/ml/datasets/Parkinson+Disease+Spiral+Drawings+Using+Digitized+Graphics+Tablet Citation  1.Isenkul M.E.; Sakar B.E.; Kursun O. . 'Improved spiral test using digitized graphics tablet for monitoring Parkinson's disease.' The 2nd International Conference on e-Health and Telemedicine (ICEHTM-2014) pp. 171-175 2014.  2.Erdogdu Sakar B. Isenkul M. Sakar C.O. Sertbas A. Gurgen F. Delil S. Apaydin H. Kursun O. 'Collection and Analysis of a Parkinson Speech Dataset with Multiple Types of Sound Recordings' IEEE Journal of Biomedical and Health Informatics vol. 17(4) pp. 828-834 2013.,Other,,[],CC0,,,158,1915,16,Hand drawing data using Digitized Graphics Tablet Data Set,Parkinson Disease Spiral Drawings,https://www.kaggle.com/team-ai/parkinson-disease-spiral-drawings,Tue Aug 15 2017
,UCI Machine Learning,"[mpg, cylinders, displacement, horsepower, weight, acceleration, model year, origin, car name]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]","Context The data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository Content  Title Auto-Mpg Data Sources (a) Origin  This dataset was taken from the StatLib library which is             maintained at Carnegie Mellon University. The dataset was              used in the 1983 American Statistical Association Exposition. (c) Date July 7 1993 Past Usage See 2b (above) QuinlanR. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine  Learning 236-243 University of Massachusetts Amherst. Morgan Kaufmann. Relevant Information This dataset is a slightly modified version of the dataset provided in the StatLib library.  In line with the use by Ross Quinlan (1993) in predicting the attribute ""mpg"" 8 of the original instances were removed  because they had unknown values for the ""mpg"" attribute.  The original  dataset is available in the file ""auto-mpg.data-original"". ""The data concerns city-cycle fuel consumption in miles per gallon to be predicted in terms of 3 multivalued discrete and 5 continuous attributes."" (Quinlan 1993) Number of Instances 398 Number of Attributes 9 including the class attribute Attribute Information mpg           continuous cylinders     multi-valued discrete displacement  continuous horsepower    continuous weight        continuous acceleration  continuous model year    multi-valued discrete origin        multi-valued discrete car name      string (unique for each instance) Missing Attribute Values  horsepower has 6 missing values  Acknowledgements Dataset UCI Machine Learning Repository  Data link  https//archive.ics.uci.edu/ml/datasets/auto+mpg Inspiration I have used this dataset for practicing my exploratory analysis skills.",CSV,,[automobiles],CC0,,,779,9064,0.017578125,Mileage per gallon performances of various cars,Auto-mpg dataset,https://www.kaggle.com/uciml/autompg-dataset,Sun Jul 02 2017
,National UFO Reporting Center (NUFORC),"[datetime, city, state, country, shape, duration (seconds), duration (hours/min), comments, date posted, latitude, longitude]","[dateTime, string, string, string, string, numeric, string, string, dateTime, numeric, numeric]",Context This dataset contains over 80000 reports of UFO sightings over the last century.  Content There are two versions of this dataset scrubbed and complete. The complete data includes entries where the location of the sighting was not found or blank (0.8146%) or have an erroneous or blank time (8.0237%). Since the reports date back to the 20th century some older data might be obscured. Data contains city state time description and duration of each sighting. Inspiration  What areas of the country are most likely to have UFO sightings? Are there any trends in UFO sightings over time? Do they tend to be clustered or seasonal? Do clusters of UFO sightings correlate with landmarks such as airports or government research centers? What are the most common UFO descriptions?   Acknowledgement This dataset was scraped geolocated and time standardized from NUFORC data by Sigmond Axel here.,CSV,,[space],Other,,,7256,49095,28,Reports of unidentified flying object reports in the last century,UFO Sightings,https://www.kaggle.com/NUFORC/ufo-sightings,Thu Nov 17 2016
,Kondalarao Vonteru,"[author, date, headlines, read_more, text, ctext]","[string, dateTime, string, string, string, string]",Context I am currently working on summarizing chat context where it helps an agent in understanding previous context quickly. It interests me to apply the deep learning models to existing datasets and how they perform on them. I believe news articles are rich in grammar and vocabulary which allows us to gain greater insights. Content The dataset consists of 4515 examples and contains Author_name Headlines Url of Article Short text Complete Article. I gathered the summarized news from Inshorts and only scraped the news articles from Hindu Indian times and Guardian.  Time period ranges from febrauary to august 2017. Acknowledgements I would like to thank the authors of Inshorts for their amazing work Inspiration  Generating short length descriptions(headlines) from text(news articles). Summarizing large amount of information which can be represented in compressed space  Purpose When I was working on the summarization task I didn't find any open source data-sets to work on I believe there are people just like me who are working on these tasks and I hope it helps them. Contributions It will be really helpful if anyone found nice insights from this data and can share their work. Thankyou...!!! For those who are interested here is the link for the github code which includes the scripts for scraping. https//github.com/sunnysai12345/News_Summary,CSV,,"[journalism, india, linguistics]",GPL,,,567,6191,11,Generating short length descriptions of news articles.,NEWS SUMMARY,https://www.kaggle.com/sunnysai12345/news-summary,Thu Aug 10 2017
,OpenFlights,"[1, Goroka Airport, Goroka, Papua New Guinea, GKA, AYGA, -6.081689834590001, 145.391998291, 5282, 10, U, Pacific/Port_Moresby, airport, OurAirports]","[numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string]","Context This is a database of airports train stations and ferry terminals around the world. Some of the data come from public sources and some of it comes from OpenFlights.org user contributions. Content  Airport ID    Unique OpenFlights identifier for this airport. Name  Name of airport. May or may not contain the City name. City  Main city served by airport. May be spelled differently from Name. Country   Country or territory where airport is located. See countries.dat to cross-reference to ISO 3166-1 codes. IATA  3-letter IATA code. Null if not assigned/unknown. ICAO  4-letter ICAO code. Null if not assigned. Latitude  Decimal degrees usually to six significant digits. Negative is South positive is North. Longitude Decimal degrees usually to six significant digits. Negative is West positive is East. Altitude  In feet. Timezone  Hours offset from UTC. Fractional hours are expressed as decimals eg. India is 5.5. DST   Daylight savings time. One of E (Europe) A (US/Canada) S (South America) O (Australia) Z (New Zealand) N (None) or U (Unknown). See also Help Time Tz database time zone Timezone in ""tz"" (Olson) format eg. ""America/Los_Angeles"". Type  Type of the airport. Value ""airport"" for air terminals ""station"" for train stations ""port"" for ferry terminals and ""unknown"" if not known. Source    Source of this data. ""OurAirports"" for data sourced from OurAirports ""Legacy"" for old data not matched to OurAirports (mostly DAFIF) ""User"" for unverified user contributions. In airports.csv only source=OurAirports is included.  Acknowledgements This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website so check them out! ",CSV,,"[transport, aviation, public transport]",ODbL,,,474,2225,1,Openflight.org's database of the worlds transportation hubs,"Airports, Train Stations, and Ferry Terminals",https://www.kaggle.com/open-flights/airports-train-stations-and-ferry-terminals,Tue Aug 29 2017
,LA Times Data Desk,"[CSR Number, LADBS Inspection District, Address House Number, Address House Fraction Number, Address Street Direction, Address Street Name, Address Street Suffix, Address Street Suffix Direction, Address Street Zip, Date Received, Year Received, Date Closed, Due Date, Case Flag, CSR Priority, GIS Parcel Identification Number(PIN), CSR Problem Type, Area Planning Commission (APC), Case Number Related to CSR, Response Days, Latitude/Longitude, CSR_CASE_NUMBER]","[numeric, numeric, numeric, string, string, string, string, string, numeric, dateTime, numeric, string, dateTime, string, numeric, string, string, string, string, string, string, string]","Los Angeles residents have made roughly 4000 complaints since 2011 about abandoned and vacant buildings in the city according to an analysis by the LA Times. This dataset was originally collected and analyzed for ""Fire officials were concerned about Westlake building where 5 died in a blaze"" a June 15 2016 story by the Los Angeles Times. Lists of open and closed complaints filed with the Los Angeles Department of Building and Safety were downloaded from the city's data portal. The two files were combined into a single spreadsheet. A new column called ""Year Received"" was generated from the existing ""Date Received"" field using LibreOffice's YEAR() function. The new file was named combined_complaints.csv. Acknowledgements Data and analysis originally published on the LA Times Data Desk GitHub.",CSV,,"[cities, civil engineering]",Other,,,60,890,20,Complaints filed with the Los Angeles Department of Building and Safety,LA Vacant Building Complaints,https://www.kaggle.com/la-times/la-vacant-building-complaints,Thu Apr 06 2017
,Peter Wittek,"[, id, year, month, day, scites, title, authors, abstract]","[numeric, numeric, numeric, numeric, numeric, numeric, string, string, string]",Context I was curious about the hot topics in quantum physics as reflected by the quant-ph category on arXiv. Citation counts have a long lag and so do journal publications and I wanted a more immediate measure of interest. SciRate is fairly well known in this community and I noticed that after the initial two-three weeks the number of Scites a paper gets hardly increases further. So the number of Scites is both immediate and near constant after a short while.  Content The main dataset (scirate_quant-ph.csv) is the metadata of all papers published in quant-ph between 2012-01-01 and 2016-12-31 that had at least ten Scites as crawled on 2016-12-31. It has six columns  The id column as exported by pandas. The arXiv id. The year of publication. The month of publication. The day of publication. The number of Scites (this column defines the order). The title. All authors separates by a semicolon. The abstract.  The author names were subjected to normalization and the chances are high that the same author only appears with a unique name. The name normalization was the difficult part in compiling this collection and this is why the number of Scites was lower bounded. A second file (scirate_quant-ph_unnormalized.csv) includes all papers that appeared between 2012-2016 irrespective of the number of Scites but the author names are not normalized. The actual number of Scites for each paper may show a slight variation between the two datasets because the unnormalized version was compiled more than a month later. Acknowledgements Many thanks to SciRate for tolerating my crawling trials and not blacklisting my IP address. Inspiration Unleash topic models and author analysis to find out what or who is hot in quantum physics today. Build a generative model to write trendy fake titles like SnarXiv does it for hep-th.,CSV,,"[research, physics, linguistics]",CC4,,,117,3016,32,Papers published in arXiv/quant-ph between 2012-2016 with number of Scites,SciRate quant-ph,https://www.kaggle.com/peterwittek/scirate-quant-ph,Mon Feb 13 2017
,Jacob Boysen,[],[],Context Understanding the distribution of anopheline vectors of malaria is an important prelude to the design of national malaria control and elimination programmes. A single geo-coded continental inventory of anophelines using all available published and unpublished data has not been undertaken since the 1960s. We present the largest ever geo-coded database of anophelines in Africa representing a legacy dataset for future updating and identification of knowledge gaps at national levels. The geo-coded and referenced database is made available with the related publication as a reference source for African national malaria control programmes planning their future control and elimination strategies. Information about the underlying research studies can be found at http//kemri-wellcome.org/programme/population-health/. Content Geocoded info on anopheline inventory. See key below. Acknowledgements KEMRI-Wellcome Trust assembled the data and distributed it on Dataverse. Inspiration  Where have malarial mosquito populations grown or decreased? Can you predict mosquito population growth trends? Do you seen any correlation between mosquito populations and malaria deaths from this dataset? Is the banner image mosquito capable of carrying malaria? ,Other,,"[healthcare, public health]",CC4,,,439,3209,6,Geo-coded Inventory of Anophelines in the Sub-Saharan Africa: 1898-2016,Malarial Mosquito Database,https://www.kaggle.com/jboysen/malaria-mosquito,Mon Aug 28 2017
,Mitchell J,"[id, rated, created_at, last_move_at, turns, victory_status, winner, increment_code, white_id, white_rating, black_id, black_rating, moves, opening_eco, opening_name, opening_ply]","[string, boolean, numeric, numeric, numeric, string, string, string, string, numeric, string, numeric, string, string, string, numeric]",General Info This is a set of just over 20000 games collected from a selection of users on the site Lichess.org and how to collect more. I will also upload more games in the future as I collect them. This set contains the  Game ID; Rated (T/F); Start Time; End Time; Number of Turns; Game Status; Winner; Time Increment; White Player ID; White Player Rating; Black Player ID; Black Player Rating; All Moves in Standard Chess Notation; Opening Eco (Standardised Code for any given opening list here); Opening Name; Opening Ply (Number of moves in the opening phase)  For each of these separate games from Lichess. I collected this data using the Lichess API which enables collection of any given users game history. The difficult part was collecting usernames to use however the API also enables dumping of all users in a Lichess team. There are several teams on Lichess with over 1500 players so this proved an effective way to get users to collect games from. Possible Uses Lots of information is contained within a single chess game let alone a full dataset of multiple games. It is primarily a game of patterns and data science is all about detecting patterns in data which is why chess has been one of the most invested in areas of AI in the past. This dataset collects all of the information available from 20000 games and presents it in a format that is easy to process for analysis of for example what allows a player to win as black or white how much meta (out-of-game) factors affect a game the relationship between openings and victory for black and white and more.,CSV,,"[board games, internet]",CC0,,,375,7437,7,"20,000+ Lichess Games, including moves, victor, rating, opening details and more",Chess Game Dataset (Lichess),https://www.kaggle.com/datasnaek/chess,Mon Sep 04 2017
,UCI Machine Learning,"[ARC_01_ARC, ARC_02_ARC, ARC_03_ARC, ARC_04_ARC, ARC_05_ARC, ARC_06_ARC, ARC_07_ARC, ARC_01_POS, ARC_02_POS, ARC_03_POS, ARC_04_POS, ARC_05_POS, ARC_06_POS, ARC_07_POS, ARC_01_NEG, ARC_02_NEG, ARC_03_NEG, ARC_04_NEG, ARC_05_NEG, ARC_06_NEG, ARC_07_NEG, ARC_01_POL, ARC_02_POL, ARC_03_POL, ARC_04_POL, ARC_05_POL, ARC_06_POL, ARC_07_POL, ARC_01_HAL, ARC_02_HAL, ARC_03_HAL, ARC_04_HAL, ARC_05_HAL, ARC_06_HAL, ARC_07_HAL, ARC_01_DBL, ARC_02_DBL, ARC_03_DBL, ARC_04_DBL, ARC_05_DBL, ARC_06_DBL, ARC_07_DBL, ARC_01_TRI, ARC_02_TRI, ARC_03_TRI, ARC_04_TRI, ARC_05_TRI, ARC_06_TRI, ARC_07_TRI, ARC_01_-O-, ARC_02_-O-, ARC_03_-O-, ARC_04_-O-, ARC_05_-O-, ARC_06_-O-, ARC_07_-O-, ARC_01_-OH, ARC_02_-OH, ARC_03_-OH, ARC_04_-OH, ARC_05_-OH, ARC_06_-OH, ARC_07_-OH, ARC_01_3-7, ARC_02_3-7, ARC_03_3-7, ARC_04_3-7, ARC_05_3-7, ARC_06_3-7, ARC_07_3-7, ARC_01_AMD, ARC_02_AMD, ARC_03_AMD, ARC_04_AMD, ARC_05_AMD, ARC_06_AMD, ARC_07_AMD, ARC_01_TON, ARC_02_TON, ARC_03_TON, ARC_04_TON, ARC_05_TON, ARC_06_TON, ARC_07_TON, ARC_01_HY1, ARC_02_HY1, ARC_03_HY1, ARC_04_HY1, ARC_05_HY1, ARC_06_HY1, ARC_07_HY1, ARC_01_HY2, ARC_02_HY2, ARC_03_HY2, ARC_04_HY2, ARC_05_HY2, ARC_06_HY2, ARC_07_HY2, POS_01_POS, POS_02_POS]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The drug-development process is time-consuming and expensive. In High-Throughput Screening (HTS) batches of compounds are tested against a biological target to test the compound's ability to bind to the target. Targets might be antibodies for example. If the compound binds to the target then it is active for that target and known as a hit.  Virtual screening is the computational or in silico screening of biological compounds and complements the HTS process. It is used to aid the selection of compounds for screening in HTS bioassays or for inclusion in a compound-screening library.  Drug discovery is the first stage of the drug-development process and involves finding compounds to test and screen against biological targets. This first stage is known as primary-screening and usually involves the screening of thousands of compounds.  This dataset is a collection of 21 bioassays (screens) that measure the activity of various compounds against different biological targets. Content Each bioassay is split into test and train files. Here are some descriptions of some of the assays compounds. The source unfortunately does not have descriptions for every assay. That's the nature of the beast for finding this kind data and was also pointed out in the original study. Primary screens  AID362 details the results of a primary screening bioassay for Formylpeptide Receptor Ligand Binding University from the New Mexico Center for Molecular Discovery. It is a relatively small dataset with 4279 compounds and with a ratio of 1 active to 70 inactive compounds (1.4% minority class). The compounds were selected on the basis of preliminary virtual screening of approximately 480000 drug-like small molecules from Chemical Diversity Laboratories.  AID604 is a primary screening bioassay for Rho kinase 2 inhibitors from the Scripps Research Institute Molecular Screening Center. The bioassay contains activity information of 59788 compounds with a ratio of 1 active compound to 281 inactive compounds (1.4%). 57546 of the compounds have known drug-like properties.  AID456 is a primary screen assay from the Burnham Center for Chemical Genomics for inhibition of TNFa induced VCAM-1 cell surface expression and consists of 9982 compounds with a ratio of 1 active compound to 368 inactive compounds (0.27% minority). The compounds have been selected for their known drug-like properties and 9431 meet the Rule of 5 [19].  AID688 is the result of a primary screen for Yeast eIF2B from the Penn Center for Molecular Discovery and contains activity information of 27198 compounds with a ratio of 1 active compound to 108 inactive compounds (0.91% minority). The screen is a reporter-gene assay and 25656 of the compounds have known drug-like properties.  AID373 is a primary screen from the Scripps Research Institute Molecular Screening Center for endothelial differentiation sphingolipid G-protein-coupled receptor 3. 59788 compounds were screened with a ratio of 1 active compound to 963 inactive compounds (0.1%). 57546 of the compounds screened had known drug-like properties.  AID746 is a primary screen from the Scripps Research Institute Molecular Screening Center for Mitogen-activated protein kinase. 59788 compounds were screened with a ratio of 1 active compound to 162 inactive compounds (0.61%). 57546 of the compounds screened had known drug-like properties.  AID687 is the result of a primary screen for coagulation factor XI from the Penn Center for Molecular Discovery and contains activity information of 33067 compounds with a ratio of 1 active compound to 350 inactive compounds (0.28% minority). 30353 of the compounds screened had known drug-like properties.   Primary and Confirmatory  AID604 (primary) with AID644 (confirmatory) AID746 (primary) with AID1284 (confirmatory) AID373 (primary) with AID439 (confirmatory) AID746 (primary) with AID721 (confirmatory)  Confirmatory  AID1608 is a different type of screening assay that was used to identify compounds that prevent HttQ103-induced cell death. National Institute of Neurological Disorders and Stroke Approved Drug Program. The compounds that prevent a release of a certain chemical into the growth medium are labelled as active and the remaining compounds are labelled as having inconclusive activity. AID1608 is a small dataset with 1033 compounds and a ratio of 1 active to 14 inconclusive compounds (6.58% minority class).  AID644 AID1284 AID439 AID721 AID1608 AID644 AID1284 AID439 AID721  Acknowledgements Original study https//www.ncbi.nlm.nih.gov/pmc/articles/PMC2820499/ Data downloaded form UCI ML repository Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. Inspiration Drug development is expensive. Use this virtual bio assay data to classify compounds as hits (active) against their biological targets.,CSV,,"[biology, health sciences]",CC0,,,124,2056,215,21 assays from PubChem that measure compound activity,Bioassay Datasets,https://www.kaggle.com/uciml/bioassay-datasets,Thu Sep 07 2017
,FiveThirtyEight,[],[],"Introduction Data and code behind the stories and interactives at FiveThirtyEight.  There are 80 Datasets here included in one place.  This allows you to make comparisons and utilize multiple Datasets.  airline-safety alcohol-consumption avengers bad-drivers bechdel biopics births bob-ross buster-posey-mvp classic-rock college-majors comic-characters comma-survey-data congress-age cousin-marriage ... For the complete list see the zip files listed here  Python If you're planning to use Python you might want to take a look at  How to read datasets which helps navigate the zipped Datasets.  Again there are multiple 80 Datasets in this Dataset. R If you're planning to use R it's included in the kernels.  No need to unzip files.  For example if you wanted to load ""bechdel"" you could use the following commands. library(fivethirtyeight) data(package = ""fivethirtyeight"") head(bechdel)   Reference R Quick Start for an example. References The following is FiveThirtyEight's public repository on Github. https//github.com/fivethirtyeight/data The CRAN package is maintained at the following link https//github.com/rudeboybert/fivethirtyeight  License Copyright (c) 2014 ESPN Internet Ventures Permission is hereby granted free of charge to any person obtaining a copy of this software and associated documentation files (the ""Software"") to deal in the Software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and/or sell copies of the Software and to permit persons to whom the Software is furnished to do so subject to the following conditions The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED ""AS IS"" WITHOUT WARRANTY OF ANY KIND EXPRESS OR IMPLIED INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM DAMAGES OR OTHER LIABILITY WHETHER IN AN ACTION OF CONTRACT TORT OR OTHERWISE ARISING FROM OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.",Other,,[news agencies],Other,,,3109,28145,14,80 Datasets published by FiveThirtyEight,FiveThirtyEight,https://www.kaggle.com/fivethirtyeight/fivethirtyeight,Mon Jan 30 2017
,NLTK Data,[],[],Context The maxent_ne_chunker contains two pre-trained English named entity chunkers trained on an ACE corpus (perhaps ACE  ACE 2004 Multilingual Training Corpus?) It will load an nltk.chunk.named_entity.NEChunkParser object and it is used by the nltk.ne_chunk() function. Robert M. Johnson has written an excellent expository of what the pre-trained model is doing under the hood From the relation extraction code function in NLTK it lists the following tags for the ACE tagset  LOCATION ORGANIZATION PERSON DURATION DATE CARDINAL PERCENT MONEY MEASURE FACILITY GPE  Content The maxent_ne_chunker.zip contains  - english_ace_binary.pickle  Chunks the input POS tagged sentence and labeled positive NEs as NE.   - english_ace_multiclass.pickle Chunks the input POS tagged sentence and outputs the repsective NE labels under the ACE tagset.  - PY3 Subdirectory that contains the Python3 compatiable pickles as above Acknowledgements We're not sure who exactly to credit for this pre-trained model it'll be great if anyone who knows help to document this on https//github.com/nltk/nltk/issues/1783,Other,,[],Other,,,14,591,23,ACE Named Entity Chunker (Maximum entropy),MaxEnt NE Chunker,https://www.kaggle.com/nltkdata/maxent-ne-chunker,Sun Aug 20 2017
,Moko Sharma,"[Name, Url, Race]","[string, string, string]",Context As a huge LOTR fan I was excited to have acquired this character data from the Lord of the Rings Wiki. I scraped this data using F#; the repository can be found here https//github.com/MokoSan/FSharpAdvent.  Content Data consists of character names the url in the wiki and the respective race.  Acknowledgements Wouldn't have been able to publish this data set unless it was for the work done by the great people of the wiki page.,CSV,,"[literature, classification]",Other,,,124,1604,0.984375,Character and Movie Data,Lord Of The Rings Data,https://www.kaggle.com/mokosan/lord-of-the-rings-character-data,Tue Dec 12 2017
,New America,"[plot_ID, plot_name, plot_ideology, plot_status, prevention_method, attack_date, victims_wounded, victims_killed]","[numeric, string, string, string, string, dateTime, numeric, numeric]",Content The data in this report consists of individuals accused of terrorism and related crimes since September 11 2001 who are either American citizens or who engaged in terrorist activity within the United States. The data includes some individuals who died before being charged with a crime but were widely reported to have engaged in terrorist activity. Acknowledgements This report was produced by the International Security Program at New America.,CSV,,"[history, crime]",CC0,,,522,3500,0.080078125,Terrorist activity in the United States and by Americans overseas since 9/11,"Terrorism in America, 2001-Present",https://www.kaggle.com/newamerica/terrorist-activity,Wed Feb 01 2017
,Google Natural Language Understanding Research,"[Semiotic Class, Input Token, Output Token]","[string, string, string]","Challenge Description This dataset and accompanying paper present a challenge to the community given a large corpus of written text aligned to its normalized spoken form train an RNN to learn the correct normalization function. That is a date written ""31 May 2014"" is spoken as ""the thirty first of may twenty fourteen."" We present a dataset of general text where the normalizations were generated using an existing text normalization component of a text-to-speech (TTS) system. This dataset was originally released open-source here and is reproduced on Kaggle for the community. The Data The data in this directory are the English language training development and test data used in Sproat and Jaitly (2016). The following divisions of data were used  Training      output_1 through output_21 (corresponding to output-000[0-8]?-of-00100 in the original dataset) Runtime eval  output_91 (corresponding to output-0009[0-4]-of-00100 in the original dataset) Test data     output_96 (corresponding to output-0009[5-9]-of-00100 in the original dataset)  In practice for the results reported in the paper only the first 100002 lines of output-00099-of-00100 were used (for English). Lines with """" in two columns are the end of sentence marker otherwise there are three columns the first of which is the ""semiotic class"" (Taylor 2009) the second is the input token and the third is the output following the paper cited above. All text is from Wikipedia. All data were extracted on 2016/04/08 and run through the Google Kestrel TTS text normalization system (Ebden and Sproat 2015) so that the notion of ""token"" ""semiotic class"" and reference output are all Kestrel's notion. Our Research In this paper we present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy the errors that are produced are problematic since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand we show that a simple FST-based filter can mitigate those errors and achieve a level of accuracy not achievable by the RNN alone.  Though our conclusions are largely negative on this point we are actually not arguing that the text normalization problem is intractable using an pure RNN approach merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And with open-source data we provide a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions.  Disclaimer This is not an official Google product. References Ebden Peter and Sproat Richard. 2015. The Kestrel TTS text normalization system. Natural Language Engineering. 21(3). Richard Sproat and Navdeep Jaitly. 2016. RNN Approaches to Text Normalization A Challenge. Released on arXiv.org https//arxiv.org/abs/1611.00068 Taylor Paul. 2009. Text-to-Speech Synthesis. Cambridge University Press Cambridge.",CSV,,"[languages, linguistics]",CC4,,,1546,10330,9216,"Text-to-speech synthesis text normalization data, from Sproat & Jaitly 2016",Google Text Normalization Challenge,https://www.kaggle.com/google-nlu/text-normalization,Wed Apr 26 2017
,Ed King,"[id, exception.response]","[numeric, string]","Tweets scraped by Chris Albon on the day of the 2016 United States elections. Chris Albon's site only posted tweet IDs rather than full tweets. We're in the process of scraping the full information but due to API limiting this is taking a very long time. Version 1 of this dataset contains just under 400k tweets about 6% of the 6.5 million originally posted. This dataset will be updated as more tweets become available. Acknowledgements The original data was scraped by Chris Albon and tweet IDs were posted to his Github page. The Data Since I (Ed King) used my own Twitter API key to scrape these tweets this dataset contains a couple of fields with information on whether I have personally interacted with particular users or tweets. Since Kaggle encouraged me to not remove any data from a dataset I'm leaving it in; feel free to build a classifier of the types of users I follow. The dataset consists of the following fields  text text of the tweet created_at date and time of the tweet geo a JSON object containing coordinates [latitude longitude] and a `type' lang Twitter's guess as to the language of the tweet place a Place object from the Twitter API coordinates a JSON object containing coordinates [longitude latitude] and a `type'; note that coordinates are reversed from the geo field user.favourites_count number of tweets the user has favorited user.statuses_count number of statuses the user has posted user.description the text of the user's profile description user.location text of the user's profile location user.id unique id for the user user.created_at when the user created their account user.verified bool; is user verified? user.following bool; am I (Ed King) following this user? user.url the URL that the user listed in their profile (not necessarily a link to their Twitter profile) user.listed_count number of lists this user is on (?) user.followers_count number of accounts that follow this user user.default_profile_image bool; does the user use the default profile pic? user.utc_offset positive or negative distance from UTC in seconds user.friends_count number of accounts this user follows user.default_profile bool; does the user use the default profile? user.name user's profile name user.lang user's default language user.screen_name user's account name user.geo_enabled bool; does user have geo enabled? user.profile_background_color user's profile background color as hex in format ""RRGGBB"" (no '#') user.profile_image_url a link to the user's profile pic user.time_zone full name of the user's time zone id unique tweet ID favorite_count number of times the tweet has been favorited retweeted is this a retweet? source if a link where is it from (e.g. ""Instagram"") favorited have I (Ed King) favorited this tweet? retweet_count number of times this tweet has been retweeted  I've also included a file called bad_tweets.csv  which includes all of the tweet IDs that could not be scraped along with the error message I received while trying to scrape them. This typically happens because the tweet has been deleted the user has deleted their account (or been banned) or the user has made their tweets private. The fields in this file are id and exception.response.",CSV,,"[politics, internet]",CC0,,,694,4745,209,"Tweets scraped from Twitter on November 8, 2016",Election Day Tweets,https://www.kaggle.com/kinguistics/election-day-tweets,Sun Nov 27 2016
,Rajanand Ilangovan / இராஜ்ஆனந்த் இளங்கோவன்,[],[],Connect/Follow me on LinkedIn for more updates on interesting dataset like this. Thanks. Context India - Annual Health Survey(AHS) 2012-13 The survey was conducted in Empowered Action Group (EAG) states Uttarakhand Rajasthan Uttar Pradesh Bihar Jharkhand  Odisha Chhattisgarh & Madhya Pradesh and Assam. These nine states which account for about 48 percent of the total population 59 percent of Births 70 percent of Infant Deaths 75 percent of Under 5 Deaths and 62 percent of Maternal Deaths in the country are the high focus States in view of their relatively higher fertility and mortality.  A representative sample of about 21 million population and 4.32 million households were covered 20k+ sample units which is spread across rural and urban area of these 9 states.  The objective of the AHS is to yield a comprehensive representative and reliable dataset on core vital indicators including composite ones like Infant Mortality Rate Maternal Mortality Ratio and Total Fertility Rate along with their co-variates (process and outcome indicators) at the district level and map the changes therein on an annual basis. These benchmarks would help in better and holistic understanding and timely monitoring of various determinants on well-being and health of population particularly Reproductive and Child Health. Source  Content This dataset contains the data about the below 26 key indicators.  AA. Sample Particulars Sample Units Households Population Ever Married Women (aged 15-49 years) Currently Married Women (aged 15-49 years) Children 12-23 months BB. Household Characteristics Average Household Size SC ST All Population below age 15 years (%) Dependency Ratio Currently Married Illiterate Women aged 15-49 years (%) CC. Sex Ratio Sex Ratio at Birth Sex Ratio (0- 4 years) Sex Ratio (All ages) DD. Effective Literacy Rate  EE. Marriage Marriages among Females below legal age (18 years) (%) Marriages among Males below legal age (21 years) (%) Currently Married Women aged 20-24 years married before legal age (18 years) (%) Currently Married Men aged 25-29 years married before legal age (21 years) (%) Mean age at Marriage# - Male Mean age at Marriage# - Female FF. Schooling Status Children currently attending school (Age 6-17 years) (%) Children attended before / Drop out (Age 6-17 years) (%)  GG. Work Status Children aged 5-14 years engaged in work (%)  Work Participation Rate (15 years and above)   HH. Disability Prevalence of any type of Disability (Per 100000 Population)  II. Injury Number of Injured Persons by type of Treatment received (Per 100000 Population) Severe   Major Minor JJ. Acute Illness  Persons suffering from Acute Illness (Per 100000 Population)  Diarrhoea/Dysentery  Acute Respiratory Infection (ARI) Fever (All Types)  Any type of Acute Illness  Persons suffering from Acute Illness and taking treatment from Any Source (%) Persons suffering from Acute Illness and taking treatment from Government Source (%) KK. Chronic Illness Having any kind of Symptoms of Chronic Illness (Per 100000 Population) Having any kind of Symptoms of Chronic Illness and sought Medical Care (%) Having diagnosed for Chronic Illness (Per 100000 Population)  Diabetes Hypertension Tuberculosis (TB) Asthma / Chronic Respiratory Disease Arthritis Any kind of Chronic Illness Having diagnosed for any kind of Chronic Illness and getting Regular Treatment (%) Having diagnosed for any kind of Chronic Illness and getting Regular Treatment from Government Source (%) LL. Fertility Crude Birth Rate (CBR) Natural Growth Rate Total Fertility Rate Women aged 20-24 reporting birth of order 2 & above (%) Women reporting birth of order 3 & above (%) Women with two children wanting no more children (%) Women aged 15-19 years who were already mothers or pregnant at the time of survey (%) Median age at first live birth of Women aged 15-49 years Median age at first live birth of Women aged 25-49 years Live Births taking place after an interval of 36 months (%) Mean number of children ever born to Women aged 15-49 years Mean number of children surviving to Women aged 15-49 years Mean number of children ever born to Women aged 45-49 years MM. Abortion Pregnancy to Women aged 15-49 years resulting in abortion (%) Women who received any ANC before abortion (%) Women who went for Ultrasound before abortion (%) Average Month of pregnancy at the time of abortion Abortion performed by skilled health personnel (%) Abortion taking place in Institution (%) NN. Family Planning Practices (Cmw Aged 15-49 Years) Current Usage Any Method (%) Any Modern Method (%) Female Sterilization (%) Male Sterilization (%) Copper-T/IUD (%) Pills (%) Condom/Nirodh (%) Emergency Contraceptive Pills (%) Any Traditional Method (%) Periodic Abstinence (%) Withdrawal (%) LAM (%) OO. Unmet Need For Family Planning Unmet need for Spacing (%) Unmet need for Limiting (%) Total Unmet need (%) PP. Ante Natal Care Currently Married Pregnant Women aged 15-49 years registered for ANC (%) Mothers who received any Antenatal Check-up (%) Mothers who had Antenatal Check-up in First Trimester (%) Mothers who received 3 or more Antenatal Care (%) Mothers who received at least one Tetanus Toxoid (TT) injection (%) Mothers who consumed IFA for 100 days or more (%) Mothers who had Full Antenatal Check-up (%) Mothers who received ANC from Govt. Source (%) Mothers whose Blood Pressure (BP) taken (%) Mothers whose Blood taken for Hb (%) Mothers who underwent Ultrasound (%) QQ. Delivery Care Institutional Delivery (%) Delivery at Government Institution (%) Delivery at Private Institution (%) Delivery at Home (%) Delivery at home conducted by skilled health personnel (%) Safe Delivery (%) Caesarean out of total delivery taken place in Government Institutions (%) Caesarean out of total delivery taken place in Private Institutions (%) RR. Post Natal Care Less than 24 hrs. stay in institution after delivery (%) Mothers who received Post-natal Check-up within 48 hrs. of delivery (%) Mothers who received Post-natal Check-up within 1 week of delivery (%) Mothers who did not receive any Post-natal Check-up (%) New borns who were checked up within 24 hrs. of birth (%) SS. Janani Suraksha Yojana (JSY) Mothers who availed financial assistance for delivery under JSY (%) Mothers who availed financial assistance for institutional delivery under JSY (%) Mothers who availed financial assistance for Government Institutional delivery under JSY (%) TT. Immunization Vitamin A & Iron Supplement And Birth Weight Children aged 12-23 months having Immunization Card (%) Children aged 12-23 months who have received BCG (%) Children aged 12-23 months who have received 3 doses of Polio vaccine (%) Children aged 12-23 months who have received 3 doses of DPT vaccine (%) Children aged 12-23 months who have received Measles vaccine (%) Children aged 12-23 months Fully Immunized (%) Children who have received Polio dose at birth (%) Children who did not receive any vaccination (%) Children (aged 6-35 months) who received at least one Vitamin A dose during last six months (%) Children (aged 6-35 months) who received IFA tablets/syrup during last 3 months (%) Children whose birth weight was taken (%) Children with birth weight less than 2.5 Kg. (%) UU. Childhood Diseases Children suffering from Diarrhoea (%) Children suffering from Diarrhoea who received HAF/ORS/ORT (%) Children suffering from Acute Respiratory Infection (%) Children suffering from Acute Respiratory Infection who sought treatment (%) Children suffering from Fever (%) Children suffering from Fever who sought treatment (%) VV. Breastfeeding And Supplementation Children breastfed within one hour of birth (%) Children (aged 6-35 months) exclusively breastfed for at least six months (%) Children Who Received Foods Other Than Breast Milk During First 6 Months Water (%) Animal/Formula Milk (%) Semi-Solid Mashed Food (%) Solid (Adult) Food (%) Vegetables/Fruits (%) Average Month By Which Children Received Foods Other Than Breast Milk Water (%) Animal/Formula Milk (%) Semi-Solid Mashed Food (%) Solid (Adult) Food (%) Vegetables/Fruits (%) WW. Birth Registration Birth Registered (%) Children Whose Birth Was Registered And Received Birth Certificate (%) XX. Awareness On Hiv/Aids Rti/Sti Haf/Ors/Ort/Zinc And Ari/Pneumonia Women Who Are Aware of HIV/AIDS RTI/STI HAF/ORS/ORT/ZINC Danger Signs Of ARI/Pneumonia (%)     YY. Mortality (unit level data of mortality is available here) Crude Death Rate (CDR) Infant Mortality Rate (IMR) Neo-Natal Mortality Rate Under Five Mortality Rate (U5MR) ZZ. Confidence Interval (95%) For Some Important Indicators Crude Birth Rate - (Lower and Upper Limit) Crude Death Rate - (Lower and Upper Limit) Infant Mortality Rate - (Lower and Upper Limit) Under Five Mortality Rate (U5MR) - (Lower and Upper Limit) Sex Ratio At Birth  - (Lower and Upper Limit)  Acknowledgements Department of Health and Family Welfare Govt. of India has published this data in Open Govt Data Platform India portal under Govt. Open Data License - India.,CSV,,"[india, public health, health]",CC4,,,1794,16946,2,26 health indicators (642 variables) from 9 states and 284 districts of India,Health Analytics,https://www.kaggle.com/rajanand/key-indicators-of-annual-health-survey,Thu Aug 10 2017
,Fifth Tribe,"[Magazine, Issue, Date, Type, Source, Quote, Purpose, Article Name]","[string, numeric, dateTime, string, string, string, string, string]",Context Religious texts play a key role in ISIS ideology propaganda and recruitment. This dataset is a compilation of all of the religious and ideological texts (Muslim Christian Jewish and other) used in ISIS English-based magazines.  Content We scraped 15 issues of Dabiq (6/2014 to 7/2016) and 9 issues of Rumiyah (9/2016 to 5/2017) producing a total of 2685 texts. We classified the data as follows  Magazine Dabiq or Rumiyah Issue # Date (Month and Year) Type of Text (Qur'an Hadeeth Religious Figure etc) Source What the source of the text was The quote itself Purpose Support ISIS Refute Another Group The article from which the quote is derived  Acknowledgements We would like to thank Asma Shah for helping to compile this dataset. Asma is a junior at the University of Maryland College Park where she is studying criminal justice and computer science. She is currently an intern with the Department of Justice where she does data science work. She has previously done counter-terrorism research with Fifth Tribe and the National Consortium for the Studies of Terrorism and Responses to Terrorism.  We would also like to express our gratitude to Abdul Aziz Suraqah (Canada) Saif ul Hadi (India) and Abdulbasit Kassam (Nigeria) for helping with the classification of some of the more obscure texts. Inspiration We would like this data to be analyzed by religious clerics to develop rebuttals of ISIS propaganda data scientists to generate insights from the texts and policymakers to understand how faith can shape countering violent extremism efforts. We also need help classifying some of the data that could not be identified and has been marked 'unknown.',CSV,,"[politics, terrorism]",CC0,,,313,6037,1,"A compilation of 2,685 religious texts cited by ISIS over a 3 year period",Religious Texts Used By ISIS,https://www.kaggle.com/fifthtribe/isis-religious-texts,Fri Sep 01 2017
,Kelvin Xiao,"[Cyclin-dependent kinase 2,  cdk2]","[string, string]","Outline It was reported that an estimated 4292000 new cancer cases and 2814000 cancer deaths would occur in China in 2015. Chen W. etc. (2016) Cancer statistics in China 2015. Small molecules play an non-trivial role in cancer chemotherapy. Here I focus on inhibitors of 8 protein kinases(name abbr)  Cyclin-dependent kinase 2 cdk2 Epidermal growth factor receptor erbB1 egfr_erbB1 Glycogen synthase kinase-3 beta gsk3b Hepatocyte growth factor receptor hgfr MAP kinase p38 alpha map_k_p38a Tyrosine-protein kinase LCK tpk_lck Tyrosine-protein kinase SRC tpk_src Vascular endothelial growth factor receptor 2 vegfr2  For each protein kinase several thousand inhibitors are collected  from chembl database in which molecules with IC50 lower than 10 uM are usually considered as inhibitors otherwise non-inhibitors. Challenge Based on those labeled molecules build your model and try to make the right prediction. Additionally  more than 70000 small molecules are generated from pubchem database. And you can screen these molecules to find out potential inhibitors. P.S. the majority of these molecules are non-inhibitors. DataSets(hdf5 version) There are 8 protein kinase files and 1 pubchem negative samples file. Taking ""cdk2.h5"" as an example import h5py from scipy import sparse hf = h5py.File(""../input/cdk2.h5"" ""r"") ids = hf[""chembl_id""].value # the name of each molecules ap = sparse.csr_matrix((hf[""ap""][""data""] hf[""ap""][""indices""] hf[""ap""][""indptr""]) shape=[len(hf[""ap""][""indptr""]) - 1 2039]) mg = sparse.csr_matrix((hf[""mg""][""data""] hf[""mg""][""indices""] hf[""mg""][""indptr""]) shape=[len(hf[""mg""][""indptr""]) - 1 2039]) tt = sparse.csr_matrix((hf[""tt""][""data""] hf[""tt""][""indices""] hf[""tt""][""indptr""]) shape=[len(hf[""tt""][""indptr""]) - 1 2039]) features = sparse.hstack([ap mg tt]).toarray() # the samples' features each row is a sample and each sample has 3*2039 features labels = hf[""label""].value # the label of each molecule ",Other,,[healthcare],CC4,,,1916,18407,2048,Predict small molecules' activity targeting  protein kinase,Cancer Inhibitors,https://www.kaggle.com/xiaotawkaggle/inhibitors,Fri Oct 28 2016
,Jacob Boysen,"[N-NUMBER, SERIAL NUMBER, MFR MDL CODE, ENG MFR MDL, YEAR MFR, TYPE REGISTRANT, NAME, STREET, STREET2, CITY, STATE, ZIP CODE, REGION, COUNTY, COUNTRY, LAST ACTION DATE, CERT ISSUE DATE, CERTIFICATION, TYPE AIRCRAFT, TYPE ENGINE, STATUS CODE, MODE S CODE, FRACT OWNER, AIR WORTH DATE, OTHER NAMES(1), OTHER NAMES(2), OTHER NAMES(3), OTHER NAMES(4), OTHER NAMES(5), EXPIRATION DATE, UNIQUE ID, KIT MFR, KIT MODEL, MODE S CODE HEX, X35]","[numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, numeric, numeric, string, string, string, string]","Context BuzzFeed had previously reported on flights of spy planes operated by the FBI and the Department of Homeland Security (DHS) and reasoned that it should be possible to train a machine learning algorthim to identify other aircraft performing similar surveillance based on characteristics of the aircraft and their flight patterns. You can read the story here and additional analysis and code by Peter Aldhous can be found here. Content BuzzFeed News obtained more than four months of aircraft transponder detections from the plane tracking website Flightradar24 covering August 17 to December 31 2015 UTC containing all data displayed on the site within a bounding box encompassing the continental United States Alaska Hawaii and Puerto Rico. Flightradar24 receives data from its network of ground-based receivers supplemented by a feed from ground radars provided by the Federal Aviation Administration (FAA) with a five-minute delay. After parsing from the raw files supplied by Flightradar24 the data included the following fields for each transponder detection  adshex Unique identifier for each aircraft corresponding to its ""Mode-S"" code in hexademical format. flight_id Unique identifier for each ""flight segment"" in hexadecimal format. A flight segment is a continuous series of transponder detections for one aircraft. There may be more than one segment per flight if a plane disappears from Flightradar24's coverage for a period --- for example when flying over rural areas with sparse receiver coverage. While being tracked by Fightradar24 planes were typically detected several times per minute. latitude longitude Geographic location in digital degrees. altitude Altitude in feet. speed Ground speed in knots. squawk Four-digit code transmitted by the transponder. type Aircraft manufacter and model if identified. timestamp Full UTC timestamp. track Compass bearing in degrees with 0 corresponding to north.  We also calculated  steer Change in compass bearing from the previous transponder detection for that aircraft; negative values indicate a turn to the left positive values a turn to the right.  Feature engineering First we filtered the data to remove planes registered abroad based on their adshex code common commercial airliners based on their type and aircraft with fewer than 500 transponder detections. Then we took a random sample of 500 aircraft and calculated the following for each one  duration of each flight segment recorded by Flightradar24 in minutes. boxes Area of a rectangular bounding box drawn around each flight segment in square kilometers.  Finally we calculated the following variables for each of the aircraft in the larger filtered dataset  duration1duration2duration3duration4duration5 Proportion of flight segment durations for each plane falling into each of five quantiles calculated from duration for the sample of 500 planes. The proportions for each aircraft must add up to 1; if the durations of flight segments for a plane closely matched those for a typical plane from the sample these numbers would all approximate to 0.2; a plane that mostly flew very long flights would have large decimal fraction for duration5. boxes1boxes2boxes3boxes4boxes5 Proportion of bounding box areas for each plane falling into each of five quantiles calculated from boxes for the sample of 500 planes. speed1speed2speed3speed4speed5 Proportion of speed values recorded for the aircraft falling into each of five quantiles recorded for speed for the sample of 500 planes. altitude1altitude2altitude3altitude4altitude5 Proportion of altitude values recorded for the aircraft falling into each of five quantiles recorded for altitude for the sample of 500 planes. steer1steer2steer3steer4steer5steer6steer7steer8 Proportion of steer values for each aircraft falling into bins set manually after observing the distribution for the sample of 500 planes using the breaks -180 -25 -10 -1 0 1 22 45 180. flights Total number of flight segments for each plane. squawk_1 Squawk code used most commonly by the aircraft. observations Total number of transponder detections for each plane. type Aircraft manufacter and model if identified else unknown.  The resulting data for 19799 aircraft are in the file planes_features.csv. Acknowledgements This dataset was created by Peter Aldhous from raw Flightradar24 data as well as FAA data. Inspiration  Peter used a Random Forest classifier--would another approach be better? Worse? Compare your list of candidates to his here. This data is from 2015--can you grab up to date data from ADS-B Exchange and find any new candidate planes? ",CSV,,"[government agencies, government]",CC0,,,100,1523,66,Identify Candidate US Government Spy Planes,Spy Plane Finder,https://www.kaggle.com/jboysen/spy-plane-finder,Sat Aug 12 2017
,Sanjaya Wijeratne,[],[],"Content EmoSim508 is the largest emoji similarity dataset that provides emoji similarity scores for 508 carefully selected emoji pairs. The most frequently co-occurring emoji pairs in a tweet corpus (that contains 147 million tweets) was used for creating the dataset and each emoji pair was annotated for its similarity using 10 human annotators. EmoSim508 dataset also consists of the emoji similarity scores generated from 8 different emoji embedding models proposed in ""A Semantics-Based Measure of Emoji Similarity"" paper by Wijeratne et al.    Acknowledgements EmoSim508 was developed by Sanjaya Wijeratne Lakshika Balasuriya Amit Sheth and Derek Doran. EmoSim508 is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0) license. Please cite the following paper when using EmoSim508 dataset Sanjaya Wijeratne Lakshika Balasuriya Amit Sheth and Derek Doran. 2017. A Semantics-Based Measure of Emoji Similarity. In Proceedings of WI ’17 Leipzig Germany August 23-26 2017 8 pages. DOI 10.1145/3106426.3106490 @inproceedings{DBLPconf/webi/WijeratneBSD17   author    = {Sanjaya Wijeratne and                Lakshika Balasuriya and                Amit Sheth and                Derek Doran}   title     = {A semantics-based measure of emoji similarity}   booktitle = {Proceedings of the International Conference on Web Intelligence Leipzig                Germany August 23-26 2017}   pages     = {646--653}   year      = {2017}   doi       = {10.1145/3106426.3106490} } You can also find more information about the dataset on the project website. Inspiration  Can you use these emoji similarity ratings to better group emoji in emoji keyboards? Can you use EmoSim508 as a baseline dataset to test your emoji similarity approach's performance? Can you use these emoji similarity ratings to suggest emoji domain names if they are already taken? ",{}JSON,,"[popular culture, linguistics, internet]",CC4,,,31,655,0.2490234375,An Emoji Similarity Baseline Dataset with 508 Emoji Pairs and Similarity Ratings,EmoSim508,https://www.kaggle.com/sanjayaw/emosim508,Mon Dec 25 2017
,Liza Bolton,[],[],Content Necessary shapefiles to create maps of New York City and its boroughs. Acknowledgements These files have been made available by the New York City Department of City Planning and were retrieved from http//www1.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page on 27 September 2017. Inspiration These shapefiles might pair nicely with the New York building and elevator data also on here as well as the NYC Tree Census I use it for.,Other,,"[cities, geography]",CC0,,,52,444,1,(Clipped to shoreline),NYC Borough Boundaries,https://www.kaggle.com/dataembassy/nyc-borough-boundaries,Mon Oct 02 2017
,Rachael Tatman,"[object, color]","[string, string]",Context Color terms are interesting in natural language processing because it’s an area where it’s possible to link distributional semantics (models of word meanings based on which words are used together in texts) to things in the world. This dataset was created to help link semantic models to images. Content This dataset is made up of two smaller files but were both presented and discussed in the same paper (see Acknowledgements). All data in this dataset is in English. Concrete color terms This dataset contains a list of common items manually labeled with one of the 11 colors from the set black blue brown green grey orange pink purple red white yellow. Literal vs. nonliteral colors This dataset is made up of color adjective-noun phrases randomly drawn from the most frequent 8K nouns and 4K adjectives in the concatenated ukWaC Wackypedia and BNC corpora. These were tagged by consensus by two human judges as literal (white towel black feather) or nonliteral (white wine white musician green future). Some phrases had both literal and nonliteral uses such as blue book in “book that is blue” vs. “automobile price guide”. In these cases only the most common sense (according to the judges) was taken into account for the present experiment. The dataset consists of 370 phrases. Acknowledgements If you use these datasets please cite Bruni E. G. Boleda M. Baroni N. K. Tran. 2012. Distributional semantics in technicolor. Proceedings of ACL 2012 pages 136-145 Jeju Island Korea. Inspiration  Are some colors used more often in a literal sense? Is there a relationship between how many objects are a given color and how often that color is used in a literal sense? Can you use the color of concrete and an image database of those objects to create an automatic color labeller? ,CSV,,"[visual arts, linguistics, artificial intelligence]",CC4,,,174,2024,0.0048828125,Literal & figurative use of color terms and the colors of objects,Color terms dataset,https://www.kaggle.com/rtatman/color-terms-dataset,Wed Jul 26 2017
,Ramakrishnan Srinivasan,"[symboling, normalized-losses, make, fuel-type, aspiration, num-of-doors, body-style, drive-wheels, engine-location, wheel-base, length, width, height, curb-weight, engine-type, num-of-cylinders, engine-size, fuel-system, bore, stroke, compression-ratio, horsepower, peak-rpm, city-mpg, highway-mpg, price]","[numeric, numeric, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset consist of data From 1985 Ward's Automotive Yearbook. Here are the sources Sources  1) 1985 Model Import Car and Truck Specifications 1985 Ward's Automotive Yearbook.  2) Personal Auto Manuals Insurance Services Office 160 Water Street New York NY 10038  3) Insurance Collision Report Insurance Institute for Highway Safety Watergate 600 Washington DC 20037 Content This data set consists of three types of entities (a) the specification of an auto in terms of various characteristics (b) its assigned insurance risk rating (c) its normalized losses in use as compared to other cars. The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then if it is more risky (or less) this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process ""symboling"". A value of +3 indicates that the auto is risky -3 that it is probably pretty safe.  The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small station wagons sports/speciality etc...) and represents the average loss per car per year.  Note Several of the attributes in the database could be used as a ""class"" attribute. Inspiration Please bring it on whatever inferences you can get it.",CSV,,[automobiles],Other,,,1749,13313,0.0234375,Dataset consist of various characteristic of an auto,Automobile Dataset,https://www.kaggle.com/toramky/automobile-dataset,Wed May 24 2017
,Rachael Tatman,[],[],Context Brazilian literature is the literature written in the Portuguese language by Brazilians or in Brazil including works written prior to the country’s independence in 1822. Throughout its early years literature from Brazil followed the literary trends of Portugal whereas gradually shifting to a different and authentic writing style in the course of the 19th and 20th centuries in the search for truly Brazilian themes and use of the Portuguese language. Content This dataset contains over 3.7 million words of Brazilian literature written between 1840 and 1908. There are 81 distinct works in this corpus written by Adolfo Caminha Aluisio Azevedo Bernardo Guimaraes Joaquim Manuel de Macedo Jose de Alencar Machado de Assis and Manuel Antonio de Almeida. Inspiration  Can you automatically identify topics/themes in each of these works? Can you automatically identify the author of a specific text? (You might want to split each author’s works into a test set and training set.) ,Other,,"[languages, literature, brazil, linguistics]",CC0,,,75,1129,23,3.7 million word corpus of Brazilian literature published between 1840-1908,Brazilian Portuguese Literature Corpus,https://www.kaggle.com/rtatman/brazilian-portuguese-literature-corpus,Fri Jul 28 2017
,GMAdevs,"[tourney_id, tourney_name, surface, draw_size, tourney_level, tourney_date, match_num, winner_id, winner_seed, winner_entry, winner_name, winner_hand, winner_ht, winner_ioc, winner_age, winner_rank, winner_rank_points, loser_id, loser_seed, loser_entry, loser_name, loser_hand, loser_ht, loser_ioc, loser_age, loser_rank, loser_rank_points, score, best_of, round]","[string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, numeric, string, numeric, numeric, numeric, dateTime, numeric, string]",Context A dataset of WTA matches including individual statistics. Content In these datasets there are individual csv files for WTA tournament from 2000 to 2016. The numbers in the last columns are absolute values using them you can calculate percentages. Acknowledgement Thanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile https//github.com/JeffSackmann/tennis_wta Inspiration This dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them.,CSV,,[tennis],CC4,,,657,2774,9,WTA matches from 2000 to 2016,Women's Tennis Association Matches,https://www.kaggle.com/gmadevs/wta-matches,Sun Feb 05 2017
,Sebastian Mantey,"[end_result, game, game_id, period, play, player, playoffs, score, season, shot_made, time]","[string, string, numeric, numeric, string, string, string, string, string, numeric, dateTime]",Context The data set includes information when the free throw was taken during the game who took the shot and if it went in or not. Content The data was scraped from ESPN.com. One example site is http//www.espn.com/nba/playbyplay?gameId=261229030,CSV,,[basketball],Other,,,677,5877,72,Over 600k free throws between 2006 and 2016,NBA Free Throws,https://www.kaggle.com/sebastianmantey/nba-free-throws,Thu Jan 05 2017
,Chris Crawford,"[DATE, TIME (UTC), ACID, No. A/C, TYPE A/C, ALT, MAJOR CITY, COLOR, Injury Reported, CITY, STATE]","[dateTime, numeric, string, numeric, string, numeric, string, string, string, string, string]","Context On February 14 2012 the President signed Public Law 112-95  the ""FAA Modernization and Reform Act of 2012."" Section 311 amended Title 18 of the United States Code (U.S.C.) Chapter 2 § 39 by adding § 39A which makes it a federal crime to aim a laser pointer at an aircraft. As a result of this law the FAA has compiled a report of laser incidents Content There is a datafile for each year and the column headers changed a little from year to year so keep that in mind when you're loading the data.  DATE - Date of report TIME (UTC) - Time of laser incident ACID - Aircraft ID (AC/ID Aircraft ID etc) No. A/C - Number of aircraft TYPE A/C - Type of aircraft ALT - Altitude MAJOR CITY - Nearest major city abbreviation COLOR - Color of laser Injury Reported - Were there injuries? CITY - Nearest city STATE - State  Acknowledments Original file was converted into separate CSV files for each year. Original dataset can be found here https//www.faa.gov/about/initiatives/lasers/laws/ ",CSV,,[aviation],CC0,,,64,740,1,A report of laser incidents from 2010 to 2014,FAA Laser Incident Reports,https://www.kaggle.com/crawford/laser-incident-report,Wed Aug 09 2017
,Department of Defense,"[, casenum, date, digest, keywords]","[numeric, string, dateTime, string, string]",Context Industry contractors that work for or with the United States Department of Defense and comes into contact with secret or privileged information must submit to a background check by the government as a part of their contractual obligations. Any employee who fails to get the necessary clearance will be unable to work. Employees may however appeal their decision; in this case the decision will be reviewed and finalized (or reversed) by the Department of Defense Office of Hearings and Appeals (DOHA). This dataset contains summaries of the deliberations and results of such hearings and provides a window into getting security clearance to work as a defense contractor in the United States. Content This data contains dates case numbers decisions and decisions summaries for over 20000 cases submitted for review between late 1996 and early 2016. Acknowledgements This data was published in an HTML format by the US Department of Defense. It has been converted into a CSV format before upload to Kaggle. Inspiration  What percentage of appeals were declined or upheld? What were the dominant reasons decisions were made? Have the factors behind decisions changed over times? What kinds of words appear in decision texts? ,CSV,,[military],CC0,,,67,596,10,Over 20000 security clearance appeals made to the Department of Defense,Industrial Security Clearance Adjurations,https://www.kaggle.com/usdod/dod-clearance-adjurations,Thu Sep 14 2017
,Institute of Museum and Library Services,"[State, Library ID, Submission Year, Library Name, Street Address, City, Zip Code, Longitude, Latitude, State Code, County Code, County, County Population, Public Library Definition, Legal Basis, Administrative Structure, Interlibrary Relationship, Service Population, Service Population Without Duplicates, Central Libraries, Branch Libraries, Bookmobiles, MLS Librarians, Librarians, Employees, Total Staff, Local Government Operating Revenue, State Government Operating Revenue, Federal Government Operating Revenue, Other Operating Revenue, Total Operating Revenue, Salaries, Benefits, Total Staff Expenditures, Print Collection Expenditures, Digital Collection Expenditures, Other Collection Expenditures, Total Collection Expenditures, Other Operating Expenditures, Total Operating Expenditures, Local Government Capital Revenue, State Government Capital Revenue, Federal Government Capital Revenue, Other Capital Revenue, Total Capital Revenue, Total Capital Expenditures, Print Collection, Digital Collection, Audio Collection, Downloadable Audio, Physical Video, Downloadable Video, Local Cooperative Agreements, State Licensed Databases, Total Licensed Databases, Print Subscriptions, Hours Open, Library Visits, Reference Transactions, Registered Users, Circulation Transactions, Interlibrary Loans Provided, Interlibrary Loans Received, Library Programs, Children’s Programs, Young Adult Programs, Library Program Audience, Children’s Program Audience, Young Adult Program Audience, Public Internet Computers, Internet Computer Use, Wireless Internet Sessions, Start Date, End Date]","[string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, dateTime]",Content The Public Libraries Survey (PLS) is conducted annually by the Institute of Museum and Library Services under the mandate in the Museum and Library Services Act of 2010. The data file includes all public libraries identified by state library administrative agencies in the 50 states and the District of Columbia. The reporting unit for the survey is the administrative entity defined as the agency that is legally established under local or state law to provide public library service to the population of a local jurisdiction. The FY 2014 PLS collected state characteristics data including the state total population estimate number of central and branch libraries and the total library visits and circulation transactions and data on each public library such as its name and location population of legal service area print and digital collections full-time-equivalent staff and operating revenue and expenditures. Acknowledgements The U.S. Census Bureau is the data collection agent for IMLS Public Libraries Survey.,CSV,,[libraries],CC0,,,263,1914,3,What state has the most library books per capita?,2014 Public Libraries Survey,https://www.kaggle.com/imls/public-libraries,Thu Jan 26 2017
,Dipanjan,"[instrument_token, exchange_token, tradingsymbol, name, last_price, expiry, strike, tick_size, lot_size, instrument_type, segment, exchange]","[numeric, numeric, string, string, numeric, string, numeric, numeric, numeric, string, string, string]",Context Starting something in FinTech is the most difficult thing. You have no open data. These days I'm trying to do some algo-trading. Maybe not in true sense because it's not high frequency scalping. But anyway that's that. What? The data gives almost-Realtime data for half of the Nifty 50 stocks for last week of May and first 2 Weeks of July.  Now here is the obvious question. The dataset does not have timestamp. That's because it is collected via Web-Socket streaming as it happens. Sometimes once in a couple of seconds sometimes 10-15 times in the same span. So there is no point to timestamp IMHO. Anyway it'll be client-side timestamp so not a true timestamp.  Description  tick_data.csv contains only the price-volume data. volume total volumes traded for the day  last_price denotes the quote price for latest trade List item instrument_list.csv contains description of the underlying instrument.  P.S *All the data points are not tick-by-tick update. Rather it is mostly an update after 600 ms provided a trade happened *,CSV,,[finance],CC4,,,518,9107,211,Real time price volume data for select Nifty 50 stocks from both NSE/BSE,1 M+ Real Time stock market data [NSE/BSE],https://www.kaggle.com/deeiip/1m-real-time-stock-market-data-nse,Sat Jun 24 2017
,Kheirallah Samaha,"[Event.Id, Investigation.Type, Accident.Number, Event.Date, Location, Country, Latitude, Longitude, Airport.Code, Airport.Name, Injury.Severity, Aircraft.Damage, Aircraft.Category, Registration.Number, Make, Model, Amateur.Built, Number.of.Engines, Engine.Type, FAR.Description, Schedule, Purpose.of.Flight, Air.Carrier, Total.Fatal.Injuries, Total.Serious.Injuries, Total.Minor.Injuries, Total.Uninjured, Weather.Condition, Broad.Phase.of.Flight, Report.Status, Publication.Date]","[string, string, string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Content The NTSB aviation accident database contains information from 1962 and later about civil aviation accidents and selected incidents within the United States its territories and possessions and in international waters. Acknowledgements Generally a preliminary report is available online within a few days of an accident. Factual information is added when available and when the investigation is completed the preliminary report is replaced with a final description of the accident and its probable cause. Full narrative descriptions may not be available for dates before 1993 cases under revision or where NTSB did not have primary investigative responsibility. Inspiration Hope it will teach us how to improve the quality and safety of traveling by Airplane.,CSV,,[aviation],CC0,,,2493,18200,4,The NTSB aviation accident dataset,Aviation Accident Database & Synopses,https://www.kaggle.com/khsamaha/aviation-accident-database-synopses,Mon Jan 15 2018
,Chris Crawford,"[club, last_name, first_name, position, base_salary, guaranteed_compensation]","[string, string, string, string, numeric, numeric]",Context The Major League Soccer Union releases the salaries of every MLS player each year. This is a collection of salaries from 2007 to 2017. Content Each file contains the following fields  club Team abbreviation last_name Player last name first_name Player first name position Position abbreviation base_salary Base salary guaranteed_compensation Guaranteed compensation  Acknowledgements Jeremy Singer-Vine over at Data is Plural scraped the PDF's released by the MLS Union and put the data in a nice little package of CSV files for everyone. I downloaded this dataset from https//github.com/data-is-plural/mls-salaries MIT License Inspiration Who in the MLS makes the most money? Are they worth it? I make about $900 bazillion each year can I afford a soccer team?,CSV,,"[association football, income]",Other,,,464,2604,0.197265625,Salaries from 2007 to 2017,U.S. Major League Soccer Salaries,https://www.kaggle.com/crawford/us-major-league-soccer-salaries,Fri Jul 14 2017
,US Census Bureau,[],[],The 2014 American Community Survey Public Use Microdata Sample Context The American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.  Frequency Annual Period 2014  Content Through the ACS we know more about jobs and occupations educational attainment veterans whether people own or rent their home and other topics. Public officials planners and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS you are doing your part to help your community plan hospitals and schools support school lunch programs improve emergency services build bridges and inform businesses looking to add jobs and expand to new markets and more. The data dictionary can be found here. Inspiration Kernels created using the 2013 ACS can serve as excellent starting points for working with the 2014 ACS. For example the following analyses were created using ACS data  Work arrival times and earnings in the USA Inequality in STEM careers   Acknowledgements The American Community Survey (ACS) is administered processed researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce.,Other,,"[demographics, sociology]",CC0,,,805,7776,3072,Detailed information about the American people and workforce,2014 American Community Survey,https://www.kaggle.com/census/2014-american-community-survey,Mon Oct 31 2016
,Aleksey Bilogur,"[Borough Number, Building Name, Building Address, Borough, State, Postcode, Block, Lot, FY15 Energy Usage (MMBTU) [Utility energy, excluding Fuel Oil, Latitude, Longitude, Community Board, Council District, Census Tract, BIN, BBL, NTA]","[numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context This dataset contains energy usage information for every building owned and managed by NYC DCAS. DCAS or the Department of Citywide Administrative Services is the arm of the New York City municipal government which handles ownership and management of the city's office facilities and real estate inventory. The organization voluntarily publicly discloses self-measured information about the energy use of its buildings. Content This data contains information on the name address location and 2015 financial cycle energy usage of every building managed at that time by DCAS. Acknowledgements This dataset is published as-is by of the City of New York (here). Inspiration  By combining this dataset with the New York City Buildings Database what can you learn about the energy usage of buildings in New York City? Can you use this data to model the energy consumption for city's office space at large? ,CSV,,"[government, real estate, energy]",CC0,,,207,1879,0.009765625,Energy usage for New York City owned office buildings,NYC Government Building Energy Usage,https://www.kaggle.com/residentmario/nyc-building-energy-usage,Mon Oct 23 2017
,GrubenM,"[Date, TempHighF, TempAvgF, TempLowF, DewPointHighF, DewPointAvgF, DewPointLowF, HumidityHighPercent, HumidityAvgPercent, HumidityLowPercent, SeaLevelPressureHighInches, SeaLevelPressureAvgInches, SeaLevelPressureLowInches, VisibilityHighMiles, VisibilityAvgMiles, VisibilityLowMiles, WindHighMPH, WindAvgMPH, WindGustMPH, PrecipitationSumInches, Events]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context This dataset is meant to complement the Austin Bikesharing Dataset. Content Contains the  Date (YYYY-MM-DD)  TempHighF (High temperature in Fahrenheit)  TempAvgF (Average temperature in Fahrenheit)  TempLowF (Low temperature in Fahrenheit)  DewPointHighF (High dew point in Fahrenheit)  DewPointAvgF (Average dew point in Fahrenheit)  DewPointLowF (Low dew point in Fahrenheit)  HumidityHighPercent (High humidity as a percentage)  HumidityAvgPercent (Average humidity as a percentage)  HumidityLowPercent (Low humidity as a percentage)  SeaLevelPressureHighInches (High sea level pressure in inches)  SeaLevelPressureAvgInches (Average sea level pressure in inches)  SeaLevelPressureLowInches (Low sea level pressure in inches)  VisibilityHighMiles (High visibility in miles)  VisibilityAvgMiles (Average visibility in miles)  VisibilityLowMiles (Low visibility in miles)  WindHighMPH (High wind speed in miles per hour)  WindAvgMPH (Average wind speed in miles per hour)  WindGustMPH (Highest wind speed gust in miles per hour)  PrecipitationSumInches (Total precipitation in inches)  ('T' if Trace)  Events (Adverse weather events.  ' ' if None)   This dataset contains data for every date from 2013-12-21 to 2017-07-31. Acknowledgements This dataset was obtained from WeatherUnderground.com at the Austin KATT station. Inspiration Can we use this dataset to explain some of the variation in the Austin Bikesharing Dataset?,CSV,,[],GPL,,,248,1510,0.1005859375,"Historical temperature, precipitation, humidity, and windspeed for Austin, Texas",Austin Weather,https://www.kaggle.com/grubenm/austin-weather,Tue Aug 15 2017
,WΔ,[],[],The Search for New Earths GitHub The data describe the change in flux (light intensity) of several thousand stars. Each star has a binary label of 2 or 1. 2 indicated that that the star is confirmed to have at least one exoplanet in orbit; some observations are in fact multi-planet systems. As you can imagine planets themselves do not emit light but the stars that they orbit do. If said star is watched over several months or years there may be a regular 'dimming' of the flux (the light intensity). This is evidence that there may be an orbiting body around the star; such a star could be considered to be a 'candidate' system. Further study of our candidate system for example by a satellite that captures light at a different wavelength could solidify the belief that the candidate can in fact be 'confirmed'.  In the above diagram a star is orbited by a blue planet. At t = 1 the starlight intensity drops because it is partially obscured by the planet given our position. The starlight rises back to its original value at t = 2. The graph in each box shows the measured flux (light intensity) at each time interval.  Description Trainset  5087 rows or observations. 3198 columns or features. Column 1 is the label vector. Columns 2 - 3198 are the flux values over time. 37 confirmed exoplanet-stars and 5050 non-exoplanet-stars.  Testset  570 rows or observations. 3198 columns or features. Column 1 is the label vector. Columns 2 - 3198 are the flux values over time. 5 confirmed exoplanet-stars and 565 non-exoplanet-stars.  Acknowledgements The data presented here are cleaned and are derived from observations made by the NASA Kepler space telescope. The Mission is ongoing - for instance data from Campaign 12 was released on 8th March 2017. Over 99% of this dataset originates from Campaign 3. To boost the number of exoplanet-stars in the dataset confirmed exoplanets from other campaigns were also included. To be clear all observations from Campaign 3 are included. And in addition to this confirmed exoplanet-stars from other campaigns are also included. The datasets were prepared late-summer 2016. Campaign 3 was used because 'it was felt' that this Campaign is unlikely to contain any undiscovered (i.e. wrongly labelled) exoplanets. NASA open-sources the original Kepler Mission data and it is hosted at the Mikulski Archive. After being beamed down to Earth NASA applies de-noising algorithms to remove artefacts generated by the telescope. The data - in the .fits format - is stored online. And with the help of a seasoned astrophysicist anyone with an internet connection can embark on a search to find and retrieve the datafiles from the Archive. The cover image is copyright © 2011 by Dan Lessmann,CSV,,"[astronomy, space]",CC0,,,2241,25726,278,Kepler labelled time series data,Exoplanet Hunting in Deep Space,https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data,Wed Apr 12 2017
,Tony Pino,"[Suburb, Address, Rooms, Type, Price, Method, SellerG, Date, Distance, Postcode, Bedroom2, Bathroom, Car, Landsize, BuildingArea, YearBuilt, CouncilArea, Lattitude, Longtitude, Regionname, Propertycount]","[string, string, numeric, string, string, string, string, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, string, numeric]",Update 28/11/2017 - Last few weeks clearance levels starting to decrease (I may just be seeing a pattern I want to see.. maybe I'm just evil). Anyway can any of you magicians make any sense of it?  Melbourne is currently experiencing a housing bubble (some experts say it may burst soon). Maybe someone can find a trend or give a prediction? Which suburbs are the best to buy in? Which ones are value for money? Where's the expensive side of town? And more importantly where should I buy a 2 bedroom unit? Content & Acknowledgements This data was scraped from publicly available results posted every week from Domain.com.au I've cleaned it as best I can now it's up to you to make data analysis magic. The dataset includes Address Type of Real estate Suburb Method of Selling Rooms Price Real Estate Agent Date of Sale and distance from C.B.D. ....Now with extra data including including property size land size and council area you may need to change your code! Some Key Details Suburb Suburb Address Address Rooms Number of rooms Price Price in dollars Method                S - property sold;                SP - property sold prior;                PI - property passed in;                PN - sold prior not disclosed;                SN - sold not disclosed;                NB - no bid;                VB - vendor bid;                 W - withdrawn prior to auction;                 SA - sold after auction;                 SS - sold after auction price not disclosed.                 N/A - price or highest bid not available.  Type         br - bedroom(s);          h - housecottagevilla semiterrace;          u - unit duplex;         t - townhouse;          dev site - development site;          o res - other residential. SellerG Real Estate Agent Date Date sold Distance Distance from CBD Regionname General Region (West North West North North east ...etc)  Propertycount Number of properties that exist in the suburb. Bedroom2  Scraped # of Bedrooms (from different source) Bathroom Number of Bathrooms    Car Number of carspots  Landsize Land Size BuildingArea Building Size  YearBuilt Year the house was built  CouncilArea Governing council for the area  Lattitude Self explanitory  Longtitude Self explanitory,CSV,,"[housing, demographics]",CC4,,,3545,25444,0.9072265625,Melbourne housing clearance data from Jan 2016,Melbourne Housing Market,https://www.kaggle.com/anthonypino/melbourne-housing-market,Tue Feb 13 2018
,Rachael Tatman,"[id;accession_number;artist;artistRole;artistId;title;dateText;medium;creditLine;year;acquisitionYear;dimensions;width;height;depth;units;inscription;thumbnailCopyright;thumbnailUrl;url, accession_number, artist, artistRole, artistId, title, dateText, medium, creditLine, year, acquisitionYear, dimensions, width, height, depth, units, inscription, thumbnailCopyright, thumbnailUrl, url]","[string, string, string, string, numeric, string, numeric, string, string, numeric, numeric, string, numeric, numeric, numeric, string, numeric, string, string, string]","Context ""Tate is an institution that houses the United Kingdom's national collection of British art and international modern and contemporary art. It is a network of four art museums Tate Britain London (until 2000 known as the Tate Gallery founded 1897) Tate Liverpool (founded 1988) Tate St Ives Cornwall (founded 1993) and Tate Modern London (founded 2000) with a complementary website Tate Online (created 1998). Tate is not a government institution but its main sponsor is the UK Department for Culture Media and Sport. ""The name 'Tate' is used also as the operating name for the corporate body which was established by the Museums and Galleries Act 1992 as 'The Board of Trustees of the Tate Gallery'. ""The gallery was founded in 1897 as the National Gallery of British Art. When its role was changed to include the national collection of modern art as well as the national collection of British art in 1932 it was renamed the Tate Gallery after sugar magnate Henry Tate of Tate & Lyle who had laid the foundations for the collection. The Tate Gallery was housed in the current building occupied by Tate Britain which is situated in Millbank London. In 2000 the Tate Gallery transformed itself into the current-day Tate or the Tate Modern which consists of a federation of four museums Tate Britain which displays the collection of British art from 1500 to the present day; Tate Modern which is also in London houses the Tate's collection of British and international modern and contemporary art from 1900 to the present day. Tate Liverpool has the same purpose as Tate Modern but on a smaller scale and Tate St Ives displays modern and contemporary art by artists who have connections with the area. All four museums share the Tate Collection. One of the Tate's most publicised art events is the awarding of the annual Turner Prize which takes place at Tate Britain."" -- Tate. (n.d.). In Wikipedia. Retrieved August 18 2017 from https//en.wikipedia.org/wiki/Plagiarism. Text reproduced here under a CC-BY-SA 3.0 license. Content This dataset contains the metadata for around 70000 artworks that Tate owns or jointly owns with the National Galleries of Scotland as part of ARTIST ROOMS. Metadata for around 3500 associated artists is also included. The metadata here is released under the Creative Commons Public Domain CC0 licence. Images are not included and are not part of the dataset.  This dataset contains the following information for each artwork  Id accession_number artist artistRole artistId title dateText medium creditLine year acquisitionYear dimensions width  height  depth units inscription thumbnailCopyright  thumbnailUr url  You may also like  Museum of Modern Art Collection Title artist date and medium of every artwork in the MoMA collection The Metropolitan Museum of Art Open Access Explore information on more than 420000 historic artworks ",CSV,,"[museums, visual arts, europe]",CC0,,,99,1221,26,"Metadata for 70,000 artworks from the Tate",The Tate Collection,https://www.kaggle.com/rtatman/the-tate-collection,Sat Aug 19 2017
,US Environmental Protection Agency,"[efid, id, salesArea, score, scoreAlt, smartwayScore, standard, stdText]","[string, numeric, numeric, numeric, numeric, numeric, string, string]",Fuel economy data are the result of vehicle testing done at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor Michigan and by vehicle manufacturers with oversight by EPA. Content Please see the csvs labeled with 'fields' for descriptions of the data fields; there are too many to list here. Acknowledgements This dataset was kindly provided by the US EPA. You can find the original dataset which is updated regularly here. Inspiration  How has the rate of change of fuel economy changed over time? Do simple clustering techniques on vehicles lead to the same groupings that are typically associated with manufacturers such as putting Porsche and BMW together in a luxury car group? ,CSV,,"[government agencies, vehicles]",CC0,,,326,2574,17,Mileage and more for 1948-2018,Vehicle Fuel Economy,https://www.kaggle.com/epa/vehicle-fuel-economy,Thu Sep 14 2017
,WesDuckett,"[Id, Age Cohort, Age, Gender, Expenditures, Ethnicity]","[numeric, string, numeric, string, numeric, string]",Context This data set contains data regarding the allocation of funding from the Department of Developmental Services to developmentally-disabled individuals in California in 2014.  Content The variables included are  Id [int] Age Cohort (age group) [factor] Age [int] Gender [factor] Expenditures [int] Ethnicity [factor]  This data set is well suited for exploring the effects of Simpson's Paradox and confounding variables. Acknowledgements The data was originally retrieved from the California Department of Developmental Services (http//www.dds.ca.gov) by Stanley Taylor and Amy Mickel from California State University Sacremento. The names associated with each record have been removed to protect anonymity. Taylor and Mickel explored Simpson's Paradox using this data set after a discrimination lawsuit was filed against the California DDS. The lawsuit claimed that White Non-Hispanics were receiving more funding than Hispanics. To learn more about the analysis and findings of Taylor and Mickel read the paper they published together by following this link www.amstat.org/publications/jse/v22n1/mickel.pdf Inspiration Is there any basis to the claim of discrimination? What are the confounding variables? What are other ways to organize this data to gain an alternate perspective?,CSV,,"[finance, health, demographics]",CC0,,,41,835,0.0400390625,Exploring Simpson's Paradox,California DDS Expenditures,https://www.kaggle.com/wduckett/californiaddsexpenditures,Fri Sep 29 2017
,Maxime Fuccellaro,"[, KEY, LABEL, age, capital_gain, capital_loss, education, education_num, fnlwgt, hours_per_week, marital_status, native_country, occupation, race, relationship, sex, workLABEL, LabelPage, LabelPcapital_gain, LabelPcapital_loss, LabelPeducation, LabelPeducation_num, LabelPhours_per_week, LabelPmarital_status, LabelPnative_country, LabelPoccupation, LabelPrace, LabelPrelationship, LabelPsex, LabelPworkLABEL]","[numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context This data was extracted from the 1994 Census bureau database by Ronny Kohavi and Barry Becker (Data Mining and Visualization Silicon Graphics). A set of reasonably clean records was extracted using the following conditions ((AAGE>16) && (AGI>100) && (AFNLWGT>1) && (HRSWK>0)). The prediction task is to determine whether a person makes over $50K a year. In order to make the job better we used artificial Intelligence to automatically modify the columns.  Content This Dataset contains the initial Dataset columns as well as the new ones obtained by feeding the original US Census Dataset to PredicSis.ai in order to automatically   1. Discretise continuous variables into relevant intervals.  2. Group values of categorical variables together in order to reduce the modality of the variables. Acknowledgements https//archive.ics.uci.edu/ml/datasets/Census+Income https//www.kaggle.com/uciml/adult-census-income https//predicsis.ai Inspiration We want to see by how much auto ML/AI improves the data scientist work quality.,CSV,,"[finance, politics, demographics]",CC0,,,134,1631,19,Artificial Intelligence to boost data science,Adult Census Income with AI,https://www.kaggle.com/blackbee2016/adult-census-income-with-ai,Tue Aug 08 2017
,Haohan Wang,"[subject ID,  age,  ethnicity,  gender]","[numeric, numeric, string, string]",Description We collected EEG signal data from 10 college students while they watched MOOC video clips. We extracted online education videos that are assumed not to be confusing for college students such as videos of the introduction of basic algebra or geometry. We also prepare videos that are expected to confuse a typical college student if a student is not familiar with the video topics like Quantum Mechanics and Stem Cell Research. We prepared 20 videos 10 in each category. Each video was about 2 minutes long. We chopped the two-minute clip in the middle of a topic to make the videos more confusing.  The students wore a single-channel wireless MindSet that measured activity over the frontal lobe. The MindSet measures the voltage between an electrode resting on the forehead and two electrodes (one ground and one reference) each in contact with an ear. After each session the student rated his/her confusion level on a scale of 1-7 where one corresponded to the least confusing and seven corresponded to the most confusing. These labels if further normalized into labels of whether the students are confused or not. This label is offered as self-labelled confusion in addition to our predefined label of confusion.  Data information -----data.csv  Column 1 Subject ID  Column 2 Video ID Column 3 Attention (Proprietary measure of mental focus) Column 4 Mediation (Proprietary measure of calmness) Column 5 Raw (Raw EEG signal) Column 6 Delta (1-3 Hz of power spectrum) Column 7 Theta (4-7 Hz of power spectrum) Column 8 Alpha 1 (Lower 8-11 Hz of power spectrum) Column 9 Alpha 2 (Higher 8-11 Hz of power spectrum) Column 10 Beta 1 (Lower 12-29 Hz of power spectrum) Column 11 Beta 2 (Higher 12-29 Hz of power spectrum) Column 12 Gamma 1 (Lower 30-100 Hz of power spectrum) Column 13 Gamma 2 (Higher 30-100 Hz of power spectrum) Column 14 predefined label (whether the subject is expected to be confused) Column 15 user-defined label (whether the subject is actually confused)  -----subject demographic  Column 1 Subject ID Column 2 Age Column 3 Ethnicity (Categorized according to https//en.wikipedia.org/wiki/List_of_contemporary_ethnic_groups) Column 4 Gender  -----video data Each video lasts roughly two-minute long we remove the first 30 seconds and last 30 seconds only collect the EEG data during the middle 1 minute.  Format These data are collected from ten students each watching ten videos.  Therefore it can be seen as only 100 data points for these 12000+ rows. If you look at this way then each data point consists of 120+ rows which is sampled every 0.5 seconds (so each data point is a one minute video). Signals with higher frequency are reported as the mean value during each 0.5 second.  Reference  Wang H. Li Y. Hu X. Yang Y. Meng Z. & Chang K. M. (2013 June). Using EEG to Improve Massive Open Online Courses Feedback Interaction. In AIED Workshops. [PDF]  Data Collection The data is collected from a software that we implemented ourselves. Check HaohanWang/Bioimaging for the source code.  Inspiration  This dataset is an extremely challenging data set to perform binary classification. 65% of prediction accuracy is quite decent according to our experience.  It is an interesting data set to carry out the variable selection  (causal inference) task that may help further research. Past research has indicated that Theta signal is correlated with confusion level.  It is also an interesting data set for confounding factors correction model because we offer two labels (subject id and video id) that could profoundly confound the results.  Warning The data for subject 3 might be corrupted.  Other Resources Promotion Video Source Code of Data Collection Software Contact Haohan Wang ,CSV,,"[healthcare, neuroscience]",CC0,,,2462,29864,115,For variable selection and causal inference. Challenging for classification,EEG brain wave for confusion,https://www.kaggle.com/wanghaohan/eeg-brain-wave-for-confusion,Thu Dec 01 2016
,Allan,[],[],Context The State of the Nation Address of the President of South Africa (abbreviated SONA) is an annual event in the Republic of South Africa in which the President of South Africa reports on the status of the nation normally to the resumption of a joint sitting of Parliament (the National Assembly and the National Council of Provinces). Content Full text of all the speeches from 1990 through to 2018.  In years that elections took place a State of the Nation Address happens twice once before and again after the election.,Other,,[],ODbL,,,32,674,0.4169921875,Full texts of the South African State of the Nation addresses,State of the Nation Corpus (1990 - 2018),https://www.kaggle.com/allank/state-of-the-nation-1990-2017,Wed Feb 21 2018
,UCI Machine Learning,[],[],"Context The task is to predict whether an image is an advertisement (""ad"") or not (""nonad""). Content There are 1559 columns in the data.Each row in the data represent one image which is tagged as ad or nonad in the last column.column 0 to 1557 represent the actual numerical attributes of the images Acknowledgements Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. Here is a BiBTeX citation as well @misc{Lichman2013  author = ""M. Lichman"" year = ""2013"" title = ""{UCI} Machine Learning Repository"" url = ""http//archive.ics.uci.edu/ml"" institution = ""University of California Irvine School of Information and Computer Sciences"" } https//archive.ics.uci.edu/ml/citation_policy.html",CSV,,[],ODbL,,,502,6040,10,This dataset represents a set of possible advertisements on Internet pages,Internet Advertisements Data Set,https://www.kaggle.com/uciml/internet-advertisements-data-set,Fri Sep 01 2017
,My Khe Nguyen,"[Execution, LastName, FirstName, TDCJNumber, Age, Race, CountyOfConviction, AgeWhenReceived, EducationLevel, NativeCounty , PreviousCrime, Codefendants, NumberVictim, WhiteVictim, HispanicVictim, BlackVictim, VictimOther Races, FemaleVictim, MaleVictim, LastStatement]","[numeric, string, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric, numeric, string]","1. Context Capital punishment is one of the controversial human rights issues in the United States. While surfing the Internet for an interesting dataset I came across this database by Texas Department of Criminal Justice which comprises of the offenders' last words before execution. Some of the statements are    ""...Young people listen to your parents; always do what they tell you to do go to school learn from your mistakes. Be careful before you sign anything with your name. Never despite what other people say..."" (Ramiro Hernandez executed on April 9th 2014)   ""First and foremost I'd like to say ""Justice has never advanced by taking a life"" by Coretta Scott King. Lastly to my wife and to my kids I love y'all forever and always. That's it."" (Taichin Preyor executed on July 27th 2017)    As I skimmed these lines I decided to create this dataset.  2. Content This dataset includes information on criminals executed by Texas Department of Criminal Justice from 1982 to November 8th 2017. In Furman v Georgia in 1972 the Supreme Court considered a group of consolidated cases whereby it severely restricted the death penalty. However like other states Texas adjusted its legislation to address the Court's concern and once again allow for capital punishment in 1973. Texas adopted execution by lethal injection in 1977 and in 1982 the starting year of this dataset the first offender was executed by this method.     The dataset consists of 545 observations with 21 variables. They are  - Execution The order of execution numeric.  - LastName Last name of the offender character.  - FirstName First name of the offender character.  - TDCJNumber TDCJ Number of the offender numeric.  - Age Age of the offender numeric.  - Race Race of the offender categorical  Black Hispanic White Other.  - CountyOfConviction County of conviction character.  - AgeWhenReceived Age of offender when received numeric.  - EducationLevel Education level of offender numeric.  - Native County Native county of offender categorical  0 = Within Texas 1= Outside Texas.  - PreviousCrime  Whether the offender committed any crime before categorical 0= No 1= Yes.  - Codefendants Number of co-defendants numeric.  - NumberVictim Number of victims numeric.  - WhiteVictim HispanicVictim BlackVictim VictimOtherRace. FemaleVictim MaleVictim Number of victims with specified demographic features numeric.  - LastStatement Last statement of offender character.    3. Acknowledgement  This dataset is derived from the database by Texas Department of Criminal Justice which can be found in this link http//www.tdcj.state.tx.us/death_row/dr_executed_offenders.html . It can be seen that the original one has fewer than 10 variables and is embedded with some links to sub-datasets so I manually inputted more variables based on those links.     There are some complications with this dataset. Firstly the dataset was manually created so mistakes are inevitable though I have tried my best to minimize them. Secondly the recording of offender information is not complete and consistent. For example sometimes the education level of GED is interpreted as 11 years at other times as 9 or 10 years. ""None"" and ""NA"" are used interchangeably making it hard to distinguish between 0 and NA in the coded variable. The victim's information is often omitted so I rely on the description of the crime for the names and pronouns to make a judgement of the number of victims and their gender. Finally the last statements are sometimes recorded in the first person and sometimes in the third so the word choice might not be original.  That being said I find this dataset meaningful and worth sharing. 4. Inspiration  What are the demographics of the death row inmates? What are the patterns of their last statements? What is the relationship between the two? ",CSV,,"[crime, demographics]",ODbL,,,202,1776,0.453125,Text Mining with Farewell Words,Last Words of Death Row Inmates,https://www.kaggle.com/mykhe1097/last-words-of-death-row-inmates,Sun Dec 31 2017
,PyTorch,[],[],VGG-11  Very Deep Convolutional Networks for Large-Scale Image Recognition In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.  Authors Karen Simonyan Andrew Zisserman https//arxiv.org/abs/1409.1556  VGG Architectures   What is a Pre-trained Model? A pre-trained model has been previously trained on a dataset and contains the weights and biases that represent the features of whichever dataset it was trained on. Learned features are often transferable to different data. For example a model trained on a large dataset of bird images will contain learned features like edges or horizontal lines that you would be transferable your dataset.  Why use a Pre-trained Model? Pre-trained models are beneficial to us for many reasons. By using a pre-trained model you are saving time. Someone else has already spent the time and compute resources to learn a lot of features and your model will likely benefit from it. ,Other,,"[machine learning, pre-trained model]",CC0,,,3,418,471,VGG-11 Pre-trained Model for PyTorch,VGG-11,https://www.kaggle.com/pytorch/vgg11,Thu Dec 14 2017
,Chris Crawford,"[ID number, Conveyance, Page, Researcher, Notary First Name, Notary Last Name, Sales Date, Sellers First Name, Sellers Last Name, Sellers County of Origin, Sellers State of Origin, Representing Seller, Relationship to Seller, Buyers First Name, Buyers Last Name, Buyers County of Origin, Buyers State of Origin, Representing Buyer, Relationship to Buyer, Slave Name, Sex, Age, Color, Occupation, Family Relationship, Name Child 1, Sex Child 1, Age Child 1, Name Child 2, Sex Child 2, Age Child 2, Name Child 3, Sex Child 3, Age Child 3, Name Child 4, Sex Child 4, Age Child 4, Name Child 5, Sex Child 5, Age Child 5, Name Child 6, Sex Child 6, Age Child 6, Name Child 7, Sex Child 7, Age Child 7, Name Child 8, Sex Child 8, Age Child 8, Guaranteed, Notes on Guarantee, Number of Total Slaves, Number of Adult Slaves, Number of Child Slaves, Number of Prices, Price, Payment Method, Payment flag, DUMMY credit, Down Payment, mthcred, Interest Rate, Discount Rate, predicted rate, Calculations, Ratio, PresentValue, DUMMY omission, Reason for Omission, Comments, DUMMY Estate Sale]","[numeric, numeric, numeric, string, string, string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, string, string, string]","Context Abraham Lincoln's election produced Southern secession war and abolition. This dataset was used to study connections between news and slave prices for the period 1856-1861. By August 1861 slave prices had declined by roughly one-third from their 1860 peak. That decline was similar for all age and sex cohorts and thus did not reflect expected emancipation without compensation. The decision to secede reflected beliefs that the North would not invade and that emancipation without compensation was unlikely. Both were encouraged by Lincoln's conciliatory tone before the attack on Fort Sumter and subsequently dashed by Lincoln's willingness to wage all-out war.  Calomiris Charles W. and Jonathan Pritchett. 2016. ""Betting on Secession Quantifying Political Events Surrounding Slavery and the Civil War."" American Economic Review 106(1) 1-23.  Content Data description by Jonathan Pritchett These data were collected from the office of the Orleans parish Civil Clerk of Court. The sample includes all slave sales recorded by the register of conveyance from October 1856 to August 1861. The construction of the dataset is similar to that employed previously by Fogel and Engerman (1976). The unit of observation is the individual with the exception of children who were bundled with their mothers. Fields are defined as follows  ID number Unique observation number. Conveyance Number of Conveyance volume. Page Page number of transaction. Researcher Initials of research assistant who transcribed transaction. Notary First Name First name of public notary who recorded transaction. Notary Last Name Last name of public notary. Sales Date Sales date of transaction (MM/DD/YYYY format). This is NOT the date the sale was recorded in conveyance office. Sellers First Name Seller’s First Name. Sellers Last Name Seller’s Last Name. Sellers County of Origin Seller’s county (or city) of origin. Sellers State of Origin Seller’s state of origin. Representing Seller  Name of agent representing seller if seller is not present at time of sale (normally blank). Relationship to Seller Agent’s relationship to seller. Buyers First Name Buyer’s First Name. Buyers Last Name Buyer’s Last Name. Buyers County of Origin Buyer’s County (or city) of Origin. Buyers State of Origin Buyer’s State of Origin. Representing Buyer Name of agent representing buyer if buyer is not present at time of sale (normally blank). Relationship to Buyer Agent’s relationship to buyer. Slave Name Slave’s first name; rarely last name also listed. Sex Male (M) or female (F). Gender often inferred from sale record  such as Negro vs. Negress mulatto vs. mulattress etc. Age Age in years. Color Description of slave’s skin color but may also indicate ancestry (such as mulatto). Occupation Slave’s occupation – often blank. Family Relationship Describes family relationship of slaves (if any) sold in groups Name Child 1  Name of child 1 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 1  Gender of child 1 (sold with mother). Age Child 1 Age in years of child 1 (sold with mother). Decimal equivalents for age in months. Name Child 2 Name of child 2 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 2 Gender of child 2 (sold with mother). Age Child 2 Age in years of child 2 (sold with mother). Decimal equivalents for age in months. Name Child 3 Name of child 3 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 3 Gender of child 3 (sold with mother). Age Child 3 Age in years of child 3 (sold with mother). Decimal equivalents for age in months. Name Child 4 Name of child 4 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 4 Gender of child 4 (sold with mother). Age Child 4 Age in years of child 4 (sold with mother). Decimal equivalents for age in months. Name Child 5 Name of child 5 (sold with mother). Blank when slaves are listed separately and/or with separate prices Sex Child 5 Gender of child 5 (sold with mother). Age Child 5 Age in years of child 5 (sold with mother). Decimal equivalents for age in months. Name Child 6 Name of child 6 (sold with mother). Blank when slaves are listed separately and/or with separate prices Sex Child 6 Gender of child 6 (sold with mother). Age Child 6 Age in years of child 6 (sold with mother). Decimal equivalents for age in months. Name Child 7 Name of child 7 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 7 Gender of child 7 (sold with mother). Age Child 7 Age in years of child 7 (sold with mother). Decimal equivalents for age in months. Name Child 8  Name of child 8 (sold with mother). Blank when slaves are listed separately and/or with separate prices. Sex Child 8 Gender of child 8 (sold with mother). Age Child 8 Age in years of child 8 (sold with mother). Decimal equivalents for age in months. Guaranteed Yes or No. Most conveyance records omit this information (missing value). Notes on Guarantee Description of guarantee/flaw; reason why slave doesn’t have full guarantee. Number of Total Slaves Total number of slaves listed in transaction.  Number of Adult Slaves Total number of “principal” slaves – corresponds to the number of separate entries for each transaction. Recall that children (especially those under 10 years) were bundled with mothers. Number of Child Slaves Total number of children listed in transaction. Number of Prices Total number of prices listed in transaction. Price Price associated with slaves. For group sales with a single price only one price is listed for first slave and for other slaves entry is blank. Payment Method Cash or Credit. Sometimes “Cash and Credit” for credit sales with down payment. Also slave exchange typical result of redhibition    claim. Payment flag Description of payment schedule for credit sales. DUMMY credit 0 or 1 indicator for credit sales. Down Payment Number value of cash down payment for credit sales. mthcred Maximum length of credit (in months). Interest Rate Annual interest rate charged on credit sales. Discount Rate Calculated monthly discount rate. predicted rate Predicted monthly interest rate for credit sales without explicit interest rates. Calculations Intermediate calculation – please ignore. Ratio Intermediate calculation – please ignore. PresentValue Present value calculation for credit sales; blank for cash sales. DUMMY omission 0 or 1 indicator for omitting observation. Reason for Omission Reason for excluding observation from working sample. Comments Description of unusual characteristics for observation. DUMMY Estate Sale 0 or 1 indicator for estate sale.  Acknowledgements Calomiris Charles W. and Jonathan Pritchett. 2016. ""Betting on Secession Quantifying Political Events Surrounding Slavery and the Civil War."" American Economic Review 106(1) 1-23. DOI 10.1257/aer.20131483 This dataset was converted from XLSX to CSV Inspiration As a principal port New Orleans played a major role during the antebellum era in the Atlantic slave trade.  The authors of ""Betting on Secession Quantifying Political Events Surrounding Slavery and the Civil War."" did a fantastic job of putting this dataset together to learn more about the country's connections between slave trade and the American Civil War.",Other,,[history],Other,,,238,1823,4,"A dataset of 15,377 slave sales from 1856 - 1861",New Orlean's Slave Sales,https://www.kaggle.com/crawford/new-orleans-slave-sales,Thu Jul 13 2017
,Transparency International,"[CPI 2016 Rank, Country, Country Code, Region, CPI 2016 Score, CPI 2015 Score, CPI 2014 Score, CPI 2013 Score, CPI 2012 Score]","[numeric, string, string, string, numeric, numeric, numeric, numeric, numeric]",Content The Corruption Perceptions Index scores and ranks countries/territories based on how corrupt a country’s public sector is perceived to be. It is a composite index a combination of surveys and assessments of corruption collected by a variety of reputable institutions. The CPI is the most widely used indicator of corruption worldwide. Corruption generally comprises illegal activities which are deliberately hidden and only come to light through scandals investigations or prosecutions. There is no meaningful way to assess absolute levels of corruption in countries or territories on the basis of hard empirical data. Possible attempts to do so such as by comparing bribes reported the number of prosecutions brought or studying court cases directly linked to corruption cannot be taken as definitive indicators of corruption levels. Instead they show how effective prosecutors the courts or the media are in investigating and exposing corruption. Capturing perceptions of corruption of those in a position to offer assessments of public sector corruption is the most reliable method of comparing relative corruption levels across countries. Acknowledgements The data sources used to calculate the Corruption Perceptions Index scores and ranks were provided by the African Development Bank Bertelsmann Stiftung Foundation The Economist Freedom House IHS Markit IMD Business School Political and Economic Risk Consultancy Political Risk Services World Bank World Economic Forum World Justice Project and Varieties of Democracy Project.,CSV,,"[crime, politics]",CC4,,,598,4274,0.0224609375,Perceived level of political corruption for every country in the world,Corruption Perceptions Index,https://www.kaggle.com/transparencyint/corruption-index,Thu Jan 26 2017
,US Census Bureau,[],[],The 2015 American Community Survey Public Use Microdata Sample Context The American Community Survey (ACS) is an ongoing survey that provides vital information on a yearly basis about our nation and its people. Information from the survey generates data that help determine how more than $400 billion in federal and state funds are distributed each year.  Frequency Annual Period 2015  PWGTP (Weights) Please note. Each record is weighted with PWGTP.  For accurate analysis these weights need to be applied.  Reference Getting Started 'Python' for a simple kernel on how this field gets used.  Or click on the image below to see how this can be done in R (see code in this kernel).  The Data Dictionary can be found here but you'll need to scroll down to the PERSON RECORD section.  Content Through the ACS we know more about jobs and occupations educational attainment veterans whether people own or rent their home and other topics. Public officials planners and entrepreneurs use this information to assess the past and plan the future. When you respond to the ACS you are doing your part to help your community plan hospitals and schools support school lunch programs improve emergency services build bridges and inform businesses looking to add jobs and expand to new markets and more. The data dictionary can be found here. Inspiration Kernels created using the 2014 ACS and  2013 ACS can serve as excellent starting points for working with the 2015 ACS. For example the following analyses were created using ACS data  Work arrival times and earnings in the USA Inequality in STEM careers   Acknowledgements The American Community Survey (ACS) is administered processed researched and disseminated by the U.S. Census Bureau within the U.S. Department of Commerce.,Other,,"[employment, demographics, sociology]",CC0,,,649,4447,4096,Detailed information about the American people and workforce,2015 American Community Survey,https://www.kaggle.com/census/2015-american-community-survey,Sat Feb 04 2017
,FelipeLeiteAntunes,"[V1, .id, id, nome, tipoProposicao.id, tipoProposicao.sigla, tipoProposicao.nome, numero, ano, orgaoNumerador.id, orgaoNumerador.sigla, orgaoNumerador.nome, datApresentacao, txtEmenta, txtExplicacaoEmenta, regime.codRegime, regime.txtRegime, apreciacao.id, apreciacao.txtApreciacao, autor1.txtNomeAutor, autor1.idecadastro, autor1.codPartido, autor1.txtSiglaPartido, autor1.txtSiglaUF, qtdAutores, ultimoDespacho.datDespacho, ultimoDespacho.txtDespacho, situacao.id, situacao.descricao, situacao.orgao.codOrgaoEstado, situacao.orgao.siglaOrgaoEstado, situacao.principal.codProposicaoPrincipal, situacao.principal.proposicaoPrincipal, indGenero, qtdOrgaosComEstado]","[numeric, string, numeric, string, numeric, string, string, numeric, numeric, numeric, string, string, dateTime, string, string, numeric, string, numeric, string, string, numeric, numeric, string, string, numeric, string, string, numeric, string, numeric, string, numeric, string, string, numeric]",Patterns in the Brazilian congress voting behavior The Brazilian Government House of Representatives maintains a public database that contains legislative information since 1970. One type of information that is available are the records of bills. For each bill the database gives a list of votes choices state and party of each deputy and a list of details about the bill itself like type year text of proposal benches orientations and situation (a bill can be voted more than one time in this work we will treat each votation as a single one). We retrieved more than 100000 bills (propList) where less than 1% was voted (propVotList) until November 2016.   Our objective is detect regularity patterns of legislative behavior institutional arrangements and legislative outcome. Raw data from http//www2.camara.leg.br/transparencia/dados-abertos/dados-abertos-legislativo/webservices/proposicoes-1/proposicoes,CSV,,"[crime, politics]",ODbL,,,201,2866,116,Patterns in the Brazilian congress voting behavior,Brazilian congress,https://www.kaggle.com/felipeleiteantunes/braziliancongress,Mon Nov 14 2016
,ActiveGalaXy,"[name, username, tweetid, time, tweets]","[string, string, numeric, dateTime, string]","Context The image at the top of the page is a frame from today's (7/26/2016) Isis #TweetMovie from twitter a ""normal"" day when two Isis operatives murdered a priest saying mass in a French church. (You can see this in the center left).  A selection of data from this site is being made available here to Kaggle users.   UPDATE An excellent study by Audrey Alexander titled Digital Decay? is now available which traces the ""change over time among English-language Islamic State sympathizers on Twitter. Intent This data set is intended to be a counterpoise to the How Isis Uses Twitter data set.  That data set contains 17k tweets alleged to originate with ""100+ pro-ISIS fanboys"".  This new set contains 122k tweets collected on two separate days 7/4/2016 and 7/11/2016 which contained any of the following terms with no further editing or selection  isis  isil  daesh  islamicstate  raqqa  Mosul ""islamic state""  This is not a perfect counterpoise as it almost surely contains a small number of pro-Isis fanboy tweets.  However unless some entity such as Kaggle is willing to expend significant resources on a service something like an expert level Mechanical Turk or Zooniverse a high quality counterpoise is out of reach.   A counterpoise provides a balance or backdrop against which to measure a primary object in this case the original pro-Isis data. So if anyone wants to discriminate between pro-Isis tweets and other tweets concerning Isis you will need to model the original pro-Isis data or signal against the counterpoise which is  signal + noise.  Further background and some analysis can be found in this forum thread. This data comes from postmodernnews.com/token-tv.aspx which daily collects about 25MB of Isis tweets for the purposes of graphical display. PLEASE NOTE This server is not currently active. Data Details There are several differences between the format of this data set and the pro-ISIS fanboy dataset.  1. All the twitter t.co tags have been expanded where possible  2. There are no ""description location followers numberstatuses"" data columns.    I have also included my version of the original pro-ISIS fanboy set.  This version has all the t.co links expanded where possible. ",CSV,,"[crime, internet]",CC0,,,1024,11583,29,General tweets about Isis & related words,Tweets Targeting Isis,https://www.kaggle.com/activegalaxy/isis-related-tweets,Sat Jul 30 2016
,NASA,"[EVA #, Country, Crew, Vehicle, Date, Duration, Purpose]","[numeric, string, string, string, string, dateTime, string]","Context Extra-vehicular activities are activities done by an astronaut or cosmonaut outside a spacecraft beyond the Earth's appreciable atmosphere. I like to just call it space walking )  It's unclear if this is a complete record of spacewalks. So keep that in mind. Content  EVA # Country Crew Crew members separated with | Vehicle Space craft space ship space station etc. If multiple vehicles they are separated with | Date Duration Purpose Description of the EVA. Some of these have internal commas and are enclosed with double quotes ("")  Acknowledgements These data were collected from here The original CSV was modified slightly to remove extra spaces ",CSV,,"[space, spaceflight]",CC0,,,90,1192,0.08984375,A record of Russian and U.S. extra-vehicular actvity,Space walking,https://www.kaggle.com/nasa/space-walking-russian-and-us-evas,Mon Aug 28 2017
,Sohier Dane,[],[],This dataset contains an annual summary of the assets and liabilities from the bank's founding in 1696 through 2014. Content The csv is a condensed version of the original spreadsheet. Some notes disclaimers and a small portion of the data have been discarded to enable the format conversion. Acknowledgements This dataset was kindly made available by the Bank of England. You can find the original dataset here. Inspiration  Can you back out key moments in history from this dataset using time series analysis? ,CSV,,[economics],CC0,,,185,1569,0.185546875,Annual data from 1696 to 2014,The Bank of England’s balance sheet,https://www.kaggle.com/sohier/the-bank-of-englands-balance-sheet,Sat Sep 16 2017
,Chris Roth,"[date, sentiment.comparative, sentiment.negative_count, sentiment.positive_count, sentiment.score, time]","[dateTime, numeric, numeric, numeric, numeric, dateTime]",Chronist is a project to quantitatively monitor the emotional and physical changes of an individual over periods of time. My thesis is that if you can accurately show emotional or physical change over time you can objectively pinpoint how an environmental change such as a career change moving to a new city starting or ending a relationship or starting a new habit like going to the gym affected your physical and emotional health. This can lead to important insights on an individual level and for a population as a whole. If you are interested in hearing more about this project contributing your data or collaborating contact me at chris@cjroth.com. See the GitHub repository to read more about the tools that were used to generate the dataset.,CSV,,[linguistics],Other,,,757,7829,39,Data from the Chronist project,"Emotion, Aging, and Sentiment Over Time",https://www.kaggle.com/cjroth/chronist,Mon Feb 13 2017
,shivamagrawal,"[, carat, cut, color, clarity, depth, table, price, x, y, z]","[numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context This classic dataset contains the prices and other attributes of almost 54000 diamonds. It's a great dataset for beginners learning to work with data analysis and visualization. Content price price in US dollars (\$326--\$18823) carat weight of the diamond (0.2--5.01) cut quality of the cut (Fair Good Very Good Premium Ideal) color diamond colour from J (worst) to D (best) clarity a measurement of how clear the diamond is (I1 (worst) SI2 SI1 VS2 VS1 VVS2 VVS1 IF (best)) x length in mm (0--10.74) y width in mm (0--58.9) z depth in mm (0--31.8) depth total depth percentage = z / mean(x y) = 2 * z / (x + y) (43--79) table width of top of diamond relative to widest point (43--95),CSV,,"[clothing, finance]",Other,,,3420,20013,3,"Analyze diamonds by their cut, color, clarity, price, and other attributes",Diamonds,https://www.kaggle.com/shivam2503/diamonds,Thu May 25 2017
,Time Magazine,"[Year, Honor, Name, Country, Birth Year, Death Year, Title, Category, Context]","[numeric, string, string, string, numeric, numeric, string, string, string]","Context TIME's Person of the Year hasn't always secured his or her place in the history books but many honorees remain unforgettable Gandhi Khomeini Kennedy Elizabeth II the Apollo 8 astronauts Anders Borman and Lovell. Each has left an indelible mark on the world. TIME's choices for Person of the Year are often controversial. Editors are asked to choose the person or thing that had the greatest impact on the news for good or ill — guidelines that leave them no choice but to select a newsworthy not necessarily praiseworthy cover subject. Controversial choices have included Adolf Hitler (1938) Joseph Stalin (1939 1942) and Ayatullah Khomeini (1979). TIME's choices for Person of the Year are often politicians and statesmen. Eleven American presidents from FDR to George W. Bush have graced the Person of the Year cover many of them more than once. As commander in chief of one of the world's greatest nations it's hard not to be a newsmaker. Content This dataset includes a record for every Time Magazine cover which has honored an individual or group as ""Men of the Year"" ""Women of the Year"" or (as of 1999) ""Person of the Year"". Acknowledgements The data was scraped from Time Magazine's website. Inspiration Who has been featured on the magazine cover the most times? Did any American presidents not receive the honor for their election victory? How has the selection of Person of the Year changed over time? Have the magazine's choices become more or less controversial?",CSV,,"[news agencies, history]",Other,,,541,4449,0.0107421875,Who has been featured on the magazine cover as Man/Woman of the Year?,"Person of the Year, 1927-Present",https://www.kaggle.com/timemagazine/magazine-covers,Tue Mar 07 2017
,Roam Analytics,[],[],This is the dataset used in the Roam blog post Prescription-based prediction. It is derived from a variety of US open health datasets but the bulk of the data points come from the Medicare Part D dataset and the National Provider Identifier dataset. The prescription vector for each doctor tells a rich story about that doctor's attributes including specialty gender age and region. There are 239930 doctors in the dataset. The file is in JSONL format (one JSON record per line) {     'provider_variables'          {             'brand_name_rx_count' int             'gender' 'M' or 'F'             'generic_rx_count' int             'region' 'South' or 'MidWest' or 'Northeast' or 'West'             'settlement_type' 'non-urban' or 'urban'             'specialty' str             'years_practicing' int         }      'npi' str      'cms_prescription_counts'         {             `drug_name` int              `drug_name` int              ...         } }  The brand/generic classifications behind brand_name_rx_count and generic_rx_count are defined heuristically. For more details see the blog post or go directly to the associated code.,{}JSON,,"[healthcare, artificial intelligence]",CC4,,,1523,16965,156,Predicting doctor attributes from prescription behavior,Prescription-based prediction,https://www.kaggle.com/roamresearch/prescriptionbasedprediction,Mon Oct 17 2016
,Jacob Boysen,"[countrycode, country, currency_unit, year, rgdpe, rgdpo, pop, emp, avh, hc, ccon, cda, cgdpe, cgdpo, ck, ctfp, cwtfp, rgdpna, rconna, rdana, rkna, rtfpna, rwtfpna, labsh, delta, xr, pl_con, pl_da, pl_gdpo, i_cig, i_xm, i_xr, i_outlier, cor_exp, statcap, csh_c, csh_i, csh_g, csh_x, csh_m, csh_r, pl_c, pl_i, pl_g, pl_x, pl_m, pl_k]","[string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Context The Penn World Table has long been a standard data source for those interested in comparing living standards across countries and explaining differences in cross-country growth. The article describing version 5.6 (Summers and Heston 1991) is among the most widely cited papers in economics with well over 1000 citations. This version (9.0) attempts to mitigate many concerns raised since. See this article for additional discussion. Content Database with information on relative levels of income output input and productivity covering 182 countries between 1950 and 2014. See legend user guide and source for additional information. Acknowledgements This file contains the data of PWT 9.0 as available on www.ggdc.net/pwt. Please refer to www.ggdc.net/pwt for extensive documentation of the different concepts and how these data were constructed. When using these data please refer to the following paper available for download at www.ggdc.net/pwt Feenstra Robert C. Robert Inklaar and Marcel P. Timmer (2015) ""The Next Generation of the Penn World Table"""" American Economic Review 105(10) 3150-3182.",Other,,"[finance, money, economics]",CC4,,,138,1186,7,Compare Economic Growth Across Countries,Penn World Table,https://www.kaggle.com/jboysen/penn-world-table,Wed Sep 06 2017
,Rachael Tatman,[],[],Context Pronouncing dictionaries contain a set of words as they appear in written texts as well as their pronunciations. They are often used by researchers who are working on speech technology applications. Content CMUdict (the Carnegie Mellon Pronouncing Dictionary) is a free pronouncing dictionary of English suitable for uses in speech technology. It was created and is maintained by the Speech Group in the School of Computer Science at Carnegie Mellon University. The version available here was current as of August 8 2017. The pronunciations in this dictionary are annotated in ARPABET. More information on APRABET can be found here and here. In this transcription system each speech sound is represented with a unique one or two letter code with a space between each speech sound. Vowels are followed by a 1 if they receive the primary stress in a word and a 0 if they do not. Acknowledgements The Carnegie Mellon Pronouncing Dictionary in its current and previous versions is Copyright (C) 1993-2014 by Carnegie Mellon University.  Use of this dictionary for any research or commercial purpose is completely unrestricted.  If you make use of or redistribute this material please acknowledge its origin in your descriptions. For more information on the terms under which this dataset is distributed see the LICENSE file. Inspiration  Can you create an automatic mapping from English orthography (the way a word is spelled) to a word’s pronunciation? How well does this work on out-of-domain words? Some words in the dictionary have multiple pronunciations. Can you predict which words are more likely to have more than one pronunciation? Does the length of the word have an effect? Its frequency? The sounds in it? ,Other,,"[languages, united states, linguistics]",Other,,,75,1248,3,"The pronunciation of over 134,000 North American English words",CMU Pronouncing Dictionary,https://www.kaggle.com/rtatman/cmu-pronouncing-dictionary,Tue Aug 08 2017
,Armineh Nourbakhsh,"[category, on twitter since, twitter handle, profile url, followers, following, profile location, profile lat/lon, profile description]","[string, string, string, string, dateTime, dateTime, string, numeric, string]",Content Attached is a list of Twitter users who regularly report on natural and man-made disasters violence or crime. The accounts may belong to journalists news media local fire or police departments other local authorities or disaster monitors. Disaster reporting may not be the primary function of the accounts nevertheless they are a prolific source of disaster/accident reporting especially at the location they are associated with. Background Details of the curation of this dataset once published will be added to this entry. Disclaimer The dataset does not include a measure of credibility for the users. The stories reported by them may or may not be true. Further vetting and verification is required to confirm if the stories that they report are credible.,CSV,,"[crime, internet]",CC0,,,227,2285,2,"A list of Twitter users who report on disasters, accidents & crime",Disaster/Accident Sources,https://www.kaggle.com/arminehn/disasteraccident-sources,Sat May 27 2017
,Luiza Fontana,"[Continent, Country, State, WayIn, Year, Month, Count]","[string, string, string, string, numeric, string, numeric]",Context  This dataset contains the number of international tourists arriving in Brazil each month from 1989 to 2015. Content  Continent Country State of arrival Way in (by land sea river or air) Year  Month Count  Acknowledgements  I've downloaded this dataset from dados.gov.br the Brazilian open data portal and tried to tidy it up a bit.,CSV,,[brazil],Other,,,753,6155,33,Who visited Brazil between 1989 and 2015?,Tourists Visiting Brazil,https://www.kaggle.com/zafontana/touristsinbrazil,Sun Nov 27 2016
,ShubhamMaurya,"[name, club, age, position, position_cat, market_value, page_views, fpl_value, fpl_sel, fpl_points, region, nationality, new_foreign, age_cat, club_id, big_club, new_signing]","[string, string, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric]",Context For most football fans May - July represents a lull period due to the lack of club football. What makes up for it is the intense transfer speculation that surrounds all major player transfers today. Their market valuations also lead to a few raised eyebrows lately more than ever.  I was curious to see how good a proxy popularity could be for ability and the predictive power it would have in a model estimating a player's market value.  Content name Name of the player club Club of the player age  Age of the player position  The usual position on the pitch   position_cat     1 for attackers   2 for midfielders   3 for defenders   4 for goalkeepers     market_value  As on transfermrkt.com on July 20th 2017   page_views  Average daily Wikipedia page views from September 1 2016 to May 1 2017   fpl_value  Value in Fantasy Premier League as on July 20th 2017   fpl_sel  % of FPL players who have selected that player in their team   fpl_points  FPL points accumulated over the previous season   region    1 for England   2 for EU   3 for Americas   4 for Rest of World     nationality  new_foreign  Whether a new signing from a different league for 2017/18 (till 20th July)   age_cat  club_id  big_club Whether one of the Top 6 clubs   new_signing Whether a new signing for 2017/18 (till 20th July)   Inspiration To statistically analyse the beautiful game.,CSV,,"[popular culture, association football, sports]",CC0,,,1168,8580,0.033203125,"A unique dataset containing FPL data, popularity and market values","English Premier League Players Dataset, 2017/18",https://www.kaggle.com/mauryashubham/english-premier-league-players-dataset,Thu Aug 03 2017
,Marc Velmer,"[N�mero d'expedient, Codi districte, Nom districte, Codi barri, Nom barri, Codi carrer, Nom carrer, Num postal caption, Descripci� dia setmana, Dia setmana, Descripci� tipus dia, NK Any, Mes de any, Nom mes, Dia de mes, Descripci� torn, Hora de dia, Descripci� causa vianant, Desc. Tipus vehicle implicat, Descripci� sexe, Descripci� tipus persona, Edat, Descripci� victimitzaci�, Coordenada UTM (Y), Coordenada UTM (X)]","[string, numeric, string, numeric, string, numeric, string, string, string, string, string, numeric, numeric, string, numeric, string, numeric, string, string, string, string, numeric, string, numeric, numeric]","Context This dataset is a list of people who have been involved in an accident in the city of Barcelona (Spain) from year 2010 till 2016. This data is managed by the Police in the city of Barcelona and includes several information described below. Content This dataset is composed by 7 files each one containing between 10k-12k lines. Every row contains several information like the type of injury (slightly wounded serious injuries or death). It includes a description of the person (driver passenger or pedestrian) sex age location etc... Important This dataset is uploaded as it is so it's possible that some data in some rows is missing/not correct. Description of each column  Número d'expedient Case File Number Codi districte District code where the accident was. Barcelona is divided in several districts Nom districte Name of the district Codi barri Hood code where the accident was. Every district in Barcelona has several hoods Nom barri Name of the hood Codi carrer Street code (Every street has a code) Nom carrer Name of the street Num postal caption Postal number of the street Descripció dia setmana Day of the week in text (written in Catalan) Dia setmana Shortcode of the previous field (also in Catalan) Descripció tipus dia Description of the type of the day it can be ""labor"" or ""festive"" (also in Catalan) NK Any Number of the year Mes de any Number of the month (1-12) Nom mes Name of the month (in Catalan) Dia de mes Day of the month Descripció torn Type of round of the police. It can be ""Matí"" (Morning) ""Tarda"" (Evening) or ""Nit"" (Night) Hora de dia Hour of the day (0-23) Descripció causa vianant Text in catalan. Describes the accident in case the victim is a pedestrian. If not it says ""No és causa del vianant"" Desc. Tipus vehicle implicat Type of vehicle in the accident. Also in Catalan. Descripció sexe Sex of the victim. ""Home"" means man ""Dona"" means woman. Descripció tipus persona Type of role in the accident. It describes if the victim is the pilot (Conductor ) passenger (Passatger) pedestrian (Vianant) Edat Age of the victim Descripció victimització Type of injury in Catalan (slightly wounded (Ferit lleu) serious injuries (Ferit greu) or death (Mort)) Coordenada UTM (Y) UTM coordinate Y Coordenada UTM (X) UTM coordinate X  As you can see some columns could be removed and we wouldn't loose information. My experience working with these files tells me that some rows have no correct data or no data at all. So be careful! Acknowledgements This data can be found in ""Open Data BCN - Barcelona's City Hall Open Data Service"" which is the owner of the CSV files. Inspiration I have uploaded this information here because I believe that data should be shared with everybody! So do your own ""research"" and share it also! I'm always happy to get some feedback and help each other!",CSV,,[road transport],CC4,,,246,2166,17,List of people who have been involved in an accident in Barcelona (2010 - 2016),Barcelona Accidents,https://www.kaggle.com/marcvelmer/barcelona-accidents,Fri Sep 15 2017
,Philipp Schmidt,"[author_timestamp, commit_hash, commit_utc_offset_hours, filename, n_additions, n_deletions, subject, author_id]","[numeric, string, numeric, string, numeric, numeric, string, numeric]",This dataset contains commits with detailed information about changed files from about 12 years of the linux kernel master branch. It contains about 600.000 (filtered) commits and this breaks down to about 1.4 million file change records. Each row represents a changed file in a specific commit with annotated deletions and additions to that file as well as the filename and the subject of the commit. I also included anonymized information about the author of each changed file aswell as the time of commit and the timezone of the author. The columns in detail  author_timestamp UNIX timestamp of when the commit happened commit_hash SHA-1 hash of the commit commit_utc_offset_hours Extraced UTC offset in hours from commit time filename The filename that was changed in the commit n_additions Number of added lines n_deletions Number of deleted lines subject Subject of commit author_id Anonymized author ID.  I'm sure with this dataset nice visualizations can be created let's see what we can come up with! For everybody interested how the dataset was created I've setup a github repo that contains all the required steps to reproduce it here. If you have any questions feel free to contact me via PM or discussions here.,CSV,,"[computing and society, programming]",CC4,,,89,1903,199,Anonymized git commit log with detailed file information,Linux Kernel Git Revision History,https://www.kaggle.com/philschmidt/linux-kernel-git-revision-history,Sun Mar 12 2017
,Rachael Tatman,[],[],WarOfTheRebellion is an annotated corpus of data from War of The Rebellion (a large set of American Civil War archives). It was built using GeoAnnotate. It consists of two parts a toponym corpus and a document-geolocation corpus. Document geolocation corpus The document geolocation corpus is found in two JSON files.  wotr-docgeo-jan-5-2016-625pm-by-vol.json gives the spans by volume. wotr-docgeo-jan-5-2016-625pm-80-0-20-by-split.json gives the spans by split with an 80-20 training/test split.  In both cases the JSON data for an individual span consists of the following information  The volume number from War of the Rebellion. The span character offsets from corrected OCR'd text. The text of the span in question. The counts of individual words using the tokenization algorithm followed in the paper (FIXME name of paper). They are stored in a string with a space separating word-count pairs and a colon separating the word from the count. The word itself is URL-encoded i.e. a colon is represented as %3A and a percent character as %25. The date (if available) extracted from the text using regular expressions. The full GeoJSON of the points and polygons annotated for the span. The centroid of the points and polygons computed first by taking the centroid of each polygon and then taking the centroid of the resulting set of annotated points and polygon-centroid points. The centroid is in the form of a size-2 array of longitude and latitude (the same as how points are stored in GeoJSON).  Toponym corpus The Toponym corpus otherwise known as WoTR-Topo is given in two different formats. The first format is JSON format files split into train and test. Geographic information for toponyms is given by the geojson standard with annotations done in a stand-off style.  Not everything that has been annotated is guaranteed to be correct. The creators encourage others to correct errors that they find in a branched repository and submit pull requests when corrections are made.  For questions regarding the corpus please contact its creators Ben Wing (ben@benwing.com) and Grant DeLozier (grantdelozier@gmail.com). This data is reproduced here under the MIT license. Please see the file “LICENSE” for more information. The ACL LAW paper describing the corpus and performing benchmark evaluation,{}JSON,,"[united states, geography, history]",Other,,,38,619,48,Texts from the War of The Rebellion American Civil War archives,Geographically Annotated Civil War Corpus,https://www.kaggle.com/rtatman/geographically-annotated-civil-war-corpus,Fri Aug 18 2017
,MahirKukreja,"[datetime_utc,  _conds,  _dewptm,  _fog,  _hail,  _heatindexm,  _hum,  _precipm,  _pressurem,  _rain,  _snow,  _tempm,  _thunder,  _tornado,  _vism,  _wdird,  _wdire,  _wgustm,  _windchillm,  _wspdm]","[string, string, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric]",Context This dataset contains weather data for New Delhi India. Content This data was taken out from wunderground with the help of their easy to use api. It contains various features such as temperature pressure humidity rain precipitationetc. Acknowledgements This data is owned by wunderground and although I ended up using noaa's data for my research i thought that i'd share this data here as I haven't worked on hourly data yet and this might be of huge importance. Inspiration The main target is to develop a prediction model accurate enough for predicting the weather. We can try something like predicting the weather in the next 24 hours like microsoft tried some time back. https//blogs.microsoft.com/next/2015/08/10/hows-the-weather-using-artificial-intelligence-for-better-answers/#sm.018l60051a9neka10is1m5qpi6u5y,CSV,,[climate],Other,,,1289,7697,6,Delhi Weather Data from 1997 to 2016 december,Delhi Weather Data,https://www.kaggle.com/mahirkukreja/delhi-weather-data,Tue Apr 25 2017
,Documenting the American South (DocSouth),[],[],"""The Church in the Southern Black Community"" collects autobiographies biographies church documents sermons histories encyclopedias and other published materials. These texts present a collected history of the way Southern African Americans experienced and transformed Protestant Christianity into the central institution of community life. Coverage begins with white churches' conversion efforts especially in the post-Revolutionary period and depicts the tensions and contradictions between the egalitarian potential of evangelical Christianity and the realities of slavery. It focuses through slave narratives and observations by other African American authors on how the black community adapted evangelical Christianity making it a metaphor for freedom community and personal survival. Context The North American Slave Narratives collection at the University of North Carolina contains 344 items and is the most extensive collection of such documents in the world. The physical collection was digitized and transcribed by students and library employees. This means that the text is far more reliable than uncorrected OCR output which is common in digitized archives. More information about the collection and access to individual page images can be be found here http//docsouth.unc.edu/neh The plain text files have been optimized for use in Voyant and can also be used in text mining projects such as topic modeling sentiment analysis and natural language processing. Please note that the full text contains paratextual elements such as title pages and appendices which will be included in any word counts you perform. You may wish to delete these in order to focus your analysis on just the narratives. The .csv file acts as a table of contents for the collection and includes Title Author Publication Date a url pointing to the digitized version of the text and a unique url pointing to a version of the text in plain text (this is particularly useful for use with Voyant http//voyant-tools.org/).  Copyright Statement and Acknowledgements With the exception of ""Fields's Observation The Slave Narrative of a Nineteenth-Century Virginian"" which has no known rights the texts encoding and metadata available in Open DocSouth are made available for use under the terms of a Creative Commons Attribution License (CC BY 4.0http//creativecommons.org/licenses/by/4.0/). Users are free to copy share adapt and re-publish any of the content in Open DocSouth as long as they credit the University Library at the University of North Carolina at Chapel Hill for making this material available. If you make use of this data considering letting the holder of the original collection know how you are using the data and if you have any suggestions for making it even more useful. Send any feedback to wilsonlibrary@unc.edu. About the DocSouth Data Project Doc South Data provides access to some of the Documenting The American South collections in formats that work well with common text mining and data analysis tools. Documenting the American South is one of the longest running digital publishing initiatives at the University of North Carolina. It was designed to give researchers digital access to some of the library’s unique collections in the form of high quality page scans as well as structured corrected and machine readable text. Doc South Data is an extension of this original goal and has been designed for researchers who want to use emerging technology to look for patterns across entire texts or compare patterns found in multiple texts. We have made it easy to use tools such as Voyant (http//voyant-tools.org/) to conduct simple word counts and frequency visualizations (such as word clouds) or to use other tools to perform more complex processes such as topic modeling named-entity recognition or sentiment analysis.",CSV,,"[united states, history, faith and traditions, linguistics]",Other,,,33,424,38,144 primary texts about the Church in the Southern Black Community,The Church in the Southern Black Community,https://www.kaggle.com/docsouth-data/the-church-in-the-southern-black-community,Tue Aug 15 2017
,Australian Bureau of Statistics,"[electoral_division, state, yes, no, response_clear, response_unclear, nonresponding]","[string, string, numeric, numeric, numeric, numeric, numeric]",Introduction On 9 August 2017 the Treasurer under the Census and Statistics Act 1905 directed the Australian Statistician to collect and publish statistical information from all eligible Australians on the Commonwealth Electoral Roll about their views on whether or not the law should be changed to allow same-sex couples to marry. The voluntary survey asked one question should the law be changed to allow same-sex couples to marry?  Respondents were asked to mark one box – Yes or No – on the survey form. Survey materials were mailed to eligible Australians on the Commonwealth Electoral Roll as at 24 August 2017. A range of strategies were implemented to assist all eligible Australians who wished to complete the survey to do so. A survey response was received from 12727920 (79.5%) eligible Australians. The ABS implemented robust systems and controls for the processing coding and publication of statistical data. Detailed information on the systems used as well as the accuracy and integrity of the data is available in the Survey Process and the Quality and Integrity Statement. The official statistics include a count of responses (Yes No and Response Not Clear) by Federal Electoral Division (FED) State/Territory and National. This also includes a count of eligible Australians who have not participated in the survey. Information from the Commonwealth Electoral Roll has been used to independently produce a participation rate by age and gender for each FED State/Territory and National.  This rate has been published by gender for each of the following age groups 18-19 years 20-24 years 25-29 years 30-34 years 35-39 years 40-44 years 45-49 years 50-54 years 55-59 years 60-64 years 65-69 years 70-74 years 75-79 years 80-84 years and 85+ years. National results Should the law be changed to allow same-sex couples to marry? Of the eligible Australians who expressed a view on this question the majority indicated that the law should be changed to allow same-sex couples to marry with 7817247 (61.6%) responding Yes and 4873987 (38.4%) responding No. Nearly 8 out of 10 eligible Australians (79.5%) expressed their view. All states and territories recorded a majority Yes response. 133 of the 150 Federal Electoral Divisions recorded a majority Yes response and 17 of the 150 Federal Electoral Divisions recorded a majority No response. Data Source All data presented here comes from the official ABS website https//marriagesurvey.abs.gov.au/ This data was cleaned by Myles O'Neill and Richard Dear to make it easier to work with.,CSV,,[government],CC0,,,93,1294,0.287109375,Should the law be changed to allow same-sex couples to marry?,Australian Marriage Law Postal Survey,https://www.kaggle.com/australian-bureau-of-statistics/australian-marriage-law-postal-survey,Wed Nov 15 2017
,4Quant,[],[],Context The main reason for making this dataset is the publication of the paper Learning from Simulated and Unsupervised Images through Adversarial Training and the idea of the SimGAN. The dataset and kernels should make it easier to get started making SimGAN networks and testing them out and comparing them to other approaches like KNN GAN InfoGAN and the like.  Source The synthetic images were generated with the windows version of UnityEyes http//www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/tutorial.html The real images were taken from https//www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild-mpiigaze/ which can be cited like this Appearance-based Gaze Estimation in the Wild X. Zhang Y. Sugano M. Fritz and A. Bulling Proc. of the IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) June p.4511-4520 (2015). Challenges Enhancement One of the challenges (as covered in the paper) is enhancing the simulated images by using the real images. One possible approach is using the SimGAN which is implemented for reference in one of the notebooks. There are a number of other approaches (pix2pix CycleGAN) which could have interesting results. Gaze Detection The synthetic dataset has the gaze information since it was generated by UnityEyes with a predefined look-vector. The overview notebook covers what this vector means and how each component can be interpreted. It would be very useful to have a simple quick network for automatically generating this look vector from an image,Other,,[psychometrics],CC4,,,703,7365,668,Simulated and Real datasets of eyes looking in different directions,Eye Gaze,https://www.kaggle.com/4quant/eye-gaze,Tue May 02 2017
,Rachael Tatman,"[State, Year, Mo, Day, Victim, County, Race, Sex, Mob, Offense, Note, 2nd Name, 3rd Name, Comments, Source]","[string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string]","Context ""Lynching"" historically includes not only Southern lynching but frontier lynching and vigilantism nationwide and many labor-related incidents.  Persons of any race or ethnicity and either gender may have been either perpetrators or victims of lynching. The lynchings in this dataset follow an NAACP definition for including an incident in the inventory of lynchings  There must be evidence that someone was killed; The killing must have occurred illegally; Three or more persons must have taken part in the killing; and The killers must have claimed to be serving justice or tradition.  Content The original data came from the NAACP Lynching Records at Tuskegee Institute Tuskegee Alabama.  Stewart Tolnay and E.M. Beck examined these records for name and event duplications and other errors with funding from a National Science Foundation Grant and made their findings available to Project HAL in 1998. Project HAL is inactive now but it’s original purpose was to build a data set for researchers to use and to add to.  The dataset contains the following information for each of the 2806 reported lynchings  State State where the lynching took place Year Year of the lynching Mo Month Day Day Victim Name of the victim County County where the lynching occurred (keep in mind that county names have changed & boundaries redrawn) Race Race of the victim Sex Sex of the victim Mob Information on the mob Offense Victim’s alleged offense Note Note (if any) 2nd Name Name of the 2nd victim (if any) 3rd Name Name of the 3rd victim (if any) Comments Comments (if any) Source Source of the information (if any)  Acknowledgements This dataset was compiled by Dr. Elizabeth Hines and Dr. Eliza Steelwater. If you use this dataset in your work please include the following citation  Hines E. & Steelwater E. (2006). Project Hal Historical American Lynching Data Collection Project. University of North Carolina http//people.uncw.edu/hinese/HAL/HAL%20Web%20Page.htm You may also like  Bryan Stevenson’s Equal Justice Initiative EJI posts lynching information and stories and is currently quite active  https//www.eji.org/ First Person Narratives of the American South Personal accounts of Southern life between 1860 and 1920  Inspiration  Can you use the county-level data in this dataset to create a map of lynchings in the US? What demographic qualities were most associated with lynching victims? How did patterns of lynching change over time? ",CSV,,"[united states, death, crime, violence]",CC4,,,46,850,0.1806640625,Information on 2806 lynchings in the United States,Historical American Lynching,https://www.kaggle.com/rtatman/historical-american-lynching,Thu Aug 17 2017
,Rachael Tatman,"[, at_conference., day, impressions, engagements, retweets, likes, user.profile.clicks, url.clicks, hashtag.clicks, detail.expands, follows, media.views, media.engagements]","[numeric, boolean, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context I like to livetweet conferences when I attend them for my own reference later on and to help other people who aren't attending the conference keep up-to-date. After the last conference I attended I was curious do livetweets get more or less engagement than other types of tweets? Content This dataset contains information on 314 tweets sent from my personal Twitter account between September 29 2017 and October 26 2017. For each tweet the following information is recorded  at_conference? Whether the tweet was sent during the conference day The day the tweet was sent impressions How many times the tweet was seen engagements How many times the tweet was engaged with (sum of the following columns) retweets How many times the tweet was retweeted likes How many times the tweet was liked user profile clicks How many times someone clicked on my profile from the tweet url clicks How many times someone clicked a URL in the tweet (not all tweets have URL's) hashtag clicks How many times someone clicked on a hashtag in the tweet (not all tweets have hashtags) detail expands How many times someone expanded the tweet      follows How many times someone followed me from the tweet     media views How many times someone viewed media embedded in the tweet (not all tweets have media) media engagements How many times someone clicked on media embedded in the tweet (not all tweets have media)  Inspiration  Do conference tweets get more engagement? Does my account get more engagement during a conference? Do the types of engagement differ depending on whether I'm at a conference? ,CSV,,"[social groups, marketing, twitter]",CC0,,,46,384,0.0146484375,Engagement metrics for 313 tweets (239 from a conference),Do Conference Livetweets Get More Traffic?,https://www.kaggle.com/rtatman/do-tweets-from-conferences-get-more-traffic,Fri Oct 27 2017
,US Geological Survey,"[Date, Time, Latitude, Longitude, Type, Depth, Depth Error, Depth Seismic Stations, Magnitude, Magnitude Type, Magnitude Error, Magnitude Seismic Stations, Azimuthal Gap, Horizontal Distance, Horizontal Error, Root Mean Square, ID, Source, Location Source, Magnitude Source, Status]","[dateTime, dateTime, numeric, numeric, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string]",Context The National Earthquake Information Center (NEIC) determines the location and size of all significant earthquakes that occur worldwide and disseminates this information immediately to national and international agencies scientists critical facilities and the general public. The NEIC compiles and provides to scientists and to the public an extensive seismic database that serves as a foundation for scientific research through the operation of modern digital national and global seismograph networks and cooperative international agreements. The NEIC is the national data center and archive for earthquake information. Content This dataset includes a record of the date time location depth magnitude and source of every earthquake with a reported magnitude 5.5 or higher since 1965. Start a new kernel,CSV,,[earth sciences],CC0,,,4120,24662,2,"Date, time, and location of all earthquakes with magnitude of 5.5 or higher","Significant Earthquakes, 1965-2016",https://www.kaggle.com/usgs/earthquake-database,Fri Jan 27 2017
,Aleksey Bilogur,"[, NAME, FEDERAL AGENCY, NAME OF FORMER CLIENT, LOCATION, SERVICES PROVIDED]","[numeric, string, string, string, string, string]",Context Before joining the federal executive administration new government appointees must submit amongst other things detailed information regarding their finances and previous job history. Such disclosure rules are in place in order to prevent conflicts of interest and are a fundamental part of the work done by government ethics commissions. This dataset is a condensed collection of information recovered from these forms for a selection of top Trump administration appointees. Content This dataset is split into five separate CSV files   names_and_job_titles.csv -- The names and job titles of appointees. Not all appointees are included in this dataset. jobs_before_joining_admin.csv -- Positions held by administration members immediately prior to their joining the federal government. clients_before_joining_admin.csv -- Former appointee clients before joining the federal government. income_sources_and_assets.csv -- All acknowledged and disclosed income sources and assets. The most important file. debts.csv -- Known appointee debt obligations. This record is incomplete. employee_agreements.csv -- Agreements that the appointee made as part of the conditions of their entering employment with the federal government.  Acknowledgements ProPublica The New York Times the Associated Press and others pooled their resources to collect and condense disclosure forms for many prominent members of the Trump administration. These were in turn collected into a public spreadsheet. This dataset is a further condensation of this work. Inspiration What can you discover about the finances and potential conflicts of interest of members of the Trump administration by looking at the raw government record?,CSV,,"[united states, government, money, politics]",CC0,,,42,840,3,Financial data submitted by Trump federal appointees,Trump Administration Financial Disclosures,https://www.kaggle.com/residentmario/trump-financial-disclosures,Sat Sep 30 2017
,Hacker News,"[id, title, url, num_points, num_comments, author, created_at]","[numeric, string, string, numeric, numeric, string, dateTime]",This data set is Hacker News posts from the last 12 months (up to September 26 2016).  It includes the following columns  title title of the post (self explanatory) url the url of the item being linked to num_points the number of upvotes the post received num_comments the number of comments the post received author the name of the account that made the post created_at the date and time the post was made (the time zone is Eastern Time in the US)  One fun project suggestion is a model to predict the number of votes a post will attract. The scraper is written so I can keep this up-to-date and add more historical data. I can also scrape the comments. Just make the request in this dataset's forum. The is a fork of minimaxir's HN scraper (thanks minimaxir) https//github.com/minimaxir/get-all-hacker-news-submissions-comments,CSV,,"[news agencies, internet]",CC0,,,650,6421,45,Hacker News posts from the past 12 months (including # of votes and comments),Hacker News Posts,https://www.kaggle.com/hacker-news/hacker-news-posts,Tue Sep 27 2016
,Rachael Tatman,[],[],Context Pakistan has a rich multilingual and multicultural heritage with about 70 spoken languages deriving from a diverse set of Indo-Aryan Indo-Iranian Sino-Tibetan and Dravidian language families.  More than half of these languages also have a written form employing (predominantly) Perso-Arabic Nastalique and Arabic Naskh writing styles. Gujarati Gurmuki and Tibetan scripts are also used by some communities while some others are in the process of defining their writing systems.  These languages exhibit a diverse set of sounds and underlying linguistic structures which are both linguistically and computationally exciting and challenging.  Most of these languages are not well-studied or well-modeled and present a vast training ground for researchers in linguistics and computer science. This dataset provides resources for two languages spoken in Pakistan Nepali and Urdu. Urdu is the national language of Pakistan while Nepali is mainly spoken in a small immigrant community.  Content This corpus is made of two documents one in Nepali and one in Urdu. Each document is available with and without part of speech tags. They are parallel to the 100000 words of common English source from PENN Treebank corpus available through Linguistic Data Consortium (LDC).  The part of speech tags are those in the Penn Treebank and additional information can be found in the included .csv file. Acknowledgements This dataset was collected and made available by the Center for Language Engineering at the University of  Engineering and Technology UET in Lahore. The work has been supported by the Language Resource Association (GSK) of Japan and International Development Research Center (IDRC) of Canada through PAN Localization project (www.PANL10n.net).  It is distributed here under a CC-BY-NC-SA 3.0 license.  Inspiration  Nepali and Urdu are written in two different scripts (usually Devanagari and Nastaʿlīq  respectively) but are in the same language family. Can you identify congruent characters in each writing system?  Can you automatically identify which words in Urdu and Nepali are cognates (descended from a common root)? Can you use these files to build a part of speech tagger for Nepali? Urdu? ,Other,,"[languages, india, asia, linguistics]",CC4,,,36,554,6,A part of speech tagged corpus for Urdu & Nepali,Urdu-Nepali Parallel Corpus,https://www.kaggle.com/rtatman/urdunepali-parallel-corpus,Thu Oct 05 2017
,Chase Willden,[],[],Context Netflix in the past 5-10 years has captured a large populate of viewers. With more viewers there most likely an increase of show variety. However do people understand the distribution of ratings on Netflix shows? Content Because of the vast amount of time it would take to gather 1000 shows one by one the gathering method took advantage of the Netflix’s suggestion engine. The suggestion engine recommends shows similar to the selected show. As part of this data set I took 4 videos from 4 ratings (totaling 16 unique shows) then pulled 53 suggested shows per video. The ratings include G PG TV-14 TV-MA. I chose not to pull from every rating (e.g. TV-G TV-Y etc.). Acknowledgements The data set and the research article can be found at The Concept Center Inspiration I was watching Netflix with my wife and we asked ourselves why are there so many R and TV-MA rating shows?,CSV,,"[film, internet]",CC0,,,1675,11665,0.0849609375,Understand the rating distributions of Netflix shows,1000 Netflix Shows,https://www.kaggle.com/chasewillden/netflix-shows,Sun Jun 11 2017
,NASA,"[rowid, kepid, kepoi_name, kepler_name, koi_disposition, koi_pdisposition, koi_score, koi_fpflag_nt, koi_fpflag_ss, koi_fpflag_co, koi_fpflag_ec, koi_period, koi_period_err1, koi_period_err2, koi_time0bk, koi_time0bk_err1, koi_time0bk_err2, koi_impact, koi_impact_err1, koi_impact_err2, koi_duration, koi_duration_err1, koi_duration_err2, koi_depth, koi_depth_err1, koi_depth_err2, koi_prad, koi_prad_err1, koi_prad_err2, koi_teq, koi_teq_err1, koi_teq_err2, koi_insol, koi_insol_err1, koi_insol_err2, koi_model_snr, koi_tce_plnt_num, koi_tce_delivname, koi_steff, koi_steff_err1, koi_steff_err2, koi_slogg, koi_slogg_err1, koi_slogg_err2, koi_srad, koi_srad_err1, koi_srad_err2, ra, dec, koi_kepmag]","[numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context The Kepler Space Observatory is a NASA-build satellite that was launched in 2009. The telescope is dedicated to searching for exoplanets in star systems besides our own with the ultimate goal of possibly finding other habitable planets besides our own. The original mission ended in 2013 due to mechanical failures but the telescope has nevertheless been functional since 2014 on a ""K2"" extended mission. Kepler had verified 1284 new exoplanets as of May 2016. As of October 2017 there are over 3000 confirmed exoplanets total (using all detection methods including ground-based ones). The telescope is still active and continues to collect new data on its extended mission. Content This dataset is a cumulative record of all observed Kepler ""objects of interest"" — basically all of the approximately 10000 exoplanet candidates Kepler has taken observations on. This dataset has an extensive data dictionary which can be accessed here. Highlightable columns of note are  kepoi_name A KOI is a target identified by the Kepler Project that displays at least one transit-like sequence within Kepler time-series photometry that appears to be of astrophysical origin and initially consistent with a planetary transit hypothesis kepler_name [These names] are intended to clearly indicate a class of objects that have been confirmed or validated as planets—a step up from the planet candidate designation. koi_disposition The disposition in the literature towards this exoplanet candidate. One of CANDIDATE FALSE POSITIVE NOT DISPOSITIONED or CONFIRMED. koi_pdisposition The disposition Kepler data analysis has towards this exoplanet candidate. One of FALSE POSITIVE NOT DISPOSITIONED and CANDIDATE. koi_score A value between 0 and 1 that indicates the confidence in the KOI disposition. For CANDIDATEs a higher value indicates more confidence in its disposition while for FALSE POSITIVEs a higher value indicates less confidence in that disposition.  Acknowledgements This dataset was published as-is by NASA. You can access the original table here. More data from the Kepler mission is available from the same source here. Inspiration  How often are exoplanets confirmed in the existing literature disconfirmed by measurements from Kepler? How about the other way round? What general characteristics about exoplanets (that we can find) can you derive from this dataset? What exoplanets get assigned names in the literature? What is the distribution of confidence scores?  See also the Kepler Labeled Time Series and Open Exoplanets Catalogue datasets.",CSV,,"[astronomy, space]",CC0,,,229,2847,4,10000 exoplanet candidates examined by the Kepler Space Observatory,Kepler Exoplanet Search Results,https://www.kaggle.com/nasa/kepler-exoplanet-search-results,Tue Oct 10 2017
,Chris Crawford,"[Crop, ScientificName, Symbol, NuContAvailable, PlantPartHarvested, CropCategory, YieldUnit, AvYieldUnitWeight(lb), AvMoisture%, AvN%(dry), AvP%(dry), AvK%(dry), YieldUnitWeight(lb)_set, YieldUnitWeight(lb)_Bau, YieldUnitWeight(lb)_Joh, YieldUnitWeight(lb)_Roberts, YieldUnitWeight(lb)_WEEP, YieldUnitWeight(lb)_Men, YieldUnitWeight(lb)_Guy, YieldUnitWeight(lb)_Mc, YieldUnitWeight(lb)_Mah, YieldUnitWeight(lb)_Sha, YieldUnitWeight(lb)_Sch, YieldUnitWeight(lb)_Atu, YieldUnitWeight(lb)_Zim, YieldUnitWeight(lb)_Scu, YieldUnitWeight(lb)_John, YieldUnitWeight(lb)_Arc, DryMatter%_M-FF, DryMatter%_NAS, DryMatter%_F&L, DryMatter%_F&N, DryMatter%_Alb, DryMatter%_Est1, DryMatter%_Est2, DryMatter%_Est3, DryMatter%_Est4, DryMatter%_Est5, DryMatter%_Est6, DryMatter%_M&R, DryMatter%_M&L, DryMatter%_Sun, DryMatter%_Gro, DryMatter%_AgH8-9, DryMatter%_AgH8-12, DryMatter%_B788, AvDryMatter%, N%(dry)_NAS, N%(dry)_F&L, N%(dry)_F&N, N%(dry)_Swa, N%(dry)_Chapko, N%(dry)_Hill, N%(dry)_Bru, N%(dry)_AgH8-9, N%(dry)_AgH8-12, N%(dry)_B788, N%(dry)_M&L, N%(dry)_M-FF, N%(dry)_M&R, N%(dry)_Foster, N%(dry)_Rob1, N%(dry)_Rob2, N%(dry)_Coa, N%(dry)_And, N%(dry)_Gol1, N%(dry)_Gol2, N%(dry)_Wol, N%(dry)_Pete, N%(dry)_Col, N%(dry)_Alb, N%(dry)_Arc, N%(dry)_Bis, N%(dry)_Gar, N%(dry)_Heg, N%(dry)_Flo, N%(dry)_Feil, N%(dry)_Bre, N%(dry)_Burns, N%(dry)_Coc, P%(dry)_M-FF, P%(dry)_NAS, P%(dry)_F&L, P%(dry)_F&N, P%(dry)_AgH8-9, P%(dry)_AgH8-12, P%(dry)_B788, P%(dry)_M&L, P%(dry)_L&V, P%(dry)_Foster, P%(dry)_Rob1, P%(dry)_Rob2, P%(dry)_Coa, P%(dry)_And, P%(dry)_Gol1, P%(dry)_Gol2, P%(dry)_Sims, P%(dry)_Wol, P%(dry)_Pete, P%(dry)_Col]","[string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Context The PLANTS Database provides standardized information about the vascular plants mosses liverworts hornworts and lichens of the U.S. and its territories. It includes names plant symbols checklists distributional data species abstracts characteristics images plant links references and crop information and automated tools. This particular dataset is the Crop Nutrient Database. Content These are the fields included in the dataset. I'll be honest I have no idea what some of them mean  Crop ScientificName Symbol NuContAvailable PlantPartHarvested CropCategory YieldUnit AvYieldUnitWeight(lb) AvMoisture% AvN%(dry) AvP%(dry) AvK%(dry) YieldUnitWeight(lb)_set YieldUnitWeight(lb)_Bau YieldUnitWeight(lb)_Joh YieldUnitWeight(lb)_Roberts YieldUnitWeight(lb)_WEEP YieldUnitWeight(lb)_Men YieldUnitWeight(lb)_Guy YieldUnitWeight(lb)_Mc YieldUnitWeight(lb)_Mah YieldUnitWeight(lb)_Sha YieldUnitWeight(lb)_Sch YieldUnitWeight(lb)_Atu YieldUnitWeight(lb)_Zim YieldUnitWeight(lb)_Scu YieldUnitWeight(lb)_John YieldUnitWeight(lb)_Arc DryMatter%_M-FF DryMatter%_NAS DryMatter%_F&L DryMatter%_F&N DryMatter%_Alb DryMatter%_Est1 DryMatter%_Est2 DryMatter%_Est3 DryMatter%_Est4 DryMatter%_Est5 DryMatter%_Est6 DryMatter%_M&R DryMatter%_M&L DryMatter%_Sun DryMatter%_Gro DryMatter%_AgH8-9 DryMatter%_AgH8-12 DryMatter%_B788 AvDryMatter% N%(dry)_NAS N%(dry)_F&L N%(dry)_F&N N%(dry)_Swa N%(dry)_Chapko N%(dry)_Hill N%(dry)_Bru N%(dry)_AgH8-9 N%(dry)_AgH8-12 N%(dry)_B788 N%(dry)_M&L N%(dry)_M-FF N%(dry)_M&R N%(dry)_Foster N%(dry)_Rob1 N%(dry)_Rob2 N%(dry)_Coa N%(dry)_And N%(dry)_Gol1 N%(dry)_Gol2 N%(dry)_Wol N%(dry)_Pete N%(dry)_Col N%(dry)_Alb N%(dry)_Arc N%(dry)_Bis N%(dry)_Gar N%(dry)_Heg N%(dry)_Flo N%(dry)_Feil N%(dry)_Bre N%(dry)_Burns N%(dry)_Coc P%(dry)_M-FF P%(dry)_NAS P%(dry)_F&L P%(dry)_F&N P%(dry)_AgH8-9 P%(dry)_AgH8-12 P%(dry)_B788 P%(dry)_M&L P%(dry)_L&V P%(dry)_Foster P%(dry)_Rob1 P%(dry)_Rob2 P%(dry)_Coa P%(dry)_And P%(dry)_Gol1 P%(dry)_Gol2 P%(dry)_Sims P%(dry)_Wol P%(dry)_Pete P%(dry)_Col P%(dry)_Alb P%(dry)_Arc P%(dry)_Swa P%(dry)_Rei K%(dry)_M-FF K%(dry)_NAS K%(dry)_F&L K%(dry)_F&N K%(dry)_AgH8-9 K%(dry)_AgH8-12 K%(dry)_B788 K%(dry)_Foster K%(dry)_Rob1 K%(dry)_Rob2 K%(dry)_Coa K%(dry)_And K%(dry)_Gol1 K%(dry)_Gol2 K%(dry)_Sims K%(dry)_Wol K%(dry)_Pete K%(dry)_Col K%(dry)_Alb K%(dry)_Arc K%(dry)_Swa K%(dry)_Rei Moisture%_M&R Moisture%_M&L Moisture%_Sun Moisture%_Gro gWater/100g_AgH8-9 gWater/100g_AgH8-12 gWater/100g_B788 Protein%(dry)_NAS Protein%(dry)_F&L Protein%(dry)_F&N Protein%(dry)_Swa Protein%(dry)_Chapko Protein%(dry)_Hill Protein%(dry)_Bru Protein%(dry)_Bis Protein%(dry)_Gar Protein%(dry)_Heg Protein%(dry)_Flo Protein%(dry)_Feil Protein%(dry)_Bre Protein%(dry)_Burns gProtein/100g(wet)_AgH8-9 gProtein/100g(wet)_AgH8-12 gProtein/100g(wet)_B788 Protein%(wet)_M&L N%(wet)_M-FF P%(wet)_M-FF gP/100g(wet)_AgH8-9 gP/100g(wet)_AgH8-12 gP/100g(wet)_B788 P%(wet)_M&L K%(wet)_M-FF gK/100g(wet)_AgH8-9 gK/100g(wet)_AgH8-12 gK/100g(wet)_B788 ,CSV,,"[food and drink, science and culture, united states]",CC0,,,162,1796,0.2744140625,USDA data about crop nutrients in the U.S.,Crop Nutrient Database,https://www.kaggle.com/crawford/crop-nutrient-database,Sat Aug 19 2017
,Federal Deposit Insurance Corporation,"[Financial Institution Number, Institution Name, Institution Type, Charter Type, Headquarters, Failure Date, Insurance Fund, Certificate Number, Transaction Type, Total Deposits, Total Assets, Estimated Loss (2015)]","[string, string, string, string, string, dateTime, string, string, string, numeric, numeric, string]",Content This report lists each failure of a commercial bank savings association and savings bank since the establishment of the FDIC in 1933. Each record includes the institution name and FIN number institution and charter types location of headquarters (city and state) effective date insurance fund and certificate number failure transaction type total deposits and total assets last reported prior to failure (in thousands of dollars) and the estimated cost of resolution. Data on estimated losses are not available for FDIC insured failures prior to 1986 or for FSLIC insured failures from 1934-88. Acknowledgements The bank failure report was downloaded from the FDIC website. Inspiration What type of banking institution is the most likely to fail? How have bank failure rates changed over time? What commercial bank failure cost the federal government the most to resolve?,CSV,,"[history, finance]",CC0,,,891,7055,0.38671875,Every bank failure in the United States since the Great Depression,"Commercial Bank Failures, 1934-Present",https://www.kaggle.com/fdic/bank-failures,Thu Mar 09 2017
,Jacob Boysen,[],[],"Context In the WCS investigation an average of 24 native speakers of each of 110 unwritten languages were asked   to name each of 330 Munsell chips shown in a constant random order exposed to a palette of these chips and asked to to pick out the best example(s) (""foci"") of the major terms elicited in the naming task.   See the original WCS Instructions to Fieldworkers for further information on the data elicitation method. The files in this archive display the results of that investigation. Content Demographics on speakers color chip coordinates terms used and the WCS and the Munsell coordinates as well as the CIEL*a*b* coordinates. Further details and resources at source. Acknowledgements This material is based upon work supported by the National Science Foundation under Grant No. 0130420.  Richard Cook1 Paul Kay2 and Terry Regier3    University of California at Berkeley  International Computer Science Institute Berkeley CA  University of Chicago    In any published work based on these data please cite these archives. URL http//www.icsi.berkeley.edu/wcs/data.html",Other,,[linguistics],Other,,,129,1605,12,Speakers of Unwritten Languages Name Color Chips,World Color Survey,https://www.kaggle.com/jboysen/color-survey,Wed Aug 23 2017
,Mitchell J,[],[],General Info This is a collection of over 50000 ranked EUW games from the game League of Legends as well as json files containing a way to convert between champion and summoner spell IDs and their names. For each game there are fields for  Game ID Creation Time (in Epoch format) Game Duration (in seconds) Season ID Winner (1 = team1   2 = team2) First Baron dragon tower blood inhibitor and Rift Herald (1 = team1   2 = team2 0 = none) Champions and summoner spells for each team (Stored as Riot's champion and summoner spell IDs) The number of tower inhibitor Baron dragon and Rift Herald kills each team has The 5 bans of each team (Again champion IDs are used)  This dataset was collected using the Riot Games API which makes it easy to lookup and collect information on a users ranked history and collect their games. However finding a list of usernames is the hard part in this case I am using a list of usernames scraped from 3rd party LoL sites. Possible Uses There is a vast amount of data in just a single LoL game. This dataset takes the most relevant information and makes it available easily for use in things such as attempting to predict the outcome of a LoL game analysing which in-game events are most likely to lead to victory understanding how big of an effect bans of a specific champion have and more.,{}JSON,,"[video games, internet]",CC0,,,1044,9021,9,"Details from over 50,000 ranked games of LoL",(LoL) League of Legends Ranked Games,https://www.kaggle.com/datasnaek/league-of-legends,Sat Sep 23 2017
,GMAdevs,"[tourney_id, tourney_name, surface, draw_size, tourney_level, tourney_date, match_num, winner_id, winner_seed, winner_entry, winner_name, winner_hand, winner_ht, winner_ioc, winner_age, winner_rank, winner_rank_points, loser_id, loser_seed, loser_entry, loser_name, loser_hand, loser_ht, loser_ioc, loser_age, loser_rank, loser_rank_points, score, best_of, round, minutes, w_ace, w_df, w_svpt, w_1stIn, w_1stWon, w_2ndWon, w_SvGms, w_bpSaved, w_bpFaced, l_ace, l_df, l_svpt, l_1stIn, l_1stWon, l_2ndWon, l_SvGms, l_bpSaved, l_bpFaced]","[string, string, string, numeric, string, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, numeric, numeric, dateTime, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context A dataset of ATP matches including individual statistics. Content In these datasets there are individual csv files for ATP tournament from 2000 to 2017. The numbers in the last columns are absolute values using them you can calculate percentages. Dataset legend All the match statistics are in absolute number format you can convert to percentages using the total point number ace = absolute number of aces df = number of double faults svpt = total serve points 1stin = 1st serve in 1st won = points won on 1st serve 2ndwon = points won on 2nd serve SvGms = serve games bpSaved = break point saved bpFaced = break point faced  Acknowledgement Thanks to Jeff Sackmann for the excellent work. Be sure to visit his github profile https//github.com/JeffSackmann/tennis_atp Inspiration This dataset would be likely used to develop predictive modeling of tennis matches and to do statistic research. I'm planning to add historical odds and injuries data as soon as I have the time to get them.,CSV,,"[tennis, sports]",CC4,,,1253,5650,11,ATP tournament results from 2000 to 2017,Association of Tennis Professionals Matches,https://www.kaggle.com/gmadevs/atp-matches-dataset,Tue Feb 07 2017
,Rachael Tatman,"[word, count]","[string, numeric]",Context How frequently a word occurs in a language is an important piece of information for natural language processing and linguists. In natural language processing very frequent words tend to be less informative than less frequent one and are often removed during preprocessing. Human language users are also sensitive to  word frequency. How often a word is used affects language processing in humans. For example very frequent words are read and understood more quickly and can be understood more easily in background noise. Content This dataset contains the counts of the 333333 most commonly-used single words on the English language web as derived from the Google Web Trillion Word Corpus. Acknowledgements Data files were derived from the Google Web Trillion Word Corpus (as described by Thorsten Brants and Alex Franz and distributed by the Linguistic Data Consortium) by Peter Norvig. You can find more information on these files and the code used to generate them here. The code used to generate this dataset is distributed under the MIT License.  Inspiration  Can you tag the part of speech of these words? Which parts of speech are most frequent? Is this similar to other languages like Japanese? What differences are there between the very frequent words in this dataset and the the frequent words in other corpora such as the Brown Corpus or the TIMIT corpus? What might these differences tell us about how language is used?  ,CSV,,"[languages, linguistics, internet]",Other,,,280,2270,5,⅓ Million Most Frequent English Words on the Web,English Word Frequency,https://www.kaggle.com/rtatman/english-word-frequency,Wed Sep 06 2017
,Adam Mathias Bittlingmayer,[],[],From Peter Norvig's classic How to Write a Spelling Corrector  One week in 2007 two friends (Dean and Bill) independently told me they were amazed at Google's spelling correction. Type in a search like [speling] and Google instantly comes back with Showing results for spelling. I thought Dean and Bill being highly accomplished engineers and mathematicians would have good intuitions about how this process works. But they didn't and come to think of it why should they know about something so far outside their specialty? I figured they and others could benefit from an explanation. The full details of an industrial-strength spell corrector are quite complex (you can read a little about it here or here). But I figured that in the course of a transcontinental plane ride I could write and explain a toy spelling corrector that achieves 80 or 90% accuracy at a processing speed of at least 10 words per second in about half a page of code.  A Kernel has been added with Peter's basic spell.py and evaluation code to set a baseline.  Minimal modifications were made so that it runs on this environment. Data files big.txt is required by the code.  That's how it learns the probabilities of English words. You can prepend more text data to it but be sure to leave in the little Python snippet at the end. Testing files The other files are for testing the accuracy.  The baseline code should get 75% of 270 correct on spell-testset1.txt and 68% of 400 correct on spell-testset2.txt. I've also added some other files for more extensive testing.  The example Kernel runs all of them but birkbeck.txt by default.  Here's the output Testing spell-testset1.txt 75% of 270 correct (6% unknown) at 32 words per second  Testing spell-testset2.txt 68% of 400 correct (11% unknown) at 28 words per second  Testing wikipedia.txt 61% of 2455 correct (24% unknown) at 21 words per second  Testing aspell.txt 43% of 531 correct (23% unknown) at 15 words per second   The larger datasets take a few minutes to run.  birkbeck.txt takes more than a few minutes. You can try adding other datasets or splitting these ones in meaningful ways - for example a dataset of only words of 5 characters or less or 10 characters or more or without uppercase - to understand the effect of changes you make on different types of words. Languages The data and testing files include English only for now.  In principle it is easily generalisable to other languages.,Other,,"[languages, linguistics]",Other,,,248,3060,7,Datasets from Peter Norvig's classic spelling corrector in half a page of Python,Spelling Corrector,https://www.kaggle.com/bittlingmayer/spelling,Thu May 25 2017
,UCI Machine Learning,"[Elevation, Aspect, Slope, Horizontal_Distance_To_Hydrology, Vertical_Distance_To_Hydrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area1, Wilderness_Area2, Wilderness_Area3, Wilderness_Area4, Soil_Type1, Soil_Type2, Soil_Type3, Soil_Type4, Soil_Type5, Soil_Type6, Soil_Type7, Soil_Type8, Soil_Type9, Soil_Type10, Soil_Type11, Soil_Type12, Soil_Type13, Soil_Type14, Soil_Type15, Soil_Type16, Soil_Type17, Soil_Type18, Soil_Type19, Soil_Type20, Soil_Type21, Soil_Type22, Soil_Type23, Soil_Type24, Soil_Type25, Soil_Type26, Soil_Type27, Soil_Type28, Soil_Type29, Soil_Type30, Soil_Type31, Soil_Type32, Soil_Type33, Soil_Type34, Soil_Type35, Soil_Type36, Soil_Type37, Soil_Type38, Soil_Type39, Soil_Type40, Cover_Type]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All observations are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest. There are over half a million measurements total! Content This dataset includes information on tree type shadow coverage distance to nearby landmarks (roads etcetera) soil type and local topography. Acknowledgement This dataset is part of the UCI Machine Learning Repository and the original source can be found here. The original database owners are Jock A. Blackard Dr. Denis J. Dean and Dr. Charles W. Anderson of the Remote Sensing and GIS Program at Colorado State University.  Inspiration  Can you build a model that predicts what types of trees grow in an area based on the surrounding characteristics? A past Kaggle competition project on this topic can be found here. What kinds of trees are most common in the Roosevelt National Forest? Which tree types can grow in more diverse environments? Are there certain tree types that are sensitive to an environmental factor such as elevation or soil type? ,CSV,,"[botany, ecology, plants]",Other,,,1307,13419,72,Tree types found in the Roosevelt National Forest in Colorado,Forest Cover Type Dataset,https://www.kaggle.com/uciml/forest-cover-type-dataset,Thu Nov 03 2016
,hakmesyo,"[X, Y, Z, Mixed, ClassLabel]","[numeric, numeric, numeric, numeric, numeric]","Context This dataset is partly associated to the ""Hand Tremor Based Biometric Recognition Using Leap Motion Device"" paper (doi 10.1109/ACCESS.2017.2764471 ). Objective is to investigate whether hand jitter can be treated as a new behavioral biometric recognition trait in the filed od security so that imitating and/or reproducing artificially can be avoided. Content Dataset contains five subjects. 1024 samples each subject's spatiotemporal hand tremor signals as a time series data were acquired via leap motion device. Features are X Y Z and Mixed (Average) channels. Channel represents displacement value of adjacent frames (difference between current and previous positions) and finally the last item is class label having value from 1 to 5.  Acknowledgements I would like to thanks to our volunteer donor who provides us valuable hand tremor data. Inspiration Please read the ""Hand Tremor Based Biometric Recognition Using Leap Motion Device"" paper for more details and feature extraction methods. If you have any questions related to the preprocessing and/or processing the dataset please do not hesitate to contact with me via e-mail hakmesyo@gmail.com . It should be noted that data acquisition software was implemented in Java (Netbeans) and I utilized Processing Open Cezeri Library and Weka tools alongside. ",CSV,,"[neuroscience, artificial intelligence, computer security]",Other,,,81,1537,0.23828125,New biometric recognition trait based on hand tremor via leap motion device,Hand Tremor Dataset for Biometric Recognition ,https://www.kaggle.com/hakmesyo/hand-tremor-dataset-for-biometric-recognition,Mon Nov 06 2017
,Federal Emergency Management Agency,"[Declaration Number, Declaration Type, Declaration Date, State, County, Disaster Type, Disaster Title, Start Date, End Date, Close Date, Individual Assistance Program, Individuals & Households Program, Public Assistance Program, Hazard Mitigation Program]","[string, string, dateTime, string, string, string, string, dateTime, dateTime, dateTime, string, string, string, string]",Context The president can declare an emergency for any occasion or instance when the President determines federal assistance is needed.  Emergency declarations supplement State and local or Indian tribal government efforts in providing emergency services such as the protection of lives property public health and safety or to lessen or avert the threat of a catastrophe in any part of the United States.  The total amount of assistance provided for in a single emergency may not exceed $5 million. The president can declare a major disaster for any natural event including any hurricane tornado storm high water wind-driven water tidal wave tsunami earthquake volcanic eruption landslide mudslide snowstorm or drought or regardless of cause fire flood or explosion that the President determines has caused damage of such severity that it is beyond the combined capabilities of state and local governments to respond.  A major disaster declaration provides a wide range of federal assistance programs for individuals and public infrastructure including funds for both emergency and permanent work. Content This dataset includes a record for every federal emergency or disaster declared by the President of the United States since 1953. Acknowledgements The disaster database was published by the Federal Emergency Management Agency with data from the National Emergency Management Information System. Inspiration What type of disaster is the most commonly declared by FEMA? Which disasters or emergencies have lasted the longest? What disaster was declared in the most counties or states? Has the number of disasters declared by FEMA risen or fallen over time?,CSV,,"[ecology, history]",CC0,,,311,2311,6,Has the number of emergencies declared by the president risen over time?,"Federal Emergencies and Disasters, 1953-Present",https://www.kaggle.com/fema/federal-disasters,Sun Feb 19 2017
,NORC.org,[],[],​​The GSS gathers data on contemporary American society in order to monitor and explain trends and constants in attitudes behaviors and attributes.  Hundreds of trends have been tracked since 1972. In addition since the GSS adopted questions from earlier surveys trends can be followed for up to 70 years. The GSS contains a standard core of demographic behavioral and attitudinal questions plus topics of special interest. Among the topics covered are civil liberties crime and violence intergroup tolerance morality national spending priorities psychological well-being social mobility and stress and traumatic events. Altogether the GSS is the single best source for sociological and attitudinal trend data covering the United States. It allows researchers to examine the structure and functioning of society in general as well as the role played by relevant subgroups and to compare the United States to other nations. (Source) This dataset is a csv version of the Cumulative Data File a cross-sectional sample of the GSS from 1972-current.,CSV,,[political science],CC0,,,822,5653,2048,"Longitudinal study of popular beliefs, attitudes, morality & behaviors in the US",The General Social Survey (GSS),https://www.kaggle.com/norc/general-social-survey,Fri Nov 10 2017
,MACHINE LEARNING DATASETS,"[ACTIVITY, TIME, SL, EEG, BP, HR, CIRCLUATION]","[numeric, numeric, numeric, numeric, numeric, numeric, dateTime]",Falls among the elderly is an important health issue. Fall detection and movement tracking are therefore instrumental in addressing this issue. This paper responds to the challenge of classifying different movements as a part of a system designed to fulfill the need for a wearable device to collect data for fall and near-fall analysis. Four different fall trajectories (forward backward left and right) three normal activities (standing walking and lying down) and near-fall situations are identified and detected.  Falls are a serious public health problem and possibly life threatening for people in fall risk groups. We develop an automated fall detection system with wearable motion sensor units fitted to the subjects’ body at six different positions. Each unit comprises three tri-axial devices (accelerometer gyroscope and magnetometer/compass). Fourteen volunteers perform a standardized set of movements including 20 voluntary falls and 16 activities of daily living (ADLs) resulting in a large dataset with 2520 trials. To reduce the computational complexity of training and testing the classifiers we focus on the raw data for each sensor in a 4 s time window around the point of peak total acceleration of the waist sensor and then perform feature extraction and reduction.  We successfully distinguish falls from ADLs using six machine learning techniques (classifiers) the k-nearest neighbor (k-NN) classifier least squares method (LSM) support vector machines (SVM) Bayesian decision making (BDM) dynamic time warping (DTW) and artificial neural networks (ANNs). We compare the performance and the computational complexity of the classifiers and achieve the best results with the k-NN classifier and LSM with sensitivity specificity and accuracy all above 95%. These classifiers also have acceptable computational requirements for training and testing. Our approach would be applicable in real-world scenarios where data records of indeterminate length containing multiple activities in sequence are recorded. If you are using this dataset don't forget to cite  Özdemir Ahmet Turan and Billur Barshan. “Detecting Falls with Wearable Sensors Using Machine Learning Techniques.” Sensors (Basel Switzerland) 14.6 (2014) 10691–10708. PMC. Web. 23 Apr. 2017.,CSV,,[health],Other,,,844,8550,0.5966796875,Activity of elderly patients along with their medical information,Fall Detection Data from China,https://www.kaggle.com/pitasr/falldata,Thu Apr 20 2017
,Airbnb,"[listing_id, date, available, price]","[numeric, dateTime, string, numeric]",Context Since 2008 guests and hosts have used Airbnb to travel in a more unique personalized way. As part of the Airbnb Inside initiative this dataset describes the listing activity of homestays in Seattle WA.  Content The following Airbnb activity is included in this Seattle dataset * Listings including full descriptions and average review score * Reviews including unique id for each reviewer and detailed comments * Calendar including listing id and the price and availability for that day Inspiration  Can you describe the vibe of each Seattle neighborhood using listing descriptions? What are the busiest times of the year to visit Seattle? By how much do prices spike? Is there a general upward trend of both new Airbnb listings and total Airbnb visitors to Seattle?  For more ideas visualizations of all Seattle datasets can be found here. Acknowledgement This dataset is part of Airbnb Inside and the original source can be found here.,CSV,,"[united states, home, hotels]",CC0,,,1506,11693,86,"A sneak peek into the Airbnb activity in Seattle, WA, USA",Seattle Airbnb Open Data,https://www.kaggle.com/airbnb/seattle,Thu Nov 17 2016
,Pranay Aryal,"[DRG Definition, Provider Id, Provider Name, Provider Street Address, Provider City, Provider State, Provider Zip Code, Hospital Referral Region Description,  Total Discharges ,  Average Covered Charges ,  Average Total Payments , Average Medicare Payments]","[string, numeric, string, string, string, string, numeric, string, numeric, string, string, string]",Variation of hospital charges in the various hospitals in the US for the top 100 diagnoses. The dataset is owned by the US government. It is freely available on data.gov The dataset keeps getting updated periodically  here This dataset will show you how price for the same diagnosis and the same treatment and in the same city can vary differently across different providers. It might help you or your loved one  find a better hospital for your treatment. You can also analyze to detect fraud among providers.,CSV,,"[healthcare, finance]",Other,,,2890,22965,26,How inpatient hospital charges can differ among different providers in the US,Hospital Charges for Inpatients,https://www.kaggle.com/speedoheck/inpatient-hospital-charges,Mon Sep 19 2016
,DataSF,[],[],Context This data set includes the Office of the Assessor-Recorder’s secured property tax roll spanning from 2007 to 2015 (~1.6M). It includes all legally disclosable information including location of property value of property the unique property identifier and specific property characteristics. The data is used to accurately and fairly appraise all taxable property in the City and County of San Francisco. The Office of the Assessor-Recorder makes no representation or warranty that the information provided is accurate and/or has no errors or omissions.  Potential question(s) to get started with!  Can the effects of Prop 13 been seen in the historic property tax rolls?  Fields There are 48 fields in this dataset.   A full data dictionary can be found here. We have included the following commonly used geographic shapefiles  Analysis Neighborhoods Supervisor Districts as of April 2012  Acknowledgements Data provided by the San Francisco Office of the Assessor-Recorder via the San Francisco Open Data Portal at https//data.sfgov.org/d/wv5m-vpq2 PDDL 1.0 ODC Public Domain Dedication and Licence (PDDL)  Photo from Flickr via Rebecca Morgan (CC BY-NC-SA 2.0),CSV,,"[cities, finance, politics]",Other,,,78,1452,421,SF secured property tax roll spanning from 2007 to 2015,SF Historic Secured Property Tax Rolls,https://www.kaggle.com/datasf/sf-historic-secured-property-tax-rolls,Sat Jan 07 2017
,City of New York,"[DV_DEVICE_NUMBER, Device Status, DV_DEVICE_STATUS_DESCRIPTION, BIN, TAX_BLOCK, TAX_LOT, HOUSE_NUMBER, STREET_NAME, ZIP_CODE, Borough, Device Type, DV_LASTPER_INSP_DATE, DV_LASTPER_INSP_DISP, DV_APPROVAL_DATE, DV_MANUFACTURER, DV_TRAVEL_DISTANCE, DV_SPEED_FPM, DV_CAPACITY_LBS, DV_CAR_BUFFER_TYPE, DV_GOVERNOR_TYPE, DV_MACHINE_TYPE, DV_SAFETY_TYPE, DV_MODE_OPERATION, DV_STATUS_DATE, DV_FLOOR_FROM, DV_FLOOR_TO, , LATITUDE, LONGITUDE]","[string, string, string, numeric, numeric, numeric, numeric, string, numeric, string, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, string, string, numeric, numeric]",Context This is a dataset of every registered elevator in New York City. It was generated by the NYC Department of Buildings in September 2015 in response to a journalistic Freedom of Information Law request and contains information on elevator type status and function provided in the city's BISweb interface. Content The addresses locations and statuses of elevators in New York City. Acknowledgements This data is republished as-is from its public source on GitHub. That data in turn came from the NYC Department of Buildings. Inspiration Where are the elevators how many of them are functional and what floors do they go to?,CSV,,[cities],CC0,,,100,1475,13,All registered elevators in New York City,Elevators in New York City,https://www.kaggle.com/new-york-city/nyc-elevators,Fri Sep 15 2017
,City of New York,"[, Corporate Name, Subordinate Unit, Title, Remainder Of Title, Remainder OF Title Page, Date Of Publication, Geographic Name, General Subdivision]","[numeric, string, string, string, string, string, numeric, string, string]","Context This dataset compiles the titles publication dates and other data about all reports published in the official capacities of New York City government agency work are listed in the City Hall Library catalog. The catalog functions like a city-level equivalent of the national Library of Congress and goes back very far --- at least to the 1800s. Content Columns are provided for the report name and report sub-header the year the report was issued the name of the publisher compiling the report and some other smaller fields. Acknowledgements This data was originally published in a pound (""#"") delimited dataset on the New York City Open Data Portal. It has been restructured as a CSV and lightly cleaned up for formatting prior to being uploaded to Kaggle. Inspiration  Can you separate reporting publications by the City of New York into topics? Who are the most common report issuers and what causes do they represent? What are some common elements to report titles? ",CSV,,"[cities, government]",CC0,,,30,509,5,All official reports published by the City of New York,NYC City Hall Library Catalog,https://www.kaggle.com/new-york-city/nyc-city-hall-library-catalog,Sat Sep 09 2017
,Rachael Tatman,[],[],Context Sentiment analysis is the task of computationally labeling whether the content of text is positive or negative. One common approach to this is to compile lists of words which have a positive connotation (like “wonderful” “lovely” and “best”) and a negative connotation (like “bad” “horrible” or “awful”). Then you count how many positive and how many negative   Content This dataset contains three lists of Thai words  swear words (94 words) positive words (512 words) negative words (1218 words)  Each list a .txt file with one word per line. The character encoding is UTF-8. Acknowledgements This dataset was compiled by Wannaphong Phatthiyaphaibun and is reproduced here under a CC-BY-SA 4.0 license. (You may also be interested in his translation of Python 3 documentation into Thai on this blog.) Inspiration  Can you analyze the sentiment in this corpus of Thai? Is there a difference in sentiment between the WIkipedia and Government parts of the corpus? ,Other,,"[languages, asia, linguistics]",CC4,,,89,1445,0.0341796875,"Positive, negative and swear words in Thai",Thai Sentiment Analysis Toolkit,https://www.kaggle.com/rtatman/thai-sentiment-analysis-toolkit,Thu Sep 07 2017
,DrGuillermo,"[Name, Games Played, MIN, PTS, FGM, FGA, FG%, 3PM, 3PA, 3P%, FTM, FTA, FT%, OREB, DREB, REB, AST, STL, BLK, TOV, PF, EFF, AST/TOV, STL/TOV, Age, Birth_Place, Birthdate, Collage, Experience, Height, Pos, Team, Weight, BMI]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, dateTime, string, numeric, numeric, string, string, numeric, numeric]",Context This data set can be paired with the shot logs data set from the same season. Content Full players stats from the 2014-2015 season + personal details such as height. weight etc. The data was scraped and copied from http//www.basketball-reference.com/teams/ and  http//stats.nba.com/leaders#!?Season=2014-15&SeasonType=Regular%20Season&StatCategory=MIN&CF=MIN*G*2&PerMode=Totals,CSV,,[basketball],Other,,,1165,6558,0.076171875,"Points, Assists, Height, Weight and other personal details and stats",NBA Players Stats - 2014-2015,https://www.kaggle.com/drgilermo/nba-players-stats-20142015,Wed May 03 2017
,Chris Crawford,"[User country, Nr. reviews, Nr. hotel reviews, Helpful votes, Score, Period of stay, Traveler type, Pool, Gym, Tennis court, Spa, Casino, Free internet, Hotel name, Hotel stars, Nr. rooms, User continent, Member years, Review month, Review weekday]","[string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, numeric, numeric, string, numeric, string, string]",Context This dataset includes quantitative and categorical features from online reviews from 21 hotels located in Las Vegas Strip extracted from TripAdvisor. All the 504 reviews were collected between January and August of 2015. Content The dataset contains 504 records and 20 tuned features  24 per hotel (two per each month randomly selected) regarding the year of 2015.  The CSV contains a header with the names of the columns corresponding to the features. Acknowledgements Lichman M. (2013). UCI Machine Learning Repository http//archive.ics.uci.edu/ml. Irvine CA University of California School of Information and Computer Science Downloaded form UCI Machine Learning Repository Inspiration Do machine learning algorithms take into account what happens in Vegas stays in Vegas?,CSV,,"[cities, hotels]",Other,,,261,3088,0.0576171875,500 reviews from hotels on the Las Vegas Strip,Las Vegas TripAdvisor Reviews,https://www.kaggle.com/crawford/las-vegas-tripadvisor-reviews,Tue Aug 29 2017
,Rachael Tatman,"[Have you ever taken a course in statistics?, Do you have any previous experience with programming?, What's your interest in data science?, Just for fun, do you prefer dogs or cat?]","[string, string, string, string]",Context This dataset contains survey responses to a survey that people could complete when they signed up for the 5-Day Data Challenge. On December 12 2017 survey responses for the second 5-Day Data Challenge were added. For this version of the challenge participants could sign up for either an intro version or a more in-depth regression challenge. Content The optional survey included four multiple-choice questions   Have you ever taken a course in statistics? Yep   Yes but I've forgotten everything   Nope Do you have any previous experience with programming?   Nope   I have a little bit of experience   I have quite a bit of experience   I have a whole lot of experience What's your interest in data science?   Just curious   It will help me in my current job   I want to get a job where I use data science    Other Just for fun do you prefer dogs or cat?   Dogs 🐶   Cats 🐱   Both 🐱🐶   Neither 🙅  In order to protect privacy the data has been shuffled (so there’s no temporal order to the responses) and a random 2% of the data has been removed (so even if you know that someone completed the survey you cannot be sure that their responses are included in this dataset). In addition all incomplete responses have been removed and any text entered in the “other” free response field has been replaced with the text “other”. Acknowledgements Thanks to everyone who completed the survey! )  Inspiration  Is there a relationship between how much programming experience someone has and why they’re interested in data science? Are more experienced programmers more likely to have taken statistics? Do people tend to prefer dogs cats both or neither? Is there a relationship between what people prefer and why they’re interested in data science? ,CSV,,[categorical data],CC0,,,122,1088,0.689453125,What are folks’ backgrounds? And do they prefer cats or dogs?,5-Day Data Challenge Sign-Up Survey Responses,https://www.kaggle.com/rtatman/5day-data-challenge-signup-survey-responses,Wed Dec 13 2017
,Aakaash Jois,"[JokeId, JokeText]","[numeric, string]",Context The funniness of joke is very subjective. Having more than 70000 users rate jokes can an algorithm be written to identify the universally funny joke? Content  The data file are in .csv format. The complete dataset is 100 rows and 73422 columns. The complete dataset is split into 3 .csv files. JokeText.csv contains the Id of the joke and the complete joke string. UserRatings1.csv contains the ratings provided by the first 36710 users. UserRatings2.csv contains the ratings provided by the last 36711 users. The dataset is arranged such that the initial users have rated higher number of jokes than the later users. The rating is a real value between -10.0 and +10.0. The empty values indicate that the user has not provided any rating for that particular joke.  Acknowledgements The dataset is associated with the below research paper. Eigentaste A Constant Time Collaborative Filtering Algorithm. Ken Goldberg Theresa Roeder Dhruv Gupta and Chris Perkins. Information Retrieval 4(2) 133-151. July 2001. More information and datasets can be found at http//eigentaste.berkeley.edu/dataset/ Inspiration Since funniness is a very subjective matter it will be very interesting to see if data science can bring out the details on what makes something funny.,CSV,,[humor],CC4,,,144,1676,25,"70,000+ users' rating of 100 witty jokes",Jester Collaborative Filtering Dataset,https://www.kaggle.com/aakaashjois/jester-collaborative-filtering-dataset,Fri Jun 16 2017
,Curtis Chong,[],[],"Author's Note This dataset was originally coined ""Speed Limits in New York City"". Since then I have changed the name of the dataset to ""Describing New York City Roads"" to better reflect the contents of the dataset. - Curtis Context New York City Speed Limits The New York Department of Transportation Regulates the speed limits for its roads (Afterall we can't be hitting 88 MPH on a regular day). This dataset describes the speed limits for particular road segments of New York City streets. The New York City Centerline Which streets are inherently faster? How will speed limits come into play? How will nearby bike lanes slow down vehicles (and ultimately taxis)? These are the kinds of questions that can only be answered with contextual data of the streets themselves. Fortunately most major cities provide a public Centerline file that describes the path of all railroads ferry routes and streets in the city. I've taken the New York City Centerline and packaged a dataset that tries to extract meaning out of all the road connections within the city. Content New York City Speed Limits Every speed limit region is a straight line. (Which represents a segment of road). These lines are expressed by two pairs of coordinates. lat1 - The first latitude coord lon1 - The first longitude coord lat2 - The second latitude coord lat2 - The second longitude coord street - The name of the street the speed limit is imposed on speed - The speed limit of that road section signed - Denotes if there is a physical sign on the street that displays the speed limit to cars. region - The city region that the road resides in. There are 5 regions (Bronx Brooklyn Manhattan Queens and Staten Island) distance - The length of the speed limit road section (in Miles). The New York City Centerline street - The name of the street post_type* - The extension for the street name. st_width - The width of the street (in feet). There are varying widths for the size of a street so it was hard to derive a lane count/ street using this feature. As a rule of thumb the average lane is around 12 feet wide. bike_lane - Defines which segments are part of the bicycle network as defined by the NYC Department of Transportation. There are 11 classes   1 =  Class I 2 = Class II 3 = Class III 4 = Links 5 = Class I II 6 = Class II III 7 = Stairs 8 = Class I III 9 = Class II I 10 = Class III I 11 = Class III II  Bike class information https//en.wikipedia.org/wiki/Cycling_in_New_York_City#Bikeway_types bike_traf_dir** - Describes the direction of traffic (FT = With TF = Against TW = Two-Way) traf_dir** - Describes the direction of traffic (FT = With TF = Against TW = Two-Way) rw_type - The type of road. There are 6 types of roads (1 = Street 2 = Highway 3 = Bridge 4 = Tunnel 9 = Ramp 13 = U-Turn). Note I parsed awkward path types such as ""Ferry route"" and ""trail"". start_contour*** - Numeric value indicating the vertical position of the feature's ""from"" node relative to grade level. end_contour*** - Numeric value indicating the vertical position of the feature's ""to"" node relative to grade level. snow_pri - The Department of Sanitation (DSNY) snow removal priority designation.  V = Non-DSNY C = Critical (These streets have top priority) S = Sector (These streets are second priority) H = Haulster (Small spreaders with plows attached for treating areas with limited accessibility - can hold two tons of salt)  region - The city region that the road resides in. There are 5 regions (Bronx Brooklyn Manhattan Queens and Staten Island) length - The length of the road (in Miles).  points -  The coordinates that define the road. Each coordinate is separated by '|' and the lat and lon values per coordinate are separated by ';'. (Side note Round road sections are plotted by points along the curve). *For those who may not be aware road names are based on a convention. ""Avenue""s ""Boulevard""s and ""Road""s are different for distinct reasons. I left these fields in the dataset in case you wish to find any patterns that are pertinent to those types of roads. To learn more about road conventions visit this link http//calgaryherald.com/news/local-news/in-naming-streets-strict-rules-dictate-roads-rises-trails-and-more **To explain how direction works I'll provide you with an image http//imgur.com/a/UflwX. Think of every road on the centerline as a vector. It points from one location to another. It always points from the very first coordinate to the very last coordinate. Now pay attention to the direction of the road (circled). Note how it points in the same direction as the vector denoted by the centerline data. The ""traf_dir"" attribute of the street is ""FT"" because the vector is headed in the same direction as traffic is (it is a one-way street). For ""traf_dir"" with a value of ""TW"" the direction of the vector doesn't matter as the road is a two-way street. ***I've had little luck finding what the ""grade levels"" represent. The original aliases are ""TO_LVL_CO"" and ""FRM_LVL_CO"". I'll keep searching tonight and will try to dig up what elevation these grades represent. I highly suspect the grades are contour lines because I know they have some relevance to elevation.  In the meantime here are the ""grades"" that each value represents  1 = Below Grade 1 2 = Below Grade 2 3 = Below Grade 3 4 = Below Grade 4 5 = Below Grade 5 6 = Below Grade 6 7 = Below Grade 7 8 = Below Grade 8 9 = Below Grade 9 10 = Below Grade 10 11 = Below Grade 11 12 = Below Grade 12 13 = At Grade 14 = Above Grade 1 15 = Above Grade 2 16 = Above Grade 3 17 = Above Grade 4 18 = Above Grade 5 19 = Above Grade 6 20 = Above Grade 7 21 = Above Grade 8 22 = Above Grade 9 23 = Above Grade 10 24 = Above Grade 11 25 = Above Grade 12 26 = Above Grade 13 99 = Not Applicable  All in all their documentation could be better and here is a reference to it if you want to look at the source (https//data.cityofnewyork.us/api/views/exjm-f27b/files/cba8af99-6cd5-49fd-9019-b4a6c2d9dff7?download=true&filename=Centerline.pdf) Acknowledgements I want to thank the New York City Department of Transportation (NYCDOT) and the city of New York for aggregating the original data sets. New York City Speed Limits http//www.nyc.gov/html/dot/html/about/vz_datafeeds.shtml 28‐11 Queens Plaza 8th FL Long Island City New York 11101 The New York City Centerline https//catalog.data.gov/dataset/nyc-street-centerline-cscl data.cityofnewyork.us New York NY 10007",CSV,,"[road transport, taxi services]",CC0,,,83,1639,22,A Collection of Road Variables in New York for the Taxi Playground Challenge,Describing New York City Roads,https://www.kaggle.com/splacorn/speed-limits-in-nyc-taxi-playground-challenge,Sun Aug 06 2017
,Washington University,"[case_id, docket_id, issues_id, vote_id, date_decision, decision_type, us_citation, court_citation, led_citation, lexis_citation, term, court, chief_justice, docket, case_name, date_argument, date_reargument, petitioner, petitioner_state, respondent, respondent_state, jurisdiction, administrative_action, administrative_action_state, district_court, case_origin, case_origin_state, case_source, case_source_state, lower_court_disagreement, cert_reason, lower_court_disposition, lower_disposition_direction, declaration_unconstitutionality, case_disposition, disposition_unusual, party_winning, precedent_alteration, vote_unclear, issue, issue_area, decision_direction, decision_direction_dissent, authority_decision_one, authority_decision_two, law_type, law_supplement, law_minor_supplement, majority_opinion_writer, majority_opinion_assigner, split_vote, majority_votes, minority_votes, ]","[string, string, string, string, dateTime, numeric, string, string, string, string, numeric, numeric, string, numeric, string, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string]",Content The Supreme Court database is the definitive source for researchers students journalists and citizens interested in the United States Supreme Court. The database contains more than two hundred variables regarding each case decided by the Court between the 1946 and 2015 terms. Examples include the identity of the court whose decision the Supreme Court reviewed the parties to the suit the legal provisions considered in the case and the votes of the Justices. The database codebook is available here. Acknowledgements The database was compiled by Professor Spaeth of Washington University Law and funded with a grant from the National Science Foundation.,CSV,,[law],CC4,,,235,2030,3,How have court decisions over legal issues changed over time?,"US Supreme Court Cases, 1946-2016",https://www.kaggle.com/wustl/supreme-court,Thu Jan 26 2017
,Stephan Wessels,[],[],Context CRIME STATISTICS INTEGRITY The South African Police Service (SAPS) has accepted a new and challeging objective of ensuring that its crime statistics are in line with international best practice. This will be achieved through a Memorandum of Understanding with Statistics South Africa (Stats SA) aimed at further enhancing the quality and integrity of the South African crime statistics.  The crime statistics generated by SAPS are an important link in the value chain of the statistics system informs policy development and planning in the criminal justice system. The collaboration with StatsSA will go a long way in enhancing the integrity of the SAPS crime statistics and ensuring that policy-makers have quality data to assist them with making policy decisions. Content The dataset contains South African crime statistics broken down per province station and crime type. Acknowledgements Data as published from  http//www.saps.gov.za/resource_centre/publications/statistics/crimestats/2015/crime_stats.php  Further sources  http//www.saps.gov.za/services/crimestats.php  An overview presentation  http//www.saps.gov.za/services/final-crime-stats-release-02september2016.pdf ,Other,,[crime],Other,,,2052,15824,23,A history of crime statistics from 2004 to 2015 per province and station,Crime Statistics for South Africa,https://www.kaggle.com/slwessels/crime-statistics-for-south-africa,Wed Nov 02 2016
,MuonNeutrino,"[Latitude, Longitude, BlockCode, County, State]","[numeric, numeric, numeric, string, string]",Context There are a number of Kaggle datasets that provide spatial data around New York City. For many of these it may be quite interesting to relate the data to the demographic and economic characteristics of nearby neighborhoods. I hope this data set will allow for making these comparisons without too much difficulty. Exploring the data and making maps could be quite interesting as well. Content This dataset contains two CSV files  nyc_census_tracts.csv This file contains a selection of census data taken from the ACS DP03 and DP05 tables. Things like total population  racial/ethnic demographic information employment and commuting characteristics and more are contained here. There is a  great deal of additional data in the raw tables retrieved from the US Census Bureau website so I could easily add more fields if  there is enough interest. I obtained data for individual census tracts which typically contain several thousand residents.  census_block_loc.csv For this file I used an online FCC census block lookup tool to retrieve the census block code for a 200 x 200 grid containing  New York City and a bit of the surrounding area. This file contains the coordinates and associated census block codes along  with the state and county names to make things a bit more readable to users. Each census tract is split into a number of blocks so one must extract the census tract code from the block code.  Acknowledgements The data here was taken from the American Community Survey 2015 5-year estimates (https//factfinder.census.gov/faces/nav/jsf/pages/index.xhtml). The census block coordinate data was taken from the FCC Census Block Conversions API (https//www.fcc.gov/general/census-block-conversions-api) As public data from the US government this is not subject to copyright within the US and should be considered public domain.,CSV,,"[united states, demographics]",CC0,,,458,2684,2,"Demographic, Economic, and Location Data for Census Tracts in NYC",New York City Census Data,https://www.kaggle.com/muonneutrino/new-york-city-census-data,Fri Aug 04 2017
,Abhinav Moudgil,"[ID, Joke]","[numeric, string]",Context Generating humor is a complex task in the domain of machine learning and it requires the models to understand the deep semantic meaning of a joke in order to generate new ones. Such problems however are difficult to solve due to a number of reasons one of which is the lack of a database that gives an elaborate list of jokes. Thus a large corpus of over 0.2 million jokes has been collected by scraping several websites containing funny and short jokes. Visit my Github repository for more information regarding collection of data and the scripts used.  Content This dataset is in the form of a csv file containing 231657 jokes. Length of jokes ranges from 10 to 200 characters. Each line in the file contains a unique ID and joke.  Disclaimer It has been attempted to keep the jokes as clean as possible. Since the data has been collected by scraping websites it is possible that there may be a few jokes that are inappropriate or offensive to some people.,CSV,,"[humor, linguistics]",ODbL,,,918,9406,23,"Collection of over 200,000 short jokes for humour research",Short Jokes,https://www.kaggle.com/abhinavmoudgil95/short-jokes,Tue Feb 07 2017
,Death Penalty Information Center,"[Date, Name, Age, Sex, Race, Crime, Victim Count, Victim Sex, Victim Race, County, State, Region, Method, Juvenile, Volunteer, Federal, Foreign National]","[dateTime, string, numeric, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string]",Content The execution database includes records of every execution performed in the United States since the Supreme Court reinstated the death penalty in 1976. Federal executions are indicated by FE in the state field and included in the region in which the crime occurred. The information in this database was obtained from news reports the Department of Corrections in each state and the NAACP Legal Defense Fund. Acknowledgements The execution database was compiled and published by the Death Penalty Information Center. Victim details including quantity sex and race were acquired from the Criminal Justice Project's Death Row USA report.,CSV,,"[crime, law]",Other,,,427,3209,0.150390625,"Use of capital punishment or ""death penalty"" in criminal justice system","Executions in the United States, 1976-2016",https://www.kaggle.com/usdpic/execution-database,Wed Jan 25 2017
,ChrisM!,"[API#, Operator, Operator ID, WellType, WellName, WellNumber, OrderNumbers, Approval Date, County, Sec, Twp, Rng, QQQQ, LAT, LONG, PSI, BBLS, ZONE, , , ]","[numeric, string, numeric, string, string, numeric, numeric, dateTime, string, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string]","Context Beginning in 2009 the frequency of earthquakes in the U.S. State of Oklahoma rapidly increased from an average of fewer than two 3.0+ magnitude earthquakes per year since 1978 to hundreds per year in 2014 2015 and 2016. Thousands of earthquakes have occurred in Oklahoma and surrounding areas in southern Kansas and North Texas since 2009. Scientific studies attribute the rise in earthquakes to the disposal of wastewater produced during oil extraction that has been injected deeply into the ground. (Wikipedia) Injection wells are utilized to dispose of fluid created as a byproduct of oil and gas production activities. Likewise hydraulic fracturing ie ""fracking"" produces large byproducts of water. This byproduct is then injected deep back into the earth via disposal/injection wells.  Content This dataset contains two data files. One detailing ""active"" saltwater injection wells in Oklahoma as of September 2017. The second file lists earthquakes in the Oklahoma region (Oklahoma and surrounding states) since 1977.  Acknowledgements Data was gathered from Oklahoma Corporation Commission and The United States Geological Survey.  Inspiration  Is there a correlation between earthquakes and injection well activity?  Can the data be used as a predictor of general proximity and/or time of future earthquakes ? ",CSV,,[geology],CC0,,,112,1304,4,Earthquakes in Oklahoma region and Oil and Gas fluid byproduct data.,Oklahoma Earthquakes and Saltwater Injection Wells,https://www.kaggle.com/ksuchris2000/oklahoma-earthquakes-and-saltwater-injection-wells,Tue Sep 26 2017
,LaurentBerder,"[Index, Name, Full Name, Birth, Death, Birth City, Birth Province, Succession, Reign Start, Reign End, Cause, Killer, Dynasty, Era, Notes, Verif, Image]","[numeric, string, string, dateTime, dateTime, string, string, string, dateTime, dateTime, string, string, string, string, string, string, string]",Context We all know of the Roman empire but what about its emperors specifically? Content Here you will find information on each of the emperors of the Roman empire which lasted between 26 BC and 395 AD. Specifically you can use data on their  Names Date of birth City and Province of birth Date of death Method of accession to power Date of accession to power Date of end of reign Cause of death Identity of killer Dynasty Era Photo  Acknowledgements This dataset was provided by Zonination who made it available on Wikipedia. See his repository on Github Inspiration What kind of trend can you find in these emperors' lives and reigns? What aspects of them allowed them to live longer?,CSV,,"[life, death, politicians]",ODbL,,,160,1830,0.0244140625,"Life, death and reign of Roman emperors",Roman emperors from 26 BC to 395 AD,https://www.kaggle.com/lberder/roman-emperors-from-26-bc-to-395-ad,Thu Oct 19 2017
,Sijo VM,[],[],The data set contains the details about all the ATP matches played since 1968. The data set has a lot of missing values especially for the period between 1968 - 1991.  Thanks to Xiaming Chen for making the data available to the online community.  Primarily I would like to understand how tennis matches/players have evolved over time and any other insights.,CSV,,"[tennis, sports]",Other,,,802,4095,31,Details of the ATP matches since 1968,"ATP Matches, 1968 to 2017",https://www.kaggle.com/sijovm/atpdata,Thu Mar 30 2017
,Rachael Tatman,[],[],Overview This dataset is a Wikipedia-based MR dataset called RelocaR which is tailored towards locations as well as improving previous defi- ciencies in annotation guidelines. The corpus is designed to evaluate the capability of a classifier to distinguish literal metonymic and mixed location mentions. In terms of dataset size ReLocaR contains 1026 training and 1000 test instances. The data was sampled using Wikipedia’s Random Article API. We kept the sentences which contained at least one of the places from a manually compiled list of countries and capitals of the world. The natural distribution of literal versus metonymic examples is approximately 80/20 so we had to discard the excess literal examples during sampling to balance the classes. Annotation Guidelines ReLocaR has three classes literal metonymic and mixed. Literal reading comprises territorial interpretations (the geographical territory the land soil and physical location) i.e. inanimate places that serve to point to a set of coordinates (where something might be located and/or happening) such as “The treaty was signed in Italy.” “Peter comes from Russia.” “Britain’s Andy Murray won the Grand Slam today.” “US companies increased exports by 50%.” “China’s artists are among the best in the world.” or “The reach of the transmission is as far as Brazil.”.  A metonymic reading is any location occurrence that expresses animacy (Coulson and Oakley 2003) such as “Jamaica’s indifference will not improve the negotiations.” “Sweden’s budget deficit may rise next year.”. The following are other metonymic scenarios a location name which stands for any persons or organisations associated with it such as “We will give aid to Afghanistan.” a location as a product such as “I really enjoyed that delicious Bordeaux.” a location posing as a sports team “India beat Pakistan in the playoffs.” a governmental or other legal entity posing as a location “Zambia passed a new justice law today.” events acting as locations “Vietnam was a bad experience for me”.  The mixed reading is assigned in two cases either both readings are invoked at the same time such as in “The Central European country of Slovakia recently joined the EU.” or there is not enough context to ascertain the reading i.e. both are plausible such as in “We marvelled at the art of ancient Mexico.”. In difficult cases such as these the mixed class is assigned. Acknowledgements This dataset was collected by Milan Gritta Mohammad Taher Pilehvar Nut Limsopatham and Nigel Collier. It is redistributed here under a GNU general public license. If you use this data in your work please cite the following paper Gritta M. Pilehvar M. T. Limsopatham N. & Collier N. 2017. “Vancouver Welcomes You! Minimalist Location Metonymy Resolution”. Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1 Long Papers) URL http//aclweb.org/anthology/P/P17/P17-1115.pdf In addition code to replicate the paper’s results can be found here.,Other,,"[geography, linguistics]",GPL,,,84,1139,0.3408203125,Location Metonymy Resolution Dataset,Real Location Retrieval from Text,https://www.kaggle.com/rtatman/real-location-retrieval-from-text,Sat Aug 19 2017
,UCI Machine Learning,"[ID, TITLE, URL, PUBLISHER, CATEGORY, STORY, HOSTNAME, TIMESTAMP]","[numeric, string, string, string, string, string, string, numeric]",This dataset contains headlines URLs and categories for 422937 news stories collected by a web aggregator between March 10th 2014 and August 10th 2014. News categories included in this dataset include business; science and technology; entertainment; and health. Different news articles that refer to the same news item (e.g. several articles about recently released employment statistics) are also categorized together. Content The columns included in this dataset are  ID  the numeric ID of the article TITLE  the headline of the article URL  the URL of the article PUBLISHER  the publisher of the article CATEGORY  the category of the news item; one of -- b  business -- t  science and technology -- e  entertainment -- m  health STORY  alphanumeric ID of the news story that the article discusses HOSTNAME  hostname where the article was posted TIMESTAMP  approximate timestamp of the article's publication given in Unix time (seconds since midnight on Jan 1 1970)  Acknowledgments This dataset comes from the UCI Machine Learning Repository. Any publications that use this data should cite the repository as follows Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. This specific dataset can be found in the UCI ML Repository at this URL Inspiration What kinds of questions can we explore using this dataset? Here are a few possibilities  can we predict the category (business entertainment etc.) of a news article given only its headline? can we predict the specific story that a news article refers to given only its headline? ,CSV,,"[news agencies, linguistics]",CC0,,,2289,22597,98,Headlines and categories of 400k news stories from 2014,News Aggregator Dataset,https://www.kaggle.com/uciml/news-aggregator-dataset,Tue Nov 01 2016
,Google News Lab,"[TOP TRENDING How To's on Google, ranked by spike value. Data is past 5 years]",[string],"These are the top trending ""How to"" searches on Google ranked by their spike value. Trending searches are searches with the biggest increase in search interest since the previous time period. Data covers the past 5 years. ",CSV,,[],CC4,,,161,1415,0.0029296875,"The top trending ""how to"" related searches on Google in the past 5 years",Top Trending How Tos on Google,https://www.kaggle.com/GoogleNewsLab/top-trending-how-tos-on-google,Wed Nov 01 2017
,KrisMurphy,"[ID, Name, Department, GEO, Role, Rising_Star, Will_Relocate, Critical, Trending Perf, Talent_Level, Validated_Talent_Level, Percent_Remote, EMP_Sat_OnPrem_1, EMP_Sat_OnPrem_2, EMP_Sat_OnPrem_3, EMP_Sat_OnPrem_4, EMP_Sat_OnPrem_5, EMP_Sat_Remote_1, EMP_Sat_Remote_2, EMP_Sat_Remote_3, EMP_Sat_Remote_4, EMP_Sat_Remote_5, EMP_Engagement_1, EMP_Engagement_2, EMP_Engagement_3, EMP_Engagement_4, EMP_Engagement_5, last_evaluation, number_project, average_montly_hours, time_spend_company, Work_accident, left_Company, CSR Factor, promotion_last_5years, sales, salary, Gender, LinkedIn_Hits, Emp_Work_Status2, Emp_Work_Status_3, Emp_Work_Status_4, Emp_Work_Status_5, Emp_Identity, Emp_Role, Emp_Position, Emp_Title, Women_Leave, Men_Leave, Emp_Competitive_1, Emp_Competitive_2, Emp_Competitive_3, Emp_Competitive_4, Emp_Competitive_5, Emp_Collaborative_1, Emp_Collaborative_2, Emp_Collaborative_3, Emp_Collaborative_4, Emp_Collaborative_5, Sensor_StepCount, Sensor_Heartbeat(Average/Min), Sensor_Proximity(1-highest/10-lowest)]","[numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context We are building a data set that can be used for building useful reports  understanding the difference between data and information and multivariate analysis. The data set we are building is similar to that used in several academic reports and what may be found in ERP HR subsystems. We will update the sample data set as we gain a better understanding of the data elements using the calculations that exist in scholarly journals. Specifically we will use the correlation tables to rebuild the data sets. Content The fields represent a fictitious data set where a survey was taken and actual employee metrics exist for a particular organization. None of this data is real. Acknowledgements We wouldn't be here without the help of others. If you owe any attributions or thanks include them here along with any citations of past research.  Prabhjot Singh contributed a portion of the data (the columns on the right before the survey data was added). https//www.kaggle.com/prabhjotindia https//www.kaggle.com/prabhjotindia/visualizing-employee-data/data About this Dataset Why are our best and most experienced employees leaving prematurely? Have fun with this database and try to predict which valuable employees will leave next. Fields in the dataset include Satisfaction Level Last evaluation Number of projects Average monthly hours Time spent at the company Whether they have had a work accident Whether they have had a promotion in the last 5 years Departments Salary Inspiration Your data will be in front of the world's largest data science community. What questions do you want to see answered?,CSV,,[],CC0,,,446,3289,2,To use for various exercises including multivariate analysis,Data Lab,https://www.kaggle.com/krismurphy01/data-lab,Sat Aug 19 2017
,Rachael Tatman,"[date, month, day, # of dataset links, recipients, total opens, unique opens, total clicks, unique clicks, notes]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context There are lots of really cool datasets getting added to Kaggle every day and as part of my job I want to help people find them. I’ve been tweeting about datasets on my personal Twitter accounts @rctatman and also releasing a weekly newsletter of interesting datasets. I wanted to know which method was more effective at getting the word out about new datasets Twitter or the newsletter? Content This dataset contains two .csv files. One has information on the impact of tweets with links to datasets while the other has information on the impact of the newsletter. Twitter The Twitter .csv has the following information  month The month of the tweet (1-12) day The day of the tweet (1-31) hour The hour of the tweet (1-24) impressions The number of impressions the tweet got engagement The number of total engagements clicks The number of URL clicks  Fridata Newsletter The Fridata .csv has the following information  date The Date the newsletter was sent out month The Month the newsletter was sent out (1-12) day The day the newsletter was sent out (1-31) # of dataset links How many links were in the newsletter recipients How many people received the email with the newsletter total opens How many times the newsletter was opened unique opens How many individuals opened the newsletter total clicks The total number of clicks on the newsletter unique clicks (unsure; provided by Tinyletter) notes notes on the newsletter  Acknowledgements This dataset was collected by the uploader Rachael Tatman. It is released here under a CC-BY-SA license. Inspiration  Which format receives more views? Which format receives more clicks? Which receives more clicks/view? What’s the best time of day to send a tweet? ,CSV,,"[marketing, internet]",CC4,,,173,1681,0.0029296875,Which format is best for getting the word out?,Twitter vs. Newsletter Impact,https://www.kaggle.com/rtatman/twitter-vs-newsletter,Tue Sep 19 2017
,Sohier Dane,"[Draw Date, Winning Numbers, Bonus #, Extra #]","[dateTime, string, numeric, string]",Winning numbers from the New York State Lotto since 2001. Acknowledgements This dataset was kindly made available by the state of New York. You can find the original dataset here. Inspiration  Some other state lotteries have proven to be predictable and ended up being gamed. It's extremely unlikely that any real patterns exist in a large and long running lotto like New York's but can you find any? ,CSV,,[money],CC0,,,80,1123,0.052734375,Winning numbers since 2001,NY State Lotto Winning Numbers,https://www.kaggle.com/sohier/ny-state-lotto-winning-numbers,Wed Sep 13 2017
,Centers for Disease Control and Prevention,"[Year, State, Population  < 72 months old, # of Children Tested, Total  #  of Children with Confirmed BLL ≥10 µg/dL, % of Children Tested with Confirmed BLLs ≥10 µg/dL, 5-9 µg/dL, ≥5 µg/dL, 10-14 µg/dL, 15-19 µg/dL, 20-24 µg/dL, 25-44 µg/dL, 45-69 µg/dL, ≥70 µg/dL]","[numeric, string, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",CDC began collecting childhood blood lead surveillance data in April 1995. The national surveillance system is composed of data from state and local health departments. States maintain their own child-specific databases so they can identify duplicate test results or sequential test results on individual children. These databases contain follow-up data on children with elevated blood lead levels including data on medical treatment environmental investigations and potential sources of lead exposure. States extract fields from their child-specific surveillance databases and transfer them to CDC for the national database. State child-specific databases contain follow-up data on children with elevated blood lead levels including data on medical treatment environmental investigations and potential sources of lead exposure. Surveillance fields for CDC's national database are extracted from state child-specific databases and transferred to CDC. State surveillance systems are based on reports of blood lead tests from laboratories. Ideally laboratories report results of all blood lead tests not just elevated values to state health departments. States determine the reporting level for blood lead tests and decide which data elements should accompany the blood lead test result. These data were collected for program management purposes. The data have limitations and we cannot compare across states or counties because data collection methods vary across grantees. Data are not generalizable at the national state or local level.,CSV,,"[healthcare, epidemiology, health]",CC0,,,200,1677,0.1669921875,"National and state-level surveillance data, 1997 to 2015",Childhood Blood Lead Surveillance,https://www.kaggle.com/cdc/childhood-blood-lead-surveillance,Mon May 01 2017
,Rachael Tatman,"[deceptive, hotel, polarity, source, text]","[boolean, string, string, string, string]",Context This corpus consists of truthful and deceptive hotel reviews of 20 Chicago hotels. The data is described in two papers according to the sentiment of the review. In particular we discuss positive sentiment reviews in [1] and negative sentiment reviews in [2]. While we have tried to maintain consistent data preprocessing procedures across the data there are differences which are explained in more detail in the associated papers. Please see those papers for specific details. Content This corpus contains  400 truthful positive reviews from TripAdvisor (described in [1]) 400 deceptive positive reviews from Mechanical Turk (described in [1]) 400 truthful negative reviews from Expedia Hotels.com Orbitz Priceline TripAdvisor and Yelp (described in [2]) 400 deceptive negative reviews from Mechanical Turk (described in [2])  Each of the above datasets consist of 20 reviews for each of the 20 most popular Chicago hotels (see [1] for more details). The files are named according to the following conventions Directories prefixed with fold correspond to a single fold from the cross-validation experiments reported in [1] and [2]. Hotels included in this dataset  affinia Affinia Chicago (now MileNorth A Chicago Hotel) allegro Hotel Allegro Chicago - a Kimpton Hotel amalfi Amalfi Hotel Chicago ambassador Ambassador East Hotel (now PUBLIC Chicago) conrad Conrad Chicago fairmont Fairmont Chicago Millennium Park hardrock Hard Rock Hotel Chicago hilton Hilton Chicago homewood Homewood Suites by Hilton Chicago Downtown hyatt Hyatt Regency Chicago intercontinental InterContinental Chicago james James Chicago knickerbocker Millennium Knickerbocker Hotel Chicago monaco Hotel Monaco Chicago - a Kimpton Hotel omni Omni Chicago Hotel palmer The Palmer House Hilton sheraton Sheraton Chicago Hotel and Towers sofitel Sofitel Chicago Water Tower swissotel Swissotel Chicago talbott The Talbott Hotel  References [1] M. Ott Y. Choi C. Cardie and J.T. Hancock. 2011. Finding Deceptive Opinion Spam by Any Stretch of the Imagination. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics Human Language Technologies. [2] M. Ott C. Cardie and J.T. Hancock. 2013. Negative Deceptive Opinion Spam. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics Human Language Technologies. Acknowledgements If you use any of this data in your work please cite the appropriate associated paper (described above). Please direct questions to Myle Ott (myleott@cs.cornell.edu).,CSV,,"[languages, linguistics]",CC4,,,274,1978,1,A corpus of truthful and deceptive hotel reviews,Deceptive Opinion Spam Corpus,https://www.kaggle.com/rtatman/deceptive-opinion-spam-corpus,Tue Jul 18 2017
,US Bureau of Labor Statistics,"[tucaseid, tuactivity_n, eueatsum, euedur, euedur24]","[numeric, numeric, numeric, numeric, numeric]",Context The ATUS Eating & Health (EH) Module was fielded from 2006 to 2008 and again in 2014 to 2016. The EH Module data files contain additional information related to eating meal preparation and health.  Data for 2015 currently are being processed and have not yet been released. Data collection is planned to run through December 2016. Content There are 3 datasets from 2014  The EH Respondent file which contains information about EH respondents including general health and body mass index. There are 37 variables. The EH Activity file which contains information such as the activity number whether secondary eating occurred during the activity and the duration of secondary eating. There are 5 variables. The EH Replicate weights file which contains miscellaneous EH weights. There are 161 variables.  The data dictionary can be found here. Acknowledgements The original datasets can be found here. Inspiration Some ideas for exploring the datasets are  What is the relationship between weight or BMI and meal preparation patterns consumption of fresh/fast food or snacking patterns? Do grocery shopping patterns differ by income? ,CSV,,"[nutrition, demographics]",Other,,,1829,16492,19,American Time Use Survey (ATUS) Eating & Health Module Files from 2014,Eating & Health Module Dataset,https://www.kaggle.com/bls/eating-health-module-dataset,Thu Nov 10 2016
,College Board,"[Exam Subject, Score, Students (11th Grade), Students (12th Grade), Students (Male), Students (Female), Students (White), Students (Black), Students (Hispanic/Latino), Students (Asian), Students (American Indian/Alaska Native), Students (Native Hawaiian/Pacific Islander), Students (Two or More Races), All Students (2016)]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The Advanced Placement Exam scores for the class of 2016 highlighted in this dataset show that students continue to demonstrate college-level skills and knowledge in increasing numbers. Even as AP teachers deliver rigor to an ever-diversifying population of students participation and performance continue to improve. Behind and within these data are the daily sacrifices of AP students and teachers including the late nights that students put in diligently studying and the weekends that teachers give up to help their students succeed. Their hard work and effort are worth celebrating. Acknowledgements This data was collected and released by the College Board after the May 2016 exam administration.,CSV,,[education],Other,,,775,5005,0.0244140625,What is the relationship between student demographics and exam subjects/scores?,2016 Advanced Placement Exam Scores,https://www.kaggle.com/collegeboard/ap-scores,Fri Feb 03 2017
,Thiago Balbo,"[authors, category, content, date, id, img_src, section, tags, title, topics, url]","[string, string, string, dateTime, numeric, string, string, numeric, string, numeric, string]",Inspiration I'm a big fan of TechCrunch for a while now. Kind of because I get to know about new startups that's coming up or maybe just because I find Tito Hamze videos fun. But TechCrunch got plenty of good content. And where we find good content we produce great exploratory analysis. This dataset is a great opportunity for you to boost your skills as an EDA expert! It provides several features that make you able to create different analyses such as time series clustering predictive segmenting classification and tons of others. Let's not forget about word2vec for that. It would be awesome to see that in action here! I've made the scraper available on github if you want to check it out here is the link techcrunch scraper repo Content This dataset comes with a rich set of features. You will have  authors authors of the post - can be one or multiple authors category post category content post content - each paragraph can be extracted by splitting on the \n date post date id post id - the same id used on techcrunch website img_src post main image url section post section - each section is one of the options on the main page dropdown menu tags post tags - can be zero or multiple tags title post title topics post topics url post url  Acknowledgements All posts were scraped from the TechCrunch website on mid oct-16. Each line contains information about one post and each post appear in no more than one line.,CSV,,"[linguistics, internet]",CC0,,,505,5753,136,40k compiled posts with a rich set of features will boost your visualizations,TechCrunch Posts Compilation,https://www.kaggle.com/thibalbo/techcrunch-posts-compilation,Wed Oct 19 2016
,Open Source Sports,"[abbrev_type, code, full_name]","[string, string, string]",This dataset contains stats on players coaches and teams in men's professional basketball leagues from 1937 to 2012. Acknowledgments This dataset was downloaded from the Open Source Sports website. It did not come with an explicit license but based on other datasets from Open Source Sports we treat it as follows This database is copyright 1996-2015 by Sean Lahman.  This work is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. For details see http//creativecommons.org/licenses/by-sa/3.0/ The Data This dataset contains 11 files each corresponding to a data table. There are five main tables  master biographical information for all the players and coaches teams stats on each team per year players stats for each player per year coaches stats for each coach per year series_post information on post-season winners per year  And there are six supplementary tables  abbrev a key to the abbreviations used in other tables awards_coaches coaching awards per year awards_players player awards per year draft draft information per year hof Hall of Fame information per year player_allstar individual player stats for the All-Star Game per year ,CSV,,[basketball],Other,,,1805,11472,7,"Stats on players, teams, and coaches in men's pro basketball leagues, 1937-2012",Men's Professional Basketball,https://www.kaggle.com/open-source-sports/mens-professional-basketball,Mon Nov 14 2016
,GroupLens,"[movieId, tagId, relevance]","[numeric, numeric, numeric]",Context The datasets describe ratings and free-text tagging activities from MovieLens a movie recommendation service. It contains 20000263 ratings and 465564 tag applications across 27278 movies. These data were created by 138493 users between January 09 1995 and March 31 2015. This dataset was generated on October 17 2016. Users were selected at random for inclusion. All selected users had rated at least 20 movies.  Content No demographic information is included. Each user is represented by an id and no other information is provided. The data are contained in six files. tag.csv that contains tags applied to movies by users  userId movieId tag timestamp  rating.csv that contains ratings of movies by users  userId movieId rating timestamp  movie.csv that contains movie information  movieId title genres  link.csv that contains identifiers that can be used to link to other sources  movieId imdbId tmbdId  genome_scores.csv that contains movie-tag relevance data  movieId tagId relevance  genome_tags.csv that contains tag descriptions  tagId tag  Acknowledgements The original datasets can be found here. To acknowledge use of the dataset in publications please cite the following paper F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets History and Context. ACM Transactions on Interactive Intelligent Systems (TiiS) 5 4 Article 19 (December 2015) 19 pages. DOI=http//dx.doi.org/10.1145/2827872 Inspiration Some ideas worth exploring  Which genres receive the highest ratings? How does this change over time? Determine the temporal trends in the genres/tagging activity of the movies released ,CSV,,[film],Other,,,2152,15785,885,Over 20 Million Movie Ratings and Tagging Activities Since 1995,MovieLens 20M Dataset,https://www.kaggle.com/grouplens/movielens-20m-dataset,Tue Nov 08 2016
,UC San Diego,"[Year, Month, Decimal Date, Carbon Dioxide (ppm), Seasonally Adjusted CO2 (ppm), Carbon Dioxide Fit (ppm), Seasonally Adjusted CO2 Fit (ppm)]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The carbon dioxide record from Mauna Loa Observatory known as the “Keeling Curve” is the world’s longest unbroken record of atmospheric carbon dioxide concentrations. Scientists make atmospheric measurements in remote locations to sample air that is representative of a large volume of Earth’s atmosphere and relatively free from local influences. Content This dataset includes a monthly observation of atmospheric carbon dioxide (or CO2) concentrations from the Mauna Loa Observatory (Hawaii) at a latitude of 19.5 longitude of -155.6 and elevation of 3397 meters.   Columns 1-3 Provide the date in the following redundant formats year month and decimal date Column 4 Monthly CO2 concentrations in parts per million (ppm) measured on the 08A calibration scale and collected at 2400 hours on the fifteenth of each month. Column 5 The fifth column provides the same data after a seasonal adjustment which involves subtracting from the data a 4-harmonic fit with a linear gain factor to remove the seasonal cycle from carbon dioxide measurements Column 6 The sixth column provides the data with noise removed generated from a stiff cubic spline function plus 4-harmonic functions with linear gain Column 7 The seventh column is the same data with the seasonal cycle removed.  Acknowledgements The carbon dioxide data was collected and published by the University of California's Scripps Institution of Oceanography under the supervision of Charles David Keeling with support from the US Department of Energy Earth Networks and the National Science Foundation. Inspiration How have atmospheric carbon dioxide levels changed in the past sixty years? How do carbon dioxide concentrations change seasonally? What do you think causes this seasonal cycle? When will the carbon dioxide levels exceed 450 parts per million?,CSV,,"[environment, climate]",Other,,,731,4808,0.0302734375,Atmospheric carbon dioxide data from Mauna Loa Observatory since 1958,Carbon Dioxide Levels in Atmosphere,https://www.kaggle.com/ucsandiego/carbon-dioxide,Fri Mar 10 2017
,Rio 2016,"[id, name, nationality, sex, dob, height, weight, sport, gold, silver, bronze]","[numeric, string, string, string, dateTime, numeric, numeric, string, numeric, numeric, numeric]",This dataset consists of the official statistics on the 11538 athletes and 306 events at the 2016 Olympic Games in Rio de Janeiro. The athletes file includes the name nationality (as a three letter IOC country code) gender age (as date of birth) height in meters weight in kilograms sport and quantity of gold silver and/or bronze medals won for every Olympic athlete at Rio. The events file lists the name sport discipline (if available) gender of competitors and venue(s) for every Olympic event at Rio 2016. CREDITS Source Data Rio 2016 website Data Files GitHub user flother,CSV,,[olympic games],CC0,,,1746,9814,0.7568359375,"Athletes, medals, and events from summer games",2016 Olympics in Rio de Janeiro,https://www.kaggle.com/rio2016/olympic-games,Tue Jan 10 2017
,Institute of Museum and Library Services,"[Museum ID, Museum Name, Legal Name, Alternate Name, Museum Type, Institution Name, Street Address (Administrative Location), City (Administrative Location), State (Administrative Location), Zip Code (Administrative Location), Street Address (Physical Location), City (Physical Location), State (Physical Location), Zip Code (Physical Location), Phone Number, Latitude, Longitude, Locale Code (NCES), County Code (FIPS), State Code (FIPS), Region Code (AAM), Employer ID Number, Tax Period, Income, Revenue]","[numeric, string, string, string, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The museum dataset is an evolving list of museums and related organizations in the United States. The data file includes basic information about each organization (name address phone website and revenue) plus the museum type or discipline. The discipline type is based on the National Taxonomy of Exempt Entities which the National Center for Charitable Statistics and IRS use to classify nonprofit organizations. Non-museum organizations may be included. For example a non-museum organization may be included in the data file because it has a museum-like name on its IRS record for tax-exempt organizations. Museum foundations may also be included. Museums may be missing. For example local municipal museums may be undercounted because original data sources used to create the compilation did not include them. Museums may be listed multiple times. For example one museum may be listed as both itself and its parent organization because it was listed differently in each original data sources. Duplicate records are especially common for museums located within universities. Information about museums may be outdated.  The original scan and compilation of data sources occurred in 2014.  Scans are no longer being done to update the data sources or add new data sources to the compilation.  Information about museums may have changed since it was originally included in the file. Acknowledgements The museum data was compiled from IMLS administrative records for discretionary grant recipients IRS records for tax-exempt organizations and private foundation grant recipients. Inspiration Which city or state has the most museums per capita? How many zoos or aquariums exist in the United States? What museum or related organization had the highest revenue last year? How does the composition of museum types differ across the country?,CSV,,"[museums, animals]",CC0,,,336,2665,7,"Name, location, and revenue for every museum in the United States","Museums, Aquariums, and Zoos",https://www.kaggle.com/imls/museum-directory,Tue Mar 07 2017
,Dalia Research,"[country_code, uuid, age, gender, rural, dem_education_level, dem_full_time_job, dem_has_children, question_bbi_2016wave4_basicincome_awareness, question_bbi_2016wave4_basicincome_vote, question_bbi_2016wave4_basicincome_effect, question_bbi_2016wave4_basicincome_argumentsfor, question_bbi_2016wave4_basicincome_argumentsagainst, age_group, weight]","[string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, numeric]",About this Dataset  Dalia Research conducted the first representative poll on basic income across Europe in the Spring of 2016. The results first presented together with NEOPOLIS at the Future of Work conference in Zurich showed that two thirds of Europeans would vote for basic income. Dalia's basic income poll is now an annual survey and the first wave of results from 2016 are now being made public. Although Dalia's latest research on basic income is not yet public you can visit here to see the results from the most recent Spring 2017 survey.  The study was conducted by Dalia Research in April 2016 on public opinion across 28 EU Member States. The sample of n=9.649 was drawn across all 28 EU Member States taking into account current population distributions with regard to age (14-65 years) gender and region/country. Enjoy perusing the dataset and exploring interesting connections between demographics and support for basic income.,CSV,,[income],CC4,,,533,4510,3,A survey about Europeans' opinions toward basic income,Basic Income Survey - 2016 European Dataset,https://www.kaggle.com/daliaresearch/basic-income-survey-european-dataset,Wed May 10 2017
,U.S. National Archives and Records Administration,"[identifier, source_code, source_citation, source_index_number, title_or_description_from_source, date_approximation, year, month, day, congress, congressional_session, joint_resolution_chamber, joint_resolution_number, sponsor_name, sponsor_state_or_territory, committee_of_referral, last_modified]","[string, string, string, numeric, string, string, numeric, numeric, numeric, string, string, string, string, string, string, string, dateTime]","Context Article Five of the United States Constitution describes a process that allows for alteration of the federal Constitution. So far 27 amendments have been fully added to the federal Constitution but there have been a lot of proposals that didn’t make it through. This dataset contains information about a whopping 11797 Constitutional amendments that have been proposed to Congress from 1787 to 2014!  Content This dataset consists of 11797 rows with 17 fields and was compiled by NARA volunteers that transcribed information from written records that were issued by Congress. Because a lot of the information comes from written records the dataset may not be a complete record of all amendment proposals. Proposals before 1973 were taken from various government publications and proposals after 1973 are publicly available on https//www.congress.gov.  Identifier Unique code generated by NARA that serves as a unique identifier. Source_code Code generated by NARA that represents the government publication or website where the information was transcribed. Source_citation Bibliographic citation of the government publication or website from which the information was transcribed. Source_index_number An index number listed within the source publication. Title_or_description_from_source Title or description of the proposed amendment transcribed from the source publication or website. Date_approximation When the date is estimated the term includes “circa” otherwise it is left blank. Year Year that the amendment was proposed in Congress (YYYY format). Month Month that the amendment was proposed in Congress (blank if unknown). Day Day that the amendment was proposed in Congress (blank if unknown). Congress The number for the congress in which the amendment was proposed. Congressional_session The number or designation of the session of Congress in which the amendment was proposed. Joint_resolution_chamber The chamber of Congress in which the amendment was proposed. Joint_resolution_number The number assigned to the proposed amendment by Congress. (Amendments are submitted as joint resolutions) Sponsor_name The name of the member of Congress or group who proposed the amendment. (Blank if unknown). Sponsor_state_or_territory The U.S. state or territory represented by the amendment’s sponsoring member of Congress. Committee_of_referral The committee to which the amendment was referred for further consideration following formal introduction to Congress. Last_modified the timestamp of the most recent modification made on the data contained within the particular row.  Acknowledgements The National Archives and Records Administration created this dataset as part of the Amending America initiative. To prepare for the 2016 ""Amending America"" exhibition at the National Archives Museum in Washington D.C. NARA volunteers and staff transcribed and edited over 11000 entries representing proposed amendments to the U.S. Constitution as recorded by Congress. http//www.archives.gov/amending-america Inspiration This is an interesting dataset because it contains a lot of history that isn’t necessarily reflected in the Constitution. You could use it to glean insight into the political climates at different times in the history of the United States. For instance what kind of amendments were being proposed during the Civil War? Who proposed the most amendments? Historically have there been more proposals during any particular time of year?",CSV,,"[history, politics]",CC0,,,51,860,5,"11,000+ Proposed Amendments to the United States Constitution from 1787 to 2014",Amending America,https://www.kaggle.com/national-archives/amending-america,Fri Jun 23 2017
,Chicago Police Department,[],[],Content This dataset reflects the daily volume of speed violations that have been recorded by each camera installed in the City of Chicago as part of the Automated Speed Enforcement Program. The data reflects violations that occurred from July 1 2014 until December 31 2016. The reported violations were collected by the camera and radar system and reviewed by two separate city contractors. This dataset contains all violations regardless of whether a citation was issued. Acknowledgements The speed camera data was collected and published by the Chicago Police Department on the City of Chicago data portal website. Inspiration What neighborhood has the highest density of speed cameras? Do speed cameras capture more violations on weekdays or weekends? Which camera has captured the most violations? Has the number of speed violations recorded decreased over time?,CSV,,"[crime, vehicles]",CC0,,,372,3508,17,Daily volume of speed limit violations recorded by cameras on Chicago streets,"Speed Camera Violations in Chicago, 2014-2016",https://www.kaggle.com/chicagopolice/speed-violations,Tue Sep 12 2017
,Matt Snell,"[Senator, Party Affiliation, Full Question, Comey, Comey Response]","[string, string, string, string, string]",Context Data gathered from the James Comey testimony to the Senate Intelligence Committee on June 8th 2017 regarding possible Russian influence in the 2016 U.S. presidential election. Content Content includes the full transcript in transcript.csv as well as a breakdown of questions asked by each senator their political affiliation and Comey's response.  All CSVs are UTF-8. Acknowledgements Rod Castor - Initial Python script. AppliedAI. Inspiration Initially I did analysis to determine length of question by party length of Comey response by party number of times each word is used and words with a large difference by party. (Clinton used 16x more by Republicans).  Further analysis to follow as time permits.,CSV,,"[law, politics]",Other,,,113,1699,0.3544921875,Full transcript of Comey's Testimony to Senate Intelligence Committee,James Comey Testimony,https://www.kaggle.com/mattsnellaai/comeytestimony,Wed Jun 21 2017
,Ben Hamner,"[fips, area_name, state_abbreviation, PST045214, PST040210, PST120214, POP010210, AGE135214, AGE295214, AGE775214, SEX255214, RHI125214, RHI225214, RHI325214, RHI425214, RHI525214, RHI625214, RHI725214, RHI825214, POP715213, POP645213, POP815213, EDU635213, EDU685213, VET605213, LFE305213, HSG010214, HSG445213, HSG096213, HSG495213, HSD410213, HSD310213, INC910213, INC110213, PVY020213, BZA010213, BZA110213, BZA115213, NES010213, SBO001207, SBO315207, SBO115207, SBO215207, SBO515207, SBO415207, SBO015207, MAN450207, WTN220207, RTN130207, RTN131207, AFN120207, BPS030214, LND110210, POP060210]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","This contains data relevant for the 2016 US Presidential Election including up-to-date primary results.   Exploration Ideas  What candidates within the Republican party have results that are the most anti-correlated? Which Republican candidate is Hillary Clinton most correlated with based on county voting patterns? What about Bernie Sanders? What insights can you discover by mapping this data?  Do you have answers or other exploration ideas? Add your ideas to this forum post and share your insights through Kaggle Scripts! Do you think that we should augment this dataset with more data sources? Submit a pull request to this repo or let us know here! Data Description The 2016 US Election dataset contains several main files and folders at the moment. You may download the entire archive via the ""Download Data"" link at the top of the page or interact with the data in Kaggle Scripts through the ../input directory. Original Data Sources  Primary Results from CNN New Hampshire County-Level Results County Shapefiles County QuickFacts ",CSV,,[politics],CC4,,,17274,140178,48,Explore data related to the 2016 US Election,2016 US Election,https://www.kaggle.com/benhamner/2016-us-election,Fri Jul 01 2016
,Venkat Ramakrishnan,"[State Name, District Name, Block Name, Panchayat Name, Village Name, Habitation Name, Quality Parameter, Year]","[string, string, string, string, string, string, string, dateTime]",Context There has been increased interest among the public about the Environment and living conditions in India.  Especially after since many manufacturing units are being planned people are worried about how it will affect the underground water quality and the environment.  Government of India under the Ministry of Drinking Water and Sanitation has released the Water Quality Affected Data for 2009 2010 2011 and 2012.  The objective here is to analyze this data alongside with Forest Industries Habitation and development projects data in the same area (panchayat) to figure out whether there is any connection between the development effort and the quality of water getting affected.  This effort will identify such associations and create awareness such that people and the Govt. can act in time to avoid further deterioration of the water resources. Content Currently there is this data set of areas with affected water quality for the years 2009 2010 2011 and 2012.   Further datasets are expected for subsequent years.  These datasets identify the state district and specific localities in which water quality degradation has been reported in that particular year.  Focus should be on the area (Panchayat/Village) rather than the district or the state as a whole and observations should be made if there are any associations between the other sets of data available for the same area (from industrial habitation manufacturing and other sources which I intend to add here also). Acknowledgements My deep gratitude to the Government of India for making this data available through the Open Data initiative. Inspiration  Let's explore if there are any repetitive patterns of water quality degradation in the same area for multiple years. As a whole which areas in India has a lot of water quality degradation issues over the years (heat maps) Which chemical is predominantly present in most of the water quality issues (heat maps). And why (from the associations with other developmental data like industry manufacturing development initiatives housing habitation etc.) As a whole for the country is the water quality degrading or upgrading (number of instances reported of water quality getting affected)? Let's explore if there are any associations between the water quality data and the other developmental data.  If there is then what is the extent (visualisation) and how can we address it (prescriptive). Let's predict if there are GOING TO BE water quality issues in areas that are not affected right now based on the developmental and water quality data that is available right now. Prevention is better than cure!  It would be great if we could have water quality and industrial/development experts in this analysis so that they can contribute their valuable inputs!,CSV,,[],Other,,,1453,11537,41,Government data related to the water quality of India,India Water Quality Data,https://www.kaggle.com/venkatramakrishnan/india-water-quality-data,Sat Dec 31 2016
,Cam Nugent,"[Born, City, City, City, Pr/St, Pr/St, Pr/St, Cntry, Cntry, Cntry, Nat, Nat, Nat, Ht, Ht, Ht, Wt, Wt, Wt, DftYr, DftYr, DftYr, DftRd, DftRd, DftRd, Ovrl, Ovrl, Ovrl, Hand, Hand, Hand, Last Name, Last Name, Last Name, First Name, First Name, First Name, Position, Position, Position, Team, Team, Team, GP, GP, GP, G, G, G, A, A, A, A1, A1, A1, A2, A2, A2, PTS, PTS, PTS, +/-, +/-, +/-, E+/-, E+/-, E+/-, PIM, PIM, PIM, Shifts, Shifts, Shifts, TOI, TOI, TOI, TOIX, TOIX, TOIX, TOI/GP, TOI/GP, TOI/GP, TOI/GP, TOI/GP, TOI/GP, TOI%, TOI%, TOI%, IPP%, IPP%, IPP%, SH%, SH%, SH%, SV%, SV%, SV%, PDO, PDO, PDO]","[string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context & Content This dataset features the salaries of 874 nhl players for the 2016/2017 season. I have randomly split the players into a training (612 players) and test (262 players) populations. There are 151 predictor columns (described in column legend section if you're not familiar with hockey the meaning of some of these may be a bit cryptic!) as well as a leading column with the players 2016/2017 annual salary.  For the test population the actual salaries have been broken off into a separate .csv file. Acknowledgements Raw excel sheet was acquired http//www.hockeyabstract.com/ Inspiration Can you build a model to predict NHL player's salaries? What are the best predictors of how much a player will make? Column Legend Acronym - Meaning %FOT - Percentage of all on-ice faceoffs taken by this player. +/- - Plus/minus 1G - First goals of a game A/60 - Events Against per 60 minutes defaults to Corsi but can be set to another stat A1 - First assists primary assists A2 - Second assists secondary assists BLK% - Percentage of all opposing shot attempts blocked by this player Born - Birth date C.Close - A player shot attempt (Corsi) differential when the game was close C.Down - A player shot attempt (Corsi) differential when the team was trailing C.Tied - A player shot attempt (Corsi) differential when the team was tied C.Up - A player shot attempt (Corsi) differential when the team was in the lead CA - Shot attempts allowed (Corsi SAT) while this player was on the ice Cap Hit - The player's cap hit CBar  - Crossbars hit CF - The team's shot attempts (Corsi SAT) while this player was on the ice CF.QoC - A weighted average of the Corsi percentage of a player's opponents CF.QoT - A weighted average of the Corsi percentage of a player's linemates CHIP - Cap Hit of Injured Player is games lost to injury multiplied by cap hit per game City - City of birth Cntry - Country of birth DAP - Disciplined aggression proxy which is hits and takeaways divided by minor penalties DFA - Dangerous Fenwick against which is on-ice unblocked shot attempts weighted by shot quality DFF - Dangerous Fenwick for which is on-ice unblocked shot attempts weighted by shot quality DFF.QoC - Quality of Competition metric based on Dangerous Fenwick which is unblocked shot attempts weighted for shot quality DftRd - Round in which the player was drafted DftYr - Year drafted Diff - Events for minus event against defaults to Corsi but can be set to another stat Diff/60 - Events for minus event against per 60 minutes defaults to Corsi but can be set to another stat DPS - Defensive point shares a catch-all stats that measures a player's defensive contributions in points in the standings DSA - Dangerous shots allowed while this player was on the ice which is rebounds plus rush shots DSF - The team's dangerous shots while this player was on the ice which is rebounds plus rush shots DZF - Shifts this player has ended with an defensive zone faceoff dzFOL - Faceoffs lost in the defensive zone dzFOW - Faceoffs win in the defensive zone dzGAPF - Team goals allowed after faceoffs taken in the defensive zone dzGFPF - Team goals scored after faceoffs taken in the defensive zone DZS - Shifts this player has started with an defensive zone faceoff dzSAPF - Team shot attempts allowed after faceoffs taken in the defensive zone dzSFPF - Team shot attempts taken after faceoffs taken in the defensive zone E+/- - A player's expected +/- based on his team and minutes played ENG - Empty-net goals Exp dzNGPF - Expected goal differential after faceoffs taken in the defensive zone based on the number of them Exp dzNSPF - Expected shot differential after faceoffs taken in the defensive zone based on the number of them Exp ozNGPF - Expected goal differential after faceoffs taken in the offensive zone based on the number of them Exp ozNSPF - Expected shot differential after faceoffs taken in the offensive zone based on the number of them F.Close - A player unblocked shot attempt (Fenwick) differential when the game was close F.Down - A player unblocked shot attempt (Fenwick) differential when the team was trailing F.Tied - A player unblocked shot attempt (Fenwick) differential when the team was tied F.Up - A player unblocked shot attempt (Fenwick) differential when the team was in the lead. Not the best acronym. F/60 - Events For per 60 minutes defaults to Corsi but can be set to another stat FA - Unblocked shot attempts allowed (Fenwick USAT) while this player was on the ice FF - The team's unblocked shot attempts (Fenwick USAT) while this player was on the ice First Name -  FO% - Faceoff winning percentage FO%vsL - Faceoff winning percentage against lefthanded opponents FO%vsR - Faceoff winning percentage against righthanded opponents FOL - The team's faceoff losses while this player was on the ice FOL.Close - Faceoffs lost when the score was close FOL.Down - Faceoffs lost when the team was trailing FOL.Up - Faceoffs lost when the team was in the lead FovsL - Faceoffs taken against lefthanded opponents FovsR - Faceoffs taken against righthanded opponents FOW - The team's faceoff wins while this player was on the ice FOW.Close - Faceoffs won when the score was close FOW.Down - Faceoffs won when the team was trailing FOW.Up - Faceoffs won when the team was in the lead G - Goals G.Bkhd - Goals scored on the backhand G.Dflct - Goals scored with deflections G.Slap - Goals scored with slap shots G.Snap - Goals scored with snap shots G.Tip - Goals scored with tip shots G.Wrap - Goals scored with a wraparound G.Wrst - Goals scored with a wrist shot GA - Goals allowed while this player was on the ice Game - Game Misconduct penalties GF - The team's goals while this player was on the ice GP - Games Played Grit - Defined as hits blocked shots penalty minutes and majors GS - The player's combined game score GS/G - The player's average game score GVA - The team's giveaways while this player was on the ice GWG - Game-winning goals GWG - Game-winning goals HA - The team's hits taken while this player was on the ice Hand - Handedness HF - The team's hits thrown while this player was on the ice HopFO - Opening faceoffs taken at home HopFOW - Opening faceoffs won at home Ht - Height iBLK - Shots blocked by this individual iCF - Shot attempts (Corsi SAT) taken by this individual iDS - Dangerous shots taken by this player the sum of rebounds and shots off the rush iFF - Unblocked shot attempts (Fenwick USAT) taken by this individual iFOL - Faceoff losses by this individual iFOW - Faceoff wins by this individual iGVA - Giveaways by this individual iHA - Hits taken by this individual iHDf - The difference in hits thrown by this individual minus those taken iHF - Hits thrown by this individual iMiss - Individual shots taken that missed the net. Injuries - List of types of injuries incurred if any iPEND - Penalties drawn by this individual iPenDf - The difference in penalties drawn minus those taken iPENT - Penalties taken by this individual IPP% - Individual points percentage which is on-ice goals for which this player had the goal or an assist iRB - Rebound shots taken by this individual iRS - Shots off the rush taken by this individual iSCF - All scoring chances taken by this individual iSF - Shots on goal taken by this individual iTKA - Takeaways by this individual ixG - Expected goals (weighted shots) for this individual which is shot attempts weighted by shot location Last Name -  Maj - Major penalties taken Match - Match penalties MGL - Games lost due to injury Min - Minor penalties taken Misc - Misconduct penalties Nat - Nationality NGPF - Net Goals Post Faceoff. A differential of all goals within 10 seconds of a faceoff relative to expectations set by the zone in which they took place NHLid - NHL player id useful when looking at the raw data in game files NMC - What kind of no-movement clause this player's contract has if any NPD - Net Penalty Differential is the player's penalty differential relative to a player of the same position with the same ice time per manpower situation NSPF - Net Shots Post Faceoff. A differential of all shot attempts within 10 seconds of a faceoff relative to expectations set by the zone in which they took place NZF - Shifts this player has ended with a neutral zone faceoff nzFOL - Faceoffs lost in the neutral zone nzFOW - Faceoffs won in the neutral zone nzGAPF - Team goals allowed after faceoffs taken in the neutral zone nzGFPF - Team goals scored after faceoffs taken in the neutral zone NZS - Shifts this player has started with a neutral zone faceoff nzSAPF - Team shot attempts allowed after faceoffs taken in the neutral zone nzSFPF - Team shot attempts taken after faceoffs taken in the neutral zone OCA - Shot attempts allowed (Corsi SAT) while this player was not on the ice OCF - The team's shot attempts (Corsi SAT) while this player was not on the ice ODZS - Defensive zone faceoffs that occurred without this player on the ice OFA - Unblocked shot attempts allowed (Fenwick USAT) while this player was not on the ice OFF - The team's unblocked shot attempts (Fenwick USAT) while this player was not on the ice OGA - Goals allowed while this player was not on the ice OGF - The team's goals while this player was not on the ice ONZS - Neutral zone faceoffs that occurred without this player on the ice OOZS - Offensive zone faceoffs that occurred without this player on the ice OpFO - Opening faceoffs taken OpFOW - Opening faceoffs won OppCA60 - A weighted average of the shot attempts (Corsi SAT) the team allowed per 60 minutes of a player's opponents OppCF60 - A weighted average of the shot attempts (Corsi SAT) the team generated per 60 minutes of a player's opponents OppFA60 - A weighted average of the unblocked shot attempts (Fenwick USAT) the team allowed per 60 minutes of a player's opponents OppFF60 - A weighted average of the unblocked shot attempts (Fenwick USAT) the team generated per 60 minutes of a player's opponents OppGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's opponents OppGF60 - A weighted average of the goals the team scored per 60 minutes of a player's opponents OppSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's opponents OppSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's opponents OPS - Offensive point shares a catch-all stats that measures a player's offensive contributions in points in the standings OSA - Shots on goal allowed while this player was not on the ice OSCA - Scoring chances allowed while this player was not on the ice OSCF - The team's scoring chances while this player was not on the ice OSF - The team's shots on goal while this player was not on the ice OTF - Shifts this player started with an on-the-fly change OTG - Overtime goals OTOI - The amount of time this player was not on the ice. Over - Shots that went over the net Ovrl - Where the player was drafted overall OxGA - Expected goals allowed (weighted shots) while this player was not on the ice which is shot attempts weighted by location OxGF - The team's expected goals (weighted shots) while this player was not on the ice which is shot attempts weighted by location OZF - Shifts this player has ended with an offensive zone faceoff ozFO - Faceoffs taken in the offensive zone ozFOL - Faceoffs lost in the offensive zone ozFOW - Faceoffs won in the offensive zone ozGAPF - Team goals allowed after faceoffs taken in the offensive zone ozGFPF - Team goals scored after faceoffs taken in the offensive zone OZS - Shifts this player has started with an offensive zone faceoff ozSAPF - Team shot attempts allowed after faceoffs taken in the offensive zone ozSFPF - Team shot attempts taken after faceoffs taken in the offensive zone Pace - The average game pace as estimated by all shot attempts per 60 minutes Pass - An estimate of the player's setup passes (passes that result in a shot attempt) Pct% - Percentage of all events produced by this team defaults to Corsi but can be set to another stat PDO - The team's shooting and save percentages added together times a thousand PEND - The team's penalties drawn while this player was on the ice PENT - The team's penalties taken while this player was on the ice PIM - Penalties in minutes Position - Positions played. NHL source listed first followed by those listed by any other source. Post - Times hit the post Pr/St - Province or state of birth PS - Point shares a catch-all stats that measures a player's contributions in points in the standings PSA - Penalty shot attempts PSG - Penalty shot goals PTS - Points. Goals plus all assists PTS/60 - Points per 60 minutes QRelCA60  - Shot attempts allowed per 60 minutes relative to how others did against the same competition QRelCF60  - Shot attempts per 60 minutes relative to how others did against the same competition QRelDFA60  - Weighted unblocked shot attempts (Dangeorus Fenwick)  allowed per 60 minutes relative to how others did against the same competition QRelDFF60  - Weighted unblocked shot attempts (Dangeorus Fenwick) per 60 minutes relative to how others did against the same competition RBA - Rebounds allowed while this player was on the ice. Two very different sources. RBF - The team's rebounds while this player was on the ice. Two very different sources. RelA/60 - The player's A/60 relative to the team when he's not on the ice RelC/60 - Corsi differential per 60 minutes relative to his team RelC% - Corsi percentage relative to his team RelDf/60 - The player's Diff/60 relative to the team when he's not on the ice RelF/60 - The player's F/60 relative to the team when he's not on the ice RelF/60 - Fenwick differential per 60 minutes relative to his team RelF% - Fenwick percentage relative to his team RelPct% - The players Pct% relative to the team when he's not on the ice RelZS% - The player's zone start percentage when he's on the ice relative to when he's not. RopFO - Opening faceoffs taken at home RopFOW - Opening faceoffs won at home RSA - Shots off the rush allowed while this player was on the ice RSF - The team's shots off the rush while this player was on the ice S.Bkhd - Backhand shots S.Dflct - Deflections S.Slap - Slap shots S.Snap - Snap shots S.Tip - Tipped shots S.Wrap - Wraparound shots S.Wrst - Wrist shots SA - Shots on goal allowed while this player was on the ice Salary - The player's salary SCA - Scoring chances allowed while this player was on the ice SCF - The team's scoring chances while this player was on the ice sDist - The average shot distance of shots taken by this player SF - The team's shots on goal while this player was on the ice SH% - The team's (not individual's) shooting percentage when the player was on the ice SOG - Shootout Goals SOGDG - Game-deciding shootout goals SOS - Shootout Shots Status - This player's free agency status SV% - The team's save percentage when the player was on the ice Team -  TKA - The team's takeaways while this player was on the ice TMCA60 - A weighted average of the shot attempts (Corsi SAT) the team allowed per 60 minutes of a player's linemates TMCF60 - A weighted average of the shot attempts (Corsi SAT) the team generated per 60 minutes of a player's linemates TMFA60 - A weighted average of the unblocked shot attempts (Fenwick USAT) the team allowed per 60 minutes of a player's linemates TMFF60 - A weighted average of the unblocked shot attempts (Fenwick USAT) the team generated per 60 minutes of a player's linemates TMGA60 - A weighted average of the goals the team allowed per 60 minutes of a player's linemates TMGF60 - A weighted average of the goals the team scored per 60 minutes of a player's linemates TMSA60 - A weighted average of the shots on goal the team allowed per 60 minutes of a player's linemates TMSF60 - A weighted average of the shots on goal the team generated per 60 minutes of a player's linemates TmxGF - A weighted average of a player's linemates of the expected goals the team scored TmxGA - A weighted average of a player's linemates of the expected goals the team allowed TMGA - A weighted average of a player's linemates of the goals the team scored TMGF - A weighted average of a player's linemates of the goals the team allowed TOI - Time on ice in minutes or in seconds (NHL) TOI.QoC - A weighted average of the TOI% of a player's opponents. TOI.QoT - A weighted average of the TOI% of a player's linemates. TOI/GP - Time on ice divided by games played TOI% - Percentage of all available ice time assigned to this player. Wide - Shots that went wide of the net Wt - Weight xGA - Expected goals allowed (weighted shots) while this player was on the ice which is shot attempts weighted by location xGF - The team's expected goals (weighted shots) while this player was on the ice which is shot attempts weighted by location xGF.QoC - A weighted average of the expected goal percentage of a player's opponents xGF.QoT - A weighted average of the expected goal percentage of a player's linemates ZS% - Zone start percentage the percentage of shifts started in the offensive zone not counting neutral zone or on-the-fly changes,CSV,,"[sports, money]",CC0,,,543,4161,0.427734375,Build a model to predict the salaries of NHL players based on player data,Predict NHL Player Salaries,https://www.kaggle.com/camnugent/predict-nhl-player-salaries,Fri Aug 18 2017
,World Bank,"[Country Name, Country Code, Indicator Name, Indicator Code, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, ]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Focuses on financial flows trends in external debt and other major financial indicators for developing and advanced economies (data from Quarterly External Debt Statistics and Quarterly Public Sector Debt databases). Includes over 200 time series indicators from 1970 to 2014 for most reporting countries and pipeline data for scheduled debt service payments on existing commitments to 2022. Content This dataset contains country names and indicator variables from 1970 until 2024. Additional materials and detailed descriptions of the datasets can be downloaded from here. Acknowledgement The original datasets and data dictionaries can be found here. Inspiration Few ideas for exploring the dataset  Compare the current account balance across countries. Is there a pattern associated with developing vs. advanced economies? How have the debt-related indicators changed over time? Are these strongly associated with other financial indicators? ,CSV,,[finance],Other,,,1001,6997,14,Major Financial Indicators from Developing and Advanced Economies,International Debt Statistics,https://www.kaggle.com/theworldbank/international-debt-statistics,Fri Nov 25 2016
,Franklin Bradfield,"[Airline Code, Airline Name]","[string, string]","Context Have you taken a flight in the U.S. in the past 15 years? If so then you are a part of monthly data that the U.S. Department of Transportation's TranStats service makes available on various metrics for 15 U.S. airlines and 30 major U.S airports. Their website unfortunately does not include a method for easily downloading and sharing files. Furthermore the source is built in ASP.NET so extracting the data is rather cumbersome. To allow easier community access to this rich source of information I scraped the metrics for every airline / airport combination and stored them in separate CSV files. Occasionally an airline doesn't serve a certain airport or it didn't serve it for the entire duration that the data collection period covers*. In those cases the data either doesn't exist or is typically too sparse to be of much use. As such I've only uploaded complete files for airports that an airline served for the entire uninterrupted duration of the collection period. For these files there should be 174 time series points for one or more of the nine columns below. I recommend any of the files for American Delta or United Airlines for outstanding examples of complete and robust airline data. * No data for Atlas Air exists and Virgin America commenced service in 2007  so no folders for either airline are included. Content There are 13 airlines that have at least one complete dataset. Each airline's folder includes CSV file(s) for each airport that are complete as defined by the above criteria. I've double-checked the files but if you find one that violates the criteria please point it out. The file names have the format ""AIRLINE-AIRPORT.csv"" where both AIRLINE and AIRPORT are IATA codes. For a full listing of the airlines and airports that the codes correspond to check out the airline_codes.csv or airport_codes.csv files that are included or perform a lookup here. Note that the data in each airport file represents metrics for flights that originated at the airport. Among the 13 airlines in data.zip there are a total of 161 individual datasets. There are also two special folders included - airlines_all_airports.csv and airports_all_airlines.csv. The first contains datasets for each airline aggregated over all airports while the second contains datasets for each airport aggregated over all airlines. To preview a sample dataset check out all_airlines_all_airports.csv which contains industry-wide data. Each file includes the following metrics for each month from October 2002 to March 2017  Date (YYYY-MM-DD) All dates are set to the first of the month. The day value is just a placeholder and has no significance. ASM_Domestic Available Seat-Miles in thousands (000s). Number of domestic flights * Number of seats on each flight ASM_International* Available Seat-Miles in thousands (000s). Number of international flights * Number of seats on each flight Flights_Domestic  Flights_International* Passengers_Domestic Passengers_International* RPM_Domestic Revenue Passenger-Miles in thousands (000s). Number of domestic flights * Number of paying passengers RPM_International* Revenue Passenger-Miles in thousands (000s). Number of international  flights * Number of paying passengers  * Frequently contains missing values Acknowledgements Thanks to the U.S. Department of Transportation for collecting this data every month and making it publicly available to us all. Source  https//www.transtats.bts.gov/Data_Elements.aspx Inspiration The airline / airport datasets are perfect for practicing and/or testing time series forecasting with classic statistical models such as autoregressive integrated moving average (ARIMA) or modern deep learning techniques such as long short-term memory (LSTM) networks. The datasets typically show evidence of trends seasonality and noise so modeling and accurate forecasting can be challenging but still more tractable than time series problems possessing more stochastic elements e.g. stocks currencies commodities etc. The source releases new data each month so feel free to check your models' performances against new data as it comes out. I will update the files here every 3 to 6 months depending on how things go.  A future plan is to build a SQLite database so a vast array of queries can be run against the data. The data in it its current time series format is not conducive for this so coming up with a workable structure for the tables is the first step towards this goal. If you have any suggestions for how I can improve the data presentation or anything that you would like me to add please let me know. Looking forward to seeing the questions that we can answer together!",CSV,,"[transport, aviation, vehicles]",CC0,,,352,2214,2,"Monthly passengers, flights, seat-miles, and revenue-miles from 2002 to 2017",U.S. Commercial Aviation Industry Metrics,https://www.kaggle.com/shellshock1911/us-commercial-aviation-industry-metrics,Thu Jul 13 2017
,foenix,"[case, case, date cleared, call description, location, police zone, police grid, city council, x-coordinate, y-coordinate, x_gps_coords, y_gps_coords]","[string, string, dateTime, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context A collection of SLC End-of-Year Crime Reports geocoded to standard GPS coordinates. Content 2016 Crime Statistics for Salt Lake City UT. Includes  Case Numbers Offence Codes for categorization Descriptions for context IBR (National Incident-Based Reporting System Number) Occurrence Date Report Date Day of the Week (1 = Monday 7 = Sunday) Location (Addresses in SLC) City Council District SLCPD Police Zones SLCPD Grid x_coordinate note that this is based on epsg32043 projections y_coordinate note that this is based on epsg32043 projections x_gps_coords (added by yours truly converted to epsg4326) y_gps_coords (added by yours truly converted to epsg4326)  Data Accuracy Notes  Some data wrangling will still likely be required to clean up null columns.  I went ahead and lowercased column names (and corrected a spelling mistake in the y-coordinate column). epsg32043 projections were converted to epsg4326 projections using pyplot with distances preserved. Multiple year munging performed here https//github.com/octaflop/slcpd/blob/master/develop/2017-08-16-crunch.ipynb Still awaiting dataset owner clarification of Calls vs Cases  Acknowledgements Taken from the SLC Open Data Web Site.  Thank you Dean Larson the original dataset owner. Thank you to the City of Salt Lake government and the Utah.gov catalog for providing this data for public use. Thanks to the DAT Science EdEx course for inspiring me to take a look at my own city's crime stats. Thank you to the SLCPD for keeping Salt Lake City citizens safe and enforcing an internal discipline of open data-collection. Inspiration  Crime report locations by season?  Cross Reference of city council districts Time of day Offence descriptions Moving centroids based on time of day / season? Holiday rowdiness?  Coming Soon  Full 2016 reports (eta Spring 2017) ✔ 3-year combined reports (eta Summer 2017) ✔ 3-year combined cases vs calls (eta Summer 2017) year-by-year files (eta Fall 2017) ,CSV,,[crime],CC0,,,232,3092,216,"Includes latitude and longitudes, offence descriptions, and city council numbers",Salt Lake City Crime Reports,https://www.kaggle.com/foenix/slc-crime,Sat Aug 19 2017
,"Marcell ""Mazuh"" Guilherme Costa da Silva","[index, name, cpf, campus, class, situationBond, organizationalUnit, campus, hasTrustPosition, employeeSince, urlRemunerationSufix]","[numeric, string, string, string, string, string, string, string, boolean, dateTime, string]","Context Scraped data from our local federal colleges UFRN and IFRN mostly about their expenses. Soon there will be data about any other federal college.  This project (without the datasets only scripts) is available on GitHub https//github.com/mazuh/midas  Content At first there will be only data about all IFRN's workers their contractual profile and positions. Soon there will be data about their salary and remunerations. Acknowledgements Here in Brazil since 2011 the law 12.527/2011 specifies the constitutional right of every citizen to know better the public expenses. Therefore this project is entirely legal and isn't ""leaking"" anything.  Inspiration My first question about this dataset is to check remuneration profiles of our teachers. They often complain about their salary and college budget.",CSV,,"[universities and colleges, brazil, education]",CC0,,,24,731,0.7392578125,"Federal colleges expenses (RN, BR)",Midas Project,https://www.kaggle.com/mazuh69/midas-project,Tue Sep 05 2017
,kajot,"[RefSeq_accession_number, gene_symbol, gene_name, AO-A12D.01TCGA, C8-A131.01TCGA, AO-A12B.01TCGA, BH-A18Q.02TCGA, C8-A130.02TCGA, C8-A138.03TCGA, E2-A154.03TCGA, C8-A12L.04TCGA, A2-A0EX.04TCGA, AO-A12D.05TCGA, AN-A04A.05TCGA, BH-A0AV.05TCGA, C8-A12T.06TCGA, A8-A06Z.07TCGA, A2-A0CM.07TCGA, BH-A18U.08TCGA, A2-A0EQ.08TCGA, AR-A0U4.09TCGA, AO-A0J9.10TCGA, AR-A1AP.11TCGA, AN-A0FK.11TCGA, AO-A0J6.11TCGA, A7-A13F.12TCGA, BH-A0E1.12TCGA, A7-A0CE.13TCGA, A2-A0YC.13TCGA, AO-A0JC.14TCGA, A8-A08Z.14TCGA, AR-A0TX.14TCGA, A8-A076.15TCGA, AO-A126.15TCGA, BH-A0C1.16TCGA, A2-A0EY.16TCGA, AR-A1AW.17TCGA, AR-A1AV.17TCGA, C8-A135.17TCGA, A2-A0EV.18TCGA, AN-A0AM.18TCGA, D8-A142.18TCGA, AN-A0FL.19TCGA, BH-A0DG.19TCGA, AR-A0TV.20TCGA, C8-A12Z.20TCGA, AO-A0JJ.20TCGA, AO-A0JE.21TCGA, AN-A0AJ.21TCGA, A7-A0CJ.22TCGA, AO-A12F.22TCGA, A8-A079.23TCGA, A2-A0T3.24TCGA, A2-A0YD.24TCGA, AR-A0TR.25TCGA, AO-A03O.25TCGA, AO-A12E.26TCGA, A8-A06N.26TCGA, A2-A0YG.27TCGA, BH-A18N.27TCGA, AN-A0AL.28TCGA, A2-A0T6.29TCGA, E2-A158.29TCGA, E2-A15A.29TCGA, AO-A0JM.30TCGA, C8-A12V.30TCGA, A2-A0D2.31TCGA, C8-A12U.31TCGA, AR-A1AS.31TCGA, A8-A09G.32TCGA, C8-A131.32TCGA, C8-A134.32TCGA, A2-A0YF.33TCGA, BH-A0DD.33TCGA, BH-A0E9.33TCGA, AR-A0TT.34TCGA, AO-A12B.34TCGA, A2-A0SW.35TCGA, AO-A0JL.35TCGA, BH-A0BV.35TCGA, A2-A0YM.36TCGA, BH-A0C7.36TCGA, A2-A0SX.36TCGA, 263d3f-I.CPTAC, blcdb9-I.CPTAC, c4155b-C.CPTAC]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This data set contains published iTRAQ proteome profiling of 77 breast cancer samples generated by the Clinical Proteomic Tumor Analysis Consortium (NCI/NIH). It contains expression values for ~12.000 proteins for each sample with missing values present when a given protein could not be quantified in a given sample. Content File 77_cancer_proteomes_CPTAC_itraq.csv  RefSeq_accession_number RefSeq protein ID (each protein has a unique ID in a RefSeq database) gene_symbol a symbol unique to each gene (every protein is encoded by some gene) gene_name a full name of that gene Remaining columns log2 iTRAQ ratios for each sample (protein expression data most important) three last columns are from healthy individuals  File clinical_data_breast_cancer.csv First column ""Complete TCGA ID"" is used to match the sample IDs in the main cancer proteomes file (see example script). All other columns have self-explanatory names contain data about the cancer classification of a given sample using different methods. 'PAM50 mRNA' classification is being used in the example script.  File PAM50_proteins.csv Contains the list of genes and proteins used by the PAM50 classification system. The column RefSeqProteinID contains the protein IDs that can be matched with the IDs in the main protein expression data set. Past Research The original study http//www.nature.com/nature/journal/v534/n7605/full/nature18003.html (paywall warning) In brief the data were used to assess how the mutations in the DNA are affecting the protein expression landscape in breast cancer. Genes in our DNA are first transcribed into RNA molecules which then are translated into proteins. Changing the information content of DNA has impact on the behavior of the proteome which is the main functional unit of cells taking care of cell division DNA repair enzymatic reactions and signaling etc. They performed K-means clustering on the protein data to divide the breast cancer patients into sub-types each having unique protein expression signature. They found that the best clustering was achieved using 3 clusters (original PAM50 gene set yields four different subtypes using RNA data). Inspiration This is an interesting study and I myself wanted to use this breast cancer proteome data set for other types of analyses using machine learning that I am performing as a part of my PhD. However I though that the Kaggle community (or at least that part with biomedical interests) would enjoy playing with it. I added a simple K-means clustering example for that data with some comments the same approach as used in the original paper. One thing is that there is a panel of genes the PAM50 which is used to classify breast cancers into subtypes. This panel was originally based on the RNA expression data which is (in my opinion) not as robust as the measurement of mRNA's final product the protein. Perhaps using this data set someone could find a different set of proteins (they all have unique NP_/XP_ identifiers) that would divide the data set even more robustly? Perhaps into a higher numbers of clusters with very distinct protein expression signatures? Example K-means analysis script http//pastebin.com/A0Wj41DP",CSV,,[healthcare],Other,,,4437,38699,12,Dividing breast cancer patients into separate sub-classes,Breast Cancer Proteomes,https://www.kaggle.com/piotrgrabo/breastcancerproteomes,Sun Jul 03 2016
,Marc Velmer,"[Dte., Barris, Poblaci�
16-64 anys, Gener, Febrer, Mar�, Abril, Maig, Juny, Juliol, Agost, Setembre, Octubre, Novembre, Desembre]","[numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset represents the % of registered unemployment in the city of Barcelona (Spain) from year 2012 till 2016. Registered unemployment corresponds to the job demands pending cover by the last day of each month excluding employees who want to change jobs the ones that do not have readily available or incompatible situation the ones that are asking for a specific occupation and the temporary agricultural beneficiaries special unemployment benefit.  Content All files in this dataset have the same format. Every row represents a hood from the city.  District number Hood name Number of citizens from this hood with ages between 16 and 64 (legal ages for having a job) 12 columns (one per month) % of unemployment  In Barcelona we have hoods and districts. Every hood belongs to a district. A district is formed by several hoods. Acknowledgements This data can be found in ""Open Data BCN - Barcelona's City Hall Open Data Service"" which is the owner of the CSV files. Inspiration A few weeks ago I needed this datasets for testing purposes. I have uploaded this information here because in my honest opinion ""data"" and ""research"" should be shared with everybody. Enjoy!",CSV,,"[business, finance]",CC4,,,81,988,0.0400390625,Barcelona registered unemployment percentages by hood and month,Barcelona Unemployment,https://www.kaggle.com/marcvelmer/barcelona-unemployment,Tue Sep 12 2017
,Sumit Kothari,[],[],Context While searching for Alpha-Numeric handwritten dataset I came across HASYv2 dataset but it contains lot of other classes too. So I filtered the dataset which now contains only Alpha-Numeric handwritten data. Content The dataset contains 2 numpy files and 1 csv file  alphanum-hasy-data-X.npy  Contains images data-set with size (4658 32 32) alphanum-hasy-data-y.npy  Contains corresponding labels data-set with size (4658) symbols.csv  Contains mapping between symbol_id and its symbol (i.e digit/char)  More information at AlphaNum-HASYv2 Github repository.  Acknowledgements HASYv2  https//arxiv.org/abs/1701.08380,Other,,[],CC0,,,175,1640,5,Mini version of HASYv2 Dataset,Alpha-Numeric Handwritten Dataset,https://www.kaggle.com/usersumit/alphanumeric-handwritten-dataset,Tue Aug 15 2017
,Vered Shwartz,"[action bronson, rapper, TRUE]","[string, string, boolean]","Context Recognizing lexical inference is an important component in semantic tasks. Various lexical semantic relations such as synonomy class membership part-of and causality may be used to infer the meaning of one word from another in order to address lexical variability. As many of the existing lexical inference datasets are constructed from WordNet important linguistic components that are missing from them are proper-names (Lady Gaga) and recent terminology (social networks). This dataset contains both components.  To construct the dataset we sampled articles from different topics in online magazines. As candidate (x y) pairs we extracted pairs of noun phrases x and y that belonged to the same paragraph in the original text selecting those in which x is a proper-name. These pairs were manually annotated. To balance the ratio of positive and negative pairs in the dataset we sampled negative examples according to the frequency of y in positive pairs creating ""harder"" negative examples such as (Sherlock lady) and (Kylie Minogue vice president). Content This dataset contains pairs of (x y) terms in which x is a proper-name and y is a common noun annotated to whether x is a y. For instance (Lady Gaga singer) is true but (Lady Gaga film) is false.  Files  full_dataset.csv the full dataset train.csv the training set test.csv the test set validation.csv the validation set  Each file is a comma-separated file with the following format  x the x term (proper-name) y the y term (common noun) label TRUE if x is a y else FALSE  Acknowledgements If you use the dataset for any published research please include the following citation ""Learning to Exploit Structured Resources for Lexical Inference"". Vered Shwartz Omer Levy Ido Dagan and Jacob Goldberger. CoNLL 2015.",CSV,,[],GPL,,,59,531,0.076171875,Is-A relation between a name (e.g. Lady Gaga) and a common noun (e.g. singer),Proper-names Categories,https://www.kaggle.com/vered1986/propernames-categories,Sun Aug 13 2017
,ArjoonnSharma,[],[],In the pursuit of any goal the first step is invariably data collection. As put up on the OpenAI blog writing a program which can write other programs is an incredibly important problem. This dataset collects publicly available information from the Codechef site's practice section to provide about 1000 problem statements and a little over 1 million solutions in total to these problems in various languages. The ultimate aim is to allow a program to learn program generation in any language to satisfy a given problem statement.,CSV,,[programming],CC0,,,308,5371,1024,Problem statements and solutions provided by people on the codechef site,Codechef Competitive Programming,https://www.kaggle.com/arjoonn/codechef-competitive-programming,Tue Jan 24 2017
,Electoral Commission,"[Type, Code, Area, All Residents, Age 0 to 4, Age 5 to 9, Age 10 to 14, Age 15 to 19, Age 20 to 24, Age 25 to 29, Age 30 to 34, Age 35 to 39, Age 40 to 44, Age 45 to 49, Age 50 to 54, Age 55 to 59, Age 60 to 64, Age 65 to 69, Age 70 to 74, Age 75 to 79, Age 80 to 84, Age 85 to 89, Age 90 and Over]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context A referendum was held on the 23 June 2016 to decide whether the United Kingdom should remain a member of the European Union or leave. Approximately 52% or more than 17 million people voted to leave the EU. The referendum turnout was 72.2% with more than 33.5 million votes cast. Content The Electoral Commission published the results of the EU referendum by district and region after the vote. The Office of National Statistics provided the population demographics by district from the 2011 United Kingdom Census.,CSV,,"[politics, demographics]",CC0,,,548,3943,0.11328125,How did population demographics impact the Brexit vote?,2016 EU Referendum in the United Kingdom,https://www.kaggle.com/electoralcommission/brexit-results,Wed Jan 18 2017
,Allan Scott,"[mystery_type, mystery_name, mystery_grace, days_prayed]","[string, string, string, string]",Context This dataset has been uploaded primarily to help me a novice learning Python to practice coding before attempting the tutorials on Kaggle.  By all means anyone may make use of it.  For my part I'm trying to code a simple program that will print all prayers applicable to whatever sets of Rosary Mysteries someone wishes to pray.  Content A traditional Dominican Crown Rosary comprises 15 'decades' of 10 Hail Mary prayers with a few others.  In addition there are variable introductory prayers ('the Drop') and concluding prayers.  These 15 decades are made up of three groups of 'Mysteries' - Joyful Sorrowful and Glorious.  Usually one prays a five decade Rosary focusing on just one group according to the day of the week.  One CSV file contains the prayers and the other contains the mysteries. Acknowledgements I cobbled this public domain prayers from various sites; a few good ones are  http//www.preces-latinae.org/thesaurus/BVM/Rosarium.html https//www.fisheaters.com/rosary.html  Inspiration I've been having difficulties making simple input scripts work in Python (repeated EOF errors) asking the user to indicate how many decades he/she wishes to pray and which set of mysteries should be prayed first.  I have some private code uploaded but would be interested in how others do it. ,CSV,,"[languages, faith and traditions]",CC0,,,17,437,0.00390625,"All traditional Latin prayers of the Rosary, and the associated Mysteries",Rosary Prayers in Latin,https://www.kaggle.com/allanscott/rosary-prayers-in-latin,Sat Sep 16 2017
,figshare,"[Abbreviation, Meaning ]","[string, string]",This comma-separated text file contains the 27723 alien bird records that form the core of the Global AVian Invasions Atlas (GAVIA) project. These records represent 971 species introduced to 230 countries and administrative areas across all eight biogeographical realms spanning the period 6000 BCE – AD 2014. The data comprises taxonomic (species-level) spatial (geographic location realm land type) and temporal (dates of introduction and spread) components as well as details relating to the introduction event (how and why the species was introduced whether or not it is established). Each line of data consists of an individual record concerning a specific alien bird species introduced to a specific location. The data derives from both published and unpublished sources including atlases country species lists peer-reviewed articles websites and via correspondence with in-country experts. Acknowledgements Dyer Ellie; Redding David; Blackburn Tim (2016) Data from The Global Avian Invasions Atlas - A database of alien bird distributions worldwide. figshare.,CSV,,[animals],CC4,,,74,838,966,A database of alien bird distributions worldwide,The Global Avian Invasions Atlas,https://www.kaggle.com/figshare/the-global-avian-invasions-atlas,Sat Apr 08 2017
,Jordan Tremoureux,[],[],Context The federal city of Moscow Russia is divided into twelve administrative okrugs which are in turn subdivided into districts (raions). (source Wikipedia)  Content OKATO  Russian Classification on Objects of Administrative Division.  OKTMO Russian Classification on Territories of Municipal Division.  RAION Raion's name.  OKRUGS Okrugs' name.   Acknowledgements The shapefile data has been found here http//gis-lab.info/qa/moscow-atd.html.  But I have translated the okrugs and raions names in English.   Inspiration I hope you can use this shapefile to make interesting visualisations!  You can use OKATO and/or OKTMO codes to join your data to this shapefile.,Other,,"[russia, geography, politics]",Other,,,572,2178,0.666015625,"Shapefile of the 12 administrative okrugs, subdivided into 146 raions",Administrative divisions of Moscow,https://www.kaggle.com/jtremoureux/administrative-divisions-of-moscow,Wed May 03 2017
,harlfoxem,"[id, date, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, grade, sqft_above, sqft_basement, yr_built, yr_renovated, zipcode, lat, long, sqft_living15, sqft_lot15]","[numeric, string, numeric, numeric, numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",This dataset contains house sale prices for King County which includes Seattle. It includes homes sold between May 2014 and May 2015. It's a great dataset for evaluating simple regression models.,CSV,,"[home, finance]",CC0,,,16338,101297,2,Predict house price using regression,"House Sales in King County, USA",https://www.kaggle.com/harlfoxem/housesalesprediction,Thu Aug 25 2016
,73805,"[contest_key, date, title, location_str, city, state, state_full, prize, cbj_percentage, is_championship, is_standard, url]","[numeric, dateTime, string, string, string, string, string, numeric, numeric, boolean, boolean, string]","Context This data set is the aggregate of 1559 KCBS competitions from July 2013 through December 2016. The Kansas City Barbeque Society (KCBS) is ""world's largest organization of barbeque and grilling enthusiasts with over 20000 members worldwide."" The data set was constructed by scraping the KCBS events page A Standard Competition At a standard KCBS BBQ Competition 30 certified barbeque judges (CBJs) blindly judge the BBQ served by 36 teams. Judges are broken up into tables of 6. There are four categories of meat chicken pork ribs pork and brisket. Each judge receives six samples representing six different teams.  Scoring Samples are scored across three characteristics appearance taste and tenderness. Scores range from 0 to 9 with a 9 being perfect a 6 corresponding to average and a 0 given as part of a DQ or other official sanction.      A team's score within a category is calculated by a weighted sum of the six judges' scores. The KCBS scoring weights were last changed in July 2013 which aligns with the start date of this data set. The maximum score in a category is 180 (9 points * 3 characteristics * 6 judges). The maximum overall score in a standard 4-category competition is 720 (180 points * 4 categories). Not all competitions in this data set are standard 4-category; see the is_standard feature. Content Competition Data Features This CSV file contains 1559 rows of competition data. Scoring and placing resultsare stored separately across five category-specific files. The scoring results are linked to their competitions by a foreign key. There are many results for each competition in this file.  contest_key - the primary key used to link rows of results to a competition date - the date of the competition title - the name of the competition location_str - the full string of the location ie New Palestine IN city - the city extracted from location_str state - the state abbreviation extracted from location_str state_full - the full name of the state extrapolated from state prize - the total prize money awarded (I believe it's total and not just 1st place) cbj_percentage - the percentage of judges that are certified barbeque judges (CBJs) is_championsip - a boolean indicating if the competition is a ""state championship"" (note each state has more than one per year) is_standard - a boolean indicating if the competition consists of and only of the four categories chicken pork ribs pork and brisket. Some competitions include extra categories like dessert and these are considered non-standard (note overall scores in a non-standard competition may be greater that 4 * 180)  Results Data Features There are five separate tables of results; one for each of the standard categories. The features are the same across each of these tables.   contest_key - the foreign key linking a result row to its competition place - the place earned by a competing team (1st 2nd 3rd as 1 2 3) score - the total score (0-180 in a chicken ribs pork and brisket 0-720 in overall-- assuming a standard competition) team_name - the name of the team (usually clever and good for a word cloud!)  Acknowledgements I'd like to thank the KCBS for providing fun opportunities to taste great BBQ and recording event data on the website in an fairly accessible and light-weight manner. In the future I'd love to experiment with anonymized judge scoring data. A judge can view their scoring history and how it compared sample-by-sample with other judges' scores at the table.  Inspiration I've been a CBJ for 4 years!",CSV,,[food and drink],CC0,,,139,1785,11,"Competition context and results from 1,559 KCBS Barbeque Competitions",KCBS Barbeque Competitions,https://www.kaggle.com/jaysobel/kcbs-bbq,Thu Jul 20 2017
,Graham Daley,"[race_id, date, venue, race_no, config, surface, distance, going, horse_ratings, prize, race_class, sec_time1, sec_time2, sec_time3, sec_time4, sec_time5, sec_time6, sec_time7, time1, time2, time3, time4, time5, time6, time7, place_combination1, place_combination2, place_combination3, place_combination4, place_dividend1, place_dividend2, place_dividend3, place_dividend4, win_combination1, win_dividend1, win_combination2, win_dividend2]","[numeric, dateTime, string, numeric, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, string, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, numeric, numeric, string, string]",Can you beat the market? Horse racing has always intrigued me - not so much from the point of view as a sport but more from the view of it as a money market. Inspired by the pioneers of computerised horse betting I'm sharing this dataset in the hope of finding more data scientists willing to take up the challenge and find new ways of exploiting it!  As always the goal for most of us is to find information in the data that can be used to generate profit usually by finding information that has not already been considered by the other players in the game. But I'm always interested in finding new uses for the data whatever they may be. Horse racing is a huge business in Hong Kong which has two race tracks in a city that is only 1104 square km. The betting pools are bigger than all US racetracks combined which means that the opportunity is unlimited for those who are successful. So are you up for it?  Content The data was obtained from various free sources and is presented in CSV format. Personally-identifiable information such as horse and jockey names has not been included. However these should have no relevance to the purpose of this dataset which is purely for experimental use. There are two files races.csv Each line describes the condition of an individual race. race_id - unique identifier for the race date - date of the race in YYYY-MM-DD format (note that the dates given have been obscured and are not the real ones although the durations between each race should be correct) venue - a 2-character string representing which of the 2 race courses this race took place at ST = Shatin HV = Happy Valley race_no - race number of the race in the day's meeting config - race track configuration mostly related to the position of the inside rail. For more details see the HKJC website. surface - a number representing the type of race track surface 1 = dirt 0 = turf distance - distance of the race in metres going - track condition. For more details see the HKJC website. horse_ratings - the range of horse ratings that may participate in this race prize - the winning prize in HK Dollars race_class - a number representing the class of the race sec_time1 - time taken by the leader of the race to reach the end of the end of the 1st sectional point (sec) sec_time2 - time taken by the leader of the race to reach the end of the 2nd sectional point (sec) sec_time3 - time taken by the leader of the race to reach the end of the 3rd sectional point (sec) sec_time4 - time taken by the leader of the race to reach the end of the 4th sectional point if any (sec) sec_time5 - time taken by the leader of the race to reach the end of the 5th sectional point if any (sec) sec_time6 - time taken by the leader of the race to reach the end of the fourth sectional point if any (sec) sec_time7 - time taken by the leader of the race to reach the end of the fourth sectional point if any (sec) time1 - time taken by the leader of the race in the 1st section only (sec) time2 - time taken by the leader of the race in the 2nd section only (sec) time3 - time taken by the leader of the race in the 3rd section only (sec) time4 - time taken by the leader of the race in the 4th section only if any (sec) time5 - time taken by the leader of the race in the 5th section only if any (sec) time6 - time taken by the leader of the race in the 6th section only if any (sec) time7 - time taken by the leader of the race in the 7th section only if any (sec) place_combination1 - placing horse no (1st) place_combination2 - placing horse no (2nd) place_combination3 - placing horse no (3rd) place_combination4 - placing horse no (4th) place_dividend1 - placing dividend paid (for place_combination1) place_dividend2 - placing dividend paid (for place_combination2) place_dividend3 - placing dividend paid (for place_combination2) place_dividend4 - placing dividend paid (for place_combination2) win_combination1 - winning horse no win_dividend1 - winning dividend paid (for win_combination1) win_combination2 - joint winning horse no if any win_dividend2 - winning dividend paid (for win_combination2 if any) runs.csv Each line describes the characteristics of one horse run in one of the races given in races.csv. race_id - unique identifier for the race horse_no - the number assigned to this horse in the race horse_id - unique identifier for this horse result - finishing position of this horse in the race won - whether horse won (1) or otherwise (0) lengths_behind - finishing position as the number of horse lengths behind the winner horse_age - current age of this horse at the time of the race horse_country - country of origin of this horse horse_type - sex of the horse e.g. 'Gelding' 'Mare' 'Horse' 'Rig' 'Colt' 'Filly' horse_rating - rating number assigned by HKJC to this horse at the time of the race horse_gear - string representing the gear carried by the horse in the race. An explanation of the codes used may be found on the HKJC website. declared_weight - declared weight of the horse and jockey in lbs actual_weight - actual weight carried by the horse in lbs draw - post position number of the horse in this race position_sec1 - position of this horse (ranking) in section 1 of the race position_sec2 - position of this horse (ranking) in section 2 of the race position_sec3 - position of this horse (ranking) in section 3 of the race position_sec4 - position of this horse (ranking) in section 4 of the race if any position_sec5 - position of this horse (ranking) in section 5 of the race if any position_sec6 - position of this horse (ranking) in section 6 of the race if any behind_sec1 - position of this horse (lengths behind leader) in section 1 of the race behind_sec2 - position of this horse (lengths behind leader) in section 2 of the race behind_sec3 - position of this horse (lengths behind leader) in section 3 of the race behind_sec4 - position of this horse (lengths behind leader) in section 4 of the race if any behind_sec5 - position of this horse (lengths behind leader) in section 5 of the race if any behind_sec6 - position of this horse (lengths behind leader) in section 6 of the race if any time1 - time taken by the horse to pass through the 1st section of the race (sec) time2 - time taken by the horse to pass through the 2nd section of the race (sec) time3 - time taken by the horse to pass through the 3rd section of the race (sec) time4 - time taken by the horse to pass through the 4th section of the race if any (sec) time5 - time taken by the horse to pass through the 5th section of the race if any (sec) time6 - time taken by the horse to pass through the 6th section of the race if any (sec) finish_time - finishing time of the horse in this race (sec) win_odds - win odds for this horse at start of race place_odds - place (finishing in 1st 2nd or 3rd position) odds for this horse at start of race trainer_id - unique identifier of the horse's trainer at the time of the race jockey_id - unique identifier of the jockey riding the horse in this race Acknowledgements None of this research would have even started without me standing on the shoulders of giants such as William Benter Ruth Bolton and Randall Chapman and many others who have published the results of their research. Inspiration It is probably not going to be enough to just take this dataset and feed it into Google Cloud Machine Learning Azure MI etc... but let me know if you find otherwise! Questions that need to be answered include  Feature engineering - what features are needed and how best to estimate them from the data given? Modelling - what kind of model works best? Maybe more than one model? Other data - is there any other data needed apart from that given in this data set? ,CSV,,[horse racing],CC0,,,876,7855,11,Data on thoroughbred racing in Hong Kong for fun and machine learning,Horse Racing in HK,https://www.kaggle.com/gdaley/hkracing,Thu Dec 15 2016
,Jacob Boysen,"[Agency Code, Agency Name, Bureau Code, Bureau Name, Account Code, Account Name, Treasury Agency Code, Subfunction Code, Subfunction Title, BEA Category, On- or Off- Budget, 1976, TQ, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]","[numeric, string, numeric, string, numeric, string, numeric, numeric, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Each year after the President's State of the Union address the Office of Management and Budget (OMB) releases the Administration's Budget offering proposals on key priorities and newly announced initiatives. In 2016 & 2017 Obama’s OMB released all of the data included in the President's budget in a machine-readable format here on GitHub. “The budget process should be a reflection of our values as a country so we think it's important that members of the public have as many tools as possible to see the data behind the President's proposals. And if people are motivated to create their own visualizations or products from the data they should have that chance as well.” Content This branch includes three data files that contain an extract of the Office of Management and Budget (OMB) budget database. These files can be used to reproduce many of the totals published in the budget and examine unpublished details below the levels of aggregation published in the budget. The user guide file contains detailed information about this data its format and its limitations. Acknowledgements Datasets were compiled by Obama White House officials and released at this Github repo.  Inspiration  What significant changes were there between 2016 and 2017 proposals? How was the federal budget distributed across agencies? Where there any interesting changes in federal receipts? ,CSV,,"[government agencies, finance, government]",CC0,,,82,826,7,2016 & 2017 OMB Proposals,Obama White House Budgets,https://www.kaggle.com/jboysen/obama-budgets,Fri Sep 08 2017
,Ricky,"[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, convert]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This is real real-time bidding data that is used to predict if an advertiser should bid for a marketing slot e.g. a banner on a webpage. Explanatory variables are things like browser operation system or time of the day the user is online marketplace his identifiers were traded on earlier etc. The column 'convert' is 1 when the person clicked on the ad and 0 if this is not the case.  Content Unfortunately the data had to be anonymized so you basically can't do a lot of feature engineering. I just applied PCA and kept 0.99 of the linear explanatory power. However I think it's still really interesting data to just test your general algorithms on imbalanced data. ;) Inspiration Since it's heavily imbalanced data it doesn't make sense to train for accuracy but rather try to get obtain a good AUC F1Score MCC or recall rate by cross-validating your data.  It's interesting to compare different models (logistic regression decision trees svms ...) over these metrics and see the impact that your split in traintest data has on the data.  It might be good strategy to follow these  Tactics to combat imbalanced classes.,CSV,,"[business, artificial intelligence]",Other,,,527,6707,455,Predict clicks and handle imbalanced data,Real Time Bidding,https://www.kaggle.com/zurfer/rtb,Mon Feb 27 2017
,brontosaur,"[kpi, period, municipality_id, municipality_name, value, kpi_desc]","[string, numeric, numeric, string, numeric, string]","Sweden has a surprisingly large number of school fires for a small country (< 10M inhabitants) and many of these fires are due to arson. For instance according to the Division of Fire Safety Engineering at Lund University ""Almost every day between one and two school fires occur in Sweden. In most cases arson is the cause of the fire."" The associated costs can be up to a billion SEK (around 120 million USD) per year. It is hard to say why these fires are so common in Sweden compared to other countries and this dataset doesn't address that question - but could it be possible within a Swedish context to find out which properties and indicators of Swedish towns (municipalities to be exact) might be related to a high frequency of school fires? I have collected data on school fire cases in Sweden between 1998 and 2014 through a web site with official statistics from the Swedish Civil Contingencies Agency (https//ida.msb.se/ida2#page=a0087). At least at the time when I collected the data there was no API to allow easy access to schools fire data so I had to collect them using a quasi-manual process downloading XLSX report generated from the database year by year after which I joined these with an R script into a single table of school fire cases where the suspected reason was arson. (Full details on the data acquisition process are available.)  The number of such cases is reported for each municipality (of which there are currently 290) and year (i e each row is a unique municipality/year combination). The population at the time is also reported. As a complement to these data I provide a list of municipal KPIs (key performance indicators) from 1998 to 2014. There are thousands of these KPIs and it would be a futile task for me to try to translate the descriptions from Swedish to English although I might take a stab at translating a small subset of them at some point. These KPIs were extracted from Kolada (a database of Swedish municipality and county council statistics) by repeatedly querying its API (https//github.com/Hypergene/kolada). I'd be very interested to hear if anyone finds some interesting correlations between schools fire cases and municipality indicators!",CSV,,[firefighting],CC0,,,574,7155,2048,"Cases reported by municipality and year. Also, KPIs for each municipality.",School fires in Sweden 1998-2014,https://www.kaggle.com/mikaelhuss/swedish-school-fires,Wed Aug 31 2016
,Megan Risdal,[],[],"The latest hot topic in the news is fake news and many are wondering what data scientists can do to detect it and stymie its viral spread. This dataset is only a first step in understanding and tackling this problem. It contains text and metadata scraped from 244 websites tagged as ""bullshit"" by the BS Detector Chrome Extension by Daniel Sieradski.  Warning I did not modify the list of news sources from the BS Detector so as not to introduce my (useless) layer of bias; I'm not an authority on fake news. There may be sources whose inclusion you disagree with. It's up to you to decide how to work with the data and how you might contribute to ""improving it"". The labels of ""bs"" and ""junksci"" etc. do not constitute capital ""t"" Truth. If there are other sources you would like to include start a discussion. If there are sources you believe should not be included start a discussion or write a kernel analyzing the data. Or take the data and do something else productive with it. Kaggle's choice to host this dataset is not meant to express any particular political affiliation or intent. Contents The dataset contains text and metadata from 244 websites and represents 12999 posts in total from the past 30 days. The data was pulled using the webhose.io API; because it's coming from their crawler not all websites identified by the BS Detector are present in this dataset. Each website was labeled according to the BS Detector as documented here. Data sources that were missing a label were simply assigned a label of ""bs"". There are (ostensibly) no genuine reliable or trustworthy news sources represented in this dataset (so far) so don't trust anything you read. Fake news in the news For inspiration I've included some (presumably non-fake) recent stories covering fake news in the news. This is a sensitive nuanced topic and if there are other resources you'd like to see included here please leave a suggestion. From defining fake biased and misleading news in the first place to deciding how to take action (a blacklist is not a good answer) there's a lot of information to consider beyond what can be neatly arranged in a CSV file.  How Fake News Spreads (NYT) We Tracked Down A Fake-News Creator In The Suburbs. Here's What We Learned (NPR) Does Facebook Generate Over Half of its Revenue from Fake News? (Forbes) Fake News is Not the Only Problem (Points - Medium) Washington Post Disgracefully Promotes a McCarthyite Blacklist From a New Hidden and Very Shady Group (The Intercept)  Improvements If you have suggestions for improvements or would like to contribute please let me know. The most obvious extensions are to include data from ""real"" news sites and to address the bias in the current list. I'd be happy to include any contributions in future versions of the dataset. Acknowledgements Thanks to Anthony for pointing me to Daniel Sieradski's BS Detector. Thank you to Daniel Nouri for encouraging me to add a disclaimer to the dataset's page.",CSV,,"[news agencies, languages, politics]",CC0,,,6235,83701,54,Text & metadata from fake & biased news sources around the web,Getting Real about Fake News,https://www.kaggle.com/mrisdal/fake-news,Sat Nov 26 2016
,Rachael Tatman,"[Author, Title, Year, Filename, Subcorpus, Tokens]","[string, string, numeric, string, string, numeric]",Context Portuguese is a romance language that is the native language of over 215 million speakers worldwide. Like Spanish English and French it was the language of both its country of origin and also that country’s colonial possessions. This corpus contains examples of historical Portuguese  written between 1500 and 1936 both in Portugal and Brazil. Content The corpus contains complete Portuguese manuscripts published from 1500 to 1936 divided into 5 sub-corpora per century (summarized in the table below). The part of speech (POS) of words in this corpus was tagged using TreeTagger. You can find more information on this corpus on the Colonia homepage. Century Texts   Tokens 16th    13  399245 17th    18  709646 18th    14  425624 19th    38  2490771 20th    17  1132696 Total   100 5157982 Texts are balanced in terms of the variety consisting of 48 European Portuguese texts and 52 Brazilian Portuguese texts. You can find more information in the paper that describes the corpus. The complete inventory of texts is here and more detail regarding annotation can be found here. Part of Speech (POS) Tags The works in this corpus have been automatically tagged for their part of speech (POS). The tagset used to annotate the corpus is presented in the table below. It contains not only the classic POS tags (e.g. V DET N) but also a couple of compound tags such as the combination of preposition plus determiner as (PREP+DET) or verb plus pronoun (V+P). The tool used to annotate the corpus was TreeTagger. Category    POS Example Adjective   ADJ bonita Adverb  ADV muita Determiner  DET os Cardinal    CARD    primeiro Noun    NOM mesa Pronoun P   eles Preposition PRP de Verb    V   fazer Interjection    I   Oh! Commas  VIRG     Punctuation SENT    . Studies report that TreeTagger achieves performance higher than 95% accuracy in attributing the correct POS tag and lemma of a token. Acknowledgements If you use this corpus in your work please cite this paper Zampieri M. and Becker M. (2013) Colonia Corpus of Historical Portuguese. In ZSM Studien Special Volume on Non-Standard Data Sources in Corpus-Based Research. Volume 5. Shaker.  Inspiration  What changes have occurred in Portuguese over time? Have words changed? Syntactic structures? How grammatical agreement is expressed? Can you create a classifier which can classify the era and unseen work is from? Using the part of speech tags in this tagger can you train a new tagger and run it over the Brazilian Portuguese Literature Corpus linked below?  You may also like A 3.7 million word literary corpus of Brazilian Portugese,CSV,,"[languages, literature, brazil]",CC0,,,37,604,76,A 5.1 million word corpus of historical Portuguese,Colonia Corpus of Historical Portuguese,https://www.kaggle.com/rtatman/colonia-corpus-of-historical-portuguese,Sat Jul 29 2017
,Natalia,"[type;by;post_id;post_link;post_message;picture;full_picture;link;link_domain;post_published;post_published_unix;post_published_sql;likes_count_fb;comments_count_fb;reactions_count_fb;shares_count_fb;engagement_fb, by, post_id, post_link, post_message, picture, full_picture, link, link_domain, post_published, post_published_unix, post_published_sql, likes_count_fb, comments_count_fb, reactions_count_fb, shares_count_fb, engagement_fb]","[string, string, string, string, numeric, numeric, numeric, numeric, numeric, string, numeric, dateTime, numeric, numeric, numeric, numeric, numeric]","Context This dataset was obtained from Facebook groups as part of my postgraduate thesis. The objective of the thesis was to extract posts from groups related to rare diseases and compare them with the Spanish association of rare diseases (FEDER). If you want to use this open dataset or the code you should cite our paper   Reguera N. Subirats L. Armayones M. Mining Facebook data of people with rare diseases. IEEE Computer-Based Medical Systems (IEEE CBMS 2017) Thessaloniki Greece 22-24th June 2017. Content The file contains information 3917 records from 5 Facebook groups and was extracted using Netvizz. The posts were generated since each group started (as far as 2009) until the 26/11/2016. The content is as follows type Facebook's post classification (e.g. photo status etc.) by either""post_page_pageid"" (post by page) or ""post_user_pageid"" (post by user); post_id id of the post; post_link direct link to the post; post_message text of the post; picture the picture scraped from any link included with the post; full_picture the picture scraped from any link included with the post (full size); link link URL (if the post points to external content); link_domain domain name of link; post_published publishing date post_published_unix publishing date as Unix timestamp (for easy conversion and ranking); post_published_sql publishing date in SQL format (some analysis tools prefer this); likes_count_fb Facebook provided like count for posts; comments_count_fb Facebook provided comment count for posts; reactions_count_fb Facebook provided reactions count for posts (includes likes); shares_count_fb Facebook provided share count for posts; engagement_fb sum of comment reaction and share counts; Acknowledgements I would like to thank my thesis directors who guided me through all the process Laia Subirats Maté and Manuel Armayones Ruiz. Inspiration During my research (code available on https//github.com/natt77/UOC---TFP) several insights were found on the relation between the information posted on the gropus and the information in FEDER. Text analytics was performed together with sentiment analysis. It would be interesting to deepen the analysis create models to predict engagement or any other action that can help improving the life quality of people with rare diseases.",CSV,,"[diseases, epidemiology, internet]",Other,,,206,3472,2,Help improve the quality of life of people with rare diseases,Rare Diseases on Facebook Groups,https://www.kaggle.com/natt77/rare-diseases-on-facebook-groups,Tue Aug 08 2017
,FiveThirtyEight,"[DATE, TIME, PICK UP ADDRESS, , , ]","[dateTime, dateTime, string, numeric, string, string]",Uber TLC FOIL Response This directory contains data on over 4.5 million Uber pickups in New York City from April to September 2014 and 14.3 million more Uber pickups from January to June 2015. Trip-level data on 10 other for-hire vehicle (FHV) companies as well as aggregated data for 329 FHV companies is also included. All the files are as they were received on August 3 Sept. 15 and Sept. 22 2015.  FiveThirtyEight obtained the data from the NYC Taxi & Limousine Commission (TLC) by submitting a Freedom of Information Law request on July 20 2015. The TLC has sent us the data in batches as it continues to review trip data Uber and other HFV companies have submitted to it. The TLC's correspondence with FiveThirtyEight is included in the files TLC_letter.pdf TLC_letter2.pdf and TLC_letter3.pdf. TLC records requests can be made here. This data was used for four FiveThirtyEight stories Uber Is Serving New York’s Outer Boroughs More Than Taxis Are Public Transit Should Be Uber’s New Best Friend Uber Is Taking Millions Of Manhattan Rides Away From Taxis and Is Uber Making NYC Rush-Hour Traffic Worse?. The Data The dataset contains roughly four groups of files  Uber trip data from 2014 (April - September) separated by month with detailed location information Uber trip data from 2015 (January - June) with less fine-grained location information non-Uber FHV (For-Hire Vehicle) trips. The trip information varies by company but can include day of trip time of trip pickup location driver's for-hire license number and vehicle's for-hire license number. aggregate ride and vehicle statistics for all FHV companies (and occasionally for taxi companies)  Uber trip data from 2014 There are six files of raw data on Uber pickups in New York City from April to September 2014. The files are separated by month and each has the following columns  Date/Time  The date and time of the Uber pickup Lat  The latitude of the Uber pickup Lon  The longitude of the Uber pickup Base  The TLC base company code affiliated with the Uber pickup  These files are named  uber-raw-data-apr14.csv uber-raw-data-aug14.csv uber-raw-data-jul14.csv uber-raw-data-jun14.csv uber-raw-data-may14.csv uber-raw-data-sep14.csv  Uber trip data from 2015 Also included is the file uber-raw-data-janjune-15.csv This file has the following columns  Dispatching_base_num  The TLC base company code of the base that dispatched the Uber Pickup_date  The date and time of the Uber pickup Affiliated_base_num  The TLC base company code affiliated with the Uber pickup locationID  The pickup location ID affiliated with the Uber pickup  The Base codes are for the following Uber bases B02512  Unter B02598  Hinter B02617  Weiter B02682  Schmecken B02764  Danach-NY B02765  Grun B02835  Dreist B02836  Drinnen For coarse-grained location information from these pickups the file taxi-zone-lookup.csv shows the taxi Zone (essentially neighborhood) and Borough for each locationID. Non-Uber FLV trips The dataset also contains 10 files of raw data on pickups from 10 for-hire vehicle (FHV) companies. The trip information varies by company but can include day of trip time of trip pickup location driver's for-hire license number and vehicle's for-hire license number. These files are named  American_B01362.csv Diplo_B01196.csv Highclass_B01717.csv Skyline_B00111.csv Carmel_B00256.csv Federal_02216.csv Lyft_B02510.csv Dial7_B00887.csv Firstclass_B01536.csv Prestige_B01338.csv  Aggregate Statistics There is also a file other-FHV-data-jan-aug-2015.csv containing daily pickup data for 329 FHV companies from January 2015 through August 2015. The file Uber-Jan-Feb-FOIL.csv contains aggregated daily Uber trip statistics in January and February 2015.,CSV,,[road transport],CC0,,,8777,67677,835,Trip data for over 20 million Uber (and other for-hire vehicle) trips in NYC,Uber Pickups in New York City,https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city,Mon Nov 14 2016
,Bo Ju,"[Date, Total number of license issued,  lowest price , avg price, Total number of applicants]","[dateTime, numeric, numeric, numeric, numeric]","Context Shanghai uses an auction system to sell a limited number of license plates to fossil-fuel car buyers every month. The average price of this license plate is about $13000 and it is often referred to as ""the most expensive piece of metal in the world."" So our goal is to predict the avg price or the lowest price for the next month. This Data set will be updated every month constantly. Have fun! Content This data set is gathered by myself with the aid of search engine.  Inspiration This data set could be used in your first toy example project. Learning algorithms like RNN+LSTM are well fitted into this time-series prediction problem. So just have fun!",CSV,,[time series],CC0,,,133,1066,0.005859375,Time-series data set (2012-2018),Shanghai Car License Plate Auction Price,https://www.kaggle.com/bogof666/shanghai-car-license-plate-auction-price,Thu Jan 25 2018
,AMiner,[],[],"Name ambiguity has long been viewed as a challenging problem in many applications such as scientific literature management people search and social network analysis. When we search a person name in these systems many documents (e.g. papers webpages) containing that person’s name may be returned. Which documents are about the person we care about? Although much research has been conducted the problem remains largely unsolved especially with the rapid growth of the people information available on the Web. Content This data set contains 110 author names and their disambiguation results (ground truth). For each author there are 3 json entries. The most important files are xxx_xml xxx(classify)_txt and xxx_txt. The xxx(classify)_txt contains the ground truth and the other two files (xxx_xml and xxx_txt) provide features to perform the disambiguation. At the high-level the xxx_xml file includes title venue coauthor affiliation and the xxx.txt further contains citation co-affiliation-occur and homepage.  Let us use ""Ajay Gupta"" as the example to explain what information contained in each file.  Ajay Gupta.xml. The raw file. is formatted as a XML file. In the XML file the author name is associated with a number of publications. An example of a publication is as follow ""      Explanation-based Failure Recovery     1987     Ajay Gupta     AAAI     13048     0     null  "" where  denotes the title of the publication;  denotes the publication year;  denotes the publication venue;  denotes the publication id;  denotes the labeled person e.g. all publications with ""0"" can be considered as published by the same person;  denotes the affiliation of the author(s). Ajay Gupta(classify).txt the answer file is the ground truth. It is actually extracted from the raw-file by viewing publications with the same ""0"" as a person. The format is in plain text. The following is an example "" Ajay Gupta -113048 388794 596099 1265282 1179332 675629 39153 258611 -2988870 1490190 -31393934 -41398544 -51739014 -61671104 515636 1678096 -71126381 1205032 275987 277587 276300 1549674 1034401 -8600181 846439 149270 175996 264268 264291 299548 1384744 300057 302056 545651 1212517 -91316053 "" where the first line denotes the author name and each of the following line indicates a disambiguated person. For example the first line indicates that an author published 8 papers. The corresponding IDs of those papers are respectively 13048 388794 596099 1265282 1179332 675629 39153 258611. Ajay Gupta.txt the intermediate feature files. It contains 8 matrices which respectively represents 8 features co-affiliation coauthor citation co-venue google (ignored) co-affiliation-occur titleSim homepage. Each matrix records the correlation between any two papers published by Ajay Gupta. Each element e.g. m^0_{ij} the i-th row and the j-column in the 0-th matrix denotes whether the two papers (i and j) contain the same affiliation. In this sense the problem of name disambiguation can be basically considered as a pairwise clustering problem. The second matrix records the number of same coauthors except Ajay Gupta. The third matrix records whether the a paper cites another paper. The fourth matrix records whether a paper is published at the same venue with another paper. The fifth matrix records whether the two papers (titles) can be found at a same web page (e.g. conference page). (This matrix is not complete). The sixth matrix records whether the affiliation of author ""a"" of a paper appears in the content of another paper or vice versa. The seventh matrix records the cosine similarity between titles of any two papers. The eighth matrix records whether two papers appear on the same homepage. Please note that the 5th-8th matrixes cannot be extracted from the raw-data file (xxx.xml) and they are generated using other program.  This dataset is a lightly edited from the version provided by AMiner. The three core files for each author have been bundled into a single json for convenience. Acknowledgements This dataset was kindly made available by AMiner. use the data for publication please kindly cite the following papers @article{Tang12TKDE     author = {Jie Tang and Alvis C.M. Fong and Bo Wang and Jing Zhang}     title = {A Unified Probabilistic Framework for Name Disambiguation in Digital Library}     journal ={IEEE Transactions on Knowledge and Data Engineering}     volume = {24}     number = {6}     year = {2012} } @INPROCEEDINGS{ wangadana AUTHOR = ""Xuezhi Wang and Jie Tang and Hong Cheng and Philip S. Yu"" TITLE = ""ADANA Active Name Disambiguation"" BOOKTITLE = ""ICDM'11"" PAGES = {794-803} YEAR = {2011}",{}JSON,,[],Other,,,41,719,23,Practice disambiguation with this batch of research paper metadata,Author Disambiguation,https://www.kaggle.com/aminer/author-disambiguation,Thu Aug 10 2017
,SalvadorDali,"[fen, score]","[string, numeric]",1.8 million positions of racing king chess variant  Racing kings is a popular chess variant.  Each player has a standard set of pieces without pawns. The opening setup is as below.    In this game check is entirely forbidden not only is it forbidden to move ones king into check but it is also forbidden to check the opponents king.   The purpose of the game is to be the first player that moves his king to the eight row. When white moves their king to the eight row and black moves directly after that also their king to the last row the game is a draw (this rule is to compensate for the advantage of white that they may move first.)   Apart from the above pieces move and capture precisely as in normal chess.  To learn a little bit more about a game and to experience the evaluation of the position you can play a couple of games here. Do not forget to select Racing kings chess variant and to analyse the game at the end with the machine. Keep in mind that the evaluation score on lichess website is from -10 to 10 and slightly different than in my dataset.  What you get 2 csv files train.csv and validate.csv with 1.5 mln and ~0.35 mln positions. Both have an identical structure FEN of the position and the score. The score is real value in [-1 1] range. The closer it is to 1/-1 the more probable is the win of a white/black player. Due to the symmetry I will explain the score only for a white player (for black is the same just with a negative sign.  1 means that white already won (the game is already finished) 0.98 white has a guaranteed(*) win in maximum 1 move 0.96 ... 2 moves 0.94 ... 3 moves .... 0.82 ... in 9 moves 0.80 ... in 10 or more moves  from 0.4 to 0.8 - white has big advantage. For a good player it is not hard to win in such situation from 0.2 to 0.4 - white has some advantage. Might be hard to convert it to a win from 0 to 0.2 - white has tiny advantage 0 means that the position is either a draw or very close to a draw  (*) Guaranteed means that the machine has found a forced sequence of moves that allows white player to win no matter what moves the opponent will make. If the opponent makes the best moves - the game will finish in x moves but it can finish faster if the black player makes a mistake.  Your goal is to use predict a score of the position knowing its FEN. Use train.csv to build your model and evaluate the performance on the validate.csv dataset (without looking/using it). I used MAE score in my analysis. Construction of the dataset Dataset was constructed by me. I created a bot that plays many games against itself. The bot takes 1 second to analyse the position and selects the move based on the score of position. It took almost a month to generate these positions.  What is the purpose? Currently the plan is to use ML + reinforcement learning to build my own chess bot that will not use alpha-beta prunning for position evaluation and self-play. In a couple of days I will release my own findings as kernels.,CSV,,[board games],ODbL,,,134,6010,81,Over 1.5 million racing king chess variant positions,Racing Kings (chess variant),https://www.kaggle.com/salvadordali/racingkings,Tue Nov 29 2016
,Megan Risdal,"[publicaddress, controlnbr, CCN, Precinct, ReportedDate, BeginDate, Time, Offense, Description, UCRCode, EnteredDate, Long, Lat, x, y, Neighborhood, lastchanged, LastUpdateDate, OBJECTID, ESRI_OID]","[string, numeric, string, numeric, dateTime, dateTime, string, string, string, numeric, dateTime, numeric, numeric, numeric, numeric, string, dateTime, dateTime, numeric, numeric]","Thinking of making a move to the lovely Twin Cities? First check out this dataset (curtesy of Open Data Minneapolis) before you pack your bags for the ""Little Apple."" The datasets included contain information about 311 calls and crimes committed between 2010 to 2016 which will help you convince your friends family and loved ones that Minneapolis is the place to be (or not). Snow plow noise complaints be darned!",CSV,,[crime],CC3,,,784,6249,74,"What's been goin' on in Minneapolis, MN (2010 to 2016)",Minneapolis Incidents & Crime,https://www.kaggle.com/mrisdal/minneapolis-incidents-crime,Sat Aug 20 2016
,ultra-jack,"[author, content, poem name, age, type]","[string, string, string, string, string]",Context Study for poem classification. Trying to classified poems with targets age and type.  I use two Xgboost predictors to predict target and type separately.      Content Please refer to the website https//www.poetryfoundation.org/ For now I only crawl the data of   renaissance love modern love  renaissance nature modern nature renaissance  mythology & folklore modern  mythology & folklore  Some have copyrights. I only use for studying ) Acknowledgements https//www.poetryfoundation.org/ has the copyright  Inspiration classification is fun!!,CSV,,[linguistics],CC0,,,186,1958,0.578125,Modern and Renaissance poetry for classification exercises,Poems from poetryfoundation.org,https://www.kaggle.com/ultrajack/modern-renaissance-poetry,Tue Jun 27 2017
,Ksenia Sukhova,"[Season, Episode, Character, Line]","[numeric, numeric, string, string]",South Park cartoon lines +70k lines annotated with season episode and speaker It is interesting to practice NLP with ML techniques in order to guess who is speaking. Later there will be file with pre-proccesed data to train,CSV,,"[popular culture, linguistics]",Other,,,1062,9860,5,"More than 70,000 lines of dialogue by season, episode, and character",South Park Dialogue,https://www.kaggle.com/tovarischsukhov/southparklines,Wed Mar 15 2017
,NASA,"[Name, Year, Group, Status, Birth Date, Birth Place, Gender, Alma Mater, Undergraduate Major, Graduate Major, Military Rank, Military Branch, Space Flights, Space Flight (hr), Space Walks, Space Walks (hr), Missions, Death Date, Death Mission]","[string, numeric, numeric, string, dateTime, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string]","Context The term ""astronaut"" derives from the Greek words meaning ""space sailor"" and refers to all who have been launched as crew members aboard NASA spacecraft bound for orbit and beyond. Content The National Aeronautics and Space Administration (NASA) selected the first group of astronauts in 1959. From 500 candidates with the required jet aircraft flight experience and engineering training in addition to a height below 5 feet 11 inches seven military men became the nation's first astronauts. The second and third groups chosen included civilians with extensive flying experience. By 1964 requirements had changed and emphasis was placed on academic qualifications; in 1965 six scientist astronauts were selected from a group of 400 applicants who had a doctorate or equivalent experience in the natural sciences medicine or engineering. The group named in 1978 was the first of space shuttle flight crews and fourteen groups have been selected since then with a mix of pilots and mission specialists. There are currently 50 active astronauts and 35 management astronauts in the program; 196 astronauts have retired or resigned and 49 are deceased (as of April 2013). Acknowledgements This dataset was published by the National Aeronautics and Space Administration as the ""Astronaut Fact Book"" (April 2013 edition). Active astronauts' mission names and flight statistics were updated from the NASA website. Inspiration Which American astronaut has spent the most time in space? What university has produced the most astronauts? What subject did the most astronauts major in at college? Have most astronauts served in the military? Which branch? What rank did they achieve?",CSV,,"[space, employment]",CC0,,,562,4767,0.078125,Which American astronaut has spent the most time in space?,"NASA Astronauts, 1959-Present",https://www.kaggle.com/nasa/astronaut-yearbook,Wed Mar 08 2017
,Daniel Grijalva,[],[],Context I'm going straight to the point I'm obsessed with Steven Wilson. I can't help it I love his music. And I need more music with similar (almost identical) style. So what I'm trying to solve here is how to find songs that match SW's style with almost zero error?  I'm aware that Spotify gives you recommendations like similar artists and such. But that's not enough -- Spotify always gives you varied music. Progressive rock is a very broad genre and I just want those songs that sound very very similar to Steven Wilson or Porcupine Tree. BTW Porcupine Tree was Steven Wilson's band and they both sound practically the same. I made an analysis where I checked their musical similarities. Content I'm using the Spotify web API to get the data. They have an amazingly rich amount of information especially the audio features.   This repository has 5 datasets   StevenWilson.csv contains Steven Wilson discography (65 songs) PorcupineTree.csv  65 Porcupine Tree songs Complete Steven Wilson.csv a merge between the past two datasets (Steven Wilson + Porcupine Tree) Train.csv 200 songs used to train KNN. 100 are Steven Wilson songs and the rest are totally different songs Test.csv 100 songs that may or may not be like Steven Wilson's. I picked this songs from various prog rock playlists and my Discover Weekly from Spotify.    Also so far I've made two kernels   Comparing Steven Wilson and Porcupine Tree  Finding songs that match SW's style using K-Nearest Neighbors  Data There are 21 columns in the datasets.  Numerical this columns were scraped using get_audio_features from the Spotify API.   acousticness a confidence measure from 0.0 to 1.0 of whether the track is acoustic; 1.0 represents high confidence the track is acoustic danceability it describes how suitable a track is for dancing; a value of 0.0 is least danceable and 1.0 is most danceable duration_ms the duration of the track in milliseconds energy a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity instrumentalness predicts whether a track contains no vocals; values above 0.5 are intended to represent instrumental tracks but confidence is higher as the value approaches 1.0 liveness detects the presence of an audience in the recording; 1.0 represents high confidence that the track was performed live loudness the overall loudness of a track in decibels (dB) speechiness detects the presence of spoken words in a track; the more exclusively speech-like the recording (e.g. talk show) the closer to 1.0 the attribute value tempo the overall estimated tempo of a track in beats per minute (BPM) valence a measure from 0.0 to 1.0 describing the musical positiveness; tracks with high valence sound more positive (e.g. happy cheerful euphoric) while tracks with low valence sound more negative (e.g. sad depressed angry)    Categorical these features are categories represented as numbers.   key the musical key the track is in. e.g. 0 = C 1 = C♯/D♭ 2 = D and so on mode mode indicates the modality (major or minor); major is represented by 1 and minor is 0 time_signature an estimated overall time signature of a track; it is a notational convention to specify how many beats are in each bar (or measure). e.g. 4/4 4/3 3/4 8/4 etc.  Strings these fields are mostly useless (except for name album artist and lyrics)   id the Spotify ID of the song name name of the song album album of the song artist artist of the song uri the Spotify URI of the song type the type of the Spotify object track_href the Spotify API link of the song analysis_url the URL used for getting the audio features lyrics lyrics of the song in lower case  Future Ever been obsessed with a song? an album? an artist? I'm planning on building a web app that solves this. It will help you find music extremely similar to other. ,CSV,,[music],CC0,,,33,1260,0.5224609375,Finding songs that match Steven Wilson's style,Steven Wilson detector,https://www.kaggle.com/danielgrijalvas/steven-wilson-analysis,Mon Nov 27 2017
,NOAA,"[X.ZTIME, LON, LAT, WSR_ID, CELL_ID, RANGE, AZIMUTH, SEVPROB, PROB, MAXSIZE]","[numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric]",Severe Weather Data Inventory Context The Severe Weather Data Inventory (SWDI) is an integrated database of severe weather records for the United States. Severe weather is a phenomenon that risks the physical well-being of people and property. In fact the frozen precipitation resulting from fast updrafts during strong thunderstorms can lead to serious damage and harm. Each year the U.S. sees approximately $1 billion in property and crop damage due to severe weather incidents.  Frequency Event-level Period 2015  Content The records in SWDI come from a variety of sources in the National Climatic Data Center archive and cover a number of weather phenomena. This extract from 2015 covers hail detections including the probability of a weather event as well as the size and severity of hail -- all of which help understand potential damage to property and injury to people. Records are event-level records. Individual storm cells with a high probability of yielding hail are included in this dataset -- a total of n = 10824080. Inspiration Think about the geospatial and spatial statistical techniques that can be applied to this data to uncover patterns in storms.  How often does serious severe weather happen?  Where do these severe weather events normally occur? What correlations exist between severe weather and other environmental phenomena?  Acknowledgements This data is a product of NOAA's National Centers for Environmental Information (NCEI).  The dataset is generated by a variety of products that have been submitted to NOAA's weather and climate archives at NCEI. The datasets and methods are described at http//www.ncdc.noaa.gov/swdi/. SWDI provides a uniform way to access data from a variety of sources but it does not provide any additional quality control beyond the processing which took place when the data were archived. The data sources in SWDI will not provide complete severe weather coverage of a geographic region or time period due to a number of factors (eg reports for a location or time period not provided to NOAA). The absence of SWDI data for a particular location and time should not be interpreted as an indication that no severe weather occurred at that time and location. Furthermore much of the data in SWDI is automatically derived from radar data and represents probable conditions for an event rather than a confirmed occurrence. License Public Domain License,CSV,,[climate],Other,,,654,8417,667,Detections of hail storm cells based on NEXRAD radar data during 2015,Severe Weather Data Inventory,https://www.kaggle.com/noaa/severe-weather-data-inventory,Mon Oct 24 2016
,CostalAether,"[ID, primary, secondary, additional_information]","[string, string, string, string]",Disclaimer This is a data set of mine that I though might be enjoyable to the community.  It's concerning Next generation sequencing and Transcriptomics. I used several raw datasets that are public but the processing to get to this dataset is extensive.  This is my first contribution to kaggle so be nice and let me know how I can improve the experience. NGS machines are combined the biggest data producer worldwide. So why not add some (more? ) to kaggle.  A look into Yeast transcriptomics  Background Yeasts ( in this case saccharomyces cerevisiae) are used in the production of beer wine bread and a whole lot of Biotech applications such as creating complex pharmaceuticals.  They are living eukaryotic organisms  (meaning quite complex).  All living organisms store information in their DNA but action within a cell is carried out by specific Proteins. The path from DNA to Protein (from data to action) is simple. a specific region on the DNA gets transcribed to mRNA that gets translated to proteins.  Common assumption says that the translation step is linear  more mRNA means more protein.  Cells actively regulate the amount of protein by the amount of mRNA it creates. The expression of each gene depends on the condition the cell is in (starving stressed etc..) Modern methods in Biology show us all mRNA that is currently inside a cell. Assuming the linearity of the process we can get more protein the more specific mRNA is available to a cell. Making mRNA an excellent marker for what is actually happening inside a cell. It is important to consider that mRNA is fragile. It is actively replenished only when it is needed.  Both mRNA and proteins are expensive for a cell to produce . Yeasts are good model organisms for this since they only have about 6000 genes. They are also single cells which is more homogeneous and contain few advanced features (splice junctions etc.) ( all of this is heavily simplified let me know if I should go into more details ) The data  files  The following files are provided SC_expression.csv expression values for each gene over the available conditions **labels_CC.csv  ** labels for the individual genes  their status and where known intracellular localization ( see below) Maybe this would be nice as a little competition I'll see how this one is going before I'll upload the other label files.  Please provide some feedback on the presentation and whatever else you would want me to share.  background  I used 92 samples from various openly available raw datasets and ran them through a modern RNAseq pipeline.  Spanning a range of different conditions (I hid the raw names). The conditions covered stress conditions temperature and heavy metals as well as growth media changes and the deletion of specific genes. Originally I had 150 sets  92 are of good enough quality. Evaluation was done on gene level. Each gene got it's own row  Samples are columns (some are in replicates over several columns) .  Expression levels were normalized with by TPM (transcripts per million) a default normalization procedure. Raw counts would have been integers  normalized they are floats. Analysis and labels  Genes  The function of individual genes is a matter of dispute. Clearly living cells are complex. The inner machinations of cells are not visible. Gene functionality is commonly inferred indirectly by removing a gene and test the cells behavior.  This is time consuming and not very precise. As you can see in the dataset there is still much to be done to fully understand even single cell yeasts. The provided dataset is allows for a different approach to functional classification of genes. The label files contained in the set correspond a gene to a specific label. The classification is based on the official Gene Onthology associations classification. I simplified the nomenclature. Gene functionality is usually given in a hierarchical structure. [inside cell --> cytoplasma --> associated to complex A ... ] I'm only keeping high level associations and using readable terms instead of GO terms.  I'll extend if people are interested.  Labels  CC    labels  concern Cellular Component.  Where the gene is within a cell.  goes into details of found associations.  the term 'cellular_component' should be seen as  E.g the label 'cellular_component' is synonymous with 'unknown location' . CC is the easiest label to attach to a gene. It is the one that can be studied the easiest. Still there are many genes missing.   MF    labels concern Molecular Function. What is the gene doing. [upcoming] BP     labels concern Biological Processes.  What is the genes involvement. [upcoming] The core interest here is whether it is possible to improve the genes classification by modeling the data. A common assumption says that genes that are expressed in the same conditions have functional relations.  There are a bunch of possible applications out there many of which are limited by our current state of knowledge on the complex systems we observe or fail to do so. Bringing biology into the realm of data science is an ongoing effort. Having a better insight into the data might very well help.  Note  The dataset is real and therefore noisy the labels are incomplete even though I'm using the current state of the art. That is how much is known. Using expression levels for classification was already attempted by softwares like SPELL  (Serial Pattern of Expression Levels Locator).  Acknowledgements I guess I own the dataset.  It is a by product of another project of mine. If someone is interested in publishing this contact me.   Inspiration Unraveling genetic mechanisms is a complex but rewarding task. Humans and yeast are quite similar in many ways. So apart from the fact that we use it for food and medicine we might actually use knowledge gained from yeast eventually for studying diseases.   Again any feedback is welcome Enjoy CE,CSV,,[biology],CC4,,,371,5164,10,A computational bioinformatics dataset with 92 sets of yeast data on 6000 genes,Transcriptomics in yeast,https://www.kaggle.com/costalaether/yeast-transcriptomics,Tue Jan 24 2017
,Food and Drug Administration,[],[],Context The vast majority of food and food ingredients eaten today is processed in some way before they arrived at the kitchen or dinner table. Food processing equipment may leave trace amounts of various industrial chemical compounds in the foods we eat and these chemicals classed indirect food additives are regulated by the United States Food and Drug Administration. This dataset is a list of indirect food additives approved by the FDA. Content This dataset contains the names of chemical compounds and references to the federal government regulatory code approving and controlling their usage. Acknowledgements This dataset is published by the FDA and available online as a for-Excel CSV file. A few errant header columns have been cleaned up prior to upload to Kaggle but otherwise the dataset is published as-is. Inspiration  What tokens most commonly appear amongst the names contained in this list? Any identifiable elements or compounds? ,CSV,,[food and drink],CC0,,,109,1545,0.9326171875,Chemicals indirectly added during processing regulated by the FDA,Indirect Food Additives,https://www.kaggle.com/fda/indirect-food-additives,Tue Sep 12 2017
,OpenFlights,"[Airline ID, Name, Alias, IATA, ICAO, Callsign, Country, Active]","[numeric, string, string, string, string, string, string, string]","Airline database As of January 2012 the OpenFlights Airlines Database contains 5888 airlines. Some of the information is public data and some is contributed by users. Content The data is ISO 8859-1 (Latin-1) encoded. Each entry contains the following information - Airline ID    Unique OpenFlights identifier for this airline. - Name  Name of the airline. - Alias Alias of the airline. For example All Nippon Airways is commonly known as ""ANA"". - IATA  2-letter IATA code if available. - ICAO  3-letter ICAO code if available. - Callsign  Airline callsign. - Country   Country or territory where airline is incorporated. - Active    ""Y"" if the airline is or has until recently been operational ""N"" if it is defunct. This field is not reliable in particular major airlines that stopped flying long ago but have not had their IATA code reassigned (eg. Ansett/AN) will incorrectly show as ""Y"". The special value \N is used for ""NULL"" to indicate that no value is available. This is from a MySQL database where \N is used for NULL.  Notes Airlines with null codes/callsigns/countries generally represent user-added airlines. Since the data is intended primarily for current flights defunct IATA codes are generally not included. For example ""Sabena"" is not listed with a SN IATA code since ""SN"" is presently used by its successor Brussels Airlines. Acknowledgements This dataset was downloaded from Openflights.org under the Open Database license. This is an excellent resource and there is a lot more on their website so check them out! ",CSV,,[aviation],ODbL,,,640,6351,0.306640625,A database of over 5000 airlines,Airline Database,https://www.kaggle.com/open-flights/airline-database,Tue Aug 29 2017
,Starbucks,"[Brand, Store Number, Store Name, Ownership Type, Street Address, City, State/Province, Country, Postcode, Phone Number, Timezone, Longitude, Latitude]","[string, string, string, string, string, string, numeric, string, string, numeric, string, numeric, numeric]",Context Starbucks started as a roaster and retailer of whole bean and ground coffee tea and spices with a single store in Seattle’s Pike Place Market in 1971. The company now operates more than 24000 retail stores in 70 countries. Content This dataset includes a record for every Starbucks or subsidiary store location currently in operation as of February 2017. Acknowledgements This data was scraped from the Starbucks store locator webpage by Github user chrismeller. Inspiration What city or country has the highest number of Starbucks stores per capita? What two Starbucks locations are the closest in proximity to one another? What location on Earth is farthest from a Starbucks? How has Starbucks expanded overseas?,CSV,,[food and drink],Other,,,5224,35452,4,"Name, ownership type, and location of every Starbucks store in operation",Starbucks Locations Worldwide,https://www.kaggle.com/starbucks/store-locations,Tue Feb 14 2017
,UCI Machine Learning,"[Class_Number, Number_Of_Animal_Species_In_Class, Class_Type, Animal_Names]","[numeric, numeric, string, string]",This dataset consists of 101 animals from a zoo.  There are 16 variables with various traits to describe the animals.  The 7 Class Types are Mammal Bird Reptile Fish Amphibian Bug and Invertebrate The purpose for this dataset is to be able to predict the classification of the animals based upon the variables.  It is the perfect dataset for those who are new to learning Machine Learning. zoo.csv Attribute Information (name of attribute and type of value domain)  animal_name      Unique for each instance hair        Boolean feathers        Boolean eggs        Boolean milk        Boolean airborne        Boolean aquatic     Boolean predator        Boolean toothed     Boolean backbone       Boolean breathes       Boolean venomous       Boolean fins       Boolean legs      Numeric (set of values {024568}) tail      Boolean domestic      Boolean catsize      Boolean class_type       Numeric (integer values in range [17])   class.csv This csv describes the dataset  Class_Number               Numeric (integer values in range [17]) Number_Of_Animal_Species_In_Class     Numeric Class_Type                 character -- The actual word description of the class Animal_Names          character -- The list of the animals that fall in the category of the class  Acknowledgements UCI Machine Learning https//archive.ics.uci.edu/ml/datasets/Zoo Source Information    -- Creator Richard Forsyth    -- Donor Richard S. Forsyth               8 Grosvenor Avenue              Mapperley Park              Nottingham NG3 5DX              0602-621676    -- Date 5/15/1990 Inspiration What are the best machine learning ensembles/methods for classifying these animals based upon the variables given?,CSV,,[animals],ODbL,,,2677,21636,0.0048828125,Use Machine Learning Methods to Correctly Classify Animals Based Upon Attributes,Zoo Animal Classification,https://www.kaggle.com/uciml/zoo-animal-classification,Sat Dec 24 2016
,US Department of Agriculture,"[YEAR, POUNDS GRADED, PRIME, %, CHOICE, %, SELECT, %, STNDRD, %, COMRCL, %, UTILITY, %, CUTTER, %, CANNER, %, S/H, %  of FEDERAL  SLAUGHTER	 BEEF, Y1, %, Y2, %, Y3, %, Y4, %, Y5, %]","[numeric, numeric, numeric, string, numeric, string, numeric, string, string, string, numeric, string, numeric, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string]",Beef. Lamb. Veal. We might not all eat them but they are the meats whose grades the US Department of Agriculture has seen fit to publish. This dataset contains records on meat production and quality as far back as 1930.  After meat and poultry are inspected for wholesomeness producers and processors may request that they have products graded for quality by a licensed Federal grader. The USDA's Agricultural Marketing Service (http//www.ams.usda.gov) is the agency responsible for grading meat and poultry. Those who request grading must pay for the service. Grading for quality means the evaluation of traits related to tenderness juiciness and flavor of meat; and for poultry a normal shape that is fully fleshed and meaty and free of defects. USDA grades are based on nationally uniform Federal standards of quality. No matter where or when a consumer purchases graded meat or poultry it must have met the same grade criteria. The grade is stamped on the carcass or side of beef and is usually not visible on retail cuts. However retail packages of beef as well as poultry will show the U.S. grade mark if they have been officially graded. To better understand the available fields  - All fields labeled 'pounds' are really in units of indicate millions    of pounds.  - You can find a helpful explanation of what the different grades mean    here. Acknowledgements This data was kindly released by the US Department of Agriculture. You can find their most recent meat updates here. Inspiration  This is a good dataset for anyone looking to do basic data cleanup. I've converted it into a properly formed CSV but there are still numerous missing values footnoted fields and exceptions. This is a good candidate for regression analysis especially in conjunction with other datasets. Can you identify correlates for the amount of beef produced? Validate how well cattle futures predict annual yields? 2015 was a banner year for beef production. What happened? ,CSV,,"[government, agriculture]",Other,,,96,1029,0.0615234375,USDA's data on beef and mutton production since the 1930s,The National Summary of Meats,https://www.kaggle.com/usda/the-national-summary-of-meats,Wed Aug 23 2017
,def love(x):,"[name, status, salary, pay_basis, title, year]","[string, string, string, string, string, numeric]",Context For democracy to function we need transparency. Part of the transparency is given to us through government salaries and names of employees. Since 1996 the White House has been required by Congress to disclose a list of staff and their salaries. Content The Obama_staff_salaries covers salaries under the Obama administration from 2009-2016 (by Julianna Langston).  The White_house_2017_salaries was released by the White House as a 16-page PDF detailing the salaries of Trump Administration's employees. This is a CSV scraped from the PDF using Tabula (by Carl V. Lewis). Acknowledgements I would like to thank @julilangston and @carblewis at data.world for providing these datasets. Inspiration How do WH staff salaries compare to Trump staff salaries?  How much did each administration spend on Immigration advisor/assistant staff?    Who has more executive assistants?    How did salaries under the Obama's administration change from 2009-2016?,CSV,,"[income, politics]",CC0,,,415,2538,0.37890625,Trump vs. Obama Administration,White House Salaries,https://www.kaggle.com/adamschroeder/white-house-salaries,Mon Jul 10 2017
,National Park Service,"[Park Code, Park Name, State, Acres, Latitude, Longitude]","[string, string, string, numeric, numeric, numeric]",Context The National Park Service publishes a database of animal and plant species identified in individual national parks and verified by evidence — observations vouchers or reports that document the presence of a species in a park. All park species records are available to the public on the National Park Species portal; exceptions are made for sensitive threatened or endangered species when widespread distribution of information could pose a risk to the species in the park. Content National Park species lists provide information on the presence and status of species in our national parks. These species lists are works in progress and the absence of a species from a list does not necessarily mean the species is absent from a park. The time and effort spent on species inventories varies from park to park which may result in data gaps. Species taxonomy changes over time and reflects regional variations or preferences; therefore records may be listed under a different species name. Each park species record includes a species ID park name taxonomic information scientific name one or more common names record status occurrence (verification of species presence in park) nativeness (species native or foreign to park) abundance (presence and visibility of species in park) seasonality (season and nature of presence in park) and conservation status (species classification according to US Fish & Wildlife Service). Taxonomic classes have been translated from Latin to English for species categorization; order family and scientific name (genus species subspecies) are in Latin. Acknowledgements The National Park Service species list database is managed and updated by staff at individual national parks and the systemwide Inventory and Monitoring department.,CSV,,"[ecology, animals, plants]",CC0,,,1638,9347,17,Plant and animal species found in the American national park system,Biodiversity in National Parks,https://www.kaggle.com/nationalparkservice/park-biodiversity,Fri Jan 20 2017
,United States Drought Monitor,"[USPS, GEOID, ANSICODE, NAME, ALAND, AWATER, ALAND_SQMI, AWATER_SQMI, INTPTLAT, INTPTLONG                                                                                                               ]","[string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric]","The United States Drought Monitor collects weekly data on drought conditions around the U.S. Acknowledgements All data was downloaded from the United States Drought Monitor webpage. The U.S. Drought Monitor is jointly produced by the National Drought Mitigation Center at the University of Nebraska-Lincoln the United States Department of Agriculture and the National Oceanic and Atmospheric Administration. Map courtesy of NDMC-UNL. The Data The data contains weekly observations about the extent and severity of drought in each county of the United States. The dataset contains the following fields  releaseDate when this data was released on the USDM website FIPS the FIPS code for this county county the county name state the state the county is in NONE percentage of the county that is not in drought D0 percentage of the county that is in abnormally dry conditions D1 percentage of the county that is in moderate drought D2 percentage of the county that is in severe drought D3 percentage of the county that is in extreme drought D4 percentage of the county that is in exceptional drought validStart the starting date of the week that these observations represent validEnd the ending date of the week that these observations represent domStatisticFormatID seems to always be 1  Note the drought categories are cumulative if an area is in D3 then it is also in D2 D1 and D0. This means that for every observation D4 <= D3 <= D2 <= D1 <= D0. County Info To make some analyses slightly easier I've also included *county_info_2016.csv* which contains physical size information about each county. This file contains the following fields  USPS     United States Postal Service State Abbreviation GEOID    FIPS code ANSICODE     American National Standards Institute code NAME     Name ALAND    Land Area (square meters) - Created for statistical purposes only AWATER   Water Area (square meters) - Created for statistical purposes only ALAND_SQMI   Land Area (square miles) - Created for statistical purposes only AWATER_SQMI  Water Area (square miles) - Created for statistical purposes only INTPTLAT     Latitude (decimal degrees) First character is blank or ""-"" denoting North or South latitude respectively INTPTLONG    Longitude (decimal degrees) First character is blank or ""-"" denoting East or West longitude respectively ",CSV,,"[geography, climate]",CC0,,,577,4494,251,Weekly data on extent and severity of drought in each US county (2000-present),United States Droughts by County,https://www.kaggle.com/us-drought-monitor/united-states-droughts-by-county,Tue Nov 08 2016
,Karmanya Aggarwal,[],[],Context List of all UFC fights since 2013 with summed up entries of each fighter's round by round record preceding that fight. Created in the attempt to create a UFC fight winner predictor. Dataset may not be great I'm still new to this thing so appreciate any tips on cleaning up the set.  Content Each row represents a single fight - with each fighter's previous records summed up  prior to the fight. blank stats mean its the fighter's first fight since 2013 which is where granular data for UFC fights beings Acknowledgements https//github.com/valish/ufc-api for the UFC api  Beautifulsoup and it's creators and Hitkul my partner in crime Inspiration Can we draw decent predictions from this dataset?,CSV,,[sports],CC0,,,866,8320,2,Fight-by-fight list of all UFC fights from 2013,UFC Fight Data,https://www.kaggle.com/calmdownkarm/ufcdataset,Mon Jun 12 2017
,Rush Kirubi,"[Name, Platform, Year_of_Release, Genre, Publisher, NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales, Critic_Score, Critic_Count, User_Score, User_Count, Developer, Rating]","[string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context Motivated by Gregory Smith's web scrape of VGChartz Video Games Sales this data set simply extends the number of variables with another web scrape from Metacritic. Unfortunately there are missing observations as Metacritic only covers a subset of the platforms. Also a game may not have all the observations of the additional variables discussed below. Complete cases are ~ 6900 Content Alongside the fields Name Platform Year_of_Release Genre Publisher NA_Sales EU_Sales JP_Sales Other_Sales Global_Sales we have-  Critic_score - Aggregate score compiled by Metacritic staff Critic_count - The number of critics used in coming up with the Critic_score User_score - Score by Metacritic's subscribers User_count - Number of users who gave the user_score Developer - Party responsible for creating the game Rating  - The ESRB ratings  Acknowledgements This repository https//github.com/wtamu-cisresearch/scraper after a few adjustments worked extremely well! Inspiration It would be interesting to see any machine learning techniques or continued data visualizations applied on this data set.,CSV,,[video games],Other,,,10622,61631,2,Video game sales from Vgchartz and corresponding ratings from Metacritic,Video Game Sales with Ratings,https://www.kaggle.com/rush4ratio/video-game-sales-with-ratings,Fri Dec 30 2016
,Steve Palley,"[county, fips, cand, st, pct_report, votes, total_votes, pct, lead]","[string, string, string, string, numeric, numeric, numeric, numeric, string]",This dataset includes county-level data from the 2016 US Presidential Election. Data are from Michael W. Kearney's GitHub page by way of Max Galka's County-Level Results Map on metrocosm.com.,CSV,,"[geography, politics]",Other,,,790,6626,2,County-level data on presidential voting,2016 US Presidential Election Vote By County,https://www.kaggle.com/stevepalley/2016uspresidentialvotebycounty,Sun Nov 20 2016
,Brian Roach,"[longitude, latitude, school_code, countyMatch, isSchool]","[numeric, numeric, numeric, boolean, boolean]","Context Vaccinations provide people the ability to develop immunity to particular diseases.  When the majority of a population is vaccinated “herd immunity” protects those who have not been vaccinated by blocking the spread of these diseases.  A medical research paper published by The Lancet in 1998 suggested an association between the Measles/Mumps/Rubella (MMR) vaccine and Autism spectrum disorders.  The paper was later fully-retracted due controversy surrounding the lead author who had financial conflicts of interest and allegedly manipulated the study data.  However it generated worldwide concern over the safety of MMR and other types of vaccines including Diphtheria/Tetanus/Pertussis (DTP). In California by 2010 the growing trend for parents to opt out of having their children receive vaccines over the following decade coincided with the largest Pertussis outbreak in more than 60 years.  Reduced vaccination frequency was also linked to a high-profile measles outbreak in 2014 that began at Disneyland.  The resulting California state legislation (Senate Bill 277) signed June 2015 made it much more difficult for parents to opt out of vaccinations for their children.  The data set will allow you to explore individual public and private school vaccination rates of incoming Kindergarten students for the 2000 to 2014 school years. Content The data are records for every school with ten or more students reporting the number of incoming Kindergarteners who provided either proof of immunization personal beliefs exemption (PBE) or permanent medical exemption (PME).  Annual records for the 2000-2001 through 2014-2015 school years have been formatted and combined.  Common variables in these annual data sets included in the merged file are the number of students school name school county the number of PBEs PMEs and number of students vaccinated for  Diphtheria/Tetanus/Pertussis (DTP)   Polio  Measles/Mumps/Rubella (MMR)  One additional file contains 5 years of county-level Pertussis case numbers and rates.  Another additional data file contains the number of infant Pertussis cases for infants under three months old for each county in California between 2014-2015. Geographic data are available in a file based on scripted geocode calls using the ggmap R package to find latitude and longitude data using the school names and county names.  Not all calls returned a valid coordinate so additional indicator variables in this file indicate the quality of the match.  The isSchool indicator variable is 1 if the geocode search meta data included ""school"" and the countyMatch indicator is 1 if the latitude and longitude coordinates are contained within the appropriate county in CA.  References  Retracted Lancet Research Article  Report on 2010 Pertussis Outbreak  Acknowledgements Individual data files and detailed annual reports for every school year in this data set are provided by the California Department of Public Health (CDPH).  Individual schools and licensed child care facilities are required to report immunization information to CDPH every year to maintain compliance with the California Health and Safety Code.  Additional details as well as child care and 7th grade data files can be found on the CDPH website https//www.cdph.ca.gov/programs/immunize/Pages/ImmunizationLevels.aspx County level case data were pulled from the following report https//archive.cdph.ca.gov/programs/immunize/Documents/Pertussis_Report_1-7-2015.pdf Infant Pertussis data were reported to CDPH as of 2/10/2016.  Additional Pertussis reports can be found here https//www.cdph.ca.gov/programs/immunize/Pages/PertussisSummaryReports.aspx Inspiration While the Disneyland measles outbreak received much media attention Pertussis outbreaks in California present great health risks to infants and the elderly.  Can you predict which counties and schools are at greatest risk for outbreaks and/or quantify the association between vaccination rates and the number infant Pertussis cases?",CSV,,"[diseases, public health]",CC0,,,327,2626,7,How many new students contributed to “herd immunity” between 2000 and 2015?,California Kindergarten Immunization Rates,https://www.kaggle.com/broach/california-kindergarten-immunization-rates,Wed Aug 30 2017
,dmi3kno,"[Year, Month, Make, Quantity, Pct]","[numeric, numeric, string, numeric, numeric]","Context On the morning of 10 January 2017 Opplysningsrådet for Veitrafikken (OFV) Norwegian road association  held a business breakfast for its member organizations where they presented the annual presentation under the title ""Car Year 2016. Status and trends"" (Bilåret 2016 – status og trender).  Among the highlights for the year OFV reported all-time-high sales of electric cars with fully electric and plug-in hybrid cars accounting for 402% of all new car sales (compare to 7.4% for Sweden and 3.6% for Denmark). No other country in the world has this level of popularity of battery-equipped vehicles! In November 2016 12 out of 15 most popular cars sold in Norway were either hybrids of fully electric vehicles with BWM-i3 snapping the title as the most popular car in Norway ahead of undisputed leader of the last decade VW Golf (including eGolf) according to bilnorge.no. Among 10 most popular cars for the year OFV reported there was only one(!) fossil fuel vehicle. OFV makes annual forecast of new passenger car sales. Short summary of their methodology  Based on OFV statistics over several years Taking into account the actual monthly figures for the last four years Actual same-month sales for the previous year is combined with the average for the eight previous months  weighed by the month's proportion in a year adjusted by year's actual sales compared with those of the last year.  OFV forecast for 2016 was 157 500 new passenger cars. Actual sales were 154 603 cars. Applying the same model for 2017 OFV forecasts 152 400 new passenger cars to be sold in Norway. Content Dataset includes two tables  1) Monthly sales of new passenger cars by make (manufacturer brand) - norway_new_car_sales_by_make.csv  Year - year of sales Month - month of sales Make - car make (e.g. Volkswagen Toyota Tesla) Quantity - number of units sold Pct - percent share in monthly total  2) Monthly summary of top-20 most popular models (by make and model) - norway_new_car_sales_by_model.csv  Year - year of sales Month - month of sales Make - car make (e.g. Volkswagen Toyota Tesla) Model - car model (e.g. BMW-i3 Volkswagen Golf Tesla S75) Quantity - number of units sold Pct - percent share in monthly total  3) Summary stats for car sales in Norway by month - norway_new_car_sales_by_month.csv  Year - year of sales Month - month of sales Quantity - total number of units sold Quantity_YoY - change YoY in units Import - total number of units imported (used cars) Import_YoY - change YoY in units Used - total number of units owner changes inside the country (data available from 2012) Used_YoY - change YoY in units Avg_CO2 - average CO2 emission of all cars sold in a given month (in g/km) Bensin_CO2 - average CO2 emission of bensin-fueled cars sold in a given month (in g/km) Diesel_CO2 - average CO2 emission of diesel-fueled cars sold in a given month (in g/km) Quantity_Diesel - number of diesel-fueled cars sold in the country in a given month Diesel_Share - share of diesel cars in total sales (Quantity_Diesel / Quantity) Diesel_Share_LY - share of diesel cars in total sales a year ago Quantity_Hybrid - number of new hybrid cars sold in the country (both PHEV and BV) Quantity_Electric - number of new electric cars sold in the country (zero emission vehicles) Import_Electric - number of used electric cars imported to the country (zero emission vehicles)  Note The numbers on sales of hybrid and electric cars is unavailable prior to 2011. Data is complied from monthly tables published on OFV website (example here). Additional datapoints added from summary tables published on dinside.no Acknowledgements Opplysningsrådet for Veitrafikken (OFV) is a politically independent membership organization that works to get politicians and authorities to build safer and more efficient roads in Norway. The organization has about 60 members representing different types of road users. Members are leading players in road safety car owner associations public transportation companies shippers car dealers oil companies banking finance and insurance road builders and general contractors.  Site http//www.ofvas.no and http//www.ofv.no  Monthly summary statistics and market news http//www.dinside.no/emne/bilsalget and http//statistikk.ofv.no/ofv_bilsalg_small.asp Detailed sales per model http//www.ofvas.no/co2-utslippet/category406.html (using http//www.newocr.com/)  Inspiration 1) How did Norway get here? When did they start on the journey towards electric-powered vehicles and what might have contributed? 2) Did you now that until recently (September 2016) Norway has been second most important market for Tesla Motors (after US)? 3) Can you beat the forecast accuracy of OFV for 2016 and produce a better estimate for 2017?",CSV,,"[business, automobiles]",Other,,,2317,12616,0.2236328125,Monthly car sales for 2007-2017 by make and most popular models,New Car Sales in Norway,https://www.kaggle.com/dmi3kno/newcarsalesnorway,Sat Feb 18 2017
,Rachael Tatman,"[comment_text, label]","[string, numeric]","Context Irony in language is when a statement is produced with one meaning but the intended meaning is exactly the opposite. For instance someone who has burned toast might serve it and say ironically “it’s a little underdone”.  Automatically detecting when language is ironic is an especially difficult task in Natural Language Processing. Content This dataset contains 1950 comments which have been labeled as ironic (1) or not ironic (-1) by human annotators. The text was taken from Reddit comments. Acknowledgements This dataset and analysis of it is presented in the following paper. Wallace B. C. Do Kook Choe L. K. Kertz L. & Charniak E. (2014 April). Humans Require Context to Infer Ironic Intent (so Computers Probably do too). In ACL (2) (pp. 512-516). Url http//www.byronwallace.com/static/articles/wallace-irony-acl-2014.pdf Made possible by support from the Army Research Office (ARO) grant 64481-MA / W9111F-13-1-0406 ""Sociolinguistically Informed Natural Language Processing Automating Irony Detection"" Inspiration  Is irony more likely when discussing certain topics? Does ironic text tend to have more positive or more negative sentiment? What novel features can you develop to help detect irony? ",CSV,,"[languages, linguistics]",CC4,,,116,1589,0.4609375,1950 sentences labeled for ironic content,Ironic Corpus,https://www.kaggle.com/rtatman/ironic-corpus,Tue Jul 25 2017
,Medicare,"[provnum, Provname, address, city, state, zip, survey_date_output, SurveyType, defpref, tag, tag_desc, scope, defstat, statdate, cycle, standard, complaint, filedate]","[numeric, string, string, string, string, numeric, dateTime, string, string, numeric, string, string, string, dateTime, numeric, string, string, dateTime]",Context This official dataset from the Medicare.gov Nursing Home Compare website allows for comparison of over 15000 Medicare and Medicaid-certified nursing homes in the country. Content Separate data collections include  Deficiencies including fire safety health and inspection cycle types Ownership details including ownership percentage Penalties including filing date fee and payment date Provider details including non or for profit status staff ratings and survey scores Quality MSR (Minimum Savings Rate) claims including adjusted and observed scores MDS (Minimum Data Set) quality measures scored on a quarterly basis State averages including total number of quarterly deficiencies and nurse staffing hours Survey summaries for each nursing home  Inspiration  How would you determine what the top ten best nursing homes in the country are? The least? Which states have the best level of nursing home care? The least? In general what are the most common types of complaints and deficiencies?   Acknowledgements This dataset was collected by Medicare.gov and the original files can be accessed here.,CSV,,[healthcare],Other,,,313,2785,318,"Comparing the quality of care of over 15,000 nursing homes in the U.S.",Nursing Home Compare,https://www.kaggle.com/medicare/nursing-home-compare,Mon Nov 28 2016
,Mike Chirico,[],[],"Introduction Each year schools in Pennsylvania are required to report weapons violations substance abuse cyber harassment and other crimes committed during school at school events or on a bus (or waiting at a bus stop) to and from school. The raw data can be found at   www.safeschools.state.pa.us Important  (LEA Types) The rows in this Dataset include several ""LEA Types"" (Legal Entity Types) ""School"" ""School District"" ""County""... etc. Please note that several Schools may fall under a single ""School District"" and there maybe several ""School Districts"" in a single county.  Hence if you include both ""LEA Types"" the counts could be off. Key Pennsylvania Safe Schools Legislation The Safe Schools Act of 1995 (Act 26) was amended in 1997 (Act 30) to mandate annual reporting of all incidents of violence weapons alcohol drugs and tobacco possession to the Department of Education. Local education agencies also are required to develop a Memorandum of Understanding with local law enforcement agencies and provide for other procedural safeguards to enhance school safety. Another amendment to Act 26 (Act 36 of 1999) empowers schools to acquire the tools and resources needed to develop and enhance safe learning environments. How this data was collected for Kaggle. See the following gist.",CSV,,"[crime, education]",CC0,,,136,1768,7,Self reporting statistics from Pennyslvania schools,Pennsylvania Safe Schools Report,https://www.kaggle.com/mchirico/pennsylvania-safe-schools-report,Wed Dec 06 2017
,Academy of Motion Picture Arts and Sciences,"[Year, Ceremony, Award, Winner, Name, Film]","[string, numeric, string, numeric, string, string]",Context Each January the entertainment community and film fans around the world turn their attention to the Academy Awards. Interest and anticipation builds to a fevered pitch leading up to the Oscar telecast in February when hundreds of millions of movie lovers tune in to watch the glamorous ceremony and learn who will receive the highest honors in filmmaking. Achievements in up to 25 regular categories will be honored on February 26 2017 at the 89th Academy Awards presentation at the Dolby Theatre at Hollywood & Highland Center.  Content The Academy Awards Database contains the official record of past Academy Award winners and nominees. The data is complete through the 2015 (88th) Academy Awards presented on February 28 2016. Acknowledgements The awards data was scraped from the Official Academy Awards Database; nominees were listed with their name first and film following in some categories such as Best Actor/Actress and in the reverse for others. Inspiration Do the Academy Awards reflect the diversity of American films or are the #OscarsSoWhite? Which actor/actress has received the most awards overall or in a single year? Which film has received the most awards in a ceremony? Can you predict who will receive the 2016 awards?,CSV,,[film],Other,,,1848,12161,0.7568359375,What actors and films have received the most Oscars?,"The Academy Awards, 1927-2015",https://www.kaggle.com/theacademy/academy-awards,Mon Feb 13 2017
,NOAA,"[ID, Name, Date, Time, Event, Status, Latitude, Longitude, Maximum Wind, Minimum Pressure, Low Wind NE, Low Wind SE, Low Wind SW, Low Wind NW, Moderate Wind NE, Moderate Wind SE, Moderate Wind SW, Moderate Wind NW, High Wind NE, High Wind SE, High Wind SW, High Wind NW]","[string, string, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The National Hurricane Center (NHC) conducts a post-storm analysis of each tropical cyclone in the Atlantic basin (i.e. North Atlantic Ocean Gulf of Mexico and Caribbean Sea) and and the North Pacific Ocean to determine the official assessment of the cyclone's history. This analysis makes use of all available observations including those that may not have been available in real time. In addition NHC conducts ongoing reviews of any retrospective tropical cyclone analyses brought to its attention and on a regular basis updates the historical record to reflect changes introduced. Content The NHC publishes the tropical cyclone historical database in a format known as HURDAT short for HURricane DATabase. These databases (Atlantic HURDAT2 and NE/NC Pacific HURDAT2) contain six-hourly information on the location maximum winds central pressure and (starting in 2004) size of all known tropical cyclones and subtropical cyclones.,CSV,,[weather],CC0,,,1311,7372,9,"Location, wind, and pressure of tropical cyclones in Atlantic and Pacific Oceans","Hurricanes and Typhoons, 1851-2014",https://www.kaggle.com/noaa/hurricane-database,Fri Jan 20 2017
,Aleksey Bilogur,"[SUR_ID, SUR_CPO, SUR_PANEL, DEMO_S1AGE1, QCOMM, QRECRUIT, DEMO_S1PT, DEMO_S1GND, DEMO_S1AGE, S1Q1, S1Q2, S1Q3_1, S1Q3_2, S1Q3_3, S1Q3_4, S1Q3_5, S1Q3_6, S1Q3_7, S1Q3_8, S1Q3_9, S1Q3_10, S1Q5_1, S1Q5_2, S1Q5_3, S1Q5_4, S1Q5_5, S1Q5_6, S1Q5_7, S1Q5_8, S1Q5_9, S1Q5_10, S1Q5_11, S1Q6_1, S1Q6_2, S1Q6_3, S1Q6_4, S1Q6_5, S1Q6_6, S1Q6_7, S1Q6_8, S1Q6_9, S1Q6_10, S1Q8_1, S1Q8_2, S1Q8_3, S1Q8_4, S1Q8_5, S1Q8_6, S1Q8_7, S1Q8_8, S1Q8_9, S1Q8_10, S1Q8_11, S1Q9_1, S1Q9_2, S1Q9_3, S1Q9_4, S1Q9_5, DEMO_S1EDU, DEMO_S1INC, DEMO_S1EMPL, DEMO_S1CAN, DEMO_S1LOC, SUR_S1, SUR_WGT1, S2Q1_1, S2Q1_2, S2Q1_3, S2Q1_4, S2Q2, S2Q3, S2Q4A_1, S2Q4A_2, S2Q8, S2Q9, S2Q10, S2Q11, S2Q12A, S2Q12B, S2Q13, S2Q16_1, S2Q16_2, S2Q16_3, S2Q17_1, S2Q17_2, S2Q17_3, S2Q18_1, S2Q18_2, S2Q20_1, S2Q20_2, S2Q20_3, S2Q20_4, S2Q21_1, S2Q21_2, S2Q21_3, S2Q21_4, S2Q22A, S2Q22B, S2Q22C, S2Q23_1]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset is the anonymized result of responses submitted to a survey collected by the Canadian Department of Justice in 2016.  This survey ""...focuses on the criminal justice system (CJS) to inform the current criminal justice system review...[this] involved a traditional public opinion research survey in informed choice survey and in person and online focus groups...this work was undertaken to support reforms and new initiatives in this area."" This dataset is the survey component of this review. Content Respondents were asked over 50 questions on their perception of how the Canadian Justice system works at large. This dataset was published in a typical survey output format in that most questions are 1-10 rating scales or 0-1 True/False questions with some free-text responses intermixed. To understand the fields please see the attached data dictionary or otherwise access it here. Acknowledgements This data was published as-is by the Government of Canada here. It is licensed under the Open Government License - Canada. Inspiration In a time of increasingly invective dialogue between police forces and the people they police this dataset provides a window on the general level of satisfaction and concern that Canadian government citizens have with their country's justice systems. These results are mostly generalizable to the developed world as a whole.",CSV,,"[government agencies, crime, politics]",Other,,,82,803,4,Canadian government justice system survey results,Canada National Justice Survey 2016,https://www.kaggle.com/residentmario/national-justice-survey-2016,Wed Oct 11 2017
,Punxsutawney Groundhog Club,"[Year, Punxsutawney Phil, February Average Temperature, February Average Temperature (Northeast), February Average Temperature (Midwest), February Average Temperature (Pennsylvania), March Average Temperature, March Average Temperature (Northeast), March Average Temperature (Midwest), March Average Temperature (Pennsylvania)]","[numeric, string, string, string, string, string, string, string, string, string]",Context Thousands gather at Gobbler’s Knob in Punxsutawney Pennsylvania on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend if Phil sees his shadow the United States is in store for six more weeks of winter weather. But if Phil doesn’t see his shadow the country should expect warmer temperatures and the arrival of an early spring. Acknowledgements The historical weather predictions were provided by the Punxsutawney Groundhog Club and the average monthly temperatures were published by NOAA's National Climatic Data Center.,CSV,,[climate],CC0,,,298,3145,0.0068359375,How accurate is Punxsutawney Phil's winter weather forecast?,Groundhog Day Forecasts and Temperatures,https://www.kaggle.com/groundhogclub/groundhog-day,Wed Feb 01 2017
,Centers for Disease Control and Prevention,"[Year, State, Yes, No, Category, Condition, Location 1]","[numeric, string, string, string, string, string, string]",Context This dataset contains the prevalence and trends of health care access/coverage for 1995-2010. Percentages are weighted to population characteristics. Data are not available if it did not meet Behavioral Risk Factor Surveillance System (BRFSS) stability requirements. For more information on these requirements as well as risk factors and calculated variables see the Technical Documents and Survey Data for a specific year - http//www.cdc.gov/brfss/annual_data/annual_data.htm.  Content This dataset has 7 variables  Year State Yes No Category Condition Location 1  Acknowledgements The original dataset can be found here. Recommended citation Centers for Disease Control and Prevention (CDC). Behavioral Risk Factor Surveillance System. Atlanta Georgia U.S. Department of Health and Human Services Centers for Disease Control and Prevention [appropriate year]. Inspiration  How does health care coverage change over time? Does health care access differ by state? ,CSV,,[healthcare],Other,,,716,6456,0.2509765625,Prevalence and Trends of Health Care Acess,Health Care Access/Coverage for 1995-2010,https://www.kaggle.com/cdc/health-care-access-coverage,Thu Nov 17 2016
,Eurostat,"[Country, European Union, Accession Year, Council Votes, European Parliament Seats, European Free Trade Agreement, European Single Market, European Monetary Union, Currency, Currency Code, Language, Population, Area (km²), Population Density, GDP (€, millions), GDP ($, millions), GDP per capita ($, millions)]","[string, string, numeric, numeric, numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context The European Union is a unique economic and political union between twenty-eight countries that together cover much of the continent. It was created in the aftermath of the Second World War to foster economic cooperation and thus avoid conflict. The result was the European Economic Community (EEC) established in 1958 with Belgium Germany France Italy Luxembourg and the Netherlands as its members. What began as an economic union has evolved into an organization spanning policy areas from climate environment and health to external relations and security justice and migration. The 1993 name change from the European Economic Community (EEC) to the European Union (EU) reflected this. The European Union has delivered more than half a century of peace stability and prosperity helped raise living standards and launched a single European currency the Euro. In 2012 the EU was awarded the Nobel Peace Prize for advancing the causes of peace reconciliation democracy and human rights in Europe. The single market is the EU's main economic engine enabling most goods services money and people to move freely. Content The European Union covers over 4 million square kilometers and has 508 million inhabitants — the world’s third largest population after China and India. This dataset includes information on each EU member state candidate state or European Free Trade Agreement (EFTA) signatory state. Acknowledgements The membership population and economic data was published by the European Commission's Eurostat. Gross domestic product and GDP per capita in US dollars was provided by the World Bank. Inspiration How has the European Union grown in the past fifty years? What is the largest country by population or surface area? Which country has the largest economy by gross domestic product? How many different languages are spoken across all the member states?,CSV,,"[politics, international relations]",CC0,,,320,2914,0.00390625,What divides and unites the 28 countries in the European Union?,Member States of the European Union,https://www.kaggle.com/eurostat/european-union,Tue Mar 14 2017
,Backblaze,"[date, serial_number, model, capacity_bytes, failure, smart_1_normalized, smart_1_raw, smart_2_normalized, smart_2_raw, smart_3_normalized, smart_3_raw, smart_4_normalized, smart_4_raw, smart_5_normalized, smart_5_raw, smart_7_normalized, smart_7_raw, smart_8_normalized, smart_8_raw, smart_9_normalized, smart_9_raw, smart_10_normalized, smart_10_raw, smart_11_normalized, smart_11_raw, smart_12_normalized, smart_12_raw, smart_13_normalized, smart_13_raw, smart_15_normalized, smart_15_raw, smart_22_normalized, smart_22_raw, smart_183_normalized, smart_183_raw, smart_184_normalized, smart_184_raw, smart_187_normalized, smart_187_raw, smart_188_normalized, smart_188_raw, smart_189_normalized, smart_189_raw, smart_190_normalized, smart_190_raw, smart_191_normalized, smart_191_raw, smart_192_normalized, smart_192_raw, smart_193_normalized, smart_193_raw, smart_194_normalized, smart_194_raw, smart_195_normalized, smart_195_raw, smart_196_normalized, smart_196_raw, smart_197_normalized, smart_197_raw, smart_198_normalized, smart_198_raw, smart_199_normalized, smart_199_raw, smart_200_normalized, smart_200_raw, smart_201_normalized, smart_201_raw, smart_220_normalized, smart_220_raw, smart_222_normalized, smart_222_raw, smart_223_normalized, smart_223_raw, smart_224_normalized, smart_224_raw, smart_225_normalized, smart_225_raw, smart_226_normalized, smart_226_raw, smart_240_normalized, smart_240_raw, smart_241_normalized, smart_241_raw, smart_242_normalized, smart_242_raw, smart_250_normalized, smart_250_raw, smart_251_normalized, smart_251_raw, smart_252_normalized, smart_252_raw, smart_254_normalized, smart_254_raw, smart_255_normalized, smart_255_raw]","[dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string]",Context Each day Backblaze takes a snapshot of each operational hard drive that includes basic hard drive information (e.g. capacity failure) and S.M.A.R.T. statistics reported by each drive. This dataset contains data from the first two quarters in 2016. Content This dataset contains basic hard drive information and 90 columns or raw and normalized values of 45 different S.M.A.R.T. statistics. Each row represents a daily snapshot of one hard drive.  date Date in yyyy-mm-dd format serial_number Manufacturer-assigned serial number of the drive model Manufacturer-assigned model number of the drive capacity_bytes Drive capacity in bytes failure Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing. 90 variables that begin with 'smart' Raw and Normalized values for 45 different SMART stats as reported by the given drive  Inspiration Some items to keep in mind as you process the data  S.M.A.R.T. statistic can vary in meaning based on the manufacturer and model. It may be more informative to compare drives that are similar in model and manufacturer Some S.M.A.R.T. columns can have out-of-bound values When a drive fails the 'failure' column is set to 1 on the day of failure and starting the day after the drive will be removed from the dataset. Each day new drives are also added. This means that total number of drives each day may vary.  S.M.A.R.T. 9 is the number of hours a drive has been in service. To calculate a drive's age in days divide this number by 24.  Given the hints above below are a couple of questions to help you explore the dataset  What is the median survival time of a hard drive? How does this differ by model/manufacturer? Can you calculate the probability that a hard drive will fail given the hard drive information and statistics in the dataset?  Acknowledgement The original collection of data can be found here. When using this data Backblaze asks that you cite Backblaze as the source; you accept that you are solely responsible for how you use the data; and you do not sell this data to anyone.,CSV,,[computer science],Other,,,801,8441,1024,Daily Snapshot of Each Operational Hard Drive in 2016,Hard Drive Test Data,https://www.kaggle.com/backblaze/hard-drive-test-data,Sat Nov 05 2016
,Starbucks,"[Beverage_category, Beverage, Beverage_prep, Calories,  Total Fat (g), Trans Fat (g) , Saturated Fat (g),  Sodium (mg),  Total Carbohydrates (g) , Cholesterol (mg),  Dietary Fibre (g),  Sugars (g),  Protein (g) , Vitamin A (% DV) , Vitamin C (% DV),  Calcium (% DV) , Iron (% DV) , Caffeine (mg)]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, numeric]",Context Starbucks is an American coffee chain founded in Seattle. It serves both beverages and food.  Content This dataset includes the nutritional information for Starbucks’ food and drink menu items. All nutritional information for drinks are for a 12oz serving size. Acknowledgements Food composition data is in the public domain but product names marked with ® or ™ remain the registered trademarks of Starbucks. Inspiration  Can you train a Markov Chain to generate new Starbucks drink or food items? Can you design an easy-to-interpret visualization for the nutrition of each item? How to Starbucks menu items compare to McDonald’s menu items (see link to dataset below) in terms of nutrition?  You may also like  Nutrition Facts for McDonald's Menu  Starbucks Locations Worldwide  ,CSV,,"[food and drink, nutrition]",Other,,,1253,8387,0.04296875,"Nutrition information for Starbucks menu items, including food and drinks",Nutrition facts for Starbucks Menu,https://www.kaggle.com/starbucks/starbucks-menu,Fri Jul 21 2017
,National Snow and Ice Data Center,"[Year, Month, Day, Extent, Missing, Source Data, hemisphere]","[numeric, numeric, numeric, numeric, numeric, string, string]",Context The National Snow and Ice Data Center (NSIDC) supports research into our world’s frozen realms the snow ice glaciers frozen ground and climate interactions that make up Earth’s cryosphere. NSIDC manages and distributes scientific data creates tools for data access supports data users performs scientific research and educates the public about the cryosphere.  Content The dataset provides the total extent for each day for the entire time period (1978-2015). There are 7 variables  Year Month Day Extent unit is 10^6 sq km Missing unit is 10^6 sq km Source Source data product web site http//nsidc.org/data/nsidc-0051.html hemisphere  Acknowledgements The original datasets can be found here and here. Inspiration  Can you visualize the change in sea ice over time? Do changes in sea ice differ between the two hemispheres? ,CSV,,"[environment, climate]",Other,,,1225,10609,4,Total sea ice extent from 1978 to present,Daily Sea Ice Extent Data,https://www.kaggle.com/nsidcorg/daily-sea-ice-extent-data,Fri Jun 09 2017
,UCI Machine Learning,"[A01, 010-000-024-033, 633790226051280329, 27.05.2009 14:03:25:127, 4.062931060791016, 1.8924342393875122, 0.5074254274368286, walking]","[string, string, numeric, string, numeric, numeric, numeric, string]",Context These datums  represent a multi-agent system for the care of elderly people living at home on their own with the aim to prolong their independence. The system was designed to provide a reliable robust and flexible monitoring by sensing the user in the environment reconstructing the position and posture to create the physical awareness of the user in the environment reacting to critical situations calling for help in the case of an emergency and issuing warnings if unusual behavior was detected.  Content Columns descriptions  Sequence Name A01 A02 A03 A04 A05 B01 B02 B03 B04 B05 ...E05 A-E represent a person (5 total) 01 02 03 04 05 = Tag Numbers Tag identifiers ANKLE_LEFT = 010-000-024-033 ANKLE_RIGHT = 010-000-030-096 CHEST = 020-000-033-111 BELT = 020-000-032-221 Time stamp Date Format = dd.MM.yyyy HHmmssSSS x coordinate of the tag  y coordinate of the tag  z coordinate of the tag  activity walking falling 'lying down' lying 'sitting down' sitting 'standing up from lying' 'on all fours' 'sitting on the ground' 'standing up from sitting' 'standing up from sitting on the ground  Acknowledgements B. Kaluza V. Mirchevska E. Dovgan M. Lustrek M. Gams An Agent-based Approach to Care in Independent Living International Joint Conference on Ambient Intelligence (AmI-10) Malaga Spain In press Inspiration Given these data can you classify the persons activity from the tags they wore?,CSV,,"[healthcare, biotechnology, programming]",Other,,,82,1610,21,Recordings of five people while wearing localization tags,Localization Data for Posture Reconstruction,https://www.kaggle.com/uciml/posture-reconstruction,Wed Sep 06 2017
,Human Computation,"[task_id, unit_id, assignment_id, session_id, dt_start, key]","[numeric, numeric, string, numeric, numeric, numeric]","Context Data Scientists often use crowdsourcing platforms such as Amazon Mechanical Turk or CrowdFlower to collect labels for their data. Controlling high quality and timeless execution of tasks is an important part of such collection process. It is not possible (or not efficient) to manually check every worker assignment. There is an intuition that there quality could be predicted based on workers task browser behaviour (e.g. key presses scrolling mouse clicks tab switching). In this dataset there are assignment results for 3 different crowdsourcing tasks launched on CrowdFlower along with associated workers behaviour. Content We collected data running 3 tasks   Image labelling   Receipt Transcription   Business Search.   Tasks are described in tasks.csv. Results for corresponding tasks are given in files results_{task_id}.csv. Workers's activity could be found in the following files  activity_keyboard.csv - timestamps of keyboard keys pressed activity_mouse.csv - timestamps of mouse clicks with associated HTML elements activity_tab.csv - timestamps of event task browser tab changes (opened active hidden closed) activity_page.csv - a summary of events happened in the task page every 2 seconds (boolean keyboard activity boolean mouse movement activity boolean scrolling activity the position of the screen boolean if text was selected)  Result files have a similar structure to the original one given by CrowdFlower  _unit_id A unique ID number created by the system for each row _created_at The time the contributor submitted the judgement  _golden This will be ""true"" if this is a test question otherwise it is ""false"" _id A unique ID number generated for this specific judgment _missed This will be ""true"" if the row is an incorrect judgment on a test question. _started_at The time at which the contributor started working on the judgement _tainted This will be ""true"" if the contributor has been flagged for falling below the required accuracy. This judgment will not be used in the aggregation. _channel The work channel that the contributor accessed the job through _trust The contributor's accuracy. Learn more about trust here _worker_id A unique ID number assigned to the contributor (in the current dataset MD5 value is given) _country The country the contributor is from _region A region code for the area the contributor is from _city The city the contributor is from _ip The IP address for the contributor (in the current dataset MD5 value is given) {{field}} There will be a column for each field in the job with a header equal to the field's name. {{field}}_gold The correct answer for the test question  Acknowledgements We thank crowd workers who accomplished our not always exciting tasks on CrowdFlower. ",CSV,,[web sites],CC4,,,82,1940,10,In-page behaviour of crowdworkers performing tasks on CrowdFlower,Workers Browser Activity in CrowdFlower Tasks,https://www.kaggle.com/humancomp/worker-activity-crowdflower,Fri Oct 20 2017
,Jacob Boysen,"[GWNO, EVENT_ID_CNTY, EVENT_ID_NO_CNTY, EVENT_DATE, YEAR, TIME_PRECISION, EVENT_TYPE, ACTOR1, ALLY_ACTOR_1, INTER1, ACTOR2, ALLY_ACTOR_2, INTER2, INTERACTION, COUNTRY, ADMIN1, ADMIN2, ADMIN3, LOCATION, LATITUDE, LONGITUDE, GEO_PRECISION, SOURCE, NOTES, FATALITIES]","[numeric, string, numeric, dateTime, numeric, numeric, string, string, string, numeric, string, string, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, string, string, numeric]",Context The Armed Conflict Location and Event Data Project is designed for disaggregated conflict analysis and crisis mapping. This dataset codes the dates and locations of all reported political violence and protest events in developing Asian countries in. Political violence and protest includes events that occur within civil wars and periods of instability public protest and regime breakdown. The project covers 2015 to the present. Content These data contain information on  Dates and locations of conflict events; Specific types of events including battles civilian killings riots protests and recruitment activities; Events by a range of actors including rebels governments militias armed groups protesters and civilians; Changes in territorial control; and Reported fatalities.  Event data are derived from a variety of sources including reports from developing countries and local media humanitarian agencies and research publications. Please review the codebook and user guide for additional information the codebook is for coders and users of ACLED whereas the brief guide for users reviews important information for downloading reviewing and using ACLED data. A specific user guide for development and humanitarian practitioners is also available as is a guide to our sourcing materials. Acknowledgements ACLED is directed by Prof. Clionadh Raleigh (University of Sussex). It is operated by senior research manager Andrea Carboni (University of Sussex) for Africa and Hillary Tanoff for South and South-East Asia. The data collection involves several research analysts including Charles Vannice James Moody Daniel Wigmore-Shepherd Andrea Carboni Matt Batten-Carew Margaux Pinaud Roudabeh Kishi Helen Morris Braden Fuller Daniel Moody and others. Please cite Raleigh Clionadh Andrew Linke Håvard Hegre and Joakim Karlsen. 2010. Introducing ACLED-Armed Conflict Location and Event Data. Journal of Peace Research 47(5) 651-660. Inspiration Do conflicts in one region predict future flare-ups? How do the individual actors interact across time?,CSV,,[war],CC0,,,35,567,13,35k Conflicts Across Developing Asian Countries,"ACLED Asian Conflicts, 2015-2017",https://www.kaggle.com/jboysen/asian-conflicts,Wed Aug 09 2017
,Jemilu Mohammed,"[, id, homeTacklesTotalHT, homeShotsTotalHT, awayDispossessedFT, awayPassSuccessFT, awayRatingsFT, awayDribbleSuccessFT, awayDribblesAttemptedHT, awayTeamLineUp, awayShotsBlockedHT, awayShotsTotalHT, homeDribbleSuccessHT, homeFoulsCommitedFT, homeAerialsTotalFT, homeRatingsHT, awayShotsOnTargetFT, awayShotsBlockedFT, homeInterceptionsHT, awayPossessionHT, homePassesKeyFT, awayShotsOnTargetHT, awayDribblesWonFT, awayTackleSuccessHT, homeCornersTotalFT, homeAerialsTotalHT, homeShotsBlockedFT, awayCornersTotalHT, homeCornersTotalHT, homeDribbleSuccessFT, homeTeamLineUp, awayPassSuccessHT, awayDribblesWonHT, homeDispossessedHT, awayAerialsTotalFT, homeShotsBlockedHT, awayPassesKeyFT, homeTackleSuccessHT, awayPassesKeyHT, homeFormation, awayInterceptionsHT, awayDispossessedHT, refereeName, homeDribblesWonHT, homePossessionFT, awayAerialsTotalHT, awayGoalHT, awayManagerName, awayInterceptionsFT, homeDribbledPastFT, homeGoalHT, awayDribbleSuccessHT, homeGoalFT, awayTacklesTotalHT, homeDribblesWonFT, awayTackleSuccessFT, awayTeam, homeDispossessedFT, awayOffsidesCaughtFT, awayDribbledPastFT, homeShotsOnTargetHT, awayFormation, awayOffsidesCaughtHT, homeDribbledPastHT, awayFoulsCommitedHT, homeShotsTotalFT, homePassSuccessFT, homeFoulsCommitedHT, awayCornersTotalFT, homeTeam, homeManagerName, awayFoulsCommitedFT, homeShotsOnTargetFT, homeDribblesAttemptedHT, awayRatingsHT, homeOffsidesCaughtHT, homeTacklesTotalFT, awayDribbledPastHT, awayGoalFT, homePassesKeyHT, homeOffsidesCaughtFT, homePossessionHT, venueName, awayDribblesAttemptedFT, homeInterceptionsFT, homePassSuccessHT, date, awayTacklesTotalFT, homeRatingsFT, homeDribblesAttemptedFT, homeTackleSuccessFT, awayPossessionFT, awayShotsTotalFT, division]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context I am a student exploring the possibility of making money in football betting. I am currently doing a literature review on modelling association football scores and trying to put together a machine learning system to use for my first betting campaign next season. What I have learned thus far is that outcomes of football events are partly deterministic and partly random. I do not know exactly how to go about implementing this in a machine learning system yet. I am also hoping to find useful features from this dataset. Content The data here contains match statistics collected from whoscored.com europes top five leagues from 2012-2013 to 2016-2017 season. It contains just about all match statistics that anyone can ever hope for including but not limited to  Goals Corners Possession Ratings Coaches LineUps and other relevent match statistics The features are simply just self explanatory and have been given long but meaningful names Acknowledgement I collected the data from the whoscored.com website. I scraped it using beautifulSoup in python and just extracted the features I thought could have some use. Inspiration This is just something I hope could become something but hey it may be nothing. I am just interested to know the kind of insights that could be generated from this.,CSV,,[association football],CC0,,,603,4386,6,"Italy, Spain, England, Germany, France 2012-2017",Match Statistics from top 5 European Leagues,https://www.kaggle.com/jangot/ligue1-match-statistics,Fri Jul 07 2017
,DrGuillermo,"[Country, Region, Population, Area, PopDensity, Coastline, Net migration, Infant_mortality, GDP, Literacy, Phones, Arable, Crops, Other, Climate, Birthrate, Deathrate, Agriculture, Industry, Service, FIFA_Rank, UEFA_Rank, Attendance, Home_Away_Contrast]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context The data set includes information about different leagues in different sports (Basketball and Soccer) all around the world as well as some basic facts about each country regarding the home advantage phenomenon in sports.  Content The data is comprised of 3 data sets  The home and away performance of 8365 soccer teams from a few dozens of countries during the years 2010-2016. The home and away performance of 1216 NBA teams during the years 1968-2010 General facts about 88 countries including soccer data such as their FIFA rank the average attendance of soccer matches and the Home Advatnage Factor of the leagure  Acknowledgement The soccer data was scraped from here http//footballdatabase.com/competitions-index The NBA data was scraped from NBA.com. The world facts were copied from Wikipedia.,CSV,,"[association football, basketball]",Other,,,726,4633,0.7890625,Home and away performance of 9K+ Teams from 88 leagues around the world,Home Advantage in Soccer and Basketball,https://www.kaggle.com/drgilermo/home-advantage-in-soccer-and-basketball,Sat Jan 21 2017
,Datafiniti,"[address, categories, city, country, key, lat, long, name, phones, postalCode, province, websites]","[string, string, string, string, string, numeric, numeric, string, numeric, numeric, string, string]",About This Data This is a list of over 7000 breweries and brewpubs in the USA provided by Datafiniti's Business Database.  The dataset includes the category name address city state and more for each listing.  What You Can Do With This Data You can use this geographical and categorical information for business locations to determine which cities and states have the most breweries. E.g.   What is the number of breweries in each state? What are the cities with the most breweries per person? What are the states with the most breweries per person? What are the top cities for breweries? What are the top states for breweries? What industry categories are typically grouped with breweries?  Data Schema A full schema for the data is available in our support documentation. About Datafiniti Datafiniti provides instant access to web data. We compile data from thousands of websites to create standardized databases of business product and property information. Learn more. Want More? You can get more data like this by joining Datafiniti or requesting a demo.,CSV,,"[databases, food and drink, business]",CC4,,,457,4170,22,"A list of over 7,000 breweries and brew pubs in the USA.",Breweries and Brew Pubs in the USA,https://www.kaggle.com/datafiniti/breweries-brew-pubs-in-the-usa,Tue Sep 19 2017
,Mario Pasquato,[],[],"Context Stars mostly form in clusters and associations rather than in isolation. Milky Way star clusters are easily observable with small telescopes and in some cases even with the naked eye. Depending on a variety of conditions star clusters may dissolve quickly or be very long lived. The dynamical evolution of star clusters is a topic of very active research in astrophysics. Some popular models of star clusters are the so-called direct N-body simulations [1 2] where every star is represented by a point particle that interacts gravitationally with every other particle. This kind of simulation is computationally expensive as it scales as O(N^2) where N is the number of particles in the simulated cluster. In the following the words ""particle"" and ""star"" are used interchangeably. Content This dataset contains the positions and velocities of simulated stars (particles) in a direct N-body simulation of a star cluster. In the cluster there are initially 64000 stars distributed in position-velocity space according to a King model [3]. Each .csv file named c_xxxx.csv corresponds to a snapshot of the simulation at time t = xxxx. For example c_0000.csv contains the initial conditions (positions and velocities of stars at time t=0). Times are measured in standard N-body units [4]. This is a system of units where G = M = −4E = 1 (G is the gravitational constant M the total mass of the cluster and E its total energy). x y z Columns 1 2 and 3 of each file are the x y z positions of the stars. They are also expressed in standard N-body units [4]. You can switch to units of the median radius of the cluster by finding the cluster center and calculating the median distance of stars from it and then dividing x y and z by this number. In general the median radius changes in time. The initial conditions are approximately spherically symmetric (you can check) so there is no particular physical meaning attached to the choice of x y and z.  vx vy vz Columns 4 5 and 6 contain the x y and z velocity also in N-body units. A scale velocity for the stars can be obtained by taking the standard deviation of velocity along one direction (e.g. z). You may check that the ratio between the typical radius (see above) and the typical velocity is of order unity. m Column 7 is the mass of each star. For this simulation this is identically 1.5625e-05 i.e. 1/64000. The total mass of the cluster is initially 1. More realistic simulations (coming soon) have a spectrum of different masses and live stelar evolution that results in changes in the mass of stars. This simulation is a pure N-body problem instead. Star id number The id numbers of each particle are listed in the last column (8) of the files under the header ""id"". The ids are unique and can be used to trace the position and velocity of a star across all files. There are initially 64000 particles. At end of the simulation there are 63970. This is because some particles escape the cluster. Acknowledgements This simulation was run on a Center for Galaxy Evolution Research (CGER) workstation at Yonsei University (Seoul Korea) using the NBODY6 software (https//www.ast.cam.ac.uk/~sverre/web/pages/nbody.htm). Inspiration Some stars hover around the center of the cluster while some other get kicked out to the cluster outskirts or even leave the cluster altogether. Can we predict where a star will be at any given time based on its initial position and velocity? Can we predict its velocity? How correlated are the motions of stars? Can we predict the velocity of a given star based on the velocity of its neighbours? The size of the cluster can be measured by defining a center (see below) and finding the median distance of stars from it. This is called the three-dimensional effective radius. Can we predict how it evolves over time? What are its properties as a time series? What can we say about other quantiles of the radius? How to define the cluster center? Just as the mode of a KDE of the distribution of stars? How does it move over time and how to quantify the properties of its fluctuations? Is the cluster symmetric around this center? Some stars leave the cluster over time they exchange energy in close encounters with other stars and reach the escape velocity. This can be seen by comparing later snapshots with the initial one some IDs are missing and there is overall a lower number of stars. Can we predict which stars are more likely to escape? When will a given star escape? References [1] Heggie D. Hut P. 2003 The Gravitational Million-Body Problem A Multidisciplinary Approach to Star Cluster Dynamics ~ Cambridge University Press 2003 [2] Aarseth S.~J. 2003 Gravitational N-Body Simulations - Cambridge University Press 2003 [3] King I. 1966 AJ 71 64 [4] Heggie D. C. Mathieu R. D. 1986 Lecture Notes in Physics Vol. 267 The Use of Supercomputers in Stellar Dynamics Berlin Springer",CSV,,"[astronomy, space]",CC0,,,1328,12766,109,Direct N-body simulation of a star cluster: Position and velocities of stars,Star Cluster Simulations,https://www.kaggle.com/mariopasquato/star-cluster-simulations,Sat Jan 07 2017
,David Schwertfeger,[],[],"Context People have been making music for tens of thousands of years [1]. Today making music is easier and more accessible than ever before. The technological developments of the last few decades allow people to simulate playing every imaginable instrument on their computers. Audio sequencers enable users to arrange their songs on a time line sample by sample. Digital audio workstations (DAWs) ship with virtual instruments and synthesizers which allow users to virtually play a whole band or orchestra in their bedrooms. One challenge in working with DAWs is organizing samples and recordings in a structured way; so users can easily access them. In addition to their own recordings many users download samples. Browsing through sample collections to find the perfect sound is time consuming and may impede the user's creative flow [2]. On top of this manually naming and tagging recordings is a time-consuming and tedious task so not many users do [3]. The consequence is that finding the right sound at the right moment becomes a challenging problem [4]. Modeling the relationship between the acoustic content and semantic descriptions of sounds could allow users to retrieve sounds using text queries. This dataset was collected to support research on content-based audio retrieval systems focused on sounds used in creative context. Content This dataset was collected from Freesound [5] in June 2016. It contains the frame-based MFCCs of about 230000 sounds and the associated tags.  sounds.json Sound metadata originally downloaded from the Freesound API. This file includes the id associated tags links to previews and links to an analysis_frames file which contains frame-based low-level features for each sound. preprocessed_tags.csv Preprocessed tags. Contains only tags which are associated to at least 0.01% of sounds. Moreover tags were split on hyphens and stemmed. Tags containing numbers and short tags with less than three characters were removed. queries.csv An aggregated query-log of real user-queries against the Freesound database collected between May 11 and November 24 in 2016. preprocessed_queries.csv Queries were preprocessed in the same way tags were preprocessed. *_mfccs.csv.bz2 The original MFCCs for each sound extracted from the URL provided in the analysis_frames field of sounds.json split across ten files. cb_{512|1024|2048|4096}_sparse.pkl Codebook representation of sounds saved as sparse  pd.DataFrame. The first-order and second-order derivatives of the 13 MFCCs were appended to the MFCC feature vectors of each sound. All frames were clustered using K-Means (Mini-Batch K-Means) to find {512|1024|2048|4096} cluster centers. Each frame was then assigned to its closest cluster center and the counts used to represent a sound as a single {512|1024|2048|4096}-dimensional vector.   Acknowledgements Thanks to the Music Technology Group of the Universitat Pompeu Fabra in Barcelona for creating and maintaining the Freesound [5] database and for providing the aggregated query-logs. Inspiration Who can create the best content-based audio retrieval system measured by precision-at-k for values of k in {1 ... 20} and mean average precision. Getting started Here's the accompanying GitHub repository https//github.com/dschwertfeger/cbar  References [1] N. L. Wallin and B. Merker The Origins of Music. MIT Press 2001. [2] M. Csikszentmihalyi Flow The Psychology of Optimal Experience. New York Harper Perennial Modern Classics 2008. [3] E. Pampalk A. Rauber and D. Merkl ""Content-based organization and visualization of music archives"" in Proceedings of the tenth ACM international conference on Multimedia 2002 pp. 570–579. [4] T. Bertin-Mahieux D. Eck and M. Mandel ""Automatic tagging of audio The state-of-the-art"" Machine audition Principles algorithms and systems pp. 334–352 2010. [5] F. Font G. Roma and X. Serra ""Freesound technical demo"" 2013 pp. 411–412.",CSV,,"[linguistics, sound technology]",CC4,,,264,3565,5120,Find sounds with text-queries based on their acoustic content,Freesound: Content-Based Audio Retrieval,https://www.kaggle.com/dschwertfeger/freesound,Tue Dec 27 2016
,University of Pittsburgh,"[week, state, state_name, disease, cases, incidence_per_capita]","[numeric, string, string, string, numeric, numeric]",Context The Project Tycho database was named after the Danish nobleman Tycho Brahe who is known for his detailed astronomical and planetary observations. Tycho was not able to use all of his data for breakthrough discoveries but his assistant Johannes Kepler used Tycho's data to derive the laws of planetary motion. Similarly this project aims to advance the availablity of large scale public health data to the worldwide community to accelerate advancements in scientific discovery and technological progress. Content The Project Tycho database (level one) includes standardized counts at the state level for smallpox polio measles mumps rubella hepatitis A and whooping cough from weekly National Notifiable Disease Surveillance System (NNDSS) reports for the United States. The time period of data varies per disease somewhere between 1916 and 2010. The records include cases and incidence rates per 100000 people based on historical population estimates. These data have been used by investigators at the University of Pittsburgh to estimate the impact of vaccination programs in the United States recently published in the New England Journal of Medicine. Acknowledgements The Project Tycho database was digitized and standardized by a team at the University of Pittsburgh including Professor Wilbert van Panhuis MD PhD Professor John Grefenstette PhD and Dean Donald Burke MD.,CSV,,"[healthcare, diseases]",Other,,,787,4781,20,"Weekly case reports for polio, smallpox, and other diseases in the United States",Project Tycho: Contagious Diseases,https://www.kaggle.com/pitt/contagious-diseases,Thu Feb 02 2017
,Daisuke Ishii,"[Date, Open, High, Low, Close, Volume, Stock Trading]","[dateTime, numeric, numeric, numeric, numeric, numeric, numeric]","Context We are doing Fintech data hakathon in Tokyo everyweek. Let's predict stock price in Tokyo Stock Exchange. 毎週水曜日東京・渋谷で開催している、Team AI ""FinTech Data Hackathon""の題材として、 身近なユニクロ(ファーストリテイリング)の株価予測モデルをオープンイノベーションで構築します。 https//www.meetup.com/Machine-Learning-Meetup-by-team-ai/events/242154425/ Content Training; 5 year daily stock price info of FastRetailing(Uniqlo). You should predict ""close"" price. Test 1 week daily stock price Acknowledgements Thanks to open market data http//k-db.com/ Inspiration Let's build basic stock prediction model together!  公開されたモデルを実際の取引に使う場合は十分注意ください。弊社側やコミュニティメンバー側では損失の責任は持てません。",CSV,,"[time series, finance]",CC0,,,332,3696,0.064453125,Tokyo Stock Exchange Data (LightWeight CSV) in 2016 for Beginners,Uniqlo (FastRetailing) Stock Price Prediction,https://www.kaggle.com/daiearth22/uniqlo-fastretailing-stock-price-prediction,Mon Aug 07 2017
,ArcGIS Open Data,"[OBJECTID, ATTRIBUTE, WETLAND_TYPE, ACRES, GLOBALID, ShapeSTArea, ShapeSTLength]","[numeric, string, string, numeric, string, numeric, numeric]","Context The data delineate the areal extent of wetlands and surface waters as defined by Cowardin et al. (1979). Certain wetland habitats are excluded from the National mapping program because of the limitations of aerial imagery as the primary data source used to detect wetlands. These habitats include seagrasses or submerged aquatic vegetation that are found in the intertidal and subtidal zones of estuaries and near shore coastal waters. Some deepwater reef communities (coral or tuberficid worm reefs) have also been excluded from the inventory. These habitats because of their depth go undetected by aerial imagery. By policy the Service also excludes certain types of ""farmed wetlands"" as may be defined by the Food Security Act or that do not coincide with the Cowardin et al. definition. Contact the Service's Regional Wetland Coordinator for additional information on what types of farmed wetlands are included on wetland maps.  Content The dataset includes  OBJECTID ATTRIBUTE WETLAND_TYPE ACRES GLOBALID ShapeSTArea ShapeSTLength  Acknowledgement The original dataset and metadata can be found here. Inspiration  Can you visualizes the differences in the wetlands shape by type? ",CSV,,[ecology],Other,,,91,1876,80,Location and Type of Wetlands and Deepwater Habitats in the United States,National Wetlands Inventory,https://www.kaggle.com/arcgisopendata/national-wetlands-inventory,Thu Nov 17 2016
,Berkeley Earth,"[dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]","[dateTime, numeric, numeric, string, string, string, string]",Some say climate change is the biggest threat of our age while others say it’s a myth based on dodgy science. We are turning some of the data over to you so you can form your own view.  Even more than with other data sets that Kaggle has featured there’s a huge amount of data cleaning and preparation that goes into putting together a long-time study of climate trends. Early data was collected by technicians using mercury thermometers where any variation in the visit time impacted measurements. In the 1940s the construction of airports caused many weather stations to be moved. In the 1980s there was a move to electronic thermometers that are said to have a cooling bias. Given this complexity there are a range of organizations that collate climate trends data. The three most cited land and ocean temperature data sets are NOAA’s MLOST NASA’s GISTEMP and the UK’s HadCrut.  We have repackaged the data from a newer compilation put together by the Berkeley Earth which is affiliated with Lawrence Berkeley National Laboratory. The Berkeley Earth Surface Temperature Study combines 1.6 billion temperature reports from 16 pre-existing archives. It is nicely packaged and allows for slicing into interesting subsets (for example by country). They publish the source data and the code for the transformations they applied. They also use methods that allow weather observations from shorter time series to be included meaning fewer observations need to be thrown away.  In this dataset we have include several files Global Land and Ocean-and-Land Temperatures (GlobalTemperatures.csv)    Date starts in 1750 for average land temperature and 1850 for max and min land temperatures and global ocean and land temperatures LandAverageTemperature global average land temperature in celsius   LandAverageTemperatureUncertainty the 95% confidence interval around the average   LandMaxTemperature global average maximum land temperature in celsius   LandMaxTemperatureUncertainty the 95% confidence interval around the maximum land temperature   LandMinTemperature  global average minimum land temperature in celsius   LandMinTemperatureUncertainty the 95% confidence interval around the minimum land temperature   LandAndOceanAverageTemperature global average land and ocean temperature in celsius   LandAndOceanAverageTemperatureUncertainty the 95% confidence interval around the global average land and ocean temperature    Other files include    Global Average Land Temperature by Country (GlobalLandTemperaturesByCountry.csv)   Global Average Land Temperature by State (GlobalLandTemperaturesByState.csv)   Global Land Temperatures By Major City (GlobalLandTemperaturesByMajorCity.csv)   Global Land Temperatures By City (GlobalLandTemperaturesByCity.csv)  The raw data comes from the Berkeley Earth data page.,CSV,,"[environment, climate]",CC4,,,23013,134942,573,Exploring global temperatures since 1750,Climate Change: Earth Surface Temperature Data,https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data,Mon May 01 2017
,Crowdflower,"[_unit_id, _golden, _unit_state, _trusted_judgments, _last_judgment_at, gender, gender:confidence, profile_yn, profile_yn:confidence, created, description, fav_number, gender_gold, link_color, name, profile_yn_gold, profileimage, retweet_count, sidebar_color, text, tweet_coord, tweet_count, tweet_created, tweet_id, tweet_location, user_timezone]","[numeric, string, string, numeric, dateTime, string, numeric, string, numeric, dateTime, string, numeric, numeric, string, string, numeric, string, numeric, numeric, string, numeric, numeric, dateTime, numeric, numeric, numeric]","This data set was used to train a CrowdFlower AI gender predictor. You can read all about the project here. Contributors were asked to simply view a Twitter profile and judge whether the user was a male a female or a brand (non-individual). The dataset contains 20000 rows each with a user name a random tweet account profile and image location and even link and sidebar color. Inspiration Here are a few questions you might try to answer with this dataset  how well do words in tweets and profiles predict user gender? what are the words that strongly predict male or female gender? how well do stylistic factors (like link color and sidebar color) predict user gender?  Acknowledgments Data was provided by the Data For Everyone Library on Crowdflower. Our Data for Everyone library is a collection of our favorite open data jobs that have come through our platform. They're available free of charge for the community forever. The Data The dataset contains the following fields  _unit_id a unique id for user _golden whether the user was included in the gold standard for the model; TRUE or FALSE _unit_state state of the observation; one of finalized (for contributor-judged) or golden (for gold standard observations) _trusted_judgments number of trusted judgments (int); always 3 for non-golden and what may be a unique id for gold standard observations _last_judgment_at date and time of last contributor judgment; blank for gold standard observations gender one of male female or brand (for non-human profiles) genderconfidence a float representing confidence in the provided gender profile_yn ""no"" here seems to mean that the profile was meant to be part of the dataset but was not available when contributors went to judge it profile_ynconfidence confidence in the existence/non-existence of the profile created date and time when the profile was created description the user's profile description fav_number number of tweets the user has favorited gender_gold if the profile is golden what is the gender? link_color the link color on the profile as a hex value name the user's name profile_yn_gold whether the profile y/n value is golden  profileimage a link to the profile image retweet_count number of times the user has retweeted (or possibly been retweeted) sidebar_color color of the profile sidebar as a hex value text text of a random one of the user's tweets tweet_coord if the user has location turned on the coordinates as a string with the format ""[latitude longitude]"" tweet_count number of tweets that the user has posted tweet_created when the random tweet (in the text column) was created tweet_id the tweet id of the random tweet tweet_location location of the tweet; seems to not be particularly normalized user_timezone the timezone of the user ",CSV,,"[gender, twitter, internet]",CC0,,,3860,32882,8,Predict user gender based on Twitter profile information,Twitter User Gender Classification,https://www.kaggle.com/crowdflower/twitter-user-gender-classification,Mon Nov 21 2016
,Ron Graf,"[Player_Id, Year, Rnd, Pick, Tm, Player, Pos, Position Standard, First4AV, Age, To, AP1, PB, St, CarAV, DrAV, G, Cmp, Pass_Att, Pass_Yds, Pass_TD, Pass_Int, Rush_Att, Rush_Yds, Rush_TDs, Rec, Rec_Yds, Rec_Tds, Tkl, Def_Int, Sk, College/Univ, ]","[string, numeric, numeric, numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string]",Context Consolidated draft data from http//www.pro-football-reference.com/ for all drafts from 1985 to 2015. Content Pro-Football-Reference AV Approximate Value is PFR's attempt to attach a single number to every player-season since 1960. Methodology can be found here http//www.pro-football-reference.com/blog/indexd961.html?page_id=8061  Player_Id    Pro Football Reference Player Id Year Draft Year Rnd  Draft Round Pick Draft Pick Tm   Team Player   Player first and last name Pos  Position unfiltered Position Standard    Position standardized to one of the following QB LB WR T DE RB DB DT C C G TE FB P LS K First4AV AV accumulated for this player's first four seasons Age  Age at time of draft To   Year of last season played AP1  # of first team all-pro selections PB   # of pro-bowl selections St   # of years as a primary starter in their primary position CarAV    Weighted Career AV - 100% of best season 95% of second best season 90% of third best season and so on DrAV AV accumulated for team that drafted this  player G    Games played Cmp  Pass completions Pass_Att Pass attempts Pass_Yds Yards gained by passing Pass_TD  Passing touchdowns Pass_Int Interceptions thrown Rush_Att Rushing attempts Rush_Yds Rushing yards gained Rush_TDs Rushing touchdowns Rec  Receptions Rec_Yds  Receiving yards gained Rec_Tds  Receiving touchdowns Tkl  Tackles Def_Int  Defensive interceptions Sk   Sacks College/Univ College/University attended by player  Acknowledgements http//www.pro-football-reference.com/,CSV,,[american football],Other,,,978,7356,0.76171875,All players selected in the NFL Draft from 1985 to 2015 with outcome statistics,NFL Draft Outcomes,https://www.kaggle.com/ronaldjgrafjr/nfl-draft-outcomes,Mon Dec 19 2016
,Christian Vorhemus,"[DATETIME, COMPLEXITY, MAP, MONSTERS_SELECTED, MONSTERS_HIT]","[dateTime, numeric, string, string, string]","The Game ""Who Dies?"" is a simple physics puzzle available for Android. Randomly a world full of stones monsters coil springs slingshots and other objects is created. The user has to guess which monster will get hit by a stone or falls of the platform when gravity is turned on. He gets point for every right guess the more points he collects the more complex the worlds will get.  The Development For development Phaser was used the game map is a 35x20 grid. Each tile in this grid can contain different blockers or objects.   The Data Every time a user is playing the game the position of all objects is recorded as well as the selection the user has made and the final set of monsters who died. The dataset consists of 5 columns DATETIME is a timestamp of when a user played the game. COMPLEXITY is a parameter that measures the difficulty of the game (1 = easy 100 = hard). MAP is a JSON array containing 3 arrays The first array contains immobile foreground objects described by a ""type"" property including x and y coordinates. The following list gives an overview about the the most commonly used types  The second array contains immobile background objects that don't interact with the game objects and are therefore not relevant. The third array contains movable foreground objects. These could be monsters (""guys"") balls (""smallball"" ""ball"" and ""bigball"" with or without an initial rotation to the left or right) spring (catapults boxes and balls up in the air but not monsters) box chain seesaw spin switch and switchwall (if a ball touches a switch all switchwalls with the same color as the switch change their visibility and become transparent to foreground objects or vice versa). Column ""MONSTERS_SELECTED"" contains all the monsters a player thought will get hit by a ball (ordered by the selection time) ""MONSTERS_HIT"" contains all monsters that were actually killed by balls or fell of the platform (ordered by time) The Goal There are several interesting outcomes for example  Creating a ML algorithm that is able to correctly predict the outcome of the game (which monster will die) Creating a ML algorithm that is able to correctly predict which monsters a user will most likely pick  Creating a ML algorithm that is able to create new (better?) game worlds ",CSV,,"[video games, artificial intelligence]",CC0,,,65,1935,365,Who can predict the outcome of a physics puzzle better - human or machine?,Who Dies? Physics Puzzle Dataset,https://www.kaggle.com/christianvorhemus/physicspuzzlewhodies,Thu Jun 22 2017
,toby jolly,"[Case Number, Date, Year, Type, Country, Area, Location, Activity, Name, Sex , Age, Injury, Fatal (Y/N), Time, Species , Investigator or Source, pdf, href formula, href, Case Number, Case Number, original order, , ]","[string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string]",Context This is a table of shark attack incidents compiled by the Global Shark Attack File Please see their website for more details on where this data comes from. Acknowledgements This data was downloaded with permission from the Global Shark Attack File's website,CSV,,[oceans],Other,,,3452,21505,0.5302734375,Data compiled by the global shark attack file,Global Shark Attacks,https://www.kaggle.com/teajay/global-shark-attacks,Tue Jan 02 2018
,dish,"[, days, nepalitext, urls]","[numeric, string, string, string]",Context The headlines with links in most cases were harvested for a quick viewing of the kinds of election talk the online news media were carrying as the campaign picked up steam ahead of Nepal's first federal and provincial elections less than a month away.  Content The headlines with links preceding them were scraped from 24 news websites of Nepal on 11/14/2017. They comprise 510 lines of Nepali texts in UTF-8 after removing the links in English. Acknowledgements The dataset is part of a personal hobby of the author to take stock of the election talk going on in Nepali media at the moment. This was possible thanks to the Python libraries requests and bs4 available with the Jupyter notebooks on Anaconda.    Inspiration Media headlines tend to be a succinct gist of the election talk going on in the campaign period with their potential to throw light on the kind of rhetoric and quality of arguments used for election gains. What can a text analysis of the headlines show?    ,CSV,,"[politics, linguistics, internet]",CC0,,,14,472,0.07421875,A day's harvest of election headlines from Nepal's news homepages ,Election News Headlines,https://www.kaggle.com/blogdish/election-news-headlines,Sat Nov 18 2017
,Union of Concerned Scientists,"[Official Name of Satellite, Country/Organization of UN Registry, Operator/Owner, Country of Operator/Owner, Users, Purpose, Detailed Purpose, Class of Orbit, Type of Orbit, Longitude of Geosynchronous Orbit (Degrees), Perigee (Kilometers), Apogee (Kilometers), Eccentricity, Inclination (Degrees), Period (Minutes), Launch Mass (Kilograms), Dry Mass (Kilograms), Power (Watts), Date of Launch, Expected Lifetime (Years), Contractor, Country of Contractor, Launch Site, Launch Vehicle, COSPAR Number, NORAD Number]","[string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, numeric, string, string, string, string, string, numeric]",Content The Satellite Database is a listing of active satellites currently in orbit around the Earth. The database includes basic information about the satellites and their orbits but does not contain the detailed information necessary to locate individual satellites. The information included in the database is publicly accessible and free and was collected from corporate scientific government military non-governmental and academic websites available to the public. No copyrighted material was used nor did we subscribe to any commercial databases for information. We have attempted to include all currently active satellites. However satellites are constantly being launched decommissioned or simply abandoned and the list may inadvertently contain some satellites that are no longer active but for which we have not yet received information. Acknowledgements The Satellite Database is produced and updated quarterly by Teri Grimwood.,CSV,,[space],CC4,,,287,2651,0.328125,Which country has the most satellites in orbit? What are they used for?,Active Satellites in Orbit Around Earth,https://www.kaggle.com/ucsusa/active-satellites,Mon Jan 30 2017
,XavierMartinezBartra,"[REGION,  2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Some countries have a very divergent GDP per capita between its regions. Sometimes a given country's regions tend to converge over time while in other cases the disparity between the poorer and the richer regions is kept over the decades. In this DataSet we can examine the Spanish case. Which has been the evolution of the nominal GDP per capita by regions in Spain since the year 2000 ? Have the regions converged ? Which is the spread between regions ?   Can we make a cluster analysis of the regions ?  Content We have a DataFrame of the evolution of the nominal GDP per capita across the 19 Spanish regions (autonomous communities & cities) since 2000 to 2016. ** Acknowledgements ** The Data used has been compiled by the INE (spanish institute of statistics). http//www.ine.es/,CSV,,"[business, social sciences, economics]",CC0,,,127,3512,0.001953125,Series from the years 2000 to 2016,Nominal € GDP per capita of Spain (by regions),https://www.kaggle.com/xavier14/nominal-gdp-per-capita-of-spain-by-regions,Thu Aug 10 2017
,Daia Alexandru,[],[],,CSV,,[geology],ODbL,,,0,0,0.091796875,Data from 1975  until   published  date,Romania Earthquake Historical Data,https://www.kaggle.com/alexandrudaia/romania-earthquake-historical-data,Sat May 06 2017
,US Environmental Protection Agency,"[ID, PMNNO, ACCNO, GN, FL, CS]","[numeric, string, numeric, string, string, string]","Context What is the TSCA Chemical Substances Control Inventory? Section 8 (b) of the Toxic Substances Control Act (TSCA) requires EPA to compile keep current and publish a list of each chemical substance that is manufactured or processed including imports in the United States for uses under TSCA. Also called the “TSCA Inventory” or simply “the Inventory” it plays a central role in the regulation of most industrial chemicals in the United States. The initial reporting period by manufacturers processors and importers was January to May of 1978 for chemical substances that had been in commerce since January of 1975. The Inventory was initially published in 1979 and a second version containing about 62000 chemical substances was published in 1982. The TSCA Inventory has continued to grow since then and now lists about 85000 chemicals. EPA’s compilation of the public TSCA Inventory information is updated twice a year to include new and corrected TSCA Inventory chemical listings and it contains none of the chemical identities claimed as confidential. Thus it is not as complete nor current as the information contained in EPA's TSCA Master Inventory File which includes the chemical identities claimed as confidential and is updated continuously as new and corrected information is received by EPA. Consequently for the purposes of TSCA compliance the TSCA Master Inventory File maintained by EPA's Office of Pollution Prevention and Toxics is the only complete and accurate source that can provide authoritative and conclusive information about which chemical substances are currently included in the TSCA Inventory. Content TSCAINV_062017.csv  ID Record ID Number RN Chemical Abstracts Service (CAS) Registry Number casregno CAS registry number without ""-"" [dashes] IN Index Name (Chemical name) DF Chemical substance definition FL EPA TSCA Regulatory Flag UV UVCB Flag CS Commercial Status Designation  PMNACC_062017.csv  ID Record Number ID PMNNO PMN Number/Form Number ACCNO EPA Accession Number GN Generic Name FL EPA TSCA Regulatory Flag CS Commercial Status Designation  Acknowledgements The EPA updates this registry is twice per year. The version here was downloaded on Oct 18th 2017. Check the EPA website for updated versions https//www.epa.gov/tsca-inventory/how-access-tsca-inventory.  Inspiration There are lots of air quality and pollution datasets that you can use in conjunction with this TSCA Registry to learn more about contaminants and chemicals in general.",CSV,,"[science and culture, chemistry, health]",CC0,,,28,830,9,The EPA's Toxic Substances Control Act Chemical Substance Inventory,Chemical Substance Registry (CAS registry numbers),https://www.kaggle.com/epa/cas-registry-numbers,Thu Oct 19 2017
,traceyvanp,"[Parent Airline, Airline, Aircraft Type, Current, Future, Historic, Total, Orders, Unit Cost, Total Cost (Current), Average Age]","[string, string, string, numeric, string, numeric, numeric, string, string, string, numeric]","Planespotters.net has a full database on airlines around the world and the airplanes that each owns and operates. This dataset collects the top 100+ airlines in the world (by the size of their fleet). It is combined with information found on Wikipedia on the respective airline's fleet and the average value/cost of the manufactured airplane. Updated January 2017. Dataset includes  Parent Airline i.e. International Airlines Group (IAG) Airline i.e. Iberia Aer Lingus British Airways...etc. which are owned by IAG Aircraft Type Manufacturer & Model Current Quantity of airplanes in Operation Future Quantity of airplanes on order from planespotter.net Order Quantity airplanes on order from Wikipedia Unit Cost Average unit cost ($M) of Aircraft Type as found by Wikipedia and various google searches Total Cost Current quantity * Unit Cost ($M) Average Age Average age of ""Current"" airplanes by ""Aircraft Type""  Sources Planespotters.net Wikipedia.org",CSV,,[aviation],Other,,,848,6680,0.09765625,The top 100+ airlines and their fleet specifics.,Airline Fleets,https://www.kaggle.com/traceyvanp/airlinefleet,Thu Feb 09 2017
,Andrey,"[UNIXTime, Data, Time, Radiation, Temperature, Pressure, Humidity, WindDirection(Degrees), Speed, TimeSunRise, TimeSunSet]","[numeric, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, dateTime, dateTime]","Context Space Apps Moscow was held on April 29th & 30th. Thank you to the 175 people who joined the International Space Apps Challenge at this location! Content The dataset contains such columns as ""wind direction"" ""wind speed"" ""humidity"" and temperature. The response parameter that is to be predicted is ""Solar_radiation"". It contains measurements for the past 4 months and you have to predict the level of solar radiation. Just imagine that you've got solar energy batteries and you want to know will it  be reasonable to use them in future? Acknowledgements Thanks NASA for the dataset. Inspiration Predict the level of solar radiation. Here are some intersecting dependences that i have figured out 1. Humidity & Solar_radiation. 2.Temeperature & Solar_radiation. The best result of accuracy  I could get using cross-validation was only 55%.",CSV,,"[space, energy]",ODbL,,,882,7112,3,Task from NASA Hackathon,Solar Radiation Prediction,https://www.kaggle.com/dronio/SolarEnergy,Sun May 21 2017
,The Marshall Project,"[report_year, agency_code, agency_jurisdiction, population, violent_crimes, homicides, rapes, assaults, robberies, months_reported, crimes_percapita, homicides_percapita, rapes_percapita, assaults_percapita, robberies_percapita]","[numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Is crime in America rising or falling? The answer is not as simple as politicians make it out to be because of how the FBI collects crime data from the country’s more than 18000 police agencies. National estimates can be inconsistent and out of date as the FBI takes months or years to piece together reports from those agencies that choose to participate in the voluntary program. To try to fill this gap The Marshall Project collected and analyzed more than 40 years of data on the four major crimes the FBI classifies as violent — homicide rape robbery and assault — in 68 police jurisdictions with populations of 250000 or greater. We obtained 2015 reports which have yet to be released by the FBI directly from 61 of them. We calculated the rate of crime in each category and for all violent crime per 100000 residents in the jurisdiction based on the FBI’s estimated population for that year. We used the 2014 estimated population to calculate 2015 crime rates per capita. Acknowledgements The crime data was acquired from the FBI Uniform Crime Reporting program's ""Offenses Known and Clearances by Arrest"" database for the year in question held at the National Archives of Criminal Justice Data. The data was compiled and analyzed by Gabriel Dance Tom Meagher and Emily Hopkins of The Marshall Project; the analysis was published as Crime in Context on 18 August 2016.",CSV,,"[history, crime]",Other,,,1303,7123,0.251953125,Are violent crime rates rising or falling in American cities?,"Crime in Context, 1975-2015",https://www.kaggle.com/marshallproject/crime-rates,Fri Feb 10 2017
,Zeeshan-ul-hassan Usmani,"[S#, Date, Time, Location, City, Province, No of Strike, Al-Qaeda, Taliban, Civilians Min, Civilians Max, Foreigners Min, Foreigners Max, Total Died Min, Total Died Mix, Injured Min, Injured Max, Women/Children  , Special Mention (Site), Comments, References, Longitude, Latitude, Temperature(C), Temperature(F)]","[numeric, dateTime, string, string, string, string, numeric, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric]",Context Pakistan Drone Attacks (2004-2016) The United States has targeted militants in the Federally Administered Tribal Areas [FATA] and the province of Khyber Pakhtunkhwa [KPK] in Pakistan via its Predator and Reaper drone strikes since year 2004. Pakistan Body Count (www.PakistanBodyCount.org) is the oldest and most accurate running tally of drone strikes in Pakistan. The given database (PakistanDroneAttacks.CSV) has been populated by using majority of the data from Pakistan Body Count and building up on it by canvassing open source newspapers media reports think tank analyses and personal contacts in media and law enforcement agencies. We provide a count of the people killed and injured in drone strikes including the ones who died later in hospitals or homes due to injuries caused or aggravated by drone strikes making it the most authentic source for drone related data in this region. We will keep releasing the updates every quarter at this page. Content Geography Pakistan Time period 2004-2016  Unit of analysis Attack Dataset The dataset contains detailed information of 397 drone attacks in Pakistan that killed an estimated 3558 and injured 1333 people including 2539 civilians.  Variables The dataset contains Serial No Incident Day & Date Approximate Time of the attack Specific Location City Province Number of people killed who claimed to be from Al-Qaeeda  Number of people killed who claimed to be from Taliban minimum and maximum count of foreigners killed minimum and maximum count of civilians killed minimum and maximum count of civilians injured special mention (more details) and comments about the attack longitude and latitude of the location.  Sources Unclassified media articles hospital reports think tank analysis and reports and government official press releases. Acknowledgements & References Pakistan Body Count has been leveraged extensively in scholarly publications reports media articles and books. The website and the dataset has been collected and curated by the founder Zeeshan-ul-hassan Usmani.  Users are allowed to use copy distribute and cite the dataset as follows “Zeeshan-ul-hassan Usmani Pakistan Body Count Drone Attacks Dataset Kaggle Dataset Repository Jan 25 2017.” Past Research Zeeshan-ul-hassan Usmani and Hira Bashir “The Impact of Drone Strikes in Pakistan” Cost of War Project Brown University December 16 2014 Inspiration Some ideas worth exploring •   How many people got killed and injured per year in last 12 years? •   How many attacks involved killing of actual terrorists from Al-Qaeeda and Taliban? •   How many attacks involved women and children? •   Visualize drone attacks on timeline •   Find out any correlation with number of drone attacks with specific date and time for example do we have more drone attacks in September? •   Find out any correlation with drone attacks and major global events (US funding to Pakistan and/or Afghanistan Friendly talks with terrorist outfits by local or foreign government?) •   The number of drone attacks in Bush Vs Obama tenure? •   The number of drone attacks versus the global increase/decrease in terrorism? •   Correlation between number of drone strikes and suicide bombings in Pakistan Questions? For detailed visit www.PakistanBodyCount.org  Or contact Pakistan Body Count staff at info@pakistanbodycount.org ,CSV,,[military],CC0,,,979,11263,0.4638671875,"Most authentic count of drone strikes in Pakistan, 2004-2016",Pakistan Drone Attacks,https://www.kaggle.com/zusmani/pakistandroneattacks,Fri Dec 01 2017
,Chase Willden,"[Username, Repository Name, Description, Last Update Date, Language, Number of Stars, Tags, Url]","[string, string, string, dateTime, string, string, string, string]",Context GitHub is the leader in hosting open source projects. For those who are not familiar with open source projects a group of developers share and contribute to common code to develop software. Example open source projects include Chromium (which makes Google Chrome) WordPress and Hadoop. Open source projects are said to have disrupted the software industry (2008 Kansas Keynote). Content For this study I crawled the leader in hosting open source projects GitHub.com and extracted a list of the top starred open source projects. On GitHub a user may choose the star a repository representing that they “like” the project. For each project I gathered the repository username or Organization the project resided in the repository name a description the last updated date the language of the project the number of stars any tags and finally the url of the project. Acknowledgements This data wouldn't be available if it weren't for GitHub. An example micro-study can be found at The Concept Center,CSV,,"[internet, programming languages, programming]",CC0,,,223,4722,0.173828125,Which types of projects are the most popular on GitHub?,Top 980 Starred Open Source Projects on GitHub,https://www.kaggle.com/chasewillden/topstarredopensourceprojects,Sun Jun 25 2017
,0rangutan,[],[],"Context Information reproduced from the National Archives ""The Vietnam Conflict Extract Data File of the Defense Casualty Analysis System (DCAS) Extract Files contains records of 58220 U.S. military fatal casualties of the Vietnam War.  These records were transferred into the custody of the National Archives and Records Administration in 2008.  The earliest casualty record contains a date of death of June 8 1956 and the most recent casualty record contains a date of death of May 28 2006.  The Defense Casualty Analysis System Extract Files were created by the Defense Manpower Data Center (DMDC) of the Office of the Secretary of Defense. The records correspond to the Vietnam Conflict statistics on the DMDC web site which is accessible online at https//www.dmdc.osd.mil/dcas/pages/main.xhtml .  A full series description for the Defense Casualty Analysis System (DCAS) Extract Files is accessible online via the National Archives Catalog under the National Archives Identifier 2163536. The Vietnam Conflict Extract Data File is also accessible for direct download via the National Archives Catalog file-level description National Archives Identifier 2240992. "" Content The raw data files have been cleaned and labelled as best as I can with reference to the accompanying Supplemental Code Lists. Names and ID numbers have been removed out of respect and to provide anonymity. Acknowledgements Data provided by The U.S. National Archives and Records Administration. Raw data can be accessed via the following link https//catalog.archives.gov/id/2240992 Inspiration By cleaning the data I hope to give wider access to this resource.",Other,,"[history, war]",Other,,,410,3928,24,Data from The U.S. National Archives and Records Administration,US Casualties of the Vietnam War,https://www.kaggle.com/orangutan/vietnamconfilct,Thu Feb 16 2017
,Kheirallah Samaha,"[flare, start.date, start.time, peak, end, duration.s, peak.c/s, total.counts, energy.kev, x.pos.asec, y.pos.asec, radial, active.region.ar, flag.1, flag.2, flag.3, flag.4, flag.5]","[numeric, dateTime, dateTime, dateTime, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric, string, string, string, string, string]","Context Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI originally High Energy Solar Spectroscopic Imager or HESSI) is a NASA solar flare observatory. It is the sixth mission in the Small Explorer program selected in October 1997 and launched on 5 February 2002. Its primary mission is to explore the physics of particle acceleration and energy release in solar flares. HESSI was renamed to RHESSI on 29 March 2002 in honor of Reuven Ramaty a pioneer in the area of high energy solar physics. RHESSI is the first space mission named after a NASA scientist. RHESSI was built by Spectrum Astro for Goddard Space Flight Center and is operated by the Space Sciences Laboratory in Berkeley California. The principal investigator from 2002 to 2012 was Robert Lin who was succeeded by Säm Krucker. useful links https//en.wikipedia.org/wiki/Reuven_Ramaty_High_Energy_Solar_Spectroscopic_Imager https//hesperia.gsfc.nasa.gov/hessi/objectives.htm Content Ramaty High Energy Solar Spectroscopic Imager (RHESSI) Notes  Note that only events with non-zero position and energy range not equal to 3-6 keV are confirmed as solar sources. Events which have no position and show up mostly in the front detectors but were not able to be imaged  are flagged as ""PS"". Events which do not have valid position are only confirmed to be non-solar if the NS flag is set. Peak Rate  peak counts/second in energy range 6-12 keV averaged over active collimators including background. Total Counts  counts in energy range 6-12 keV integrated over duration of flare summed over all subcollimators  including background. Energy  the highest energy band in which the flare was observed. Electron Kev (kilo electron volt) https//en.wikipedia.org/wiki/Electronvolt Radial Distance  distance from Sun center Quality Codes Qn where n is the total number of data gap SAA particle eclipse or decimation flags set for event.  n ranges from 0 to 11.  Use care when analyzing the data when the quality is not zero. Active_Region A number for the closest active region if available radial_offset the offset of the flare position from the spin axis of the spacecraft in arcsec. This is used i spectroscopy. peak_c/s peak count rate in corrected counts. Flare Flag Codes    a0 - In attenuator state 0 (None) sometime during flare   a1 - In attenuator state 1 (Thin) sometime during flare   a2 - In attenuator state 2 (Thick) sometime during flare   a3 - In attenuator state 3 (Both) sometime during flare   An - Attenuator state (0=None 1=Thin 2=Thick 3=Both) at peak of flare   DF - Front segment counts were decimated sometime during flare   DR - Rear segment counts were decimated sometime during flare   ED - Spacecraft eclipse (night) sometime during flare   EE - Flare ended in spacecraft eclipse (night)   ES - Flare started in spacecraft eclipse (night)   FE - Flare ongoing at end of file   FR - In Fast Rate Mode   FS - Flare ongoing at start of file   GD - Data gap during flare   GE - Flare ended in data gap   GS - Flare started in data gap   MR - Spacecraft in high-latitude zone during flare   NS - Non-solar event   PE - Particle event Particles are present   PS - Possible Solar Flare; in front detectors but no position   Pn - Position Quality P0 = Position is NOT valid P1 = Position is valid   Qn - Data Quality Q0 = Highest Quality Q11 = Lowest Quality   SD - Spacecraft was in SAA sometime during flare   SE - Flare ended when spacecraft was in SAA   SS - Flare started when spacecraft was in SAA Acknowledgements What is a solar flare? A Solar flare is the rapid release of a large amount of energy stored in the solar atmosphere. During a flare gas is heated to 10 to 20 million degrees Kelvin (K) and radiates soft X rays and longer-wavelength emission. Unable to penetrate the Earth's atmosphere the X rays can only be detected from space. Instruments on Skylab SMM the Japanese/US Yohkoh mission and other spacecraft have recorded many flares in X rays over the last twenty years or so. Ground-based observatories have recorded the visible and radio outputs. These data form the basis of our current understanding of a solar flare. But there are many possible mechanisms for heating the gas and observations to date have not been able to differentiate between them. HESSI's new approach Researchers believe that much of the energy released during a flare is used to accelerate to very high energies electrons (emitting primarily X-rays) and protons and other ions (emitting primarily gamma rays). The new approach of the HESSI mission is to combine for the first time high-resolution imaging in hard X-rays and gamma rays with high-resolution spectroscopy so that a detailed energy spectrum can be obtained at each point of the image. This new approach will enable researchers to find out where these particles are accelerated and to what energies. Such information will advance understanding of the fundamental high-energy processes at the core of the solar flare problem. https//hesperia.gsfc.nasa.gov/hessi/objectives.htm Inspiration  Explore Know something new Predict the solar flare Respect the Sun and value it and Take care of the environments.  Thanks",CSV,,"[astronomy, space]",CC0,,,231,3150,10,Reuven Ramaty High Energy Solar Spectroscopic Imager,Solar Flares from RHESSI Mission,https://www.kaggle.com/khsamaha/solar-flares-rhessi,Thu Feb 09 2017
,NOAA,"[SOURCE_ID, YEAR, MONTH, DAY, HOUR, MINUTE, CAUSE, VALIDITY, FOCAL_DEPTH, PRIMARY_MAGNITUDE, REGION_CODE, COUNTRY, STATE/PROVINCE, LOCATION, LATITUDE, LONGITUDE, MAXIMUM_HEIGHT, MAGNITUDE_ABE, MAGNITUDE_IIDA, INTENSITY_SOLOVIEV, WARNING_STATUS, MISSING, MISSING_ESTIMATE, INJURIES, INJURY_ESTIMATE, FATALITIES, FATALITY_ESTIMATE, DAMAGE_MILLIONS_DOLLARS, DAMAGE_ESTIMATE, HOUSES_DAMAGED, HOUSE_DAMAGE_ESTIMATE, HOUSES_DESTROYED, HOUSE_DESTRUCTION_ESTIMATE, ALL_MISSING, MISSING_TOTAL, ALL_INJURIES, INJURY_TOTAL, ALL_FATALITIES, FATALITY_TOTAL, ALL_DAMAGE_MILLIONS, DAMAGE_TOTAL, ALL_HOUSES_DAMAGED, HOUSE_DAMAGE_TOTAL, ALL_HOUSES_DESTROYED, HOUSE_DESTRUCTION_TOTAL]","[numeric, numeric, numeric, numeric, string, string, numeric, numeric, string, numeric, numeric, string, string, string, numeric, numeric, numeric, string, string, numeric, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, string, string, string, string]","Context Tsunami is a Japanese word that translates to ""harbor wave"". It is a wave or a series of waves generated by an impulsive vertical displacement of the surface of the ocean or other body of water. Tsunamis have been responsible for over 500000 fatalities throughout the world — almost half from the 2004 Indian Ocean earthquake and tsunami! Content The NOAA/WDS tsunami database is a listing of historical tsunami source events and runup locations throughout the world from 2000 B.C. to the present. The events were gathered from scientific and scholarly sources regional and worldwide catalogs tide gauge data deep ocean sensor data individual event reports and unpublished works. There are currently over 2000 source events in the database with event validities greater than one and over 13000 runup locations where tsunami effects were observed. Acknowledgements NOAA's National Centers for Environmental Information (NCEI) and the World Data Service for Geophysics compiled and published this tsunami database for tsunami warning centers engineers oceanographers seismologists and the general public.",CSV,,[oceans],CC0,,,899,6673,3,"Cause, magnitude, and intensity of every tsunami since 2000 BC",Tsunami Causes and Waves,https://www.kaggle.com/noaa/seismic-waves,Fri Feb 03 2017
,Myles O'Neill,"[accession_no, accounting_period, acquisition_history, alternative_years, ark_number, atf_source, atf_up, author, author_remarks, cdli_collation, cdli_comments, citation, collection, composite, condition_description, date_entered, date_of_origin, date_remarks, date_updated, dates_referenced, db_source, designation, dumb, dumb2, electronic_publication, elevation, excavation_no, external_id, findspot_remarks, findspot_square, genre, google_earth_collection, google_earth_provenience, height, id, id_text2, id_text, join_information, language, lineart_up, material, museum_no, object_preservation, object_type, period, period_remarks, photo_up, primary_publication, provenience, provenience_remarks, publication_date, publication_history, published_collation, seal_id, seal_information, stratigraphic_level, subgenre, subgenre_remarks, surface_preservation, text_remarks, thickness, translation_source, width]","[string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, dateTime, string, string, dateTime, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric]","What is CDLI? The Cuneiform Digital Library Initiative (CDLI) is an international digital library project aimed at putting text and images of an estimated 500000 recovered cuneiform tablets created from between roughly 3350 BC and the end of the pre-Christian era online. The initiative is a joint project of the University of California Los Angeles the University of Oxford and the Max Planck Institute for the History of Science Berlin.  This dataset includes the full CDLI catalogue (metadata) transliterations of tablets in the catalogue and word/sign lists from old akkadian and Ur III. This data was downloaded on the 9th of May 2017. Transliterations are in .atf format find out more about this format here http//oracc.museum.upenn.edu/doc/help/editinginatf/cdliatf/index.html Find more about CDLI here http//cdli.ucla.edu/ What is Cuneiform? Cuneiform script one of the earliest systems of writing was invented by the Sumerians. It is distinguished by its wedge-shaped marks on clay tablets made by means of a blunt reed for a stylus. The name cuneiform itself simply means ""wedge shaped"". Cuneiform is not a language nor is it an alphabet. Cuneiform uses between 600-1000 characters to write words or syllables. It has been used by many different cultural groups to represent many different languages but it was primarily used to write Sumerian and Akkadian. Deciphering cuneiform is very difficult to this day though the difficulty varies depending on the language. https//en.wikipedia.org/wiki/Cuneiform_script What is Assyriology? Assyriology is the study of the languages history and culture of the people who used the ancient writing system called cuneiform. Cuneiform was used primarily in an area called the Near East centred on Mesopotamia (modern Iraq and eastern Syria) where cuneiform was invented but including the Northern Levant (Western Syria and Lebanon) parts of Anatolia and western Iran. The sources for Assyriology are all archaeological and include both inscribed and uninscribed objects. Most Assyriologists focus on the rich textual record from the ancient Near East and specialise in either the study of language literature or history of the ancient Near East. Assyriology began as an academic discipline with the recovery of the monuments of ancient Assyria and the decipherment of cuneiform in the middle of the 19th century. Large numbers of archaeological objects including texts were brought to museums in Europe and later the US following the early excavations of Nineveh Kalhu Babylon Girsu Assur and so forth. Today Assyriology is studied in universities across the globe both as an undergraduate and a graduate subject and knowledge from the ancient Near East informs students of numerous other disciplines such as the History of Science Archaeology Classics Biblical studies and more.",CSV,,"[languages, history, linguistics]",CC4,,,97,1577,192,Explore thousands of ancient tablet transliterations,Cuneiform Digital Library Initiative,https://www.kaggle.com/mylesoneill/cuneiform-digital-library-initiative,Wed May 10 2017
,Rob Harrand,[],[],These are 5 texts taken from Project Gutenberg uploaded to Kaggle to encourage things like text-mining and sentiment analysis. These are fun skills to develop and many existing datasets on Kaggle don't lend themselves to these sorts of analyses. The 5 books are  The King James Bible  The Quran  The Book Of Mormon  The Gospel of Buddha  Meditations by Marcus Aurelius  Project Gutenberg is an online archive of books that are free to download and distribute. These files are taken without alteration (filenames or contents) from the Project Gutenberg website,Other,,"[languages, faith and traditions, linguistics]",Other,,,913,7180,8,"5 texts from Project Gutenberg to encourage text-mining, sentiment analysis, etc",Religious and philosophical texts,https://www.kaggle.com/tentotheminus9/religious-and-philosophical-texts,Wed Sep 07 2016
,Aaron7sun,"[Date, Label, Top1, Top2, Top3, Top4, Top5, Top6, Top7, Top8, Top9, Top10, Top11, Top12, Top13, Top14, Top15, Top16, Top17, Top18, Top19, Top20, Top21, Top22, Top23, Top24, Top25]","[dateTime, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]","Actually I prepare this dataset for students on my Deep Learning and NLP course.  But I am also very happy to see kagglers play around with it. Have fun! Description There are two channels of data provided in this dataset  News data I crawled historical news headlines from Reddit WorldNews Channel (/r/worldnews). They are ranked by reddit users' votes and only the top 25 headlines are considered for a single date. (Range 2008-06-08 to 2016-07-01) Stock data Dow Jones Industrial Average (DJIA) is used to ""prove the concept"". (Range 2008-08-08 to 2016-07-01)  I provided three data files in .csv format  RedditNews.csv two columns The first column is the ""date"" and second column is the ""news headlines"". All news are ranked from top to bottom based on how hot they are. Hence there are 25 lines for each date. DJIA_table.csv  Downloaded directly from Yahoo Finance check out the web page for more info. Combined_News_DJIA.csv To make things easier for my students I provide this combined dataset with 27 columns. The first column is ""Date"" the second is ""Label"" and the following ones are news headlines ranging from ""Top1"" to ""Top25"".  ========================================= To my students I made this a binary classification task. Hence there are only two labels ""1"" when DJIA Adj Close value rose or stayed as the same; ""0"" when DJIA Adj Close value decreased. For task evaluation please use data from 2008-08-08 to 2014-12-31 as Training Set and Test Set is then the following two years data (from 2015-01-02 to 2016-07-01). This is roughly a 80%/20% split. And of course use AUC as the evaluation metric. ========================================= +++++++++++++++++++++++++++++++++++++++++ To all kagglers Please upvote this dataset if you like this idea for market prediction. If you think you coded an amazing trading algorithm friendly advice  do play safe with your own money ) +++++++++++++++++++++++++++++++++++++++++ Feel free to contact me if there is any question~  And remember me when you become a millionaire P",CSV,,"[news agencies, finance]",CC4,,,10845,95979,14,Using 8 years daily news headlines to predict stock market movement,Daily News for Stock Market Prediction,https://www.kaggle.com/aaron7sun/stocknews,Thu Aug 25 2016
,Binks,[],[],Context Youtube has introduced automatic generation of subtitles based on speech recognition of uploaded video. This dataset provides collection of subtitles Robert Phoenix The 11th House uploaded podcasts. It serves as database for an introduction to algorithmic analysis of spoken language. From the podcasts author description “The Eleventh House is the home of Robert Phoenix a journalist blogger interviewer astrologer and psychic medium with over 30 years experience in personal readings and coaching and has been a media personality in TV and radio. The 11th house delves into the supernatural geopolitics exopolitics conspiracy theories and pop culture.”  Content The 11th House speeches dataset consists of 543 subtitles (sets of words) retrieved from Youtube playlists  https//www.youtube.com/user/FreeAssociationRadio/videos This dataset consists of a single CSV file RobertPhoenixThe11thHouse.csv. The columns are 'id' 'playlist' 'upload_date' 'title' 'view_count' 'average_rating' 'like_count' 'dislike_count' 'subtitles' which are delimited with a comma. Text data in columns 'subtitles' is not sentence based there are not commas or dots. It is only stream of words being translated from speech into text by GoogleVoice (more here https//googleblog.blogspot.com.au/2009/11/automatic-captions-in-youtube.html). Acknowledgements The data was downloaded using youtube-dl package. Inspiration I'm interested in a deeper meaning behind current affairs. (For example see http//www.blogtalkradio.com/freeassociationradio),CSV,,"[popular culture, storytelling, politics]",Other,,,26,521,19,Find out hidden meaning behind current affairs,Subtitles of The Eleventh House podcast,https://www.kaggle.com/binksbiz/robert,Sun Aug 20 2017
,Chris Crawford,"[date, l_ipn, r_asn, f]","[dateTime, numeric, numeric, numeric]","Context Computer Network Traffic Data - A ~500K CSV with summary of some real network traffic data from the past. The dataset has ~21K rows and covers 10 local workstation IPs over a three month period. Half of these local IPs were compromised at some point during this period and became members of various botnets.  Content Each row consists of four columns  date yyyy-mm-dd (from 2006-07-01 through 2006-09-30) l_ipn local IP (coded as an integer from 0-9) r_asn remote ASN (an integer which identifies the remote ISP) f flows (count of connnections for that day)  Reports of ""odd"" activity or suspicions about a machine's behavior triggered investigations on the following days (although the machine might have been compromised earlier) Date  IP 08-24  1 09-04  5 09-18  4 09-26  3 6 Acknowledgements This public dataset was found on http//statweb.stanford.edu/~sabatti/data.html Inspiration Can you discover when a compromise has occurred by a change in the pattern of communication?",CSV,,[],CC0,,,369,4358,0.41015625,Traffic from workstation IPs where at least half were compromised,Computer Network Traffic,https://www.kaggle.com/crawford/computer-network-traffic,Fri Aug 18 2017
,Paul Magda,"[year, category, winner, nominee, detail]","[numeric, string, numeric, string, string]",Context We are working on a project relating to predicting and voting for Academy Award. With the Primetime Emmy Awards coming up this week I thought it would be interesting to see if I could integrated those. I couldn't find too many well organized datasets relating to those awards.  I decided to spend the afternoon and build my own.  We probably won't use this information this year  but it might be something we could use in the future.  Content I created a simple web parser in Go and parsed the data from the Emmy Awards Website. The data is a representation of Primetime Emmy Nominees from the first Emmy Awards (1949)... to the current ones that will air Sunday September 17th 2017.  After this date the winner will have to be updated.  In work we've done with Academy Awards we used movie title and name as the main structure for the data. I kind of felt this was a little inconsistent as certain awards focus on one or the other. With the Emmy Nominees I made it more general with nominee and additional detail I believe this will make the data more consistent and easier to manipulate. Acknowledgements I based the structure of the data from the Kaggle dataset of the Academy Awards . I would also like to acknowledge the Academy of Television Arts & Sciences for providing the data on their website. Inspiration Who won the most Emmys for Outstanding Comedy Series? I think it would be cool if we could answer Who will win the Emmy for Outstanding Comedy Series in 2018?  But I think we more than just historical data. ,CSV,,"[entertainment, telecommunications]",CC0,,,120,1272,2,Which television show has won the most Emmy Awards?,"Primetime Emmy Awards, 1949-2017",https://www.kaggle.com/pmagda/primetime-emmy-awards,Tue Sep 19 2017
,City of New York,"[ELECTION, OFFICECD, CANDID, CANCLASS, CANDLAST, CANDFIRST, CANDMI, COMMITTEE, FILING, SCHEDULE, PAGENO, SEQUENCENO, REFNO, DATE, REFUNDDATE, NAME, C_CODE, STRNO, STRNAME, APARTMENT, BOROUGHCD, OCCUPATION, EMPNAME, EMPSTRNO, EMPSTRNAME, AMNT, MATCHAMNT, PREVAMNT, PAY_METHOD, INTERMNO, INTERMNAME, INTSTRNO, INTSTRNM, INTAPTNO, INTST, INTZIP, INTEMPNAME, INTEMPSTNO, INTEMPSTNM, INTEMPCITY, INTEMPST, INTOCCUPA, PURPOSECD, EXEMPTCD, ADJTYPECD, RR_IND, SEG_IND, INT_C_CODE, Location 1, Location 2, Location 3]","[numeric, numeric, numeric, string, string, string, string, string, numeric, string, string, string, string, dateTime, dateTime, string, string, numeric, string, string, string, string, string, numeric, string, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, numeric, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string]",Context A record of every campaign donation made during the 2013 New York City election cycle. This dataset includes the donor recipient and dollar amount of contributions to campaigns both for the mayor of New York City (for the 2013-2017 term which was won by Bill de Blasio) and for the variety of other publicly elected positions in the City of New York. Content This dataset includes identifying information about the name of the person and/or corporation which made the donation the dollar amount of the donation whether or not it was later refunded (if so a date of refund is provided) the archetype the donation falls under the name of the candidate being donated to and identifying information about the donor (including self-identified work area or profession). This dataset includes a field for campaign finance codes read here for more information on what these are. Acknowledgements This data is published as-is by the City of New York. Inspiration  What is the distribution of donations made across candidates? What is the distribution of small-to-large donations? Who is doing the donating? How are they spatially distributed through the city? ,CSV,,"[money, politics]",CC0,,,35,583,5,"Donors, recipients, and dollar amounts from the 2013 NYC election cycle",New York City - 2013 Campaign Contributions,https://www.kaggle.com/new-york-city/nyc-2013-campaign-contributions,Fri Sep 08 2017
,FiveThirtyEight,"[movie, type, word, minutes_in]","[string, string, string, numeric]",Context I found this dataset after reading a Five Thirty Eight article. The author got a tally of every death and cuss word in Tarantino's movies. That's no small feat considering the content of Tarantino flicks! Such endurance!    Content  movie Film title type  Whether the event was a profane word or a death word The specific profane word if the event was a word minutes_in The number of minutes into the film the event occurred  Acknowledgements Thanks to FiveThirtyEight for throwing this dataset up on github and sharing with everyone. The original article can be found on FiveThirtyEight's website here https//fivethirtyeight.com/features/complete-catalog-curses-deaths-quentin-tarantino-films/ And the dataset is can be found here https//github.com/fivethirtyeight/data/tree/master/tarantino Inspiration Try some word counting and see how Tarantino's murderdeathcuss ratios have changed over time.  What are his favorite cuss words? Which movies have the most deaths? Shared under MIT License,CSV,,"[news agencies, film, death, linguistics]",Other,,,70,988,0.060546875,A tally of every cuss word and death in Tarentino's films up to 2012,Cuss words and Deaths in Quentin Tarantino Films,https://www.kaggle.com/fivethirtyeight/cuss-words-and-deaths-in-quentin-tarantino-films,Thu Jul 20 2017
,UCI Machine Learning,"[Observation,  Year,  Month,  Day,  Date,  Latitude,  Longitude,  Zonal Winds,  Meridional Winds,  Humidity,  Air Temp,  Sea Surface Temp]","[numeric, numeric, numeric, numeric, dateTime, numeric, numeric, numeric, numeric, string, numeric, numeric]",Context This dataset contains oceanographic and surface meteorological readings taken from a series of buoys positioned throughout the equatorial Pacific. This data was collected with the Tropical Atmosphere Ocean (TAO) array which consists of nearly 70 moored buoys spanning the equatorial Pacific measuring oceanographic and surface meteorological variables critical for improved detection understanding and prediction of seasonal-to-interannual climate variations originating in the tropics.  Content The data consists of the following variables date latitude longitude zonal winds (west<0 east>0) meridional winds (south<0 north>0) relative humidity air temperature sea surface temperature and subsurface temperatures down to a depth of 500 meters. Data taken from the buoys from as early as 1980 for some locations. Other data that was taken in various locations are rainfall solar radiation current levels and subsurface temperatures.  The latitude and longitude in the data showed that the bouys moved around to different locations. The latitude values stayed within a degree from the approximate location. Yet the longitude values were sometimes as far as five degrees off of the approximate location.  There are missing values in the data. Not all buoys are able to measure currents rainfall and solar radiation so these values are missing dependent on the individual buoy. The amount of data available is also dependent on the buoy as certain buoys were commissioned earlier than others.  All readings were taken at the same time of day.  Acknowledgement This dataset is part of the UCI Machine Learning Repository and the original source can be found here. The original owner is the NOAA Pacific Marine Environmental Laboratory.   Inspiration  How can the data be used to predict weather conditions throughout the world?  How do the variables relate to each other?  Which variables have a greater effect on the climate variations?  Does the amount of movement of the buoy effect the reliability of the data? ,CSV,,"[oceans, climate]",Other,,,690,6788,10,Meteorological readings taken from a series of buoys in the equatorial Pacific,El Nino Dataset,https://www.kaggle.com/uciml/el-nino-dataset,Mon Nov 07 2016
,US Customs and Border Protection,"[Border, Sector, State/Territory, 2000 (All Illegal Immigrants), 2000 (Mexicans Only), 2001 (All Illegal Immigrants), 2001 (Mexicans Only), 2002 (All Illegal Immigrants), 2002 (Mexicans Only), 2003 (All Illegal Immigrants), 2003 (Mexicans Only), 2004 (All Illegal Immigrants), 2004 (Mexicans Only), 2005 (All Illegal Immigrants), 2005 (Mexicans Only), 2006 (All Illegal Immigrants), 2006 (Mexicans Only), 2007 (All Illegal Immigrants), 2007 (Mexicans Only), 2008 (All Illegal Immigrants), 2008 (Mexicans Only), 2009 (All Illegal Immigrants), 2009 (Mexicans Only), 2010 (All Illegal Immigrants), 2010 (Mexicans Only), 2011 (All Illegal Immigrants), 2011 (Mexicans Only), 2012 (All Illegal Immigrants), 2012 (Mexicans Only), 2013 (All Illegal Immigrants), 2013 (Mexicans Only), 2014 (All Illegal Immigrants), 2014 (Mexicans Only), 2015 (All Illegal Immigrants), 2015 (Mexicans Only), 2016 (All Illegal Immigrants), 2016 (Mexicans Only)]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This report provides statistics for the number of illegal immigrants arrested or apprehended by the border patrol in each division (or sector) of the United States borders with Canada Mexico and Caribbean islands; this data is a partial measure of the flow of people illegally entering the United States. Acknowledgements Data was compiled and published by the US Border Patrol on the Customs and Border Protection webpage.,CSV,,"[crime, international relations]",CC0,,,940,7921,0.005859375,Has the number of Mexican citizens crossing the border increased or decreased?,Illegal Immigrants Arrested by US Border Patrol,https://www.kaggle.com/cbp/illegal-immigrants,Fri Jan 27 2017
,Oleksii Nidzelskyi,"[date, msg]","[dateTime, string]",Context Collection of chat messages in night urban city between boys and girls. Content Data set of messages (more than 1 million of rows) in Russian language from teenager population taken in period from 2012 to 2016 inclusive Acknowledgements All personal info in the message' body were taken from public web source and though are free of use.  Inspiration This dataset can be used to classify chat messages as male / female. Key objectives  Extract phone numbers from messages. All phone numbers are located in Ukraine and belongs to one from next operators +380 50 +380 95 +380 66 +380 99 +380 63 +380 73 +380 93 +380 68 +380 67 +380 96 +380 97 +380 98 Classify chat messages by gender (male/female) ,CSV,,"[cities, linguistics, telecommunications]",CC0,,,539,6871,119,Urban night city chat messages,Chat messages,https://www.kaggle.com/onidzelskyi/chat-messages,Mon Mar 06 2017
,Liling Tan,[],[],Context A broad-coverage corpus such as the Human Language Project envisioned by Abney and Bird (2010) would be a powerful resource for the study of endangered languages. SeedLing was created as a seed corpus for the Human Language Project to cover a broad range of languages (Guy et al. 2014). TAUS (Translation Automation User Society) also see the importance of the Human Language Project in the context of keeping up with the demand for capacity and speed for translation.  TAUS' definition of the Human Language Project can be found on https//www.taus.net/knowledgebase/index.php/Human_Language_Project A detailed explanation of how to use the corpus can be found on https//github.com/alvations/SeedLing Content The SeedLing corpus on this repository includes the data from  ODIN Online Database of Interlinear Text Omniglot Useful foreign phrases from www.omniglot.com UDHR Universal Declaration of Human Rights  Acknowledgements Citation Guy Emerson Liling Tan Susanne Fertmann Alexis Palmer and Michaela Regneri . 2014. SeedLing Building and using a seed corpus for the Human Language Project. In Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop. Baltimore USA. @InProceedings{seedling2014   author    = {Guy Emerson Liling Tan Susanne Fertmann Alexis Palmer and Michaela Regneri}   title     = {SeedLing Building and using a seed corpus for the Human Language Project}   booktitle = {Proceedings of The use of Computational methods in the study of Endangered Languages (ComputEL) Workshop}   month     = {June}   year      = {2014}   address   = {Baltimore USA}   publisher = {Association for Computational Linguistics}   pages     = {}   url       = {} }  References Steven Abney and Steven Bird. 2010. The Human Language  Project Building a universal corpus of the world’s languages.  In Proceedings of the 48th Annual Meeting of the Association  for Computational Linguistics pages 88–97.  Sime Ager. Omniglot - writing systems and languages  of the world. Retrieved from www.omniglot.com.  William D Lewis and Fei Xia. 2010. Developing ODIN A multilingual  repository of annotated language data for hundreds of the world’s  languages. Literary and Linguistic Computing 25(3)303–319.  UN General Assembly Universal Declaration of Human Rights  10 December 1948 217 A (III) available at  http//www.refworld.org/docid/3ae6b3712c.html  [accessed 26 April 2014]  Inspiration This corpus was created in a span a semester in Saarland University by a linguist a mathematician a data geek and two amazing mentors from the COLI department. It wouldn't have been possible without the cross-disciplinary synergy and the common goal we had.  Expand/Explore the Human Language Project. Go to the field and record/document their language. Make them computationally readable. Grow the Seedling! ,Other,,"[culture and humanities, languages, linguistics]",Other,,,33,541,6,A Seed Corpus for the Human Language Project,SeedLing,https://www.kaggle.com/alvations/seedling,Thu Aug 24 2017
,Kelvin Wellington,"[ranking_date, rank, name, title, country, rating, games, birth_year]","[string, numeric, string, string, string, numeric, numeric, numeric]",Context Rankings are a constant phenomenon in society with a  persistent interest in the stratification of items in a set across various disciplines. In sports rankings are a direct representation of the performance of a team or player over a certain period.  Given the straightforward nature of rankings in sports (points based system) there is the opportunity to statistically explore rankings of sports disciplines.  Content The dataset comprises monthly rankings data of the Top 100 Chess players between July 2000 and June 2017 . The data is housed in a single csv file.  Acknowledgements Data was sourced from the official site of the World Chess Federation fide.com Inspiration This dataset could be of use to anyone interested in the distribution of rankings in competitive events.,CSV,,[board games],Other,,,213,2620,0.5537109375,The Top 100 ranked players in Chess between July 2000 and June 2017,Top 100 Chess Players Historical,https://www.kaggle.com/odartey/top-chess-players,Wed Jun 28 2017
,McDonald's,"[Category, Item, Serving Size, Calories, Calories from Fat, Total Fat, Total Fat (% Daily Value), Saturated Fat, Saturated Fat (% Daily Value), Trans Fat, Cholesterol, Cholesterol (% Daily Value), Sodium, Sodium (% Daily Value), Carbohydrates, Carbohydrates (% Daily Value), Dietary Fiber, Dietary Fiber (% Daily Value), Sugars, Protein, Vitamin A (% Daily Value), Vitamin C (% Daily Value), Calcium (% Daily Value), Iron (% Daily Value)]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Ray Kroc wanted to build a restaurant system that would be famous for providing food of consistently high quality and uniform methods of preparation. He wanted to serve burgers buns fries and beverages that tasted just the same in Alaska as they did in Alabama. To achieve this he chose a unique path persuading both franchisees and suppliers to buy into his vision working not for McDonald’s but for themselves together with McDonald’s. Many of McDonald’s most famous menu items – like the Big Mac Filet-O-Fish and Egg McMuffin – were created by franchisees. Content This dataset provides a nutrition analysis of every menu item on the US McDonald's menu including breakfast beef burgers chicken and fish sandwiches fries salads soda coffee and tea milkshakes and desserts. Acknowledgements The menu items and nutrition facts were scraped from the McDonald's website. Inspiration How many calories does the average McDonald's value meal contain? How much do beverages like soda or coffee contribute to the overall caloric intake? Does ordered grilled chicken instead of crispy increase a sandwich's nutritional value? What about ordering egg whites instead of whole eggs? What is the least number of items could you order from the menu to meet one day's nutritional requirements? Start a new kernel,CSV,,"[food and drink, health]",Other,,,8471,53132,0.0283203125,"Calories, fat, and sugar for every cheeseburger, fries, and milkshake on menu",Nutrition Facts for McDonald's Menu,https://www.kaggle.com/mcdonalds/nutrition-facts,Fri Mar 03 2017
,Rachael Tatman,"[Accused Witch,  Residence , Month of Accusation, Month of Execution, Sort]","[string, string, numeric, string, numeric]","Context Few events in American history are better known than the Salem witchcraft trials of 1692. Its popularity is doubtless attributable to a number of things a persistent fascination with the occult; a perverse pleasure to expose the underbelly of an American culture that boasts of toleration social harmony and progress; and an appreciation for a compelling dramatic narrative replete with heroes and villains. Skeptics like the preeminent twentieth-century historian Perry Miller question whether the Salem trials constituted anything more than an inconsequential episode in colonial history. But most historians consider Salem worthy of continuing investigation even if it was less than a major turning point in history. Indeed Salem has been an unusually fertile field for historical research because it readily lends itself to new approaches insights and methodologies. To understand what happened in Salem historians have profitably applied the perspectives of politics anthropology economic and social analysis religion social psychology and demography. If the ultimate meaning of Salem is still elusive these investigations have broadened and deepened our understanding of the 1692 witchcraft outbreak. Content The Salem Witchcraft Website contains eight data sets. They provide only a small portion of the historical record about Salem. They do not contain transcripts of examinations or trials or contemporary narrative accounts for example. Instead they provide information primarily of a quantitative nature about three major aspects of the outbreak its chronology its geographic spread and the social and economic divisions in Salem Village that shaped events. The data were derived primarily from four published sources Paul Boyer and Stephen Nissenbaum's three-volume transcription of the legal records of the witchcraft trials The Salem Witchcraft Papers; the new and now authoritative Records of the Salem Witch-Hunt edited by Bernard Rosenthal et. al.; Boyer and Nissenbaum's edited collection of documents Salem-Village Witchcraft; and Salem Village's Book of Record which contain tax records and other information relating to Salem Village. Photocopies of the original Salem Village record book and church records were examined at the Danvers Archival Center.  The Accused Witches Data Set contains information about those who were formally accused of witchcraft during the Salem episode. This means that there exists evidence of some form of direct legal involvement such as a complaint made before civil officials an arrest warrant an examination or court record. Accused witches were almost always detained in jail to await further action by a grand jury which had the authority to indict and hold the accused for trial. Trials by a special Court of Oyer and Terminer began in June 1692. In October 1692 this court was discontinued due to mounting criticism of its methods. It was replaced by another court the Superior Court of Judicature which held trials from January to May 1693. The ""Accused Witch"" column records the names of the 152 people mentioned in legal records as having been formally accused of witchcraft. Their names are alphabetically arranged. Spelling generally follows that of Paul Boyer and Stephen Nissenbaum Salem Witchcraft Papers but has been sometimes changed in accordance with the newer Records of the Salem Witch-Hunt and other sources. ""Residence"" identifies the community in which the accused person was living when accused of witchcraft. In a few cases the residence of an accused witch is problematic. For Elizabeth How for example some records cite Ipswich while others name Topsfield as her home. In such cases the most likely residence has been used. In a few instances the residence entry does not reflect the actual geographic relationship of the accused with the trials. George Burroughs was living in Wells Maine in 1692 but he had been a controversial minister in Salem Village in the early 1680s. ""Month of Accusation"" numerically expresses the month of the year in which an alleged witch was accused ""1""=January 1692; ""6""=June 1692; and ""13""=January 1693. A negative 1 (-1) indicates that the actual month of accusation is not known with sufficient certainty to be included. Some of these ""unknowns"" can be approximated from available records and users may choose to substitute their estimate. Users should also recognize an artificial quality to this data those accused in one month May (5) for example may have been charged only a day or two before someone in June (6). ""Month of Execution"" numerically expresses the month in which an alleged witch was executed as a result of the legal process. The data do not include entries for those who died as a result of their incarceration. In one case Giles Corey the month of execution does record the month in which he was pressed to death for refusing to plead to the charges against him. The Towns Data Set provides a convenient way to construct histograms of the communities whose residents were charged with witchcraft in 1692. It contains 25 columns Twenty-four columns record each town for which at least one formal accusation occurred (Salem Village and Salem Town are listed separately). Each cell lists the month of an accusation numerically expressed 1=January 1692 2=February 1692 and so forth.  The negative number -1 indicates that the month of accusation is unknown. A ""Bin"" column contains the range of months of witchcraft accusations from 1 (January 1692) to 12 (December 1692) with -1 for unknown months of accusation. Both the Pro-Parris and Anti-Parris data sets contain the same four columns ""Name"" identifies each signer of the pro-Parris petition. ""Identification"" indicates the category in the petition under which the signer was placed. ""Sex"" indicates whether the signer was male or female. ""Sort"" locates each signer in the data set so that it can be returned to its original order. To compare the social make-up of Salem Village's pro- and anti-Parris factions to the village's general population download the Salem Village Data Set. The data set contains four columns ""Name"" lists every person in Salem Village who appeared on any village tax assessments for 1690 1695 and 1697. The 137 names are a good though imperfect indicator of the village's adult male population in the period of the witch hunt. Only a few women all widows appear. Young men not yet independent or paying taxes do not appear. ""Petition"" notes whether the taxpayer signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that the person did not sign either petition. ""Church to 1696"" indicates whether a person was a church member though 1696. No distinction is made as to whether a person was a member of the Salem Village church or another church.  The list is compiled from the pro- and anti-Parris petitions as well as from the records of the Salem Village church as recorded by Samuel Parris.  Additional information came from the published records of the First Church in Salem Town. ""Sort"" permits data to be easily restored to their original order after a statistical manipulation. The Committee Yearly Data Set contains information about Salem Village's committees from 1685 to 1698 a period that covers the last years Deodat Lawson's ministry and the entire tenure of Samuel Parris. The data set contains three columns of information for each committee ""Committee"" lists the names of committeemen for a particular year (in 1688 only four men were elected). ""Petition"" indicates whether the committeeman signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this committeeman did not sign either petition. Signing a petition strongly suggests but does not conclusively establish a petitioner's earlier position regarding Parris or the witchcraft outbreak. ""Social"" indicates whether the committeeman was a church member or a householder.  Three committeemen (William Sibley James Smith and Jacob Fuller) have been listed as householders in the absence of information linking them to a church.) The Committee List Data Set provides information about all Salem Village committee members who held office from 1685 to 1698. The data set contains 18 columns ""Committee Members"" records the names of the thirty-one villagers who held committee office from 1685-1698. They are listed in the order in which they first appeared in the village's Book of Record. ""Petition"" notes whether the committeeman signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this committeeman did not sign either petition. ""Social"" indicates whether the committeeman was a church member or a householder.  (Three committeemen William Sibley James Smith and Jacob Fuller have been listed as householders in the absence of information linking them to a church; the three did not sign either petition.) Columns 4-17 indicate committee membership for each year. ""Sort"" permits data to be easily restored to their original order. The Tax Comparison Data Set was compiled by listing all Salem Village taxpayers who were assessed rates in the period between 1681 and 1700. The rates are recorded in Salem Village's Book of Record (see Bibliography). ""Name"" lists in alphabetical order all assessments on Salem Village's tax lists for 1681 1690 1694 1695 1697 and 1700. ""Tax"" records the taxpayer's assessment in shillings. Since the village's revenue needs changed the total assessment (and individual allocations) changed accordingly. ""Petition"" indicates whether the taxpayer signed either the pro- or anti-Parris petition in 1695. ""NoS"" (no signature) means that this taxpayer did not sign either petition. ""Sort"" permits data to be easily restored to their original order after a statistical manipulation.  Acknowledgements Users who copy share adapt and re-publish any of the content in Salem Witchcraft Dataset should credit Professor Richard Latner of Tulane University for making this material available. More information and guided exercises can be found on this website. Inspiration  What was the relationship between economic success and support for Parris? Can you split the list of accused witches and predict who would be accused based on other acquisitions? ",CSV,,"[united states, history, crime, violence]",CC4,,,239,1936,0.03125,Explore and visualize the Salem witchcraft trials,Salem Witchcraft Dataset,https://www.kaggle.com/rtatman/salem-witchcraft-dataset,Fri Sep 01 2017
,Randy Betancourt,"[hpi_type, hpi_flavor, frequency, level, place_name, place_id, yr, period, index_nsa, index_sa]","[string, string, string, string, string, string, numeric, numeric, numeric, numeric]",Context The Federal Housing Finance Agency House Price Index (HPI) is a broad measure of the movement of single-family house prices. The HPI is a weighted repeat-sales index meaning that it measures average price changes in repeat sales or refinancings on the same properties.  The technical methodology for devising the index collection and publishing the data is at http//www.fhfa.gov/PolicyProgramsResearch/Research/PaperDocuments/1996-03_HPI_TechDescription_N508.pdf Content Contains monthly and quarterly time series from January 1991 to August 2016 for the U.S. state and MSA categories.  Analysis variables are the aggregate non-seasonally adjusted value and seasonally adjusted index values.  The index value is 100 beginning January 1991.   Acknowledgements This data is found on Data.gov Inspiration Can this data be combined with the corresponding census growth projections either at the state or MSA level to forecast 24 months out the highest and lowest home index values?,CSV,,[home],CC0,,,1199,9843,8,Housing indexed prices from January 1991 to August 2016,Home Price Index,https://www.kaggle.com/PythonforSASUsers/hpindex,Wed Dec 07 2016
,danerbland,"[can_id, can_nam, can_off, can_off_sta, can_off_dis, can_par_aff, can_inc_cha_ope_sea, can_str1, can_str2, can_cit, can_sta, can_zip, ind_ite_con, ind_uni_con, ind_con, par_com_con, oth_com_con, can_con, tot_con, tra_fro_oth_aut_com, can_loa, oth_loa, tot_loa, off_to_ope_exp, off_to_fun, off_to_leg_acc, oth_rec, tot_rec, ope_exp, exe_leg_acc_dis, fun_dis, tra_to_oth_aut_com, can_loa_rep, oth_loa_rep, tot_loa_rep, ind_ref, par_com_ref, oth_com_ref, tot_con_ref, oth_dis, tot_dis, cas_on_han_beg_of_per, cas_on_han_clo_of_per, net_con, net_ope_exp, deb_owe_by_com, deb_owe_to_com, cov_sta_dat, cov_end_dat, winner, votes]","[string, string, string, string, numeric, string, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, dateTime, dateTime, string, numeric]",Context This dataset was assembled to investigate the possibility of predicting congressional election results by campaign finance reports from the period leading up to the election. Content Each row represents a candidate with information on their campaign including the state district office total contributions total expenditures etc.  The content is specific to the year leading up to the 2016 election (1/1/2015 through 10/19/2016). Acknowledgements Campaign finance information came directly from FEC.gov. Election results and vote totals for house races were taken from CNN's election results page. Inspiration How much of an impact does campaign spending and fundraising have on an election?  Is the impact greater in certain areas?  Given this dataset to what degree of accuracy could we have predicted the election results?,CSV,,"[finance, politics]",CC0,,,546,5093,0.580078125,Can an election be predicted from the preceding campaign finance reports?,Campaign Finance versus Election Results,https://www.kaggle.com/danerbland/electionfinance,Thu Dec 08 2016
,Hacker News,"[title, url, text, dead, by, score, time, type, id, parent, descendants, ranking, deleted, timestamp]","[string, string, string, string, string, string, numeric, string, numeric, string, string, string, string, dateTime]","Context This dataset contains a randomized sample of roughly one quarter of all stories and comments from Hacker News from its launch in 2006. Hacker News is a social news website focusing on computer science and entrepreneurship. It is run by Paul Graham's investment fund and startup incubator Y Combinator. In general content that can be submitted is defined as ""anything that gratifies one's intellectual curiosity"". Content Each story contains a story ID the author that made the post when it was written and the number of points the story received. Please note that the text field includes profanity. All texts are the author’s own do not necessarily reflect the positions of Kaggle or Hacker News and are presented without endorsement. Acknowledgements This dataset was kindly made publicly available by Hacker News under the MIT license. Inspiration  Recent studies have found that many forums tend to be dominated by a very small fraction of users. Is this true of Hacker News? Hacker News has received complaints that the site is biased towards Y Combinator startups. Do the data support this?  Is the amount of coverage by Hacker News predictive of a startup’s success?  Use this dataset with BigQuery You can use Kernels to analyze share and discuss this data on Kaggle but if you’re looking for real-time updates and bigger data check out the data in BigQuery too https//cloud.google.com/bigquery/public-data/hacker-news The BigQuery version of this dataset has roughly four times as many articles.",CSV,,"[news agencies, internet]",Other,,,326,10697,1024,A subset of all Hacker News articles,Hacker News Corpus,https://www.kaggle.com/hacker-news/hacker-news-corpus,Fri Jun 30 2017
,Jessica Yung,"[LOCATION, Country, SUBJECT, Subject, MEASURE, Measure, TIME, Time, Unit Code, Unit, PowerCode Code, PowerCode, Reference Period Code, Reference Period, Value, Flag Codes, Flags]","[string, string, string, string, string, string, numeric, numeric, string, string, numeric, string, string, string, numeric, string, string]",Context The data was obtained from the OECD website's Productivity Statistics section on 4 May 2017. Productivity = output per units of input. To be expanded on. Content To be filled in. Acknowledgements The data in this dataset is the property of the Organisation for Economic Co-operation and Development (the “OECD”).  The OECD makes data (the “Data”) available for use and consultation by   the public.  As stated in Section I(a) above Data may be subject to   restrictions beyond the scope of these Terms and Conditions either   because specific terms apply to those Data or because third parties   may have ownership interests. It is the User’s responsibility to   verify either in the metadata or source information whether the Data   is fully or partially owned by third parties and/or whether additional   restrictions may apply and to contact the owner of the Data before   incorporating it in your work in order to secure the necessary   permissions. The OECD in no way represents or warrants that it owns or   controls all rights in all Data and the OECD will not be liable to   any User for any claims brought against the User by third parties in   connection with the use of any Data.  Permitted use Except where additional restrictions apply as stated above You can extract from download copy adapt print distribute   share and embed Data for any purpose even for commercial use. You   must give appropriate credit to the OECD by using the citation   associated with the relevant Data or if no specific citation is   available You must cite the source information using the following   format OECD (year) (dataset name)(data source) DOI or URL (accessed   on (date)). When sharing or licensing work created using the Data You   agree to include the same acknowledgment requirement in any   sub-licenses that You grant along with the requirement that any   further sub-licensees do the same.  Inspiration Research Questions  Why is productivity growth slowing down in many advanced and emerging economies? ,CSV,,[economics],Other,,,283,1790,26,"Productivity, labour costs and GDP per capita for 32 OECD countries",OECD Productivity Data,https://www.kaggle.com/jessicayung/oecd-productivity-data,Thu May 04 2017
,Thomas Nelson,"[ID, N1, N2, N3, N4, N5, N6, N7, N8, N9, N10, N11, N12, N13, N14, N15, N16, N17, N18, N19, N20, N21, N22, N23, N24, N25, N26, N27, N28, N29, T1, T2, T3, T4, T5, T6, T7, T8, T9, T10, T11, T12, T13, T14, T15, T16, T17, T18, T19, T20, T21, T22, T23, T24, T25, T26, T27, T28, T29]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context If you use this data please be sure to give credit to Witten et. al. since it is their data set. Cervical cancer tumor vs matched control data.  Data set is gene expression profiling data from tumor and matched normal samples (29 each).  The data are the raw read counts (not normalized) from sequencing of microRNA.  This is not my data but was published by Witten D. et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology 858 Content The rows are each micro RNA name and the columns are the sample names (N=normal T=tumor).  The values are raw read counts. Acknowledgements Witten D. et al. (2010) Ultra-high throughput sequencing-based small RNA discovery and discrete statistical biomarker analysis in a collection of cervical tumours and matched controls. BMC Biology 858 Inspiration Use this data to practice making predictive models from machine learning/deep learning algorithms on gene expression profiling data.,CSV,,[healthcare],Other,,,412,4401,0.1025390625,gene expression profiling data from tumor and matched normal samples (29 each),Cervical cancer tumor vs matched control,https://www.kaggle.com/thomasnelson/cervical-cancer-tumor-vs-matched-control,Mon May 08 2017
,Chicago Police Department,"[Tow Date, Make, Style, Model, Color, Plate, State, Towed to Address, Tow Facility Phone, Inventory Number]","[dateTime, string, string, string, string, numeric, string, string, string, numeric]",Context This dataset displays location for vehicles that have been towed and impounded by the City of Chicago within the last 90 days. Illegally parked vehicles abandoned vehicles and vehicles used for illegal activities may be towed by the Chicago Police Department the Department of Streets and Sanitation the Department of Revenue Aviation and the office of the City Clerk. After a tow request is issued an inventory number is assigned by the Department of Streets and Sanitation and a truck is dispatched to tow the requested vehicle to a City auto pound. Disclaimer This dataset includes vehicles towed or relocated by the City of Chicago; it does not include vehicles towed by a private towing company.  Content Collected July 7-Sept 7. Updated data can be found here for past 90 days. Columns include  TowDate Make Style Model Color Plate State TowedToAddress TowFacilityPhone InventoryNumber  Acknowledgements Dataset was compiled by the City of Chicago here. Inspiration  When do most tows occur? What are the most commonly towed cars? Where are most tows occurring? ,CSV,,"[government agencies, government]",CC0,,,55,654,0.404296875,"5800 Violation Records, July 7- Sept 7 2017",Chicago Towing Records,https://www.kaggle.com/chicagopolice/chicago-towing,Tue Sep 12 2017
,Open Knowledge International,"[Country Code, Country Name, 2015 Rank, 2015 Score, 2014 Rank, 2014 Score, 2013 Rank, 2013 Score]","[string, string, numeric, numeric, numeric, numeric, numeric, numeric]",Context The Global Open Data Index is an annual effort to measure the state of open government data around the world. The crowdsourced survey is designed to assess the openness of specific government datasets according to the Open Definition. The Global Open Data Index is not an official government representation of the open data offering in each country but an independent assessment from a citizen’s perspective. It is a civil society audit of open data and it enables government progress on open data by giving them a measurement tool and a baseline for discussion and analysis of the open data ecosystem in their country and internationally from a key user’s perspective. The Global Open Data Index plays a powerful role in sustaining momentum for open data around the world and in convening civil society networks to use and collaborate around this data. Governments and open data practitioners can review the index results to see how accessible the open data they publish actually appears to their citizens see where improvements are necessary to make open data truly open and useful and track their progress year to year. According to the common open data assessment framework there are four different ways to evaluate data openess — context data use and impact. The Global Open Data Index is intentionally narrowly focused on the data aspect hence limiting its inquiry only to the datasets publication by national governments. It does not look at the broader societal context seek to assess use or impact in a systematic way or evaluate the quality of the data. This narrow focus of data publication enables it to provide a standardized robust comparable assessment of the state of the publication of key data by governments around the world. Acknowledgements The Global Open Data Index is compiled by Open Knowledge International with the assistance of volunteers from the Open Knowledge Network around the world. Inspiration What is the state of open data around the world? Which countries or regions score the highest in all the data categories? Did any countries receive lower open data scores than in previous years?,CSV,,[research],CC0,,,265,2204,0.2509765625,What is the state of open data around the world?,2015 Global Open Data Index,https://www.kaggle.com/okfn/open-data,Fri Mar 03 2017
,JamesS,"[Order, File_Type, SKU_number, SoldFlag, SoldCount, MarketingType, ReleaseNumber, New_Release_Flag, StrengthFactor, PriceReg, ReleaseYear, ItemCount, LowUserPrice, LowNetPrice]","[numeric, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context Attached is a set of products in which we are trying to determine which products we should continue to sell and which products to remove from our inventory. The file contains BOTH historical sales data AND active inventory which can be discerned with the column titled ""File Type"".  We suspect that data science applied to the set--such as a decision tree analysis or logistic regression or some other machine learning model---can help us generate a value (i.e. probability score) for each product that can be used as the main determinant evaluating the inventory. Each row in the file represents one product.  It is important to note that we have MANY products in our inventory and very few of them tend to sell (only about 10% sell each year) and many of the products only have a single sale in the course of a year. Content The file contains historical sales data (identified with the column titled File_Type) along with current active inventory that is in need of evaluation (i.e. File Type = ""Active""). The historical data shows sales for the past 6 months. The binary target (1 = sale 0 = no sale in past six months) is likely the primary target that should drive the analysis.  The other columns contain numeric and categorical attributes that we deem relevant to sales.  Note that some of the historical sales SKUs are ALSO included in the active inventory.  A few comments about the attributes included as we realize we may have some attributes that are unnecessary or may need to be explained.   SKU_number This is the unique identifier for each product. Order Just a sequential counter. Can be ignored. SoldFlag 1 = sold in past 6 mos. 0 = Not sold MarketingType = Two categories of how we market the product. This should probably be ignored or better yet each type should be considered independently.  New_Release_Flag = Any product that has had a future release (i.e.  Release Number > 1)  Inspiration (1) What is the best model to use that will provide us with a probability estimate of a sale for each SKU? We are mainly interested in a relative unit that we can continuously update based on these attributes (and others that we add as we are able).  (2) Is it possible to provide a scored file (i.e. a probability score for each SKU in the file) and to provide an evaluation of the accuracy of the selected model?  (3) What are the next steps we should take? Thanks very much for any suggestions you may provide.",CSV,,"[business, product]",Other,,,2486,18929,13,Records of sold and unsold products and their characteristics,Historical Sales and Active Inventory,https://www.kaggle.com/flenderson/sales-analysis,Thu Dec 08 2016
,sab30226,[],[],"Context Since 2001 Palestinian militants have launched thousands of rocket and mortar attacks on Israel from the Gaza Strip as part of the continuing Arab–Israeli conflict. From 2004 to 2014 these attacks have killed 27 Israeli civilians 5 foreign nationals 5 IDF soldiers and at least 11 Palestinians and injured more than 1900 people but their main effect is their creation of widespread psychological trauma and disruption of daily life among the Israeli population. source Wikipedia[https//en.wikipedia.org/wiki/Palestinian_rocket_attacks_on_Israel] Content This dataset contains the latest rocket alerts released by the Israeli ""Home Front Security"". The data was aggregated in the http//tzeva-adom.com site. The column contains date in dd/mm/yy format time mmhh format and the name of the area in Hebrew (And sometimes messages like הופעלה בישוב בעוטף עזה- ככה''נ איכון שווא which means that it's a false alarm. Those messages can be easily distinguished by the length of them. Area sizes may differ and does not report exact coordinates. A list of all the possible areas and messages is in a separate file. Data is reported from 2013-2014. Acknowledgements Context was taken from the Wikipedia page Palestinian rocket attacks on Israel. Data generated by the Israeli Home Front Command and was made easily accessible to developers(Many apps were created based on it. Reporting the alarm during the last conflict). The data was aggregated at the site http//tzeva-adom.com. Inspiration Israel has a system called ""Iron Dome"" which intercepts rockets(But only from certain distance).  The challenge for Israel is where those systems should be deployed and at what times. Furthermore it will be interesting to find patterns in the times rockets were being launched trying to see if different places were targeted in different times of day. Also areas that were targeted at the same time find if it's possible to cluster the places into different groups of areas. Operation ""Protective Edge"" took place from 8 July until 26 August 2014. After it ended  The rocket attack ended (more or less) until today(June 2017). It will be interesting to check out how the operation effects the alerts the launching patterns targeted areas etc. Though the names in this dataset are in Hebrew no Hebrew knowledge is needed for working with this dataset. I tried to find an appropriate automatic transliteration service to English but non of them proved useful. If anyone knows how to get them in English even using some list from the internet of the cities names in English and their corresponding Hebrew names  I'll appreciate your contribution to the dataset's GitHub repository https//github.com/tomersa/tzeva_adom_dataset.git Also you may contact me and I'll add the changes.",Other,,"[war, international relations]",CC4,,,88,1344,0.9931640625,List of rocket alerts in Israel published by the Israeli Home Front Command,"Rocket alerts in Israel made by ""Tzeva Adom""",https://www.kaggle.com/sab30226/rocket-alerts-in-israel-made-by-tzeva-adom,Sun Jun 25 2017
,Airbnb,"[listing_id, date, available, price]","[numeric, dateTime, string, numeric]",Context Since 2008 guests and hosts have used Airbnb to travel in a more unique personalized way. As part of the Airbnb Inside initiative this dataset describes the listing activity of homestays in Boston MA.  Content The following Airbnb activity is included in this Boston dataset * Listings including full descriptions and average review score * Reviews including unique id for each reviewer and detailed comments * Calendar including listing id and the price and availability for that day Inspiration  Can you describe the vibe of each Boston neighborhood using listing descriptions? What are the busiest times of the year to visit Boston? By how much do prices spike? Is there a general upward trend of both new Airbnb listings and total Airbnb visitors to Boston?  For more ideas visualizations of all Boston datasets can be found here. Acknowledgement This dataset is part of Airbnb Inside and the original source can be found here.,CSV,,"[united states, home, hotels]",CC0,,,3439,26773,72,"A sneak peek into the Airbnb activity in Boston, MA, USA",Boston Airbnb Open Data,https://www.kaggle.com/airbnb/boston,Thu Nov 17 2016
,US Patent and Trademark Office,[],[],Patent Assignment Daily XML (PADX) This dataset contains daily patent assignment (ownership) text (no drawings/images) for 10/18/2016 derived from patent assignment recordations made at the USPTO. Context Each day the US Patent and Trademark Office (USPTO) records patent assignments (changes in ownership). These assignments can be used to track chain-of-ownership for patents and patent applications. For more information about Ownership/Assignability of Patents and Applications please see the USPTO Manual of Patent Examining Procedure (MPEP) Section 301.  Frequency Weekly (MON-SUN) Period 10/18/2016  Inspiration This dataset provides insight into where new knowledge and technological advances originate in the United States. To get started working with XML files fork the kernel Exploring Daily Patent Assignment Files. You can use this dataset to understand what sector is currently up-and-coming or which companies are filing a lot of patents which can proxy their level of innovation a corporate strength or a focus of new research and development.  Acknowledgements The USPTO owns the dataset. These files are extracted nightly from the Assignment Historical Database (AHD). License Creative Commons - Public Domain Mark 1.0,Other,,[law],Other,,,148,3075,273,Contains daily patent assignment text for 10/18/2016,Patent Assignment Daily,https://www.kaggle.com/uspto/patent-assignment-daily,Thu Oct 20 2016
,Harvard University,"[Election Year, Expert ID, State, Sex, Age Range, Politician, Candidate, Activist, Monitor, Election Official, Voter, Citizen, Political Scale, Candidate Supported, Perceptions of Electoral Integrity Index, State Electoral Integrity, National Electoral Integrity, Laws Discriminated, Laws Favored Incumbent, Laws Restricted Citizen Rights, Electoral Laws Index, Election Managed Well, Voting Information Available, Election Officials Fair, Election Followed Law, Election Procedures Index, Boundaries Discriminated, Boundaries Favored Incumbent, Boundaries Impartial, Voting District Boundaries Index, Citizens Not Found in Register, Register Inaccurate, Ineligible Voters Registered, Voter Registration Index, Opposition Prevented, Women Provided Opportunity, Leaders Selected Candidates, Campaign Rallies Restricted, Party/Candidate Registration Index, Newspapers Balanced, Television Favored Incumbent, Media Access Fair, Media Coverage Fair, Social Media Exposed Fraud, Media Coverage Index, Subsidy Access Fair, Donation Access Fair, Campaign Accounts Transparent, Elections Bought by Rich, Resources Improperly Used, Campaign Finance Index, Voters Threatened with Violence, Fradulent Votes Cast, Voting Process Easy, Choice of Candidates, Postal Ballots Available, Disabled Provided Access, Expatriates Able to Vote, Internet Ballots Available, Voting Process Index, Ballot Boxes Secure, Results Announced Without Delay, Vote Count Fair, International Monitors Restricted, Domestic Monitors Restricted, Vote Count Index, Results Challenged, Protests Peaceful, Protests Violent, Dispute Resolved in Courts, Election Results Index, Authorities Impartial, Information Distributed, Authorities Scrutinized, Authorities Performed Well, Electoral Authorities Index, Election Rigged, Deadlines Restrictive, Voters Waited, Voters Intimidated, Multiple Ballots Cast, Machines Accurate, Records Secure, Votes Counted Quickly, Outcome Reflected Popular Will]","[numeric, numeric, string, string, string, string, string, string, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string, numeric, string, string, string, string, numeric, string, string, string, numeric, string, string, string, numeric, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, string, string, string, string, numeric, string, string, string, string, string, numeric, string, string, string, string, numeric, string, string, string, string, numeric, string, string, string, string, string, string, string, string, string]",Context Electoral integrity refers to international standards and global norms governing the appropriate conduct of elections. These standards have been endorsed in a series of authoritative conventions treaties protocols and guidelines by agencies of the international community and apply universally to all countries throughout the electoral cycle including during the pre-electoral period the campaign on polling day and in its aftermath. Content The Perceptions of Electoral Integrity (PEI) survey asks experts to evaluate elections according to 49 indicators grouped into eleven categories reflecting the whole electoral cycle. The PEI dataset is designed to provide a comprehensive systematic and reliable way to monitor the quality of elections worldwide. It includes disaggregated scores for each of the individual indicators summary indices for the eleven dimensions of electoral integrity and a PEI index score out of 100 to summarize the overall integrity of the election. Acknowledgements This study was conducted by Pippa Norris Alessandro Nai and Max Grömping for Harvard University's Electoral Integrity Project.,CSV,,[politics],CC0,,,198,1784,0.5576171875,How did 700+ experts perceive the integrity of the presidential election?,Electoral Integrity in 2016 US Election,https://www.kaggle.com/harvard-university/electoral-integrity,Thu Feb 09 2017
,Kevin Mader,[],[],Overview The dataset is designed to allow for different methods to be tested for examining the trends in CT image data associated with using contrast and patient age. The basic idea is to identify image textures statistical patterns and features correlating strongly with these traits and possibly build simple tools for automatically classifying these images when they have been misclassified (or finding outliers which could be suspicious cases bad measurements or poorly calibrated machines) Data The data are a tiny subset of images from the cancer imaging archive. They consist of the middle slice of all CT images taken where valid age modality and contrast tags could be found. This results in 475 series from 69 different patients. TCIA Archive Link - https//wiki.cancerimagingarchive.net/display/Public/TCGA-LUAD License  http//creativecommons.org/licenses/by/3.0/ After the publication embargo period ends these collections are freely available to browse download and use for commercial scientific and educational purposes as outlined in the Creative Commons Attribution 3.0 Unported License. Questions may be directed to help@cancerimagingarchive.net. Please be sure to acknowledge both this data set and TCIA in publications by including the following citations in your work  Data Citation  Albertina B. Watson M. Holback C. Jarosz R. Kirk S. Lee Y. … Lemmerman J. (2016). Radiology Data from The Cancer Genome Atlas Lung Adenocarcinoma [TCGA-LUAD] collection. The Cancer Imaging Archive. http//doi.org/10.7937/K9/TCIA.2016.JGNIHEP5  TCIA Citation  Clark K Vendt B Smith K Freymann J Kirby J Koppel P Moore S Phillips S Maffitt D Pringle M Tarbox L Prior F. The Cancer Imaging Archive (TCIA) Maintaining and Operating a Public Information Repository Journal of Digital Imaging Volume 26 Number 6 December 2013 pp 1045-1057. (paper) ,Other,,"[healthcare, tutorial, image data]",Other,,,1738,20351,437,CT images from cancer imaging archive with contrast and patient age,CT Medical Image Analysis Tutorial,https://www.kaggle.com/kmader/siim-medical-image-analysis-tutorial,Tue May 23 2017
,The Guardian,"[ID, Name in English, Name in French, Name in Spanish, Countries, Country codes alpha 3, ISO639-3 codes, Degree of endangerment, Alternate names, Name in the language, Number of speakers, Sources, Latitude, Longitude, Description of the location]","[numeric, string, string, string, string, string, string, string, string, string, numeric, string, numeric, numeric, string]","Context A recent Guardian blog post asks ""How many endangered languages are there in the World and what are the chances they will die out completely?"" The United Nations Education Scientific and Cultural Organisation (UNESCO) regularly publishes a list of endangered languages using a classification system that describes its danger (or completion) of extinction.  Content The full detailed dataset includes names of languages number of speakers the names of countries where the language is still spoken and the degree of endangerment. The UNESCO endangerment classification is as follows  Vulnerable most children speak the language but it may be restricted to certain domains (e.g. home) Definitely endangered children no longer learn the language as a 'mother tongue' in the home Severely endangered language is spoken by grandparents and older generations; while the parent generation may understand it they do not speak it to children or among themselves Critically endangered the youngest speakers are grandparents and older and they speak the language partially and infrequently Extinct there are no speakers left  Acknowledgements Data was originally organized and published by The Guardian and can be accessed via this Datablog post.  Inspiration  How can you best visualize this data? Which rare languages are more isolated (Sicilian for example) versus more spread out? Can you come up with a hypothesis for why that is the case? Can you compare the number of rare speakers with more relatable figures? For example are there more Romani speakers in the world than there are residents in a small city in the United States? ",CSV,,"[languages, linguistics]",Other,,,1401,12467,0.7197265625,"Number of endangered languages in the world, and their likelihood of extinction",Extinct Languages,https://www.kaggle.com/the-guardian/extinct-languages,Wed Dec 07 2016
,NASA,"[Center, Center Search Status, Facility, FacilityURL, Occupied, Status, URL Link, Record Date, Last Update, Country, Location, City, State, Zipcode]","[string, string, string, string, string, string, string, dateTime, dateTime, string, string, string, string, string]","Context NASA has something like 400 different facilities across the United States! This dataset is a collection of those facilities and their locations. Content  Center Name of the ""Center"" a collection facilities Center Search Status Public or...? Facility Name of the facility FacilityURL Occupied Status URL Link Record Date Last Update Country Location City State Zipcode  Acknowledgements This dataset was downloaded from https//data.nasa.gov/Management-Operations/NASA-Facilities/gvk9-iz74. The original file was modified to remove contact information for each facility.",CSV,,"[space, spaceflight]",CC0,,,66,904,0.0986328125,A dataset of NASA facility names and locations,NASA Facilities,https://www.kaggle.com/nasa/nasa-facilities,Wed Aug 23 2017
,Megan Risdal,"[Line number, Entity type, Payee name, Payee state, Date, Amount, Purpose]","[string, string, string, string, dateTime, string, string]",Did you know that Donald J. Trump For President Inc paid for a subscription to The New York Times on November 30th 2016? Curious to see where else over six million USD was spent and for what purposes? This dataset was downloaded from Pro Publica so you can find out. Content Here's what you get   Line number Entity type Payee name Payee state Date Amount Purpose  Contributions Want to contribute to this dataset? Download contributions to Donald J. Trump Inc here and share on the dataset's discussion forum. Acknowledgements This data was downloaded from Pro Publica. You can read about their data terms of use here.,CSV,,"[presidents, finance, politics]",Other,,,76,1346,0.0830078125,Itemized expenditures totaling over 6M USD,"Donald J. Trump For President, Inc",https://www.kaggle.com/mrisdal/donald-j-trump-for-president-inc,Mon Apr 17 2017
,The BGU Cyber Security Research Center,"[UserId, UUID, Extras, Action, timestamp]","[string, numeric, string, string, dateTime]","What is the SherLock dataset? A long-term smartphone sensor dataset with a high temporal resolution. The dataset also offers explicit labels capturing the to activity of malwares running on the devices. The dataset currently contains 10 billion data records from 30 users collected over a period of 2 years and an additional 20 users for 10 months (totaling 50 active users currently participating in the experiment).  The primary purpose of the dataset is to help security professionals and academic researchers in developing innovative methods of implicitly detecting malicious behavior in smartphones. Specifically from data obtainable without superuser (root) privileges. However this dataset can be used for research in domains that are not strictly security related. For example context aware recommender systems event prediction user personalization and awareness location prediction and more. The dataset also offers opportunities that aren't available in other datasets. For example the dataset contains the SSID and signal strength of the connected WiFi access point (AP) which is sampled once every second over the course of many months.  To gain full free access to the SherLock Dataset follow these two steps 1) Read complete and sign the license agreement. The general restrictions are -The license lasts for 3 years afterwhich the data must be deleted. -Do not share the data with those who are not bound by the license agreement. -Do not attempt to de-anonymize the individuals (volunteers) who have contributed the data. -Any of your publication that benefit from the SherLock project must cite the following article Mirsky Yisroel et al. ""SherLock vs Moriarty A Smartphone Dataset for Cybersecurity Research."" Proceedings of the 2016 ACM Workshop on Artificial Intelligence and Security. ACM 2016. 2)Send the scanned document as a PDF to bgu.sherlock@gmail.com and provide a gmail account to share a google drive folder with. More information can be found here or in this publication (download link). A 2 week data sample from a single user is provided on this Kaggle page. To access the full dataset for free please visit our site. Note The format of the sample dataset may differ from the full dataset. ",CSV,,[computer science],Other,,,518,6778,468,A long-term smartphone sensor dataset with a high temporal resolution,SherLock,https://www.kaggle.com/BGU-CSRC/sherlock,Wed Dec 07 2016
,NCAA,"[SCHOOL_ID, SCHOOL_NAME, SCHOOL_TYPE, ACADEMIC_YEAR, SPORT_CODE, SPORT_NAME, NCAA_DIVISION, NCAA_SUBDIVISION, NCAA_CONFERENCE, FOURYEAR_ATHLETES, FOURYEAR_SCORE, FOURYEAR_ELIGIBILITY, FOURYEAR_RETENTION, 2014_ATHLETES, 2014_SCORE, 2014_ELIGIBILITY, 2014_RETENTION, 2013_ATHLETES, 2013_SCORE, 2013_ELIGIBILITY, 2013_RETENTION, 2012_ATHLETES, 2012_SCORE, 2012_ELIGIBILITY, 2012_RETENTION, 2011_ATHLETES, 2011_SCORE, 2011_ELIGIBILITY, 2011_RETENTION, 2010_ATHLETES, 2010_SCORE, 2010_ELIGIBILITY, 2010_RETENTION, 2009_ATHLETES, 2009_SCORE, 2009_ELIGIBILITY, 2009_RETENTION, 2008_ATHLETES, 2008_SCORE, 2008_ELIGIBILITY, 2008_RETENTION, 2007_ATHLETES, 2007_SCORE, 2007_ELIGIBILITY, 2007_RETENTION, 2006_ATHLETES, 2006_SCORE, 2006_ELIGIBILITY, 2006_RETENTION, 2005_ATHLETES, 2005_SCORE, 2005_ELIGIBILITY, 2005_RETENTION, 2004_ATHLETES, 2004_SCORE, 2004_ELIGIBILITY, 2004_RETENTION]","[numeric, string, numeric, numeric, numeric, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context College presidents across the nation recognized a need to track how student-athletes are doing academically prior to graduation. Starting in 2003 colleges and universities in NCAA Division I — the largest and highest profile athletics programs — implemented a comprehensive academic reform package designed to improve the academic success and graduation of all student-athletes. The centerpiece of the academic reform package was the development of a real-time academic measurement for sports teams known as the Academic Progress Rate (APR). The APR includes student-athlete eligibility retention and graduation as factors in a formula that yields a single number providing a much clearer picture of the current academic culture on each Division I sports team in the country. Since its inception the APR has become an important measure of student-athlete academic success. For high APR scores the NCAA recognizes member institutions for ensuring that student-athletes succeed in the classroom. If however low APR scores are earned consistently member institutions can be subjected to penalties including scholarship reductions and the loss of eligibility to compete in championships. Content This study was created by the National Collegiate Athletic Association (NCAA) to provide public access to team-level APR scores eligibility rates retention rates and athlete counts on Division I athletic programs starting with the 2003-2004 season through the 2013-2014 season Inspiration Which sport or school has the highest academic score? Which schools' scores have increased or decreased significantly in the past decade? Are men's or women's team academic performance better? What about public and private colleges?,CSV,,"[sports, education]",Other,,,516,3896,2,Survey of team's program eligibility and retention by year and institution,Academic Scores for NCAA Athletic Programs,https://www.kaggle.com/ncaa/academic-scores,Thu Feb 16 2017
,Jacob Boysen,"[ID, M/F, Hand, Age, Educ, SES, MMSE, CDR, eTIV, nWBV, ASF, Delay]","[string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context The Open Access Series of Imaging Studies (OASIS) is a project aimed at making MRI data sets of the brain freely available to the scientific community. By compiling and freely distributing MRI data sets we hope to facilitate future discoveries in basic and clinical neuroscience. OASIS is made available by the Washington University Alzheimer’s Disease Research Center Dr. Randy Buckner at the Howard Hughes Medical Institute (HHMI)( at Harvard University the Neuroinformatics Research Group (NRG) at Washington University School of Medicine and the Biomedical Informatics Research Network (BIRN). Content  Cross-sectional MRI Data in Young Middle Aged Nondemented and Demented Older Adults This set consists of a cross-sectional collection of 416 subjects aged 18 to 96.  For each subject 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included.  The subjects are all right-handed and include both men and women.  100 of the included subjects over the age of 60 have been clinically diagnosed with very mild to moderate Alzheimer’s disease (AD).  Additionally a reliability data set is included containing 20 nondemented subjects imaged on a subsequent visit within 90 days of their initial session. Longitudinal MRI Data in Nondemented and Demented Older Adults  This set consists of a longitudinal collection of 150 subjects aged 60 to 96. Each subject was scanned on two or more visits separated by at least one year for a total of 373 imaging sessions. For each subject 3 or 4 individual T1-weighted MRI scans obtained in single scan sessions are included. The subjects are all right-handed and include both men and women. 72 of the subjects were characterized as nondemented throughout the study. 64 of the included subjects were characterized as demented at the time of their initial visits and remained so for subsequent scans including 51 individuals with mild to moderate Alzheimer’s disease. Another 14 subjects were characterized as nondemented at the time of their initial visit and were subsequently characterized as demented at a later visit.  Acknowledgements When publishing findings that benefit from OASIS data please include the following grant numbers in the acknowledgements section and in the associated Pubmed Central submission P50 AG05681 P01 AG03991 R01 AG021910 P20 MH071616 U24 RR0213 Inspiration Can you predict dementia? Alzheimer’s?,CSV,,"[healthcare, health sciences]",CC0,,,763,6610,0.0478515625,Magnetic Resonance Imaging Comparisons of Demented and Nondemented Adults,MRI and Alzheimers,https://www.kaggle.com/jboysen/mri-and-alzheimers,Wed Aug 16 2017
,Sauro Grandi,"[Date, Time, Location, Operator, Flight #, Route, Type, Registration, cn/In, Aboard, Fatalities, Ground, Summary]","[dateTime, numeric, string, string, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, string]","Analysis of the public dataset ""Airplane Crashes and Fatalities Since 1908"" (Full history of airplane crashes throughout the world from 1908-present) hosted by Open Data by Socrata available at https//opendata.socrata.com/Government/Airplane-Crashes-and-Fatalities-Since-1908/q2te-8cvq Questions  Yearly how many planes crashed? how many people were on board? how many survived? how many died? Highest number of crashes by operator and Type of aircrafts. ‘Summary’ field has the details about the crashes. Find the reasons of the crash and categorize them in different clusters i.e Fire shot down weather (for the ‘Blanks’ in the data category can be UNKNOWN) you are open to make clusters of your choice but they should not exceed 7. Find the number of crashed aircrafts and number of deaths against each category from above step. Find any interesting trends/behaviors that you encounter when you analyze the dataset.   My solution The following bar charts display the answers requested by point 1. of the assignment in particular  the planes crashed per year people aboard per year during crashes people dead per year during crashes people survived per year during crashes   The following answers regard point 2 of the assignment  Highest number of crashes by operator Aeroflot with 179 crashes By Type of aircraft Douglas DC-3 with 334 crashes  I have identified 7 clusters using k-means clustering technique on a matrix obtained by a text corpus created by using Text Analysis (plain text remove punctuation to lower etc.) The following table summarize for each cluster the number of crashes and death.  Cluster 1 258 crashes 6368 deaths Cluster 2 500 crashes 9408 deaths Cluster 3 211 crashes 3513 deaths    Cluster 4 1014 crashes 14790 deaths  Cluster 5 2749 crashes 58826 deaths  Cluster 6 195 crashes 4439 deaths    Cluster 7 341 crashes 8135 deaths  The following picture shows clusters using the first 2 principal components  For each clusters I will summarize the most used words and I will try to identify the causes of the crash Cluster 1 (258) aircraft crashed plane shortly taking.  No many information about this cluster can be deducted using Text Analysis Cluster 2 (500) aircraft airport altitude crashed crew due engine failed failure fire flight landing lost pilot plane runway takeoff taking.  Engine failure on the runway after landing or takeoff Cluster 3 (211) aircraft crashed fog  Crash caused by fog Cluster 4 (1014) aircraft airport attempting cargo crashed fire land landing miles pilot plane route runway struck takeoff  Struck a cargo during landing or takeoff Cluster 5 (2749) accident aircraft airport altitude approach attempting cargo conditions control crashed crew due engine failed failure feet fire flight flying fog ground killed land landing lost low miles mountain pilot. plane poor route runway short shortly struck takeoff taking weather  Struck a cargo due to engine failure or bad weather conditions mainly fog Cluster 6 (195) aircraft crashed engine failure fire flight left pilot plane runway  Engine failure on the runway Cluster 7 (341) accident aircraft altitude cargo control crashed crew due engine failure flight landing loss lost pilot plane takeoff  Engine failure during landing or takeoff  Better solutions are welcome. Thanks.",CSV,,[aviation],ODbL,,,15240,63558,2,"Full history of airplane crashes throughout the world, from 1908-present",Airplane Crashes Since 1908,https://www.kaggle.com/saurograndi/airplane-crashes-since-1908,Sat Sep 10 2016
,Anu,"[Age as of 9/4/2017, Number (rounded), Percent]","[string, numeric, numeric]","Context Aggregate data from US Citizenship and Immigration Services on Deferred Action for Childhood Arrivals program as of Sept 4 2017 https//www.uscis.gov/tools/reports-studies/immigration-forms-data/data-set-form-i-821d-deferred-action-childhood-arrivals Content  Country of birth approximate active DACA recipients  Notes  This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4 2017.  The number of individuals who were ever granted DACA as of September 4 2017 was approximately 800000. This total excludes persons who applied for an initial grant of DACA and were not approved as well as initial DACA requestors that were approved at first but later had their initial request denied or terminated. Nearly 40000 DACA recipients have adjusted to lawful permanent resident (LPR) status leaving about 760000 who are not LPRs.  About 70000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal leaving approximately 690000 active DACA recipients as of September 4 2017. Totals do not add due to rounding. Countries  with fewer than 10 active DACA recipients are included in other. Not available  data are not available in electronic systems. Source  U.S. Citizenship and Immigration Services""                               State of Residence  Notes  This table refers to individuals who were  granted Deferred Action for Childhood Arrivals (DACA) as of September 4 2017.  The number of individuals who were ever granted DACA as of September 4 2017 was approximately 800000. This total excludes persons who applied for an initial grant of DACA and were not approved as well as initial DACA requestors that were approved at first but later had their initial request denied or terminated. Nearly 40000 DACA recipients have adjusted to lawful permanent resident (LPR) status leaving about 760000 who are not LPRs.  About 70000 individuals who were granted for DACA either failed to renew at the end of their 2-year validity period or were denied on renewal leaving approximately 690000 active DACA recipients as of September 4 2017. State of residence at the time of most recent application. Totals do not add due to rounding. Territories with less than 10 residents are included in other. Not available  data are not available in electronic systems. Source  U.S. Citizenship and Immigration Services CLAIMS3 and ELIS Systems.""                                 Core-based Statistical Areas  Notes  This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4 2017.  The number of individuals who were ever granted DACA as of September 4 2017 was approximately 800000. This total excludes persons who applied for an initial grant of DACA and were not approved as well as initial DACA requestors that were approved at first but later had their initial request denied or terminated. Nearly 40000 DACA recipients have adjusted to lawful permanent resident (LPR) status leaving about 760000 who are not LPRs.  About 70000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal leaving approximately 690000 active DACA recipients as of September 4 2017. Core Based Statistical Areas (CBSA) at the time of most recent application.  CBSAs are defined  by the Office of Management and Budget. Totals may not add due to rounding. CBSAs with fewer than 1000 residents are included in other. Not available  data are not available in electronic systems. Source  U.S. Citizenship and Immigration Services CLAIMS3 and ELIS Systems.""                                                 Age and Sex  Notes  These tables refer to individuals who were  granted Deferred Action for Childhood Arrivals (DACA) as of September 4 2017. The number of individuals who were ever granted DACA as of September 4 2017 was approximately 800000. This total excludes persons who applied for an initial grant of DACA and were not approved as well as initial DACA requestors that were approved at first but later had their initial request denied or terminated.  Nearly 40000 DACA recipients have adjusted to lawful permanent resident (LPR) status leaving about 760000 who are not LPRs.  About 70000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal leaving approximately 690000 active DACA recipients as of September 4 2017. Age as of September 4 2017 and marital status as of the time of most recent application. Totals do not add due to rounding. Interquartile range is the range between the 25th percentile and the 75th percentile.  About half of the active DACA recipients are 20 to 27 years old. Not available  data are not available in electronic systems. Source  U.S. Citizenship and Immigration Services CLAIMS3 and ELIS Systems.""                                     Marital status  Notes  This table refers to individuals who were granted Deferred Action for Childhood Arrivals (DACA) as of September 4 2017.  The number of individuals who were ever granted DACA as of September 4 2017 was approximately 800000. This total excludes persons who applied for an initial grant of DACA and were not approved as well as initial DACA requestors that were approved at first but later had their initial request denied or terminated.  Nearly 40000 DACA recipients have adjusted to lawful permanent resident (LPR) status leaving about 760000 who are not LPRs.  About 70000 individuals who were granted DACA either failed to renew at the end of their 2-year validity period or were denied on renewal leaving approximately 690000 active DACA recipients as of September 4 2017. Marital status at time of most recent application. Not available  data are not available in electronic systems.  Inspiration I converted this data set from the published PDF so I could practice making choropleth maps.  Acknowledgements I got the CBSA topojson from here https//discourse.looker.com/t/custom-topojson-for-2014-core-based-statistical-areas-cbsa/2028",CSV,,"[politics, demographics]",Other,,,125,1316,1,Aggregate data as of Sept 4 2017,DACA Recipients,https://www.kaggle.com/anupamakhan/daca-recipients-as-of-sept-4-2017,Tue Oct 10 2017
,alimbekovkz,[],[],Context This data set is a collection of all StarCraft pro-player 2 matches. The data is taken from the site - http//aligulac.com/ Content Dataset data - 18 October 2017. You can parse actual data. Just use my script (Github) This dataset contains 10 variables  match_date -Date of match in format mm/dd/yyyy player_1 - Player 1 Nickname player_1_match_status - Match status for Player 1 winner or loser score - match score (example 1-0 1-2 etc) player_2 - Player 2 Nickname player_2_match_status - Match status for Player 2 winner or loser player_1_race - Player 1 Race Z - Zerg P - Protoss T - Terran player_2_race - Player 2 Race Z - Zerg P - Protoss T - Terran addon - Game addon WoL- Wings of Liberty HotS - Heart of the Swarm LotV - Legacy of the Void tournament_type - online or offline  Acknowledgements The source is http//aligulac.com/ Inspiration Questions worth exploring  Predict the outcome of a match between two players or whatever you want .... ,CSV,,"[games and toys, video games, history, strategy]",Other,,,83,1428,23,Predict the results of matches in StarCraft 2 using historical data,StarCraft II matches history,https://www.kaggle.com/alimbekovkz/starcraft-ii-matches-history,Wed Oct 18 2017
,citylines.co,"[id, name, coords, start_year, url_name, country, country_state]","[numeric, string, string, numeric, string, string, string]",Context What did the expansion of the London Underground the world’s first underground railway which opened in 1863 look like? What about the transportation system in your home city? Citylines collects data on transportation lines across the world so you can answer questions like these and more. Content This dataset originally shared and updated here includes transportation line data from a number of cities from around the world including London Berlin Mexico City Barcelona Washington D.C. and others covering many thousands of kilometers of lines.  Inspiration You can explore geometries to generate maps and even see how lines have changed over time based on historical records. Want to include shapefiles with your analysis? Simply publish a shapefile dataset here and then create a new kernel (R or Python script/notebook) adding your shapefile as an additional datasource.,CSV,,"[cities, transport]",ODbL,,,464,4143,2,Explore the transport systems of the world's cities,City Lines,https://www.kaggle.com/citylines/city-lines,Sat Aug 05 2017
,UCI Machine Learning,"[Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age, Outcome]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular all patients here are females at least 21 years old of Pima Indian heritage. Content The datasets consists of several medical predictor variables and one target variable Outcome. Predictor variables includes the number of pregnancies the patient has had their BMI insulin level age and so on. Acknowledgements Smith J.W. Everhart J.E. Dickson W.C. Knowler W.C. & Johannes R.S. (1988). Using the ADAP learning algorithm to forecast the onset of diabetes mellitus. In Proceedings of the Symposium on Computer Applications and Medical Care (pp. 261--265). IEEE Computer Society Press. Inspiration Can you build a machine learning model to accurately predict whether or not the patients in the dataset have diabetes or not?,CSV,,[healthcare],CC0,,,9474,65687,0.0224609375,Predict the onset of diabetes based on diagnostic measures,Pima Indians Diabetes Database,https://www.kaggle.com/uciml/pima-indians-diabetes-database,Fri Oct 07 2016
,Department of Transportation,"[Report Number, Supplemental Number, Accident Year, Accident Date/Time, Operator ID, Operator Name, Pipeline/Facility Name, Pipeline Location, Pipeline Type, Liquid Type, Liquid Subtype, Liquid Name, Accident City, Accident County, Accident State, Accident Latitude, Accident Longitude, Cause Category, Cause Subcategory, Unintentional Release (Barrels), Intentional Release (Barrels), Liquid Recovery (Barrels), Net Loss (Barrels), Liquid Ignition, Liquid Explosion, Pipeline Shutdown, Shutdown Date/Time, Restart Date/Time, Public Evacuations, Operator Employee Injuries, Operator Contractor Injuries, Emergency Responder Injuries, Other Injuries, Public Injuries, All Injuries, Operator Employee Fatalities, Operator Contractor Fatalities, Emergency Responder Fatalities, Other Fatalities, Public Fatalities, All Fatalities, Property Damage Costs, Lost Commodity Costs, Public/Private Property Damage Costs, Emergency Response Costs, Environmental Remediation Costs, Other Costs, All Costs]","[numeric, numeric, numeric, dateTime, numeric, string, string, string, string, string, string, string, string, string, string, numeric, numeric, string, string, numeric, numeric, numeric, numeric, string, string, string, dateTime, dateTime, numeric, string, string, string, string, string, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This database includes a record for each oil pipeline leak or spill reported to the Pipeline and Hazardous Materials Safety Administration since 2010. These records include the incident date and time operator and pipeline cause of incident type of hazardous liquid and quantity lost injuries and fatalities and associated costs. Acknowledgements The oil pipeline accident reports were collected and published by the DOT's Pipeline and Hazardous Materials Safety Administration.,CSV,,"[environment, energy]",CC0,,,1220,8474,0.8662109375,"Causes, injuries/fatalities, and costs of pipeline leaks and spills","Oil Pipeline Accidents, 2010-Present",https://www.kaggle.com/usdot/pipeline-accidents,Wed Feb 08 2017
,David de la Iglesia Castro,[],[],"Context The aim of this dataset is to provide a simple way to get started with 3D computer vision problems such as 3D shape recognition. Accurate 3D point clouds can (easily and cheaply) be adquired nowdays from different sources  RGB-D devices Google Tango Microsoft Kinect etc. Lidar. 3D reconstruction from multiple images.  However there is a lack of large 3D datasets (you can find a good one here based on triangular meshes); it's especially hard  to find datasets based on point clouds (wich is the raw output from every 3D sensing device). This dataset contains 3D point clouds generated from the original images of the MNIST dataset to bring a familiar introduction to 3D to people used to work with 2D datasets (images). In the 3D_from_2D notebook you can find the code used to generate the dataset. You can use the code in the notebook to generate a bigger 3D dataset from the original. Content full_dataset_vectors.h5 The entire dataset stored as 4096-D vectors obtained from the voxelization (x16 y16 z16) of all the 3D point clouds. In adition to the original point clouds it contains randomly rotated copies with noise. The full dataset is splitted into arrays  X_train (10000 4096) y_train (10000) X_test(2000 4096) y_test (2000)  Example python code reading the full dataset  with h5py.File(""../input/train_point_clouds.h5"" ""r"") as hf          X_train = hf[""X_train""][]      y_train = hf[""y_train""][]          X_test = hf[""X_test""][]        y_test = hf[""y_test""][]    train_point_clouds.h5 & test_point_clouds.h5 5000 (train)  and 1000 (test) 3D point clouds stored in HDF5 file format. The point clouds have zero mean and a maximum dimension range of 1. Each file is divided into HDF5 groups Each group is named as its corresponding array index in the original mnist dataset and it contains  ""points"" dataset x y z coordinates of each 3D point in the point cloud. ""normals"" dataset nx ny nz components of the unit normal associate to each point. ""img"" dataset the original mnist image. ""label"" attribute the original mnist label.  Example python code reading 2 digits and storing some of the group content in tuples with h5py.File(""../input/train_point_clouds.h5"" ""r"") as hf         a = hf[""0""]     b = hf[""1""]         digit_a = (a[""img""][] a[""points""][] a.attrs[""label""])      digit_b = (b[""img""][] b[""points""][] b.attrs[""label""])   voxelgrid.py Simple Python class that generates a grid of voxels from the 3D point cloud. Check kernel for use. plot3D.py Module with functions to plot point clouds and voxelgrid inside jupyter notebook. You have to run this locally due to Kaggle's notebook lack of support to rendering Iframes. See github issue here Functions included  array_to_color Converts 1D array to rgb values use as kwarg color in plot_points() plot_points(xyz colors=None size=0.1 axis=False) plot_voxelgrid(v_grid cmap=""Oranges"" axis=False)  Acknowledgements  Website of the original MNIST dataset Website of the 3D MNIST dataset  Have fun!",Other,,"[writing, artificial intelligence]",Other,,,833,14303,244,A 3D version of the MNIST database of handwritten digits,3D MNIST,https://www.kaggle.com/daavoo/3d-mnist,Thu Nov 10 2016
,PULKIT KHANDELWAL,"[event-id, visible, timestamp, location-long, location-lat, manually-marked-outlier, visible, sensor-type, individual-taxon-canonical-name, tag-local-identifier, individual-local-identifier, study-name, ECMWF Interim Full Daily Invariant Low Vegetation Cover, NCEP NARR SFC Vegetation at Surface, ECMWF Interim Full Daily Invariant High Vegetation Cover]","[numeric, boolean, dateTime, numeric, numeric, string, boolean, string, string, numeric, string, string, numeric, numeric, numeric]",Context Movebank is a free online database of animal tracking data hosted by the Max Planck Institute for Ornithology. It is designed to help animal tracking researchers to manage share protect analyze and archive their data. Movebank is an international project with over 11000 users including people from research and conservation groups around the world. The animal tracking data accessible through Movebank belongs to researchers all over the world. These researchers can choose to make part or all of their study information and animal tracks visible to other registered users or to the public. Animal tracking Animal tracking data helps us understand how individuals and populations move within local areas migrate across oceans and continents and evolve through millennia. This information is being used to address environmental challenges such as climate and land use change biodiversity loss invasive species and the spread of infectious diseases. Read more.,CSV,,[animals],Other,,,202,2864,21,Analyzing migratory patterns of animals,Movebank: Animal Tracking,https://www.kaggle.com/pulkit8595/movebank-animal-tracking,Mon Jan 09 2017
,Chris Crawford,"[patient, cancer]","[numeric, string]","Context This dataset comes from a proof-of-concept study published in 1999 by Golub et al. It showed how new cases of cancer could be classified by gene expression monitoring (via DNA microarray) and thereby provided a general approach for identifying new cancer classes and assigning tumors to known classes. These data were used to classify patients with acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL). Content Golub et al ""Molecular Classification of Cancer Class Discovery and Class Prediction by Gene Expression Monitoring"" There are two datasets containing the initial (training 38 samples) and independent (test 34 samples) datasets used in the paper.  These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.  Acknowledgements Molecular Classification of Cancer Class Discovery and Class Prediction by Gene Expression Science 286531-537. (1999). Published 1999.10.14  T.R. Golub D.K. Slonim P. Tamayo C. Huard M. Gaasenbeek J.P. Mesirov H. Coller M. Loh J.R. Downing M.A. Caligiuri C.D. Bloomfield and E.S. Lander These datasets have been converted to a comma separated value files (CSV). Inspiration These datasets are great for classification problems. The original authors used the data to classify the type of cancer in each patient by their gene expressions.",CSV,,"[human genetics, biology, health sciences, biotechnology]",CC0,,,975,7921,4,Molecular Classification of Cancer by Gene Expression Monitoring,Gene expression dataset (Golub et al.),https://www.kaggle.com/crawford/gene-expression,Wed Aug 09 2017
,The Guardian,"[Country, Code, Population, GDP per Capita]","[string, string, numeric, numeric]",Content Which Olympic athletes have the most gold medals? Which countries are they from and how has it changed over time?  More than 35000 medals have been awarded at the Olympics since 1896. The first two Olympiads awarded silver medals and an olive wreath for the winner and the IOC retrospectively awarded gold silver and bronze to athletes based on their rankings. This dataset includes a row for every Olympic athlete that has won a medal since the first games. Acknowledgements Data was provided by the IOC Research and Reference Service and published by The Guardian's Datablog.,CSV,,[olympic games],CC4,,,3961,15892,3,Which countries and athletes have won the most medals at the Olympic games?,"Olympic Sports and Medals, 1896-2014",https://www.kaggle.com/the-guardian/olympic-games,Tue Jan 24 2017
,Ankit,"[seat_allotment, phase, ac_no, ac, district, candidate, party, votes]","[string, numeric, numeric, string, string, string, string, numeric]",Context The assembly election results for Uttar Pradesh(UP) were surprising to say the least. Never in the past has any single party secured a similar mandate. UP with a population of around 220 million is as big as the whole of united states. It has 403 constituencies each having its own demographic breakup. The election was conducted in 7 phases. Content The dataset has 8 variables seat_allotment As some of you might be aware that there was a coalition between INC and SP which materialized pretty late into the campaign. Hence in a few constituencies the high command of the 2 parties could not convince contestants to forfeit their nomination. In such constituencies there is a situation that is called a friendly fight(FF) where candidates from both parties INC and SP are contesting instead of just one.  These constituencies are marked by the flag FF (Friendly Fight). Others are INC (contested by INC) SP(Contested by SP) and DNC(Contested by none) phase The phase in which the election was conducted. ac_no Assembly constituency number ac Assembly constituency name district District to which the ac belongs candidate Candidate name party Party name votes votes for each candidate Source Scraped from eciresults.nic.in,CSV,,[politics],CC0,,,388,3079,0.28515625,Dataset for constituency wise results for all candidates,Uttar Pradesh Assembly Elections 2017,https://www.kaggle.com/ankit2106/uttar-pradesh-assembly-elections-2017,Sat Apr 22 2017
,Luiz Gustavo Schiller,"[id, apelido, clube_id, posicao_id]","[numeric, string, numeric, numeric]","Context CartolaFC is the most popular fantasy football in Brazil. Before each round of the Brazilian Football League players choose which athletes they want for their teams and they score points based on their real-life performances.  Content Data is divided in 7 kinds of files Athletes (atletas)  ""atleta_id"" id ""nome"" athlete's full name ""apelido"" athlete's nickname  Clubs (clubes)  ""id"" id ""nome"" club's name ""abreviacao"" name abbreviation ""slug"" used for some API calls  Matches (partidas)  ""rodada_id"" current round ""clube_casa_id"" home team id ""clube_visitante_id"" away team id ""clube_casa_posicao"" home team's position on the league ""clube_visitante_posicao"" away team's position on the league ""aproveitamento_mandante"" home team's outcome on the last five matches (d loss e draw v victory) ""aproveitamento_visitante"" away team's outcome on the last five matches (d loss e draw v victory) ""placar_oficial_mandante"" home team's score ""placar_oficial_visitante"" away team's score ""partida_data"" match date ""local"" stadium ""valida"" match valid for scoring  Scouts  ""atleta_id"" reference to athlete ""rodada_id"" current round ""clube_id"" reference to club ""posicao_id"" reference to position ""status_id"" reference to status ""pontos_num"" points scored on current round ""preco_num"" current price ""variacao_num"" price variation from previous round ""media_num"" average points per played round ""jogos_num"" number of matches played ""FS"" suffered fouls ""PE"" missed passes ""A"" assistances ""FT"" shots on the post ""FD"" defended shots ""FF"" shots off target ""G"" goals ""I"" offsides ""PP"" missed penalties ""RB"" successful tackes ""FC"" fouls commited ""GC"" own goals ""CA"" yellow cards ""CV"" red cards ""SG"" clean sheets (only defenders) ""DD"" difficult defenses (only goalies) ""DP"" defended penalties (only goalies) ""GS"" suffered goals (only goalies)  Positions (posicoes)  ""id"" id ""nome"" name ""abreviacao"" abbreviation  Status  ""id"" id ""nome"" name  Points (pontuacao)  ""abreviacao"" abbreviation ""nome"" name ""pontuacao"" points earned for respective scout  Acknowledgements The datasets from 2014 to 2016 were taken from here https//github.com/thevtm/CartolaFCDados. Data from 2017 until round 11 was taken from this repo https//github.com/henriquepgomide/caRtola. From 2017 round 12 and on I've been extracting the data from CartolaFC's API (which is not officially public). Inspiration It would be interesting to see analyses on which factors make an athlete or team more likely to score points and also predictive models for future scores.",CSV,,"[association football, brazil, sports]",CC0,,,441,2903,8,Data from the popular Brazilian Fantasy Football (2014 to 2017) ⚽️,CartolaFC,https://www.kaggle.com/schiller/cartolafc,Wed Aug 09 2017
,The Washington Post,"[season, week_num, day_of_week, gametime_local, home_team, away_team, home_score, away_score, OT_flag, arrests, division_game]","[numeric, numeric, string, numeric, string, string, numeric, numeric, string, numeric, string]",Context This dataset consists of public records requests made by the Washington Post to police departments that oversee security at each NFL stadium.  Content Twenty-nine of the 31 jurisdictions provided at least partial data though reporting methods differed from agency to agency; Cleveland and New Orleans did not submit data. Certain data were omitted if found to be incomplete or unreliable. Among those jurisdictions sending partial arrest figures for home games between 2011 and 2015 were Buffalo Miami and Oakland. St. Louis provided only year-by-year arrest data rather than game-by-game numbers. Detroit Minneapolis and Atlanta did not provide data for arrests that took place in stadium parking lots. The main dataset includes fields such as the day of the week which teams were playing on which home field and the score of the game between 2011 and 2015. Inspiration  Which stadiums had the most arrests? The least? Are arrests more likely when the home team lost a game? Does the score correlate with number of arrests? (For example if the game ended in a narrow loss for the home team does this correlate with more arrests?) Are there any stadiums with consistent arrest rates regardless of how the game ended?  Acknowledgements Data was collected and reported by Kent Babb and Steven Rich of the Washington Post and the original dataset can be found here.,CSV,,"[american football, crime]",CC4,,,338,2436,0.0576171875,Public records of police arrests at NFL stadiums,NFL Arrests,https://www.kaggle.com/washingtonpost/nfl-arrests,Mon Nov 28 2016
,Scott Cole,"[Location, Burrito, Date, Neighborhood, Address, URL, Yelp, Google, Chips, Cost, Hunger, Mass (g), Density (g/mL), Length, Circum, Volume, Tortilla, Temp, Meat, Fillings, Meat:filling, Uniformity, Salsa, Synergy, Wrap, overall, Rec, Reviewer, Notes, Unreliable, NonSD, Beef, Pico, Guac, Cheese, Fries, Sour cream, Pork, Chicken, Shrimp, Fish, Rice, Beans, Lettuce, Tomato, Bell peper, Carrots, Cabbage, Sauce, Salsa, Cilantro, Onion, Taquito, Pineapple, Ham, Chile relleno, Nopales, Lobster, Queso, Egg, Mushroom, Bacon, Sushi, Avocado, Corn, Zucchini]","[string, string, dateTime, string, string, string, string, string, string, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string]",Mexican cuisine is often the best food option is southern California. And the burrito is the hallmark of delicious taco shop food tasty cheap and filling. Appropriately an effort was launched to critique burritos across the county and make this data open to the lay burrito consumer. At this time the data set contains ratings from over 200 burritos from around 50 restaurants. There are 10 core dimensions of the San Diego burrito. * Volume * Tortilla quality *Temperature * Meat quality * Non-meat filling quality * Meat-to-filling ratio * Uniformity * Salsa quality * Flavor synergy * Wrap integrity All of these measures (except for Volume) are rated on a scale from 0 to 5 0 being terrible and 5 being optimal. Other information available for each burrito includes an overall rating cost Yelp rating of the restaurant and more. More information about the data set as well as a link to the continuously updated dataset can be found here.,CSV,,[food and drink],CC4,,,1031,7381,0.0654296875,Mexican food enthusiasts rate 10 dimensions of hundreds of burritos in San Diego,Burritos in San Diego,https://www.kaggle.com/srcole/burritos-in-san-diego,Tue Jan 02 2018
,jmataya,"[id, cause_of_death, region_origin, affected_nationality, missing, dead, incident_region, date, source, reliability, lat, lon]","[numeric, string, string, numeric, numeric, numeric, string, dateTime, string, string, numeric, numeric]",About the Missing Migrants Data This data is sourced from the International Organization for Migration.  The data is part of a specific project called the Missing Migrants Project which tracks deaths of migrants including refugees  who have gone missing along mixed migration routes worldwide. The research behind this project began with the October 2013 tragedies when at least 368 individuals died in two shipwrecks near the Italian island of Lampedusa. Since then Missing Migrants Project has developed into an important hub and advocacy source of information that media researchers and the general public access for the latest information. Where is the data from? Missing Migrants Project data are compiled from a variety of sources. Sources vary depending on the region and broadly include data from national authorities such as Coast Guards and Medical Examiners; media reports; NGOs; and interviews with survivors of shipwrecks. In the Mediterranean region data are relayed from relevant national authorities to IOM field missions who then share it with the Missing Migrants Project team. Data are also obtained by IOM and other organizations that receive survivors at landing points in Italy and Greece. In other cases media reports are used. IOM and UNHCR also regularly coordinate on such data to ensure consistency. Data on the U.S./Mexico border are compiled based on data from U.S. county medical examiners and sheriff’s offices as well as media reports for deaths occurring on the Mexico side of the border. Estimates within Mexico and Central America are based primarily on media and year-end government reports. Data on the Bay of Bengal are drawn from reports by UNHCR and NGOs. In the Horn of Africa data are obtained from media and NGOs. Data for other regions is drawn from a combination of sources including media and grassroots organizations. In all regions Missing Migrants Projectdata represents minimum estimates and are potentially lower than in actuality. Updated data and visuals can be found here https//missingmigrants.iom.int/  Who is included in Missing Migrants Project data? IOM defines a migrant as any person who is moving or has moved across an international border or within a State away from his/her habitual place of residence regardless of      (1) the person’s legal status;      (2) whether the movement is voluntary or involuntary;      (3) what the causes for the movement are; or      (4) what the length of the stay is.[1]  Missing Migrants Project counts migrants who have died or gone missing at the external borders of states or in the process of migration towards an international destination. The count excludes deaths that occur in immigration detention facilities during deportation or after forced return to a migrant’s homeland as well as deaths more loosely connected with migrants’ irregular status such as those resulting from labour exploitation. Migrants who die or go missing after they are established in a new home are also not included in the data so deaths in refugee camps or housing are excluded.  This approach is chosen because deaths that occur at physical borders and while en route represent a more clearly definable category and inform what migration routes are most dangerous. Data and knowledge of the risks and vulnerabilities faced by migrants in destination countries including death should not be neglected rather tracked as a distinct category. How complete is the data on dead and missing migrants? Data on fatalities during the migration process are challenging to collect for a number of reasons most stemming from the irregular nature of migratory journeys on which deaths tend to occur.  For one deaths often occur in remote areas on routes chosen with the explicit aim of evading detection. Countless bodies are never found and rarely do these deaths come to the attention of authorities or the media. Furthermore when deaths occur at sea frequently not all bodies are recovered - sometimes with hundreds missing from one shipwreck - and the precise number of missing is often unknown.  In 2015 over 50 per cent of deaths recorded by the Missing Migrants Project refer to migrants who are presumed dead and whose bodies have not been found mainly at sea.       Data are also challenging to collect as reporting on deaths is poor and the data that does exist are highly scattered. Few official sources are collecting data systematically. Many counts of death rely on media as a source. Coverage can be spotty and incomplete. In addition the involvement of criminal actors in incidents means there may be fear among survivors to report deaths and some deaths may be actively covered-up. The irregular immigration status of many migrants and at times their families as well also impedes reporting of missing persons or deaths. The varying quality and comprehensiveness of data by region in attempting to estimate deaths globally may exaggerate the share of deaths that occur in some regions while under-representing the share occurring in others.  What can be understood through this data? The available data can give an indication of changing conditions and trends related to migration routes and the people travelling on them which can be relevant for policy making and protection plans.  Data can be useful to determine the relative risks of irregular migration routes. For example Missing Migrants Project data show that despite the increase in migrant flows through the eastern Mediterranean in 2015 the central Mediterranean remained the more deadly route.  In 2015 nearly two people died out of every 100 travellers (1.85%) crossing the Central route as opposed to one out of every 1000 that crossed from Turkey to Greece (0.095%).  From the data we can also get a sense of whether groups like women and children face additional vulnerabilities on migration routes. However it is important to note that because of the challenges in data collection for the missing and dead basic demographic information on the deceased is rarely known. Often migrants in mixed migration flows do not carry appropriate identification. When bodies are found it may not be possible to identify them or to determine basic demographic information. In the data compiled by Missing Migrants Project sex of the deceased is unknown in over 80% of cases. Region of origin has been determined for the majority of the deceased. Even this information is at times extrapolated based on available information – for instance if all survivors of a shipwreck are of one origin it was assumed those missing also came from the same region.  The Missing Migrants Project dataset includes coordinates for where incidents of death took place which indicates where the risks to migrants may be highest.  However it should be noted that all coordinates are estimates. Why collect data on missing and dead migrants? By counting lives lost during migration even if the result is only an informed estimate we at least acknowledge the fact of these deaths. What before was vague and ill-defined is now a quantified tragedy that must be addressed.  Politically the availability of official data is important. The lack of political commitment at national and international levels to record and account for migrant deaths reflects and contributes to a lack of concern more broadly for the safety and well-being of migrants including asylum-seekers. Further it drives public apathy ignorance and the dehumanization of these groups. Data are crucial to better understand the profiles of those who are most at risk and to tailor policies to better assist migrants and prevent loss of life. Ultimately improved data should contribute to efforts to better understand the causes both direct and indirect of fatalities and their potential links to broader migration control policies and practices. Counting and recording the dead can also be an initial step to encourage improved systems of identification of those who die. Identifying the dead is a moral imperative that respects and acknowledges those who have died. This process can also provide a some sense of closure for families who may otherwise be left without ever knowing the fate of missing loved ones.                  Identification and tracing of the dead and missing As mentioned above the challenge remains to count the numbers of dead and also identify those counted. Globally the majority of those who die during migration remain unidentified. Even in cases in which a body is found identification rates are low. Families may search for years or a lifetime to find conclusive news of their loved one. In the meantime they may face psychological practical financial and legal problems.        Ultimately Missing Migrants Project would like to see that every unidentified body for which it is possible to recover is adequately “managed” analysed and tracked to ensure proper documentation traceability and dignity.  Common forensic protocols and standards should be agreed upon and used within and between States.  Furthermore data relating to the dead and missing should be held in searchable and open databases at local national and international levels to facilitate identification.  For more in-depth analysis and discussion of the numbers of missing and dead migrants around the world and the challenges involved in identification and tracing read our two reports on the issue  Fatal Journeys Tracking Lives Lost during Migration (2014) and Fatal Journeys Volume 2 Identification and Tracing of Dead and Missing Migrants         Content The data set records incidents of missing persons and deaths of migrants   columns in the data   ID - unique key documenting incident  Cause of Death - reason for death Region of Origin  Nationality  Missing Persons - counts  Dead - counts of deaths  Incident Region - region where incident was recorded  Date - the date when the incident was recorded.  Note the data set includes records from 2014 to June 2017 Latitude - spatial coordinates  Longitude - spatial coordinates  Acknowledgements This data set was created by the International Organization for Migration.   https//www.iom.int/about-iom Established in 1951 IOM is the leading inter-governmental organization in the field of migration and works closely with governmental intergovernmental and non-governmental partners. With 166 member states a further 8 states holding observer status and offices in over 100 countries IOM is dedicated to promoting humane and orderly migration for the benefit of all. It does so by providing services and advice to governments and migrants. IOM works to help ensure the orderly and humane management of migration to promote international cooperation on migration issues to assist in the search for practical solutions to migration problems and to provide humanitarian assistance to migrants in need including refugees and internally displaced people. The IOM Constitution recognizes the link between migration and economic social and cultural development as well as to the right of freedom of movement. IOM works in the four broad areas of migration management  Migration and development Facilitating migration Regulating migration Forced migration.  IOM activities that cut across these areas include the promotion of international migration law policy debate and guidance protection of migrants' rights migration health and the gender dimension of migration. Start a new kernel,CSV,,"[demographics, international relations]",Other,,,439,4418,0.318359375,Explore missing migrants across the globe,Missing Migrants Dataset,https://www.kaggle.com/jmataya/missingmigrants,Fri Jun 16 2017
,Rob Harrand,"[, text, retweet_count, favorited, truncated, id_str, in_reply_to_screen_name, source, retweeted, created_at, in_reply_to_status_id_str, in_reply_to_user_id_str, lang, listed_count, verified, location, user_id_str, description, geo_enabled, user_created_at, statuses_count, followers_count, favourites_count, protected, user_url, name, time_zone, user_lang, utc_offset, friends_count, screen_name, country_code, country, place_type, full_name, place_name, place_id, place_lat, place_lon, lat, lon, expanded_url, url]","[numeric, string, numeric, boolean, boolean, numeric, string, string, boolean, string, numeric, numeric, string, numeric, boolean, string, numeric, string, boolean, string, numeric, numeric, numeric, boolean, string, string, string, string, numeric, numeric, string, string, string, string, string, string, string, numeric, numeric, numeric, numeric, string, string]","Context It's possible using R (and no doubt Python) to 'listen' to Twitter and capture tweets that match a certain description. I decided to test this out by grabbing tweets with the text 'good morning' in them over a 24 hours period to see if you could see the world waking up from the location information and time-stamp. The main R package used was streamR Content The tweets have been tidied up quite a bit. First I've removed re-tweets second I've removed duplicates (not sure why Twitter gave me them in the first place) third I've made sure the tweet contained the words 'good morning' (some tweets were returned that didn't have the text in for some reason) and fourth I've removed all the tweets that didn't have a longitude and latitude included. This latter step removed the vast majority. What's left are various aspects of just under 5000 tweets. The columns are  text   retweet_count  favorited  truncated  id_str     in_reply_to_screen_name    source     retweeted  created_at     in_reply_to_status_id_str  in_reply_to_user_id_str    lang   listed_count   verified   location   user_id_str    description    geo_enabled    user_created_at    statuses_count     followers_count favourites_count   protected  user_url   name   time_zone  user_lang  utc_offset     friends_count  screen_name    country_code   country    place_type     full_name  place_name     place_id   place_lat  place_lon  lat    lon    expanded_url   url  Acknowledgements I used a few blog posts to get the code up and running including this one Code The R code I used to get the tweets is as follows (note I haven't includes the code to set up the connection to Twitter. See the streamR PFD and the link above for that. You need a Twitter account) i = 1  while (i <= 280) {  filterStream(""tw_gm.json"" timeout = 300 oauth = my_oauth track = 'good morning' language = 'en') tweets_gm = parseTweets(""tw_gm.json"")  ex = grepl('RT' tweets_gm$text ignore.case = FALSE) #Remove the RTs tweets_gm = tweets_gm[!ex]  ex = grepl('good morning' tweets_gm$text ignore.case = TRUE) #Remove anything without good morning in the main tweet text tweets_gm = tweets_gm[ex]  ex = is.na(tweets_gm$place_lat) #Remove any with missing place_latitude information tweets_gm = tweets_gm[!ex]  tweets.all = rbind(tweets.all tweets_gm) #Add to the collection  i=i+1  Sys.sleep(5)  } ",CSV,,"[linguistics, sociology, internet]",Other,,,373,3747,3,Tweets captured over ~24 hours with the text 'good morning' in them,Good Morning Tweets,https://www.kaggle.com/tentotheminus9/good-morning-tweets,Fri Dec 09 2016
,Jacob Boysen,[],[],Context Shoreline bacteria are routinely monitored at fifteen stations around the perimeter of San Francisco where water contact recreation may occur. These include three stations within the Candlestick Point State Recreation Area one station at Islais Creek two stations at Aquatic Park two stations along Crissy Field Beach three stations at Baker Beach one station at China Beach and three stations along Ocean Beach. Content Dataset represents 552 samples taken across 15 locations over summer of 2017. Additional monitoring is conducted whenever a treated discharge from the City’s Combined sewer system occurs that affects a monitored beach. Acknowledgements The beach monitoring program is a cooperative effort between the San Francisco Public Utilities Commission and the San Francisco Department of Public Health. Samples are collected weekly year round.  READ MORE about the combined sewer system and a detailed explanation of the Beach Monitoring Program. Inspiration  Are there any patterns in beach water quality? ,CSV,,"[bodies of water, safety]",CC0,,,92,1169,0.0322265625,"Contaminant Sampling Across 15 Beaches, Summer 2017",SF Beaches Water Quality,https://www.kaggle.com/jboysen/sf-beaches-water,Wed Sep 06 2017
,FiveThirtyEight,"[date, time, country, country_code, group_letter, spi, spi_offense, spi_defense, group, sixteen, quarter_finals, semi_finals, final, champion]","[dateTime, dateTime, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content FiveThirtyEight’s World Cup forecasting model used ESPN’s Soccer Power Index (SPI) — a system that combines game and player ratings to estimate a team’s overall skill level — to calculate odds of each country’s performance during the two stages of the World Cup. The probabilities based on 10000 simulations were updated at the end of each match and aggregated into one file with the prediction date and time. Acknowledgements The 2014 prediction data was featured in the FiveThirtyEight article It's Brazil's World Cup to Lose and the interactive infographic 2014 World Cup Predictions. The World Cup match scores were scraped from the FIFA archive.,CSV,,[association football],CC4,,,349,3261,0.2724609375,How well did FiveThirtyEight's algorithm predict match scores?,2014 World Cup Forecasts and Scores,https://www.kaggle.com/fivethirtyeight/world-cup,Thu Jan 26 2017
,University of Virginia,"[REC_ID, COMPANY, DISPOSITION_TYPE, PRIMARY_CRIME_CODE, SWISS_BANK_PROGRAM, USAO, COUNTRY, FINANCIAL_INSTITUTION, CASE_NAME, CASE_ID, DOCKET_NO, DATE, JUDGMENT_DATE, PLEA_DATE, TICKER, US_PUBLIC_CO, ADDITIONAL_REGULATORY_FINE_OR_PAYMENT, COMMUNITY_SERVICE_OR_OTHER, FINE, FORFEITURE_DISGORGEMENT, PROBATION_LENGTH, RESTITUTION, TOTAL_PAYMENT, ACCEPTS_RESPONSIBILITY, AGREEMENT_REQUIRED_NEW_POSITIONS, AGREEMENT_REQUIRED_OUTSIDE_AUDITORS_OR_EXPERTS, CITATION_AND_DESCRIPTION, CIVIL_JUDGMENT_OR_SETTLEMENT, COMPLIANCE_PROGRAM_DESCRIPTION, COMPLIANCE_PROGRAM_REQUIRED_BY_AGREEMENT, COMPLIANCE_REQUIRED_BY_REGULATORS, CRIME_DESC, DESCRIPTION_OF_PAYMENTS, DOES_AGREEMENT_DISCUSS_REASONS_OR_RELEVENT_CONSIDERATIONS_FOR_LENIENCY, DOJ_CAN_UNILATERALLY_TERMINATE, FINE_CALCULATION_INCLUDED, FINE_DESCRIPTION, INDEP_MONITOR_REQUIRED, MUST_COMPORT_WITH_USSG_OR_AUDIT_COMPLIANCE, OTHER_AGREEMENT, OTHER_AGREEMENT_REQUIRED_GOVERNANCE_CHANGES, OTHER_COMPLIANCE_OFFICER_OR_CONSULTANT_REQUIRED, PARALLEL_CIVIL_SUIT, PARALLEL_REGULATORY_ACTION_OR_LOCAL_PROSECUTOR, PRE_AGREEMENT_COMPLIANCE, PRE_AGREEMENT_COMPLIANCE_DESCRIPTION, PRIVACY_WAIVER, REGULATORY_DISGORGEMENT_RESTITUTION_FORFEITURE, REGULATORY_FINE, REG_AGENCY, STATEMENT_OF_FACTS, TOTAL_REGULATORY, UNRELATED_TERMS, AGMT_YEAR, SOURCE, NOTES]","[numeric, string, string, string, string, string, string, string, string, string, numeric, dateTime, dateTime, dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, string, numeric, string, string]",Context The goal of this Corporate Prosecutions Registry is to provide comprehensive and up-to-date information on federal organizational prosecutions in the United States so that we can better understand how corporate prosecutions are brought and resolved. It includes detailed information about every federal organizational prosecution since 2001 as well as deferred and non-prosecution agreements with organizations since 1990. Dataset Description These data on deferred prosecution and non-prosecution agreements were collected by identifying agreements through news searches press releases by the Department of Justice and U.S. Attorney’s Office and also when practitioners brought agreements to our attention. The Government Accountability Office conducted a study of federal deferred prosecution and non-prosecution agreements with organizations and in August 2010 the GAO provided a list of those agreements in response to an information request. Finally searches of the Bloomberg dockets database located additional prosecution agreements with companies that had not previously been located. Jon Ashley has contacted U.S. Attorney’s Offices to request agreements. An effort by the First Amendment Clinic at the University of Virginia School of Law to litigate Freedom of Information Act requests resulted in locating a group of missing agreements which are now available on the Registry. This Registry only includes information about federal organizational prosecutions and not cases brought solely in state courts. Nor does this Registry include leniency agreements entered through the Antitrust Division’s leniency program which are kept confidential. The Registry also does not include convictions overturned on appeal or cases in which the indictment was dismissed or the company was acquitted at a trial. The U.S. Sentencing Commission reports sentencing data concerning organizational prosecutions each year. That data does not include cases resolved without a formal sentencing such as deferred and non-prosecution agreements. Acknowledgements The Corporate Prosecutions Registry is a project of the University of Virginia School of Law. It was created by Professor Brandon Garrett (bgarrett@virginia.edu) and Jon Ashley (jaa6c@virginia.edu). Please cite this dataset as  “Brandon L. Garrett and Jon Ashley Corporate Prosecutions Registry University of Virginia School of Law at http//lib.law.virginia.edu/Garrett/corporate-prosecution-registry/index.html” Inspiration  Which industries face the most prosecutions? Which government organizations have been the most successful at pursuing cases against corporations? Not a single case in the dataset led to a trial conviction. Can you link these corporate cases to criminal cases against the individuals involved? How many of them were convicted instead? ,CSV,,"[business, crime]",Other,,,72,858,0.90234375,Details of federal cases in the United States against corporations since 2001,Corporate Prosecution Registry,https://www.kaggle.com/university-of-virginia/corporate-prosecution-registry,Tue Jun 27 2017
,Kaan Ulgen,"[row ID, Tweet, Time, Retweet from, User]","[string, string, dateTime, string, string]",Context Elon Musk is an American business magnate. He was one of the founders of PayPal in the past and the founder and/or cofounder and/or CEO of SpaceX Tesla SolarCity OpenAI Neuralink and The Boring Company in the present. He is known as much for his extremely forward-thinking ideas and huge media presence as he is for his extremely business savvy. Musk is famously active on Twitter. This dataset contains all tweets made by @elonmusk his official Twitter handle between November 16 2012 and September 29 2017. Content This dataset includes the body of the tweet and the time it was made as well as who it was re-tweeted from (if it is a retweet). Inspiration  Can you figure out Elon Musk's opinions on various things by studying his Twitter statements? How Elon Musk's post rate increased decreased or stayed about the same over time?  ,CSV,,[],CC0,,,256,2981,0.431640625,Tweets by @elonmusk from 2012 to 2017,Elon Musk's Tweets,https://www.kaggle.com/kulgen/elon-musks-tweets,Thu Oct 12 2017
,Rachael Tatman,[],[],"This dataset contains a collection of interaction sequences between allies in online Diplomacy [1] games. A sequence consists of consecutive game seasons during which the two players exchange messages and help each other in the game. Half of the sequences end with betrayal while the other half are part of lasting friendships. Description Diplomacy [1] is a popular and engaging strategic board game that is often played online [2 3].  It is based heavily on communication between the players.  Due to its military domination setting Diplomacy is a well suited environment for studying naturally occurring betrayal and deception. From a collection of Diplomacy game logs we identified and extracted ongoing established and reciprocal friendships relationships that contain at least two consecutive and reciprocated acts of support that span at least three seasons in game time with no more than five seasons passing between two acts of friendship. We then identified 250 betrayals the subset of friendships described above that are followed by at least two attacks.  To match each betrayal we selected a friendship that is not followed by any offensive action but is otherwise nearly identical (in terms of length and relative time within the game). The current dataset consists of these selected betrayals and friendships only. Each relationship contains a sequence of seasons.  Within each season we provide features extracted from the messages sent by each player. Acknowledgements This dataset is distributed under the Open Data Commons Attribution (ODC-By 1.0) license.  It was collected by Vlad Niculae Srijan Kumar Jordan Boyd-Graber and Cristian Danescu-Niculescu-Mizil. If you use this dataset please cite this paper   Niculae V. Kumar S. Boyd-Graber J. & Danescu-Niculescu-Mizil C. (2015). Linguistic harbingers of betrayal A case study on an online strategy game. In Proceedings of the ACL.  Data format The dataset is a UTF-8 encoded JSON file which can be loaded into a Python kernel with the following code >>> import json >>> from io import open >>> with open(""diplomacy_data.json"" ""r"") as f ...     diplomacy = json.load(f) ...  It is structured as a list of dictionaries one for each of the 500 sequences. >>> len(diplomacy) 500  This is an example of one such entry with the fields explained >>> entry = diplomacy[0] >>> entry {     'idx' 0           # unique identifier of the dataset entry     'game' 74         # unique identifier of the game it comes from     'betrayal' True   # whether the friendship ended in betrayal     'people' u'AT'    # the countries represented by the two players                         # (in this case Austria and Turkey)     'seasons' ... }  The 'seasons' field is again a list of dictionaries one for each game season in the friendship sequence.  In the example below there are 8 seasons each identified by the game year.  Decimal notation is used to denote the season in each year.  For example 1906.0 is the spring of 1906 and 1906.5 is the fall of 1906.  Each season is also marked with what interaction the two players have at the end of the discussion  whether the players supported one another ('support') attacked one another ('attack') or did not have explicit military interactions (null). >>> seasons = entry['seasons'] >>> len(seasons) 8 >>> seasons[0] {     'season' 1906.5           # fall of the year 1906 (game time)     'interaction' {         'victim' u'support'   # the victim supported the betrayer         'betrayer' u'support'  # the betrayer supported the victim     }     'messages' {         'victim' ...         'betrayer' ...     }  } The ['messages']['victim'] and ['messages']['betrayer'] fields are lists of features of each message sent by the victim to the betrayer and by the betrayer to the victim respectively >>> msgs = seasons[0]['messages']['betrayer'] >>> len(msgs) 6 >>> msgs[0] {     ""n_words"" 146             # number of words in the message     ""n_sentences"" 9           # number of sentences in the message      ""n_requests"" 7            # number of request sentences     ""politeness"" 0.8320       # politeness of the requests (from 0 to 1)                                 # (using the Stanford Politeness                                 # Classifier available at [4])     ""sentiment"" {         ""positive"" 1          # no. sentences with positive sentiment         ""neutral"" 3           #      ""      ""      neutral sentiment         ""negative"" 5           #      ""      ""      negative sentiment     }                          # (using Stanford Sentiment Analysis [5])      ""lexicon_words"" {          # words and phrases matching several         ""disc_expansion"" [     # linguistic and psycholinguistic lexicons             ""until""            # (see below for details)             ""yet""             ""instead""         ]         ""premise"" [             ""for""             ""for""         ]         ...     }     ""frequent_words"" [         # frequent words in the message         ""more""                 # (occurring in at least 50 messages         ""let""                  # and 5 friendships overall)         ""keep""         ""...     ] }  The words in each list are in random order. The order of messages within a season is also randomized.  This measure is in place to protect the privacy of the players and of their conversations. The lexicons used to construct the ""lexicon_words"" field are  'claim' 'premise'  Argumentation structure markers [6] 'allsubj' Subjective markers [7] 'disc_*'  Discourse markers from the Penn Discourse Treebank. [8]     Includes 'disc_comparison' 'disc_expansion' 'disc_contingency'     'disc_temporal_future' and 'disc_temporal_rest' (we manually split     'temporal' from PDT into 'temporal_future' and 'temporal_rest' to     capture planning).  References [1] https//en.wikipedia.org/wiki/Diplomacy_%28game%29 [2] http//www.floc.net/dpjudge/ [3] http//usak.asciiking.com/ [4] http//politeness.mpi-sws.org/ [5] http//nlp.stanford.edu/sentiment/ [6] C. Stab and I. Gurevych. Identifying Argumentative Discourse Structures in     Persuasive Essays. In Proceedings of EMNLP 2014.     https//www.ukp.tu-darmstadt.de/data/argumentation-mining/ [7] E. Riloff and J. Wiebe. Learning extraction patterns for subjective     expressions. In Proceedings of EMNLP 2003.     http//www.anthology.aclweb.org/W/W03/W03-1014.pdf [8] https//www.seas.upenn.edu/~pdtb/",Other,,"[board games, linguistics, sociology, internet]",Other,,,115,2132,51,Can you predict a betrayal before it happens?,Diplomacy Betrayal Dataset,https://www.kaggle.com/rtatman/diplomacy-betrayal-dataset,Fri Jul 28 2017
,Iditarod Trail Committee,"[Number, Name, Status, Country, Checkpoint, Latitude, Longitude, Distance, Time, Speed, Arrival Date, Arrival Time, Arrival Dogs, Elapsed Time, Departure Date, Departure Time, Departure Dogs]","[numeric, string, string, string, string, numeric, numeric, string, numeric, string, string, string, string, numeric, dateTime, dateTime, numeric]",Context The dog sled race covers 1000 miles of the roughest most beautiful terrain Mother Nature has to offer from Anchorage in south central Alaska to Nome on the western Bering Sea coast. She throws jagged mountain ranges frozen river dense forest desolate tundra and miles of windswept coast at the mushers and their dog teams. Add to that temperatures far below zero winds that can cause a complete loss of visibility the hazards of overflow long hours of darkness and treacherous climbs and side hills and you have the Iditarod — the Last Great Race on Earth! The race route is alternated every other year one year going north through Cripple Ruby and Galena and the next year south through Iditarod Shageluk Anvik. Content This dataset provides a record for mushers and dog teams at all seventeen checkpoints on the Iditarod trail including the musher's name and bib number status in the race country of origin checkpoint name and location distance in miles from the last checkpoint time in hours from departure at the last checkpoint average speed in miles per hour date and time of arrival and departure layover time at the checkpoint and the number of dogs at arrival and departure. Acknowledgements The data on mushers checkpoints and race standings was scraped from the Iditarod website. Inspiration Which competitor had the highest average speed during the race? Did race veterans outpace rookies on the trail? How did their strategies differ? Did the competitors' speed slow with less dogs on their team?,CSV,,"[sports, animals]",Other,,,75,1384,0.1357421875,"What strategy won the 1,000 mile sled dog race across Alaska this year?",2017 Iditarod Trail Sled Dog Race,https://www.kaggle.com/iditarod/iditarod-race,Wed Mar 22 2017
,SIZZLE,"[timestamp, id, link, caption, author, network, likes]","[numeric, numeric, string, string, string, string, numeric]",We’re releasing 30000+ OCR’d political memes and their captions . With the election just days away we hope to contribute to the pre and post-election analysis of the most meme-orable election in modern history.  v2 of the dataset includes 8 csv’s Bern.csv 146 rows Bernie.csv 2100 rows Clinton.csv 4362 rows Donald.csv 6499 rows Gary_Johnston.csv 140 rows Hillary.csv 7398 rows Jill Stein.csv 96 rows Trump.csv 12139 rows with the following columns timestamp (date published) id (our unique identifier) link (post url) caption (meme caption via ocr) author network likes/upvotes Let us know any revisions you'd like to see. And of course if you find errors in the data do let us know. ,CSV,,"[popular culture, politics, internet]",CC4,,,1353,21464,26,"Analyze 45,000+ U.S. Presidential Election Memes",2016 U.S. Presidential Election Memes,https://www.kaggle.com/SIZZLE/2016electionmemes,Wed Nov 02 2016
,Pronto Cycle Share,"[station_id, name, lat, long, install_date, install_dockcount, modification_date, current_dockcount, decommission_date]","[string, string, numeric, numeric, dateTime, numeric, dateTime, numeric, string]","Context The Pronto Cycle Share system consists of 500 bikes and 54 stations located in Seattle. Pronto provides open data on individual trips stations and daily weather. Content There are 3 datasets that provide data on the stations trips and weather from 2014-2016.  Station dataset station_id station ID number name name of station lat station latitude long station longitude install_date date that station was placed in service install_dockcount number of docks at each station on the installation date modification_date date that station was modified resulting in a change in location or dock count current_dockcount number of docks at each station on 8/31/2016 decommission_date date that station was placed out of service Trip dataset trip_id numeric ID of bike trip taken starttime day and time trip started in PST stoptime day and time trip ended in PST bikeid ID attached to each bike tripduration time of trip in seconds  from_station_name name of station where trip originated to_station_name name of station where trip terminated  from_station_id ID of station where trip originated to_station_id ID of station where trip terminated usertype ""Short-Term Pass Holder"" is a rider who purchased a 24-Hour or 3-Day Pass; ""Member"" is a rider who purchased a Monthly or an Annual Membership gender gender of rider  birthyear birth year of rider Weather dataset contains daily weather information in the service area  Acknowledgements The original datasets can be downloaded here. Inspiration Some ideas worth exploring  What is the most popular bike route? How are bike uses or routes affected by user characteristics station features and weather? ",CSV,,"[cycling, road transport]",Other,,,2014,14475,46,Bicycle Trip Data from Seattle's Cycle Share System,Cycle Share Dataset,https://www.kaggle.com/pronto/cycle-share-dataset,Mon Nov 07 2016
,United States Department of Agriculture,[],[],Context This dataset is a checklist of plants known to grow in North America. The list is maintained by the United States Department of Agriculture. Content This data includes scientific names and common names of all plants in the United States. Acknowledgements This dataset is published as-is by the United States Department of Agriculture. Inspiration What words are commonly used in plant names? Can you detect any trends in say adjectives commonly used in plant names that are less commonly used in the English language?,CSV,,"[plants, agriculture]",CC0,,,57,990,6,Names of plants that grow in North America.,USDA PLANTS Checklist,https://www.kaggle.com/usdeptofag/usda-plants-checklist,Fri Sep 22 2017
,The Washington Post,"[id, name, date, manner_of_death, armed, age, gender, race, city, state, signs_of_mental_illness, threat_level, flee, body_camera]","[numeric, string, dateTime, string, string, numeric, string, string, string, string, boolean, string, string, boolean]",The Washington Post is compiling a database of every fatal shooting in the United States by a police officer in the line of duty since January 1 2015. In 2015 The Post began tracking more than a dozen details about each killing — including the race of the deceased the circumstances of the shooting whether the person was armed and whether the victim was experiencing a mental-health crisis — by culling local news reports law enforcement websites and social media and by monitoring independent databases such as Killed by Police and Fatal Encounters. The Post is documenting only those shootings in which a police officer in the line of duty shot and killed a civilian — the circumstances that most closely parallel the 2014 killing of Michael Brown in Ferguson Missouri which began the protest movement culminating in Black Lives Matter and an increased focus on police accountability nationwide. The Post is not tracking deaths of people in police custody fatal shootings by off-duty officers or non-shooting deaths. The FBI and the Centers for Disease Control and Prevention log fatal shootings by police but officials acknowledge that their data is incomplete. In 2015 The Post documented more than two times more fatal shootings by police than had been recorded by the FBI. The Post’s database is updated regularly as fatal shootings are reported and as facts emerge about individual cases. The Post is seeking assistance in making the database as comprehensive as possible. To provide information about fatal police shootings send us an email at policeshootingsfeedback@washpost.com. CREDITSResearch and Reporting Julie Tate Jennifer Jenkins and Steven RichProduction and Presentation John Muyskens Kennedy Elliott and Ted Mellnik,CSV,,"[crime, demographics]",CC4,,,1743,10042,0.1875,Civilians shot and killed by on-duty police officers in United States,"Fatal Police Shootings, 2015-Present",https://www.kaggle.com/washingtonpost/police-shootings,Sat Mar 11 2017
,GregorySmith,"[Rank, Name, Platform, Year, Genre, Publisher, NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales]","[numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric]",This dataset contains a list of video games with sales greater than 100000 copies.  It was generated by a scrape of vgchartz.com. Fields include  Rank - Ranking of overall sales Name - The games name Platform - Platform of the games release (i.e. PCPS4 etc.) Year - Year of the game's release Genre - Genre of the game Publisher - Publisher of the game NA_Sales - Sales in North America (in millions) EU_Sales - Sales in Europe (in millions) JP_Sales - Sales in Japan (in millions) Other_Sales - Sales in the rest of the world (in millions) Global_Sales - Total worldwide sales.  The script to scrape the data is available at https//github.com/GregorUT/vgchartzScrape. It is based on BeautifulSoup using Python. There are 16598 records.  2 records were dropped due to incomplete information.,CSV,,[video games],Other,,,13949,86236,1,"Analyze sales data from more than 16,500 games.",Video Game Sales,https://www.kaggle.com/gregorut/videogamesales,Wed Oct 26 2016
,Rachael Tatman,"[id, name]","[numeric, string]",Context When groups of people who don’t share a spoken language come together they will often create a new language which combines elements of their first languages. These languages are known as “pidgins”. If they are then learned by children as their first language they become fully-fledged languages known as “creoles”. This dataset contains information on both creoles and pidgins spoken around the world. Content This dataset includes information on the grammatical and lexical structures of 76 pidgin and creole languages. The language set contains not only the most widely studied Atlantic and Indian Ocean creoles but also less well known pidgins and creoles from Africa South Asia Southeast Asia Melanesia and Australia including some extinct varieties and several mixed languages. This dataset is made up of several tables each of which contains different pieces of information  language A table of language names & the unique id’s associated with them. language_data A table of data on the different languages including the name speakers’ call their language (Autoglossonym) other names the language is called how many speakers it has the language which contributed the most words to the language (Major lexifier) other languages which contribute to that language where it is spoken and where it is an official language fro. The column language_id has the id linked to the language table. language_source The sources referenced on each language (referencing the language and source tables). langauge_table Information on the geographic location of each language. source Information on the scholarly sources referenced for information on language.  Acknowledgements This dataset contains information from the online portion of the Atlas of Pidgin and Creole Language Structures (APiCS). It is distributed under a Creative Commons Attribution 3.0 Unported License . If you use this dataset in your work please use this citation Salikoko S. Mufwene. 2013. Kikongo-Kituba structure dataset.  In Michaelis Susanne Maria & Maurer Philippe & Haspelmath Martin & Huber Magnus (eds.)  Atlas of Pidgin and Creole Language Structures Online.  Leipzig Max Planck Institute for Evolutionary Anthropology.  (Available online at http//apics-online.info/contributions/58 Accessed on 2017-07-28.) Inspiration  Which areas of the world have the most creoles/pidgins? Which language has contributed to the most creoles/pidgins? Why might this be? Can you map the areas of influence of the various lexicalized Major Lexifier languages?   You may also be interested in  World Language Family Map The Sign Language Analyses (SLAY) Database World Atlas of Language Structures Information on the linguistic structures in 2679 languages ,CSV,,"[languages, linguistics]",Other,,,41,708,1,Information on 76 Creole and Pidgin Languages,Atlas of Pidgin and Creole Language Structures,https://www.kaggle.com/rtatman/atlas-of-pidgin-and-creole-language-structures,Fri Jul 28 2017
,NLTK Data,[],[],Context The punkt.zip file contains pre-trained Punkt sentence tokenizer (Kiss and Strunk 2006) models that detect sentence boundaries. These models are used by nltk.sent_tokenize to split a string into a list of sentences.  A brief tutorial on sentence and word segmentation (aka tokenization) can be found in Chapter 3.8 of the NLTK book. The punkt.zip file contents  README contains the information of how and what the models are trained on.  PY3/ contains the below pickled files as above for Python3 czech.pickle  danish.pickle  dutch.pickle english.pickle  estonian.pickle  finnish.pickle french.pickle  german.pickle  greek.pickle  italian.pickle  norwegian.pickle  polish.pickle portuguese.pickle slovene.pickle spanish.pickle swedish.pickle turkish.pickle   Citations Kiss Tibor and Strunk Jan (2006) Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics 32 485-525. ,Other,,[linguistics],Other,,,38,1246,35,Kiss and Strunk (2006) Unsupervised Multilingual Sentence Boundary Detection,Punkt Sentence Tokenizer Models,https://www.kaggle.com/nltkdata/punkt,Thu Aug 17 2017
,Facebook,"[Id, TeamId, DateSubmitted, PublicScore, PrivateScore, IsSelected]","[numeric, numeric, string, numeric, numeric, string]",This dataset contains the solution and the top 20 team's submissions for Facebook's 5th recruiting competition on predicting checkin places. Here's how an ensemble model leveraging the top results would have performed in the competition ,CSV,,"[geography, internet]",Other,,,996,10730,3072,The competition solution along with each top teams' final submission,Facebook V Results: Predicting Check Ins,https://www.kaggle.com/facebook/facebook-v-results,Wed Aug 03 2016
,Viktor Malyi,"[date, time, username, wrist, activity, acceleration_x, acceleration_y, acceleration_z, gyro_x, gyro_y, gyro_z]","[dateTime, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]","Context This dataset complements https//github.com/vmalyi/run-or-walk project which aims to detect whether the person is running or walking based on deep neural network and sensor data collected from iOS device. This dataset has been accumulated with help of ""Data Collection"" iOS app specially developed for this purpose https//github.com/vmalyi/run-or-walk/tree/master/ios_app_data_collection.  Please note that this app is not available in the AppStore yet. Content Currently the dataset contains a single file which represents 88588 sensor data samples collected from accelerometer and gyroscope from iPhone 5c in 10 seconds interval and ~5.4/second frequency. This data is represented by following columns (each column contains sensor data for one of the sensor's axes)   acceleration_x acceleration_y acceleration_z gyro_x gyro_y gyro_z  There is an activity type represented by ""activity"" column which acts as label and reflects following activities  ""0"" walking ""1"" running  Apart of that the dataset contains ""wrist"" column which represents the wrist where the device was placed to collect a sample on  ""0"" left wrist ""1"" right wrist  Additionally the dataset contains ""date"" ""time"" and ""username"" columns which provide information about the exact date time and user which collected these measurements.",CSV,,"[running, walking]",CC4,,,592,5962,7,A dataset containing labeled sensor data from accelerometer and gyroscope,Run or Walk,https://www.kaggle.com/vmalyi/run-or-walk,Wed Jul 19 2017
,Zillow,"[City Code, City, Metro, County, State, Population Rank, November 2010, December 2010, January 2011, February 2011, March 2011, April 2011, May 2011, June 2011, July 2011, August 2011, September 2011, October 2011, November 2011, December 2011, January 2012, February 2012, March 2012, April 2012, May 2012, June 2012, July 2012, August 2012, September 2012, October 2012, November 2012, December 2012, January 2013, February 2013, March 2013, April 2013, May 2013, June 2013, July 2013, August 2013, September 2013, October 2013, November 2013, December 2013, January 2014, February 2014, March 2014, April 2014, May 2014, June 2014, July 2014, August 2014, September 2014, October 2014, November 2014, December 2014, January 2015, February 2015, March 2015, April 2015, May 2015, June 2015, July 2015, August 2015, September 2015, October 2015, November 2015, December 2015, January 2016, February 2016, March 2016, April 2016, May 2016, June 2016, July 2016, August 2016, September 2016, October 2016, November 2016, December 2016, January 2017]","[numeric, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context Zillow operates an industry-leading economics and analytics bureau led by Zillow’s Chief Economist Dr. Stan Humphries. At Zillow Dr. Humphries and his team of economists and data analysts produce extensive housing data and analysis covering more than 500 markets nationwide. Zillow Research produces various real estate rental and mortgage-related metrics and publishes unique analyses on current topics and trends affecting the housing market. At Zillow’s core is our living database of more than 100 million U.S. homes featuring both public and user-generated information including number of bedrooms and bathrooms tax assessments home sales and listing data of homes for sale and for rent. This data allows us to calculate among other indicators the Zestimate a highly accurate automated estimated value of almost every home in the country as well as the Zillow Home Value Index and Zillow Rent Index leading measures of median home values and rents.  Content The Zillow Rent Index is the median estimated monthly rental price for a given area and covers multifamily single family condominium and cooperative homes in Zillow’s database regardless of whether they are currently listed for rent. It is expressed in dollars and is seasonally adjusted. The Zillow Rent Index is published at the national state metro county city neighborhood and zip code levels. Zillow produces rent estimates (Rent Zestimates) based on proprietary statistical and machine learning models. Within each county or state the models observe recent rental listings and learn the relative contribution of various home attributes in predicting prevailing rents. These home attributes include physical facts about the home prior sale transactions tax assessment information and geographic location as well as the estimated market value of the home (Zestimate). Based on the patterns learned these models estimate rental prices on all homes including those not presently for rent. Because of the availability of Zillow rental listing data used to train the models Rent Zestimates are only available back to November 2010; therefore each ZRI time series starts on the same date. Acknowledgements The rent index data was calculated from Zillow's proprietary Rent Zestimates and published on its website. Inspiration What city has the highest and lowest rental prices in the country? Which metropolitan area is the most expensive to live in? Where have rental prices increased in the past five years and where have they remained the same? What city or state has the lowest cost per square foot?,CSV,,"[cities, home, real estate]",Other,,,1798,11740,10,Which city has the highest median price or price per square foot?,"Zillow Rent Index, 2010-Present",https://www.kaggle.com/zillow/rent-index,Sat Mar 04 2017
,The Nobel Foundation,"[Year, Category, Prize, Motivation, Prize Share, Laureate ID, Laureate Type, Full Name, Birth Date, Birth City, Birth Country, Sex, Organization Name, Organization City, Organization Country, Death Date, Death City, Death Country]","[numeric, string, string, string, dateTime, numeric, string, string, dateTime, string, string, string, string, string, string, dateTime, string, string]","Context Between 1901 and 2016 the Nobel Prizes and the Prize in Economic Sciences were awarded 579 times to 911 people and organizations. The Nobel Prize is an international award administered by the Nobel Foundation in Stockholm Sweden and based on the fortune of Alfred Nobel Swedish inventor and entrepreneur. In 1968 Sveriges Riksbank established The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel founder of the Nobel Prize. Each Prize consists of a medal a personal diploma and a cash award. A person or organization awarded the Nobel Prize is called Nobel Laureate. The word ""laureate"" refers to being signified by the laurel wreath. In ancient Greece laurel wreaths were awarded to victors as a sign of honor. Content This dataset includes a record for every individual or organization that was awarded the Nobel Prize since 1901. Acknowledgements The Nobel laureate data was acquired from the Nobel Prize API. Inspiration Which country has won the most prizes in each category? What words are most frequently written in the prize motivation? Can you predict the age gender and nationality of next year's Nobel laureates?",CSV,,[],Other,,,586,5731,0.2763671875,Which country has won the most prizes in each category?,"Nobel Laureates, 1901-Present",https://www.kaggle.com/nobelfoundation/nobel-laureates,Thu Feb 16 2017
,Department of Justice,"[President, Fiscal Year, Petitions Pending, Petitions Received, Petitions Granted, Pardons, Commutations, Respites, Remissions, Petitions Denied, Petitions Closed Without Presidential Action, Petitions Denied or Closed Without Presidential Action]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string]",Context On April 23 2014 the Department of Justice at the behest of President Obama announced the Clemency Initiative inviting petitions for commutation of sentence from nonviolent offenders who among other criteria likely would have received substantially lower sentences if convicted of the same offenses today. As expected the announcement resulted in a record number of petitions – including thousands of petitions involving crimes not included in the initiative such as terrorism murder sex crimes public corruption and financial fraud. In the federal system commutation of sentence and pardon are different forms of executive clemency which is a broad term that applies to the President’s constitutional power to exercise leniency toward persons who have committed federal crimes. A commutation of sentence reduces a sentence either totally or partially that is then being served but it does not change the conviction signify innocence or remove civil disabilities from the criminal conviction. A commutation may include remission (or release) of the financial obligations that are imposed as part of a sentence such as payment of a fine or restitution; a remission applies only to the part of the financial obligation that has not already been paid. To be eligible to apply for commutation of sentence a person must have reported to prison to begin serving his sentence and may not be challenging his conviction in the courts. A pardon is an expression of the President’s forgiveness and is granted in recognition of the applicant’s acceptance of responsibility for the crime and established good conduct for a significant period of time after conviction or completion of sentence. It does not signify innocence. It does however remove civil disabilities – such as restrictions on the right to vote hold state or local office or sit on a jury – imposed because of the conviction. A person is not eligible to apply for a presidential pardon until a minimum of five years has elapsed since his release from any form of confinement imposed upon him as part of a sentence for his most recent criminal conviction. Acknowledgements The data was compiled and published by the Office of the Pardon Attorney. The Office of the Pardon Attorney receives and reviews petitions for all forms of executive clemency including pardon commutation (reduction) of sentence remission of fine or restitution and reprieve initiates the necessary investigations of clemency requests and prepares the report and recommendation of the Attorney General to the President.,CSV,,[],CC0,,,217,2698,0.0078125,Petitions for executive clemency granted and denied by American presidents,"Presidential Pardons, 1900-2017",https://www.kaggle.com/doj/presidential-pardons,Fri Jan 20 2017
,Rob Harrand,"[event-id, visible, timestamp, location-long, location-lat, comments, sensor-type, individual-taxon-canonical-name, tag-local-identifier, individual-local-identifier, study-name]","[numeric, boolean, dateTime, numeric, numeric, string, string, string, numeric, numeric, string]",Context This dataset comes from a study into the movement of bats by researchers at the University of Bristol UK. I found it whilst exploring the open datasets at the Movebank Data Repository a site dedicated to animal tracking data. Content The datasets contain information on the position and timestamps for multiple bats. The type of movement (individual or paired) is also included. See the readme.txt file for much more information. Acknowledgements I did not create this data. Full citations are below Data Holderied M Giuggioli L McKetterick TJ (2015) Data from Delayed response and biosonar perception explain movement coordination in trawling bats. Movebank Data Repository. doi10.5441/001/1.62h1f7k9 Associated paper (open access) Giuggioli L McKetterick TJ Holderied M (2015) Delayed response and biosonar perception explain movement coordination in trawling bats. PLOS Computational Biology. doi10.1371/journal.pcbi.1004089.t001,CSV,,[],CC0,,,25,531,14,Explore how bats interact during flight,Movement coordination in trawling bats,https://www.kaggle.com/tentotheminus9/movement-coordination-in-trawling-bats,Thu Aug 24 2017
,US Environmental Protection Agency,"[Vehicle ID, Year, Make, Model, Class, Drive, Transmission, Transmission Descriptor, Engine Index, Engine Descriptor, Engine Cylinders, Engine Displacement, Turbocharger, Supercharger, Fuel Type, Fuel Type 1, Fuel Type 2, City MPG (FT1), Unrounded City MPG (FT1), City MPG (FT2), Unrounded City MPG (FT2), City Gasoline Consumption (CD), City Electricity Consumption, City Utility Factor, Highway MPG (FT1), Unrounded Highway MPG (FT1), Highway MPG (FT2), Unrounded Highway MPG (FT2), Highway Gasoline Consumption (CD), Highway Electricity Consumption, Highway Utility Factor, Unadjusted City MPG (FT1), Unadjusted Highway MPG (FT1), Unadjusted City MPG (FT2), Unadjusted Highway MPG (FT2), Combined MPG (FT1), Unrounded Combined MPG (FT1), Combined MPG (FT2), Unrounded Combined MPG (FT2), Combined Electricity Consumption, Combined Gasoline Consumption (CD), Combined Utility Factor, Annual Fuel Cost (FT1), Annual Fuel Cost (FT2), Gas Guzzler Tax, Save or Spend (5 Year), Annual Consumption in Barrels (FT1), Annual Consumption in Barrels (FT2), Tailpipe CO2 (FT1), Tailpipe CO2 in Grams/Mile (FT1), Tailpipe CO2 (FT2), Tailpipe CO2 in Grams/Mile (FT2), Fuel Economy Score, GHG Score, GHG Score (Alt Fuel), My MPG Data, 2D Passenger Volume, 2D Luggage Volume, 4D Passenger Volume, 4D Luggage Volume, Hatchback Passenger Volume, Hatchback Luggage Volume, Start Stop Technology, Alternative Fuel/Technology, Electric Motor, Manufacturer Code, Gasoline/Electricity Blended (CD), Vehicle Charger, Alternate Charger, Hours to Charge (120V), Hours to Charge (240V), Hours to Charge (AC 240V), Composite City MPG, Composite Highway MPG, Composite Combined MPG, Range (FT1), City Range (FT1), Highway Range (FT1), Range (FT2), City Range (FT2), Highway Range (FT2)]","[numeric, numeric, string, string, string, string, string, string, numeric, string, numeric, numeric, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, string, string, string, string, boolean, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric]",Content The purpose of EPA’s fuel economy estimates is to provide a reliable basis for comparing vehicles. Most vehicles in the database (other than plug-in hybrids) have three fuel economy estimates a “city” estimate that represents urban driving in which a vehicle is started in the morning (after being parked all night) and driven in stop-and-go traffic; a “highway” estimate that represents a mixture of rural and interstate highway driving in a warmed-up vehicle typical of longer trips in free-flowing traffic; and a “combined” estimate that represents a combination of city driving (55%) and highway driving (45%). Estimates for all vehicles are based on laboratory testing under standardized conditions to allow for fair comparisons. The database provides annual fuel cost estimates rounded to the nearest $50 for each vehicle. The estimates are based on the assumptions that you travel 15000 miles per year (55% under city driving conditions and 45% under highway conditions) and that fuel costs $2.33/gallon for regular unleaded gasoline $2.58/gallon for mid-grade unleaded gasoline and $2.82/gallon for premium. EPA’s fuel economy values are good estimates of the fuel economy a typical driver will achieve under average driving conditions and provide a good basis to compare one vehicle to another. However your fuel economy may be slightly higher or lower than EPA’s estimates. Fuel economy varies sometimes significantly based on driving conditions driving style and other factors. Acknowledgements Fuel economy data are produced during vehicle testing at the Environmental Protection Agency's National Vehicle and Fuel Emissions Laboratory in Ann Arbor Michigan and by vehicle manufacturers with EPA oversight.,CSV,,[vehicles],CC0,,,664,4731,11,Which makes and models have the highest city and highway MPG?,"Vehicle Fuel Economy Estimates, 1984-2017",https://www.kaggle.com/epa/fuel-economy,Tue Jan 31 2017
,NASA,"[Catalog Number, Calendar Date, Eclipse Time, Delta T (s), Lunation Number, Saros Number, Eclipse Type, Quincena Solar Eclipse, Gamma, Penumbral Magnitude, Umbral Magnitude, Latitude, Longitude, Penumbral Eclipse Duration (m), Partial Eclipse Duration (m), Total Eclipse Duration (m)]","[numeric, string, dateTime, numeric, numeric, numeric, string, string, numeric, numeric, numeric, string, string, numeric, numeric, numeric]",Context Eclipses of the sun can only occur when the moon is near one of its two orbital nodes during the new moon phase. It is then possible for the Moon's penumbral umbral or antumbral shadows to sweep across Earth's surface thereby producing an eclipse. There are four types of solar eclipses a partial eclipse during which the moon's penumbral shadow traverses Earth and umbral and antumbral shadows completely miss Earth; an annular eclipse during which the moon's antumbral shadow traverses Earth but does not completely cover the sun; a total eclipse during which the moon's umbral shadow traverses Earth and completely covers the sun; and a hybrid eclipse during which the moon's umbral and antumbral shadows traverse Earth and annular and total eclipses are visible in different locations. Earth will experience 11898 solar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE). Eclipses of the moon can occur when the moon is near one of its two orbital nodes during the full moon phase. It is then possible for the moon to pass through Earth's penumbral or umbral shadows thereby producing an eclipse. There are three types of lunar eclipses a penumbral eclipse during which the moon traverses Earth's penumbral shadow but misses its umbral shadow; a partial eclipse during which the moon traverses Earth's penumbral and umbral shadows; and a total eclipse during which the moon traverses Earth's penumbral and umbral shadows and passes completely into Earth's umbra. Earth will experience 12064 lunar eclipses during the five millennium period -1999 to +3000 (2000 BCE to 3000 CE). Acknowledgements Lunar eclipse predictions were produced by Fred Espenak from NASA's Goddard Space Flight Center.,CSV,,"[astronomy, space]",CC0,,,685,5645,2,"Date, time, and location of every eclipse in five thousand years",Solar and Lunar Eclipses,https://www.kaggle.com/nasa/solar-eclipses,Thu Feb 09 2017
,Rachael Tatman,"[ID_CDI_I, ID_CDI_II, Word_NW, Word_CDI, Translation, AoA, VSoA, Lex_cat, Broad_lex, Freq, CDS_freq]","[string, string, string, string, string, numeric, numeric, string, string, numeric, numeric]",Context When children are born they don’t know any words. By the time they’re three most children know 200 words or more. These words aren’t randomly selected from the language they’re learning however. A two-year-old is much more likely to know the word “bottle” than the word “titration”. What words do children learn first and what qualities do those words have? This dataset was collected to explore this question. Content The main dataset includes information for 732 Norwegian words. A second table also includes measures of how frequently each word is used in Norwegian both on the internet (as observed in the Norwegian Web as Corpus dataset) and when an adult is talking to a child. The latter is commonly called “child directed speech” and is abbreviated as “CDS”.           Main data   ID_CDI_I Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories version 1 ID_CDI_II Word ID from the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories version 2 Word_NW The word in Norwegian Word_CDI The form of the word found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories Translation the English translation of the Norwegian word AoA how old a child generally is was when they this this word in months (Estimated from the MacArthur-Bates Communicative Development Inventories) VSoA how many other words a child generally knows when they learn this word (rounded up to the nearest 10) Lex_cat the specific part of speech of the word Broad_lex the broad part of speech of the word Freq a measure of how commonly this word occurs in Norwegian CDS_Freq a measure of how commonly this word occurs when a Norwegian adult is talking to a Norwegian child   Norwegian CDS Frequency   Word_CDI The word from as found in the Norwegian adaptation of the MacArthur-Bates Communicative Development Inventories Translation The English translation of the Norwegian word Freq_NoWaC How often this word is used on the internet Freq_CDS How often this word is used when talking to children (based on two Norwegian CHILDES corpora)  Acknowledgements This dataset was collected by Pernille Hansen. If you use this data please cite the following paper  Hansen (2016). What makes a word easy to acquire? The effects of word class frequency imageability and phonological neighbourhood density on lexical development. First Language. Advance online publication. doi 10.1177/0142 723716679956 http//dx.doi.org/10.1177/0142723716679956  Inspiration  How well can you predict which words a child will learn first? Are some sounds or letters found more often than chance in words learned early? Can you build topic models on earlier-acquired and later-acquired words? Which topics are over-represented in words learned very early? ,CSV,,"[languages, europe, linguistics]",CC0,,,200,2394,0.0751953125,How common 731 Norwegian words are & when children learn them,When do children learn words?,https://www.kaggle.com/rtatman/when-do-children-learn-words,Thu Jul 27 2017
,Myles O'Neill,"[Location_Name, Map_ID, X_coord, Y_coord, Z_coord]","[string, numeric, numeric, numeric, numeric]","Overview The World of Warcraft Avatar History Dataset is a collection of records that detail information about player characters in the game over time. It includes information about their character level race class location and social guild. The Kaggle version of this dataset includes only the information from 2008 (and the dataset in general only includes information from the 'Horde' faction of players in the game from a single game server).  Full Dataset Source and Information http//mmnet.iis.sinica.edu.tw/dl/wowah/  Code used to clean the data https//github.com/myles-oneill/WoWAH-parser   Ideas for Using the Dataset From the perspective of game system designers players' behavior is one of the most important factors they must consider when designing game systems. To gain a fundamental understanding of the game play behavior of online gamers exploring users' game play time provides a good starting point. This is because the concept of game play time is applicable to all genres of games and it enables us to model the system workload as well as the impact of system and network QoS on users' behavior. It can even help us predict players' loyalty to specific games. Open Questions  Understand user gameplay behavior (game sessions movement leveling) Understand user interactions (guilds) Predict players unsubscribing from the game based on activity What are the most popular zones in WoW what level players tend to inhabit each?  Wrath of the Lich King An expansion to World of Warcraft ""Wrath of the Lich King"" (Wotlk) was released on November 13 2008. It introduced new zones for players to go to a new character class (the death knight) and a new level cap of 80 (up from 70 previously). This event intersects nicely with the dataset and is probably interesting to investigate. Map This dataset doesn't include a shapefile (if you know of one that exists let me know!) to show where the zones the dataset talks about are. Here is a list of zones an information from this version of the game including their recommended levels http//wowwiki.wikia.com/wiki/Zones_by_level_(original) .  Update (Version 3) dmi3kno has generously put together some supplementary zone information files which have now been included in this dataset. Some notes about the files Note that some zone names contain Chinese characters. Unicode names are preserved as a key to the original dataset. What this addition will allow is to understand properties of the zones a bit better - their relative location to each other competititive properties type of gameplay and hopefully their contribution to character leveling. Location coordinates contain some redundant (and possibly duplicate) records as they are collected from different sources. Working with uncleaned location coordinate data will allow users to demonstrate their data wrangling skills (both working with strings and spatial data).",CSV,,"[games and toys, video games]",CC0,,,1634,22579,614,Track the players of this popular online game,World of Warcraft Avatar History,https://www.kaggle.com/mylesoneill/warcraft-avatar-history,Tue Jun 14 2016
,Dan Ofer,"[user_ID, venue_ID]","[numeric, numeric]",Context Dataset includes check-in tip and tag data of restaurant venues in NYC collected from Foursquare from 24 October 2011 to 20 February 2012. It contains 3112 users 3298 venues with 27149 check-ins and 10377 tips. Content  NY_Restauraunts_checkins.csv {Originally dataset_ubicomp2013_checkins.txt} has two columns.  Each line represents a check-in event.  The first column is user ID while the second column is venue ID.  NY_Restauraunts_tips.csv {Originally dataset_ubicomp2013_tips.txt} has three columns.  Each line represents a tip/comment a user left on a venue.  The first and second columns are user ID and venue ID repsectively.  The third column is tip text. NY_Restauraunts_tags.csv {Originally dataset_ubicomp2013_tags.txt} has two columns.  Each line represents the tags users added to a venue.  The first column is venue ID while the second column is tag set of the corresponding venues. Empty tag sets may exist for a venue if no user has ever added a tag to it.  Acknowledgements Columns headers details and file formats added manually. Source Scraped from Foursquare and downloaded from https//sites.google.com/site/yangdingqi/home/foursquare-dataset  Dingqi Yang Daqing Zhang Zhiyong Yu and Zhiwen Yu Fine-Grained Preference-Aware Location Search Leveraging Crowdsourced Digital Footprints from LBSNs. In Proceeding of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2013) September 8-12 2013 in Zurich Switzerland.   Dingqi Yang  Daqing Zhang Zhiyong Yu and Zhu Wang A Sentiment-enhanced Personalized Location Recommendation System. In Proceeding of the 24th ACM Conference on Hypertext and Social Media (HT 2013) 1-3 May 2013 Paris France.  Dingqi Yang Daqing Zhang Zhiyong Yu Zhiwen Yu Djamal Zeghlache.  SESAME Mining User Digital Footprints for Fine-Grained Preference-Aware Social Media Search. ACM Trans. on Internet Technology (TOIT) 14(4) 28 2014.  Original README included (note that columns were added). Inspiration Interesting questions  Linkage to additional data. Sentiment analysis.  Recommender systems prediction of checkins to related venues or tags.  Use for augmenting other datasets with geospatial or geotemporal data (for that period). ,CSV,,[],Other,,,93,1236,1,Check-ins for New York City restaurants over 4 months,FourSquare - NYC Restaurant Check-Ins,https://www.kaggle.com/danofer/foursquare-nyc-rest,Tue Sep 05 2017
,Ben Hamner,"[id, handle, text, is_retweet, original_author, time, in_reply_to_screen_name, in_reply_to_status_id, in_reply_to_user_id, is_quote_status, lang, retweet_count, favorite_count, longitude, latitude, place_id, place_full_name, place_name, place_type, place_country_code, place_country, place_contained_within, place_attributes, place_bounding_box, source_url, truncated, entities, extended_entities]","[numeric, string, string, boolean, string, dateTime, string, string, string, boolean, string, numeric, numeric, string, string, string, string, string, string, string, string, string, string, string, string, boolean, string, string]",Twitter has played an increasingly prominent role in the 2016 US Presidential Election. Debates have raged and candidates have risen and fallen based on tweets. This dataset provides ~3000 recent tweets from Hillary Clinton and Donald Trump the two major-party presidential nominees. ,CSV,,"[politics, internet]",Other,,,2925,25181,5,Tweets from the major party candidates for the 2016 US Presidential Election,Hillary Clinton and Donald Trump Tweets,https://www.kaggle.com/benhamner/clinton-trump-tweets,Wed Sep 28 2016
,Chase Bank,"[Institution Name, Main Office, Branch Name, Branch Number, Established Date, Acquired Date, Street Address, City, County, State, Zipcode, Latitude, Longitude, 2010 Deposits, 2011 Deposits, 2012 Deposits, 2013 Deposits, 2014 Deposits, 2015 Deposits, 2016 Deposits]","[string, numeric, string, numeric, dateTime, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This dataset includes a record for every branch of Chase Bank in the United States including the branch's name and number date established as a bank office and (if applicable) acquired by JP Morgan Chase physical location as street address city state zip and latitude and longitude coordinates and the amount deposited at the branch (or the institution for the bank's main office) between July 1 and June 30 2016 in US dollars. Acknowledgements The location data was scraped from the Chase Bank website. The deposit data was compiled from the Federal Deposit Insurance Corporation's annual Summary of Deposits reports. Inspiration Where did Chase Bank customers deposit the most money last year? Which bank branch has seen the most growth in deposits? How did the bank network of branch locations grow over the past century? What city has the most bank branches per capita?,CSV,,[finance],CC0,,,982,8418,0.9306640625,Where did Chase Bank customers deposit the most money last year?,"Chase Bank Branch Deposits, 2010-2016",https://www.kaggle.com/chasebank/bank-deposits,Wed Mar 15 2017
,Claudio Sanhueza,"[hero, comic]","[string, string]","The Marvel Universe Marvel Comics originally called Timely Comics Inc. has been publishing comic books for several decades. ""The Golden Age of Comics"" name that was given due to the popularity of the books during the first years was later followed by a period of decline of interest in superhero stories due to World War ref. In 1961 Marvel relaunched its superhero comic books publishing line. This new era started what has been known as the Marvel Age of Comics. Characters created during this period such as Spider-Man the Hulk the Fantastic Four and the X-Men together with those created during the Golden Age such as Captain America are known worldwide and have become cultural icons during the last decades. Later Marvel's characters popularity has been revitalized even more due to the release of several recent movies which recreate the comic books using spectacular modern special effects. Nowadays it is possible to access the content of the comic books via a digital platform created by Marvel where it is possible to subscribe monthly or yearly to get access to the comics. More information about the Marvel Universe can be found here. Content The dataset contains heroes and comics and the relationship between them. The dataset is divided into three files  nodes.csv Contains two columns (node type) indicating the name and the type (comic hero) of the nodes. edges.csv Contains two columns (hero comic) indicating in which comics the heroes appear. hero-edge.csv Contains the network of heroes which appear together in the comics. This file was originally taken from http//syntagmatic.github.io/exposedata/marvel/  Past Research (Acknowledgements) The Marvel Comics character collaboration graph was originally constructed by Cesc Rosselló Ricardo Alberich and Joe Miro from the University of the Balearic Islands. They compare the characteristics of this universe to real-world collaboration networks such as the Hollywood network or the one created by scientists who work together in producing research papers. Their original sources can be found here. With this dataset the authors published the paper titled ""Marvel Universe looks almost like a real social network"".",CSV,,[popular culture],CC4,,,939,10248,24,An artificial social network of heroes,The Marvel Universe Social Network,https://www.kaggle.com/csanhueza/the-marvel-universe-social-network,Sat Jan 28 2017
,SrihariRao,"[SCHOOL, STATE, CITY, NOC, PROGRAM, TYPE, DEPARTMENT, DELIVERY, DURATION, PREREQ, LINK, LOC_LAT, LOC_LONG, WORLD_RANK, COUNTRY, TEACHING, INTERNATIONAL, RESEARCH, CITATIONS, INCOME, TOTAL_SCORE, NUM_STUDENTS, STUDENT_STAFF_RATIO, INTERNATIONAL_STUDENTS, F_M_RATIO, YEAR, timesData]","[string, string, string, numeric, string, string, string, string, string, string, string, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric]",Contains DataScience programs offered by universities along with program details world ranking and a lot lot more. Happy exploring !!!,CSV,,"[education, information technology]",ODbL,,,1467,9647,0.333984375,Contains datascience programs offered and location data by university.,Datascience Universities across US,https://www.kaggle.com/sriharirao/datascience-universities-across-us,Sun Sep 25 2016
,Armineh Nourbakhsh,"[emergent_page, claim, claim_description, claim_label, tags, claim_source_domain, claim_course_url, date, body, page_domain, page_url, page_headline, page_position, page_shares, page_order]","[string, string, string, string, string, string, string, dateTime, string, string, string, string, string, numeric, numeric]","Context Emergent.info was a major rumor tracker created by veteran journalist Craig Silverman. It has been defunct for a while but its well-structured format and well-documented content provides an opportunity for analyzing rumors on the web.  Snopes.com is one of the oldest rumors trackers on the web. Originally launched by Barbara and David Mikkelson it is now run by a team of editors who investigate urban legends myths viral rumors and fake news. The investigators try to provide a detailed explanation for why they have chosen to confirm or debunk a rumor often citing several web pages and other external sources.  Politifact.com is a fact-checker that is focused on statements made by politicians and claims circulated by political campaigns blogs and similar websites. Politifact's labels range from ""true"" to ""pants on fire!""    Content This dataset consists of three files. One file is a collection of all webpages cited in Emergent.info and the second is a collection of webpages cited in Snopes.com and the third is a similar collection from Politifact.com. The webpages were often cited because they had started a rumor shared a rumor or debunked a rumor.  Emergent.info Emergent.info often provides a clean timeline of the rumor's propagation on the web and identifies which page was for the rumor which page was against it and which page was simply observing it. Please refer to the image below to learn more about the fields in this dataset.  Snopes.com The structure of posts on Snopes.com is not as well-defined. Please refer to the image below to learn more about the fields in the Snopes dataset.  Politifact.com Similar to Emergent.info Politifact.com follows a well-structured format in reporting and documenting rumors. There is a sidebar on the right side of each page that lists all of the sources cited within the page. The top link is the likeliest to be the original source of the rumor. For this link page_is_first_citation is set to true.    Inspiration I created this dataset in order to study domains that frequently start propagate or debunk rumors. By studying these domains and people who follow them I hope to gain some insight into the dynamics of rumor propagation on the web as well as social media.   Notes/Disclaimer When using the Snopes dataset please keep the following in mind   In addition to debunking rumors Snopes.com occasionally reports news and other types of content. This collection only includes data from ""Fact Check"" posts on Snopes. Snopes.com was launched years ago. Some of the older posts on the website do not follow the current format of the site therefore some of the fields might be missing. Snopes.com used to use a service named ""DoNotLink.com"" for citation purposes. That service is no longer active and as a result some of the links are missing from older posts on Snopes.  In addition some of the shortened links would time-out prior to resolution in which case they would not be added to the dataset.  Occasionally a website that has been cited has not maliciously started a rumor. For instance Andy Borowitz is a humorist who writes for The New Yorker. His satirical column is sometimes mistaken for real news; as a result The New Yorker may be cited as a source of fake news on Snopes.com. This does not mean that The New Yorker is a fake news website.  When using the Politifact dataset please keep the following in mind  The data included in this dataset are collected from the ""truth-o-meter"" page of Politifact.com. Politifact often fact-checks statements made by politicians. Since this dataset is focused on websites I have ignored all the posts in which the rumor was attributed to a person a political party a campaign or an organization. Instead I have only included rumors attributed explicitly to websites or blogs.    Useful Tips for Using the Snopes collection As opposed to the Emergent collection where each page is flagged with whether it was for or against a rumor no such information is available for the Snopes dataset. To avoid manually labeling the data you may use the following heuristics to identify which page started a rumor  Webpages that are cited in the ""Examples"" section of a post are often ""observing"" the rumor i.e. they have not started it but they are repeating it. In the snopes.csv file these webpages have been flagged as ""page_is_example."" Webpages that are cited in the ""Featured Image"" section of a post are often not related to the rumor. The editors on Snopes have simply extracted an image from those pages to embed in their posts. In the snopes.csv file these webpages have been flagged as ""page_is_image_credit."" Webpages that are cited through a secondary service (such as archive.is) are likelier to be rumor-propagators. Editors do not link to them directly so that a record of their page is available even if it is later deleted. If neither of these hints help very often (but not always) the first link cited on the page (for which ""page_is_example"" and ""page_is_image_credit"" are false) is the link to a page that started the rumor. This link is identified by the ""page_is_first_citation"" field. Pages for which both ""page_is_first_citation"" and ""page_is_archived"" are true are very likely to be rumor propagators.  To identify satirical websites that are mistaken for real news it's useful to inspect the way they are cited on Snopes. To demonstrate that a website contains satire or humor Snopes writers often cite the ""about us"" page of the site. Therefore it's useful to  see which domains often contain a URI to their ""about"" page (e.g. ""http//politicops.com/about-us/""). ",CSV,,"[linguistics, sociology]",CC0,,,345,4086,9,Webpages cited by rumor trackers,Who starts and who debunks rumors,https://www.kaggle.com/arminehn/rumor-citation,Mon Mar 27 2017
,"Huang, Peng-Hsuan","[, ID, content, time, type]","[numeric, string, string, string, string]","Context PPT is the biggest BBS in Taiwan it contained lots of news and discussion in different boards. PTT stock board is really popular because many of the users would give their opinions on trends of the market. Most of them tried to predict the trend of the  index(^TWII). It would be interesting to know if the users activities or texts was correlated with index price or trend. Content There are 3 csv files. ""PTT_stock_p3000_p3718.csv"" It contains about 2 years of the stock discussion topic name topic_URL author ID  number of people liked it or not(push type). ""daychat_push_60d_1006.csv"" It contains 60 days of instant intraday trading chats texts (2017/7/17~2017/10/06) ""TWII_20151001_20171006.csv"" Daily OHLC  volume up or down %changes and volatility level of ^TWII historical prices between 2015/10/01 and 2017/10/06 Acknowledgements All of the text data could be found on https//www.ptt.cc/bbs/Stock/index.html. The historical price data of Taiwan stock market index (^TWII) was from Yahoo Finance. Inspiration  There are few NLP data presented in Mandarin. It would be a new challenge to interpret texts not written in English. Since the text data were all about stock or index going up or down so it would be interesting to know which ID predicts accurate or misleading the others about the market trend. Is volatility level correlated with intraday trading chat amounts or topic amounts of the day? Can you tell which is the most popular stock they are observing and the trend is going up or down? ",CSV,,"[languages, finance, linguistics]",ODbL,,,83,1112,7,Find out the relationship of BBS user activity/texts and stock prices,Taiwan PTT stock topics and intraday trading chats,https://www.kaggle.com/randyrose2017/pttstock,Fri Oct 06 2017
,NASA,"[rowid, Host Name, Letter, Default Parameter Flag, Reference, Discovery Method, Number of Planets in System, Orbital Period (days), Orbital Period Upper Unc. (days), Orbital Period Lower Unc. (days), Orbital Period Limit Flag, Orbit Semi-Major Axis (AU), Orbit Semi-Major Axis Upper Unc (AU), Orbit Semi-Major Axis Lower Unc (AU), Orbit Semi-Major Axis Limit Flag, Eccentricity, Eccentricity Upper Unc., Eccentricity Lower Unc., Eccentricity Limit Flag, Inclination (deg), Inclination Upper Unc. (deg), Inclination Lower Unc. (deg), Inclination Limit Flag, Planet Mass (Jupiter mass), Planet Mass or M*sin(i)Upper Unc. (Jupiter mass), Planet Mass or M*sin(i)Lower Unc. (Jupiter mass), Planet Mass or M*sin(i) Limit Flag, Planet Radius (Jupiter radii), Planet Radius Upper Unc. (Jupiter radii), Planet Radius Lower Unc. (Jupiter radii), Planet Radius Limit Flag, Planet Density (g/cm**3), Planet Density Upper Unc. (g/cm**3), Planet Density Lower Unc. (g/cm**3), Planet Density Limit Flag, ra_str, RA [sexagesimal], dec_str, Dec [sexagesimal], Effective Temperature (K), Effective Temperature Upper Unc. (K), Effective Temperature Lower Unc. (K), Effective Temperature Limit Flag, Effective Temperature Blend Flag, Stellar Mass (Solar mass), Stellar Mass Upper Unc. (Solar mass), Stellar Mass Lower Unc. (Solar mass), Stellar Mass Limit Flag, Stellar Mass Blend Flag, Stellar Radius (Solar radii), Stellar Radius Upper Unc. (Solar radii), Stellar Radius Lower Unc. (Solar radii), Stellar Radius Limit Flag, Stellar Radius Blend Flag, Date of Last Update]","[numeric, string, string, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, dateTime]",Context On February 22 2017 NASA announces the discovery of the Trappist-1 Solar System which contains 7 earth sized exoplanets orbiting close to a dim star.  The planets' orbits are situated in the  'Goldilocks zone' making them prime candidates for extraterrestrial life. Content The data published here was pulled from NASA and CalTech's Exoplanet Archive http//exoplanetarchive.ipac.caltech.edu/index.html Column contents are explained further http//exoplanetarchive.ipac.caltech.edu/docs/API_exoplanet_columns.html Acknowledgements Use of the NASA Exoplanet Archive which is operated by the California Institute of Technology under contract with the National Aeronautics and Space Administration under the Exoplanet Exploration Program Inspiration A playground to work with data from an exciting discovery.  A lot of questions remained to be answered pending future studies.,CSV,,[space],CC0,,,240,3395,0.00390625,Data from the recently announced 7 exoplanet system,Trappist-1 Solar System,https://www.kaggle.com/nasa/trappist1,Thu Feb 23 2017
,Blitzer,"[City, Country]","[string, string]",Context Movehub city ranking as published on http//www.movehub.com/city-rankings Content movehubqualityoflife.csv Cities ranked by Movehub Rating A combination of all scores for an overall rating for a city or country. Purchase Power This compares the average cost of living with the average local wage. Health Care Compiled from how citizens feel about their access to healthcare and its quality. Pollution Low is good. A score of how polluted people find a city includes air water and noise pollution. Quality of Life A balance of healthcare pollution purchase power crime rate to give an overall quality of life score. Crime Rating Low is good. The lower the score the safer people feel in this city. movehubcostofliving.csv Unit GBP City Cappuccino Cinema Wine Gasoline Avg Rent Avg Disposable Income cities.csv Cities to countries as parsed from Wikipedia https//en.wikipedia.org/wiki/List_of_towns_and_cities_with_100000_or_more_inhabitants/cityname_A (A-Z) Acknowledgements Movehub http//www.movehub.com/city-rankings Wikipedia https//en.wikipedia.org/wiki/List_of_towns_and_cities_with_100000_or_more_inhabitants/cityname_A,CSV,,[cities],Other,,,1569,5314,0.095703125,Compare key metrics for over 200 cities,Movehub City Rankings,https://www.kaggle.com/blitzr/movehub-city-rankings,Fri Mar 24 2017
,Eric Grinstein,"[, score_phrase, title, url, platform, score, genre, editors_choice, release_year, release_month, release_day]","[numeric, string, string, string, string, numeric, string, string, numeric, numeric, numeric]",This dataset is the result of a crawl I did on http//ign.com/games/reviews . It contains 18625 lines with the fields like the release date it's platform and IGN's score. All the lines are fully filled. In 20 years the gaming industry has grown and sophisticated. By exploring this dataset one is able to find trends about the industry compare consoles against eachother search through the most popular genres and more. The dataset can also be a great place for beginners to start using Python modules such as Pandas and Seaborn.  You can find the crawl I used for the retrieval here,CSV,,"[games and toys, video games]",Other,,,7838,50420,2,18000+ rows of review data from ign.com,20 Years of Games,https://www.kaggle.com/egrinstein/20-years-of-games,Wed Sep 28 2016
,BillurEngin,"[Call Type, Call Final Disposition, Unit Type, Received DtTm, Response DtTm, On Scene DtTm, Call Type Group, Neighborhood  District, Location, elevation]","[string, string, string, dateTime, dateTime, dateTime, string, string, string, numeric]","Context This dataset is generated via merging ""San Francisco Fire Department Calls"" and ""San Francisco Elevation Data"". Fire Calls-For-Service includes all fire units responses to calls. Each record includes the call number neighborhood location unit type call type and all relevant time intervals are also included.  Content  Call Type  Type of call the incident falls into. Call Final Disposition Disposition of the call (Code). For example TH2 Transport to Hospital - Code 2 FIR Resolved by Fire Department Unit Type Unit type Received DtTm Date and time of call is received at the 911 Dispatch Center. Response DtTm Date and time this unit acknowledges the dispatch and records that the unit is en route to the location of the call. On Scene DtTm Date and time the unit records arriving to the location of the incident Call Type Group Call types are divided into four main groups Fire Alarm Potential Life Threatening and Non Life Threatening. Neighborhood  District Neighborhood District associated with this address boundaries available here Location Latitude and longitude of address obfuscated either to the midblock intersection or call box ElevationElevation in meters  Acknowledgements San Francisco Fire Department calls are downloaded from SFOpen webpage. San Francisco DEM (Digital elevation models) file is obtained from National Centers for Environmental Information web page* *Carignan K.S. L.A. Taylor B.W. Eakins R.J. Caldwell D.Z. Friday P.R. Grothe and E. Lim 2011. Digital Elevation Models of Central California and San Francisco Bay Procedures Data Sources and Analysis NOAA Technical Memorandum NESDIS NGDC-52 U.S. Dept. of Commerce Boulder CO 49 pp. Inspiration  Do fire fighters only fight fire?  There is a wide range of calls directed to FD what is the leading cause? How often do firefighters actually fight a fire on a given day/week? How fast do they respond to calls? Does the elevation lag the response?  Are there special times/months where they receive more or less calls?   Is there a relationship between the elevation and the rate or type of calls? ",CSV,,[firefighting],Other,,,110,2184,685,Do the fires climb? Do fire fighters only fight fire?,Elevation Data meets SF Fire Department Calls,https://www.kaggle.com/bengin/SanFranciscoFireDepartmentCalls,Mon Jan 09 2017
,United Nations,"[assembly_session, vote_id, resolution, amendment, vote_date, significant_vote, yes_votes, no_votes, abstain, colonization, human_rights, israel_palestine, disarmament, nuclear_weapons, economic_development]","[numeric, numeric, string, numeric, dateTime, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content This dataset documents all United Nations General Assembly votes since its establishment in 1946. The data is broken into three different files the first lists each UN resolution subject and vote records; the second records individual member state votes per resolution; and the third provides an annual summary of member state voting records with affinity scores and an ideal point estimate in relation to the United States. Acknowledgements The UN General Assembly voting data was compiled and published by Professor Erik Voeten of Georgetown University.,CSV,,"[politics, international relations]",CC0,,,373,4820,36,Votes by member states on individual resolutions and specific issues,"UN General Assembly Votes, 1946-2015",https://www.kaggle.com/unitednations/general-assembly,Fri Feb 17 2017
,Centers for Disease Control and Prevention,"[StateAbbr, PlaceName, PlaceFIPS, Population2010, ACCESS2_CrudePrev, ACCESS2_Crude95CI, ACCESS2_AdjPrev, ACCESS2_Adj95CI, ARTHRITIS_CrudePrev, ARTHRITIS_Crude95CI, ARTHRITIS_AdjPrev, ARTHRITIS_Adj95CI, BINGE_CrudePrev, BINGE_Crude95CI, BINGE_AdjPrev, BINGE_Adj95CI, BPHIGH_CrudePrev, BPHIGH_Crude95CI, BPHIGH_AdjPrev, BPHIGH_Adj95CI, BPMED_CrudePrev, BPMED_Crude95CI, BPMED_AdjPrev, BPMED_Adj95CI, CANCER_CrudePrev, CANCER_Crude95CI, CANCER_AdjPrev, CANCER_Adj95CI, CASTHMA_CrudePrev, CASTHMA_Crude95CI, CASTHMA_AdjPrev, CASTHMA_Adj95CI, CHD_CrudePrev, CHD_Crude95CI, CHD_AdjPrev, CHD_Adj95CI, CHECKUP_CrudePrev, CHECKUP_Crude95CI, CHECKUP_AdjPrev, CHECKUP_Adj95CI, CHOLSCREEN_CrudePrev, CHOLSCREEN_Crude95CI, CHOLSCREEN_AdjPrev, CHOLSCREEN_Adj95CI, COLON_SCREEN_CrudePrev, COLON_SCREEN_Crude95CI, COLON_SCREEN_AdjPrev, COLON_SCREEN_Adj95CI, COPD_CrudePrev, COPD_Crude95CI, COPD_AdjPrev, COPD_Adj95CI, COREM_CrudePrev, COREM_Crude95CI, COREM_AdjPrev, COREM_Adj95CI, COREW_CrudePrev, COREW_Crude95CI, COREW_AdjPrev, COREW_Adj95CI, CSMOKING_CrudePrev, CSMOKING_Crude95CI, CSMOKING_AdjPrev, CSMOKING_Adj95CI, DENTAL_CrudePrev, DENTAL_Crude95CI, DENTAL_AdjPrev, DENTAL_Adj95CI, DIABETES_CrudePrev, DIABETES_Crude95CI, DIABETES_AdjPrev, DIABETES_Adj95CI, HIGHCHOL_CrudePrev, HIGHCHOL_Crude95CI, HIGHCHOL_AdjPrev, HIGHCHOL_Adj95CI, KIDNEY_CrudePrev, KIDNEY_Crude95CI, KIDNEY_AdjPrev, KIDNEY_Adj95CI, LPA_CrudePrev, LPA_Crude95CI, LPA_AdjPrev, LPA_Adj95CI, MAMMOUSE_CrudePrev, MAMMOUSE_Crude95CI, MAMMOUSE_AdjPrev, MAMMOUSE_Adj95CI, MHLTH_CrudePrev, MHLTH_Crude95CI, MHLTH_AdjPrev, MHLTH_Adj95CI, OBESITY_CrudePrev, OBESITY_Crude95CI, OBESITY_AdjPrev, OBESITY_Adj95CI, PAPTEST_CrudePrev, PAPTEST_Crude95CI, PAPTEST_AdjPrev, PAPTEST_Adj95CI]","[string, string, numeric, numeric, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string, numeric, string]",Context Public health is a large and expensive problem for policymakers to understand in order to provide health services and prevent future epidemics. Self-reported data can be tricky due to many sampling issues but it can paint an interesting picture of how healthy a given area’s population might be. Content Data includes small area samples of residents from 500 US cities. Recorded is the percent of residents who answered a public health-related question affirmatively (see here). In addition to crude data additional data is provided with age adjustment applied. 95% Confidence Intervals also provided for both datapoints. Acknowledgements This data was collected by Centers for Disease Control and Prevention National Center for Chronic Disease Prevention and Health Promotion Division of Population Health. 500 Cities Project Data [online]. 2016 [accessed Aug 10 2017]. URL https//www.cdc.gov/500cities. Inspiration  Are there any regional health trends? Any unusual hotspots of declining health? Higher levels of wellness? Can you split the data by geography and predict neighboring cities health? Who's healthier larger or smaller cities? ,CSV,,[],ODbL,,,149,1612,0.55859375,Dozens of Public Health Datapoints Reported by Residents of 500 US Cities,CDC 500 Cities,https://www.kaggle.com/cdc/500-cities,Thu Aug 10 2017
,Rachael Tatman,"[word, workerID, hitID, status, score, senseID, remarks]","[string, string, string, string, numeric, numeric, string]",Context “Compositionality” is a concept from linguistics where the meaning of a phrase is made up of the meaning of each of its individual words. So “the red apple” refers literally to an apple that is red. Sometimes when you combine words however the meaning of the phrase isn’t the same as the combined meanings of the individual words. “The Big Apple” for example means “New York City” not a literal large apple. This dataset contains human judgements of the compositionality of common English phrases with two nouns. Content This dataset contains two files summary data of the human judgements and the annotations by individual judges. All judgements are on a scale of 0 to 5 with 0 being “not literal” and 5 being “literal”. Acknowledgements This dataset was collected by Siva Reddy Diana McCarthy and Suresh Manandhar. If you use this dataset in your work please cite the following paper Reddy S. McCarthy D. & Manandhar S. (2011 November). An Empirical Study on Compositionality in Compound Nouns. In IJCNLP (pp. 210-218). Inspiration  Is there a relationship between how frequent a word is and how often it’s used literally?  (You can estimate word frequency using the corpora included in the Natural Language Toolkit which is already ready to run in any Python kernel!) Given this dataset can you predict whether other compound nouns will be literal or not? Which phrases are the most literal? Which are the least literal? Are there any patterns you notice? ,CSV,,"[languages, linguistics]",CC4,,,51,1010,0.47265625,Is a flea market a market for fleas?,Noun Compositionality Judgements,https://www.kaggle.com/rtatman/noun-compositionality-judgements,Wed Jul 26 2017
,Debanjan,"[id, vendor_id, pickup_datetime, dropoff_datetime, passenger_count, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, store_and_fwd_flag, gc_distance, trip_duration, google_distance, google_duration]","[string, numeric, dateTime, dateTime, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric]","Context The idea is to measure recommended route distance and duration(average based on historical data) between two co-ordinates using Google's Distance Matrix API. Content Google's distance and duration data appended (as fetched from the API based on the co-ordinate given) to the Kaggle's dataset for ""New York City Taxi Trip Duration"" challenge. Additionally great-circle distance between two co-ordinates are also given. Acknowledgements The data was retrieved on 29th of July 2017 from the Google's Distance Matrix API based on Kaggle's dataset given for ""New York City Taxi Trip Duration"" challenge. Great Circle distance calculated between two co-ordinates using ""geopy"".",CSV,,[],CC0,,,151,1775,5,"Google's Distance Matrix API for ""New York City Taxi Trip Duration"" challenge",New York City Taxi Trip - Distance Matrix,https://www.kaggle.com/debanjanpaul/new-york-city-taxi-trip-distance-matrix,Thu Aug 03 2017
,FiveThirtyEight,"[person, dept, eow, cause, cause_short, date, year, canine, dept_name, state]","[string, string, dateTime, string, string, dateTime, numeric, string, string, string]",Context This dataset contains data behind the story The Dallas Shooting Was Among The Deadliest For Police In U.S. History. The data are scraped from ODMP and capture information on all tracked on-duty police officer deaths in the U.S. broken down by cause from 1971 until 2016. Content This dataset tags every entry as human or canine. There are 10 variables  person dept Department eow End of watch cause Cause of death cause_short Shortened cause of death date Cleaned EOW year Year from EOW canine dept_name state  Inspiration Using the data can you determine the temporal trend of police officer deaths by cause? By state? By department? Acknowledgements The primary source of data is the Officer Down Memorial Page (ODMP) started in 1996 by a college student who is now a police officer and who continues to maintain the database. The original data and code can be found on the FiveThirtyEight GitHub.,CSV,,"[death, crime]",Other,,,998,6720,4,On-Duty Police Officer Deaths from 1971-2016,Police Officer Deaths in the U.S.,https://www.kaggle.com/fivethirtyeight/police-officer-deaths-in-the-us,Sat Nov 05 2016
,NYC Open Data,"[School ID, School Name, Borough, Building Code, Street Address, City, State, Zip Code, Latitude, Longitude, Phone Number, Start Time, End Time, Student Enrollment, Percent White, Percent Black, Percent Hispanic, Percent Asian, Average Score (SAT Math), Average Score (SAT Reading), Average Score (SAT Writing), Percent Tested]","[string, string, string, string, string, string, string, numeric, numeric, numeric, string, dateTime, dateTime, numeric, string, string, string, string, numeric, numeric, numeric, string]",Content This dataset consists of a row for every accredited high school in New York City with its department ID number school name borough building code street address latitude/longitude coordinates phone number start and end times student enrollment with race breakdown and average scores on each SAT test section for the 2014-2015 school year. Acknowledgements The high school data was compiled and published by the New York City Department of Education and the SAT score averages and testing rates were provided by the College Board. Inspiration Which public high school's students received the highest overall SAT score? Highest score for each section? Which borough has the highest performing schools? Do schools with lower student enrollment perform better?,CSV,,[education],CC0,,,666,4741,0.0771484375,"Name, location, enrollment, and scores for 2014-2015 school year",Average SAT Scores for NYC Public Schools,https://www.kaggle.com/nycopendata/high-schools,Tue Mar 07 2017
,Rachael Tatman,[],[],Context SentimentWortschatz or SentiWS for short is a publicly available German-language resource for sentiment analysis opinion mining etc. It lists positive and negative polarity bearing words weighted within the interval of [-1; 1] plus their part of speech tag and if applicable their inflections. The current version of SentiWS (v1.8b) contains 1650 positive and 1818 negative words which sum up to 15649 positive and 15632 negative word forms incl. their inflections respectively. It not only contains adjectives and adverbs explicitly expressing a sentiment but also nouns and verbs implicitly containing one. Format SentiWS is organised in two utf8-encoded text files structured the following way | \t  \t ... \n where \t denotes a tab and \n denotes a new line. Acknowledgement SentiWS is licensed under a Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported License (http//creativecommons.org/licenses/by-nc-sa/3.0/). If you use SentiWS in your work please cite the following paper R. Remus U. Quasthoff & G. Heyer SentiWS - a Publicly Available German-language Resource for Sentiment Analysis. In Proceedings of the 7th International Language Ressources and Evaluation (LREC'10) 2010 This version of the data set was last updated in March 2012.,Other,,"[languages, europe, linguistics]",Other,,,113,1559,0.4208984375,3468 German words sorted by sentiment,German Sentiment Analysis Toolkit,https://www.kaggle.com/rtatman/german-sentiment-analysis-toolkit,Wed Aug 16 2017
,Jiri Roznovjak,"[ID, Question, Answer]","[numeric, string, string]","This dataset contains 38269 jokes of the question-answer form obtained from the r/Jokes subreddit. The dataset contains a csv file where a row contains a question  (""Why did the chicken cross the road"") the corresponding answer (""To get to the other side"") and a unique ID. The data comes from the end of 2016 all the way to 2008. The entries with a higher ID correspond to the ones submitted earlier. An example of what one might do with the data is build a sequence-to-sequence model where the input is a question and the output is an answer. Then given a question the model should generate a funny answer. This is what I did as the final project for my fall 2016 machine learning class. The project page can be viewed here. Disclaimer The dataset contains jokes that some may find inappropriate. License Released under reddit's API terms",CSV,,"[humor, linguistics]",Other,,,1066,8898,3,Jokes of the question-answer form from Reddit's r/jokes,Question-Answer Jokes,https://www.kaggle.com/jiriroz/qa-jokes,Fri Jan 06 2017
,Arijit Mukherjee,"[name_of_city, state_code, state_name, dist_code, population_total, population_male, population_female, 0-6_population_total, 0-6_population_male, 0-6_population_female, literates_total, literates_male, literates_female, sex_ratio, child_sex_ratio, effective_literacy_rate_total, effective_literacy_rate_male, effective_literacy_rate_female, location, total_graduates, male_graduates, female_graduates]","[string, numeric, string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric]",Context I created this data set merging  the census 2011 of Indian Cities with Population more than 1 Lac and City wise number of Graduates from the Census 2011 to create a visualization of where the future cities of India stands today I will try to add more columns [ fertility rate religion distribution health standards number of schools Mortality rate ] in the future hope people will contribute. Content   Data of 500 Cities with population more than 1 Lac by Census 2011 'name_of_city'                   Name of the City  'state_code'                     State Code of the City 'state_name'                     State Name of the City 'dist_code'                      District Code where the city belongs ( 99 means multiple district )  'population_total'               Total Population 'population_male'                Male Population  'population_female'              Female Population '0-6_population_total'           0-6 Age Total Population '0-6_population_male'            0-6 Age Male Population '0-6_population_female'          0-6 Age Female Population 'literates_total'                Total Literates 'literates_male'                 Male Literates 'literates_female'               Female Literates  'sex_ratio'                      Sex Ratio  'child_sex_ratio'                Sex ratio in 0-6 'effective_literacy_rate_total'  Literacy rate over Age 7  'effective_literacy_rate_male'   Male Literacy rate over Age 7  'effective_literacy_rate_female' Female Literacy rate over Age 7  'location'                       LatLng 'total_graduates'                Total Number of Graduates 'male_graduates'                 Male Graduates  'female_graduates'               Female Graduates  Acknowledgements  Census 2011  http//censusindia.gov.in/2011-prov-results/paper2/data_files/India2/Table_2_PR_Cities_1Lakh_and_Above.xls  Google Geocoder for Location Fetching. Graduation Data Census 2011   http//www.censusindia.gov.in/2011census/C-series/DDWCT-0000C-08.xlsx Inspiration What story do the top 500 cities of India tell to the world? I wrote a post in my blog about the dataset . ,CSV,,"[cities, sociology]",CC0,,,2826,24003,0.0712890625,What story do the top 500 cities of India tell to the world?,Top 500 Indian Cities,https://www.kaggle.com/zed9941/top-500-indian-cities,Wed Dec 21 2016
,shan,"[ID, QUERY, TWEET_ID, INSERTED DATE, TRUNCATED, LANGUAGE, possibly_sensitive, coordinates, retweeted_status, created_at_text, created_at, CONTENT, from_user_screen_name, from_user_id, from_user_followers_count, from_user_friends_count, from_user_listed_count, from_user_statuses_count, from_user_description, from_user_location, from_user_created_at, retweet_count, entities_urls, entities_urls_counts, entities_hashtags, entities_hashtags_counts, entities_mentions, entities_mentions_counts, in_reply_to_screen_name, in_reply_to_status_id, source, entities_expanded_urls, json_output, entities_media_count, media_expanded_url, media_url, media_type, video_link, photo_link, twitpic]","[numeric, string, numeric, string, numeric, string, numeric, string, string, string, string, string, string, numeric, numeric, numeric, numeric, numeric, string, string, string, numeric, string, numeric, string, numeric, string, numeric, string, string, string, string, string, numeric, string, string, string, numeric, numeric, numeric]",Withdrawal of a particular form of currency (such a gold coins currency notes) from circulation is known as demonetization . Context  On November 8th India’s Prime Minister  announced that 86% of the country’s currency would be rendered null and void in 50 days and it will withdraw all 500 and 1000 rupee notes — the country’s most popular currency denominations from circulation while a new 2000 rupee note added in.  It was posited as a move to crackdown on corruption and the country’s booming under-regulated and virtually untaxed grassroots economy. Content  The field names are following ID  QUERY  TWEET_ID  INSERTED DATE  TRUNCATED  LANGUAGE  possibly_sensitive  coordinates  retweeted_status  created_at_text  created_at  CONTENT  from_user_screen_name  from_user_id    from_user_followers_count  from_user_friends_count  from_user_listed_count  from_user_statuses_count  from_user_description  from_user_location  from_user_created_at  retweet_count  entities_urls  entities_urls_counts  entities_hashtags  entities_hashtags_counts  entities_mentions  entities_mentions_counts  in_reply_to_screen_name  in_reply_to_status_id  source  entities_expanded_urls  json_output  entities_media_count  media_expanded_url  media_url  media_type  video_link  photo_link  twitpic                                             Acknowledgements  Dataset is created by pulling tweets by hashtag from twitter. Inspiration  Dataset can be used to understand trending tweets. Dataset can be used for sentiment analysis and topic mining. Dataset can be used for time series analysis of tweets. What questions would you like answered by the community ?  What is the general sentiment of tweets ?  Conclusion regarding tweet sentiments varying over time. What feedback would be helpful on the data itself ? An in depth analysis of data.,CSV,,"[money, economics]",CC0,,,1443,12415,39,Withdrawal of  500 and 1000 bills in India,Demonetization in India,https://www.kaggle.com/shan4224/demonetization-in-india,Tue Jan 17 2017
,colemaclean,"[username, subreddit, utc]","[string, string, numeric]",Context The dataset is a csv file compiled using a python scrapper developed using Reddit's PRAW API. The raw data is a list of 3-tuples of [usernamesubredditutc timestamp]. Each row represents a single comment made by the user representing about 5 days worth of Reddit data. Note that the actual comment text is not included only the user subreddit and comment timestamp of the users comment. The goal of the dataset is to provide a lens in discovering user patterns from reddit meta-data alone. The original use case was to compile a dataset suitable for training a neural network in developing a subreddit recommender system. That final system can be found here A very unpolished EDA for the dataset can be found here. Note the published dataset is only half of the one used in the EDA and recommender system to meet kaggle's 500MB size limitation. Content user - The username of the person submitting the comment  subreddit - The title of the subreddit the user made the comment in  utc_stamp - the utc timestamp of when the user made the comment   Acknowledgements The dataset was compiled as part of a school project. The final project report with my collaborators can be found here Inspiration We were able to build a pretty cool subreddit recommender with the dataset. A blog post for it can be found here and the stand alone jupyter notebook for it here. Our final model is very undertuned so there's definitely improvements to be made there but I think there are many other cool data projects and visualizations that could be built from this dataset. One example would be to analyze the spread of users through the Reddit ecosystem whether the average user clusters in close communities or traverses wide and far to different corners. If you do end up building something on this please share! And have fun! Released under Reddit's API licence,CSV,,"[sociology, internet]",Other,,,181,2848,484,Modeling Reddit users from their metadata,"Subreddit Interactions for 25,000 Users",https://www.kaggle.com/colemaclean/subreddit-interactions,Sun Feb 19 2017
,NASA,"[Object Name, Period Start, Period End, Possible Impacts, Cumulative Impact Probability, Asteroid Velocity, Asteroid Magnitude, Asteroid Diameter (km), Cumulative Palermo Scale, Maximum Palermo Scale, Maximum Torino Scale]","[string, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Context An asteroid's orbit is computed by finding the elliptical path about the sun that best fits the available observations of the object. That is the object's computed path about the sun is adjusted until the predictions of where the asteroid should have appeared in the sky at several observed times match the positions where the object was actually observed to be at those same times. As more and more observations are used to further improve an object's orbit we become more and more confident in our knowledge of where the object will be in the future. When the discovery of a new near Earth asteroid is announced by the Minor Planet Center Sentry automatically prioritizes the object for an impact risk analysis. If the prioritization analysis indicates that the asteroid cannot pass near the Earth or that its orbit is very well determined the computationally intensive nonlinear search for potential impacts is not pursued. If on the other hand a search is deemed necessary then the object is added to a queue of objects awaiting analysis. Its position in the queue is determined by the estimated likelihood that potential impacts may be found. Content Sentry is a highly automated collision monitoring system that continually scans the most current asteroid catalog for possibilities of future impact with Earth over the next 100 years. This dataset includes the Sentry system's list of possible asteroid impacts with Earth and their probability in addition to a list of all known near Earth asteroids and their characteristics. Acknowledgements The asteroid orbit and impact risk data was collected by NASA's Near Earth Object Program at the Jet Propulsion Laboratory (California Institute of Technology). Inspiration During which year is Earth at the highest risk of an asteroid impact? How do asteroid impact predictions change over time? Which possible asteroid impact would be the most devastating given the asteroid's size and speed?,CSV,,"[astronomy, space]",CC0,,,788,7149,2,"Name, orbit, and year range for impacts predicted by Sentry system",Possible Asteroid Impacts with Earth,https://www.kaggle.com/nasa/asteroid-impacts,Sat Feb 11 2017
,United States Department of Agriculture,"[Annotate Code, Annotated Information]","[string, string]",Context This dataset contains information on pesticide residues in food. The U.S. Department of Agriculture (USDA) Agricultural Marketing Service (AMS) conducts the Pesticide Data Program (PDP) every year to help assure consumers that the food they feed themselves and their families is safe. Ultimately if EPA determines a pesticide is not safe for human consumption it is removed from the market. The PDP tests a wide variety of domestic and imported foods with a strong focus on foods that are consumed by infants and children. EPA relies on PDP data to conduct dietary risk assessments and to ensure that any pesticide residues in foods remain at safe levels. USDA uses the data to better understand the relationship of pesticide residues to agricultural practices and to enhance USDA’s Integrated Pest Management objectives. USDA also works with U.S. growers to improve agricultural practices. Content While the original 2015 MS Access database can be found [here (https//www.ams.usda.gov/datasets/pdp/pdpdata) the data has been transferred to a SQLite database for easier more open use. The database contains two tables Sample Data and Results Data. Each sampling includes attributes such as extraction method the laboratory responsible for the test and EPA tolerances among others. These attributes are labeled with codes which can be referenced in PDF format here or integrated into the database using the included csv files.  Inspiration  What are the most common types of pesticides tested in this study? Do certain states tend to use one particular pesticide type over another? Does pesticide type correspond more with crop type or location (state)? Are any produce types found to have higher pesticide levels than assumed safe by EPA standards? By combining databases from several years of PDP tests can you see any trends in pesticide use?  Acknowledgement This dataset is part of the USDA PDP yearly database and the original source can be found here.,CSV,,"[food and drink, agriculture]",Other,,,622,4840,123,Study of pesticide residues in food,Pesticide Data Program (2015),https://www.kaggle.com/usdeptofag/pesticide-data-program-2015,Wed Nov 16 2016
,Julian Simon de Castro,"[CET, Max TemperatureC, Mean TemperatureC, Min TemperatureC, Dew PointC, MeanDew PointC, Min DewpointC, Max Humidity,  Mean Humidity,  Min Humidity,  Max Sea Level PressurehPa,  Mean Sea Level PressurehPa,  Min Sea Level PressurehPa,  Max VisibilityKm,  Mean VisibilityKm,  Min VisibilitykM,  Max Wind SpeedKm/h,  Mean Wind SpeedKm/h,  Max Gust SpeedKm/h, Precipitationmm,  CloudCover,  Events, WindDirDegrees]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, string, numeric]","Weather data Barajas Airport Madrid between 1997 and 2015. Gathered web https//www.wunderground.com/ The Weather Company LLC Fields  Max TemperatureC Mean TemperatureC Min TemperatureC Dew PointC MeanDew PointC Min DewpointC Max Humidity Mean Humidity Min Humidity Max Sea Level PressurehPa Mean Sea Level PressurehPa Min Sea Level PressurehPa Max VisibilityKm Mean VisibilityKm Min VisibilitykM Max Wind SpeedKm/h Mean Wind SpeedKm/h Max Gust SpeedKm/h Precipitationmm CloudCover Events WindDirDegrees  Script for download the data #!/bin/bash  LOCAL='weather_madrid' STATION='LEMD' INI=1997 END=2015 FILE=${LOCAL}_${STATION}_${INI}_${END}.csv SITE='airport'  echo ""CETMax TemperatureCMean TemperatureCMin TemperatureCDew PointCMeanDew PointCMin DewpointCMax Humidity Mean Humidity Min Humidity Max Sea Level PressurehPa Mean Sea Level PressurehPa Min Sea Level PressurehPa Max VisibilityKm Mean VisibilityKm Min VisibilitykM Max Wind SpeedKm/h Mean Wind SpeedKm/h Max Gust SpeedKm/hPrecipitationmm CloudCover EventsWindDirDegrees"" > ${FILE}  for YEAR in $(seq ${INI} ${END}) do    echo ""Year $YEAR""    wget ""https//www.wunderground.com/history/${SITE}/${STATION}/${YEAR}/1/1/CustomHistory.html?dayend=31&monthend=12&yearend=${YEAR}&req_city=&req_state=&req_statename=&reqdb.zip=&reqdb.magic=&reqdb.wmo=&format=1"" -O ""${LOCAL}_${YEAR}.csv""    tail -n +3 ${LOCAL}_${YEAR}.csv > ${LOCAL}_${YEAR}_1.csv    sed 's/<br\ \/>//g' ${LOCAL}_${YEAR}_1.csv >> ${FILE}    rm ${LOCAL}_${YEAR}.csv ${LOCAL}_${YEAR}_1.csv done ",CSV,,"[climate, history]",Other,,,908,6730,0.5048828125,"Location: Barajas Airport, Madrid. Data: The Weather Company, LLC",Weather Madrid 1997 - 2015,https://www.kaggle.com/juliansimon/weather_madrid_lemd_1997_2015.csv,Tue Sep 06 2016
,University of Michigan,"[year, christianity_protestant, christianity_romancatholic, christianity_easternorthodox, christianity_anglican, christianity_other, christianity_all, judaism_orthodox, judaism_conservative, judaism_reform, judaism_other, judaism_all, islam_sunni, islam_shi’a, islam_ibadhi, islam_nationofislam, islam_alawite, islam_ahmadiyya, islam_other, islam_all, buddhism_mahayana, buddhism_theravada, buddhism_other, buddhism_all, zoroastrianism_all, hinduism_all, sikhism_all, shinto_all, baha’i_all, taoism_all, jainism_all, confucianism_all, syncretism_all, animism_all, noreligion_all, otherreligion_all, religion_all, population, world_population, protestant_percent, romancatholic_percent, easternorthodox_percent, anglican_percent, otherchristianity_percent, christianity_percent, orthodox_percent, conservative_percent, reform_percent, otherjudaism_percent, judaism_percent, sunni_percent, shi’a_percent, ibadhi_percent, nationofislam_percent, alawite_percent, ahmadiyya_percent, otherislam_percent, islam_percent, mahayana_percent, theravada_percent, otherbuddhism_percent, buddhism_percent, zoroastrianism_percent, hinduism_percent, sikhism_percent, shinto_percent, baha’i_percent, taoism_percent, jainism_percent, confucianism_percent, syncretism_percent, animism_percent, noreligion_percent, otherreligion_percent, religion_sumpercent, total_percent]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Content The World Religion Project aims to provide detailed information about religious adherence worldwide since 1945. It contains data about the number of adherents by religion in each of the states in the international system for every half-decade period. Some of the religions are divided into religious families and the breakdown of adherents within a given religion into religious families is provided to the extent data are available. The project was developed in three stages. The first stage consisted of the formation of a religions tree. A religion tree is a systematic classification of major religions and of religious families within those major religions. To develop the religion tree we prepared a comprehensive literature review the aim of which was to define a religion to find tangible indicators of a given religion of religious families within a major religion and to identify existing efforts at classifying world religions. The second stage consisted of the identification of major data sources of religious adherence and the collection of data from these sources according to the religion tree classification. This created a dataset that included multiple records for some states for a given point in time yet contained multiple missing data for specific states specific time periods and specific religions. The third stage consisted of cleaning the data reconciling discrepancies of information from different sources and imputing data for the missing cases. Acknowledgements The dataset was created by Zeev Maoz University of California-Davis and Errol Henderson Pennsylvania State University and published by the Correlates of War Project.,CSV,,"[faith and traditions, politics, war]",Other,,,1211,7757,0.6123046875,"World, regional, and national populations by religious beliefs",Correlates of War: World Religions,https://www.kaggle.com/umichigan/world-religions,Sat Jan 28 2017
,Freedom House,"[Country, 2001 Legal Score, 2001 Political Score, 2001 Economic Score, 2001 Score, 2001 Status, 2002 Legal Score, 2002 Political Score, 2002 Economic Score, 2002 Score, 2002 Status, 2003 Legal Score, 2003 Political Score, 2003 Economic Score, 2003 Score, 2003 Status, 2004 Legal Score, 2004 Political Score, 2004 Economic Score, 2004 Score, 2004 Status, 2005 Legal Score, 2005 Political Score, 2005 Economic Score, 2005 Score, 2005 Status, 2006 Legal Score, 2006 Political Score, 2006 Economic Score, 2006 Score, 2006 Status, 2007 Legal Score, 2007 Political Score, 2007 Economic Score, 2007 Score, 2007 Status, 2008 Legal Score, 2008 Political Score, 2008 Economic Score, 2008 Score, 2008 Status, 2009 Legal Score, 2009 Political Score, 2009 Economic Score, 2009 Score, 2009 Status, 2010 Legal Score, 2010 Political Score, 2010 Economic Score, 2010 Score, 2010 Status, 2011 Legal Score, 2011 Political Score, 2011 Economic Score, 2011 Score, 2011 Status, 2012 Legal Score, 2012 Political Score, 2012 Economic Score, 2012 Score, 2012 Status, 2013 Legal Score, 2013 Political Score, 2013 Economic Score, 2013 Score, 2013 Status, 2014 Legal Score, 2014 Political Score, 2014 Economic Score, 2014 Score, 2014 Status, 2015 Legal Score, 2015 Political Score, 2015 Economic Score, 2015 Score, 2015 Status]","[string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string, numeric, numeric, numeric, numeric, string]",Content The 2016 edition of Freedom of the Press which provides analytical reports and numerical scores for 199 countries and territories continues a process conducted by Freedom House since 1980. Each country and territory is given a total press freedom score from 0 (best) to 100 (worst) on the basis of 23 methodology questions divided into three subcategories. The total score determines the status designation of Free Partly Free or Not Free. The scores and reports included in Freedom of the Press 2016 cover events that took place between January 1 2015 and December 31 2015. The level of press freedom in each country and territory is evaluated through 23 methodology questions divided into three broad categories the legal environment the political environment and the economic environment. For each methodology question a lower number of points is allotted for a more free situation while a higher number of points is allotted for a less free environment. A country or territory’s final score (from 0 to 100) represents the total of the points allotted for each question. A total score of 0 to 30 results in a press freedom status of Free; 31 to 60 results in a status of Partly Free; and 61 to 100 indicates a status of Not Free. The legal environment category encompasses an examination of both the laws and regulations that could influence media content and the extent to which they are used in practice to enable or restrict the media’s ability to operate. We assess the positive impact of legal and constitutional guarantees for freedom of expression; the potentially negative aspects of security legislation the penal code and other statutes; penalties for libel and defamation; the existence of and ability to use freedom of information legislation; the independence of the judiciary and official regulatory bodies; registration requirements for both media outlets and journalists; and the ability of journalists’ organizations to operate freely. Under the political environment category we evaluate the degree of political influence in the content of news media. Issues examined include the editorial independence of both state-owned and privately owned outlets; access to information and sources; official censorship and self-censorship; the vibrancy of the media and the diversity of news available within each country or territory; the ability of both foreign and local reporters to cover the news in person without obstacles or harassment; and reprisals against journalists or bloggers by the state or other actors including arbitrary detention violent assaults and other forms of intimidation. Our third category examines the economic environment for the media. This includes the structure of media ownership; transparency and concentration of ownership; the costs of establishing media as well as any impediments to news production and distribution; the selective withholding of advertising or subsidies by the state or other actors; the impact of corruption and bribery on content; and the extent to which the economic situation in a country or territory affects the development and sustainability of the media.,CSV,,"[news agencies, law]",Other,,,221,1834,0.04296875,"Scores on legal, political, and economic freedom in all countries","Freedom of the Press, 2001-2015",https://www.kaggle.com/freedomhouse/press-freedom,Sat Feb 04 2017
,Federal Emergency Management Agency,"[First Name, Last Name, Age, Rank, Classification, Date of Incident, Date of Death, Cause Of Death, Nature Of Death, Duty, Activity, Emergency, Property Type, ]","[string, string, numeric, string, string, dateTime, dateTime, string, string, string, string, string, string, string]",Content The U. S. Fire Administration tracks and collects information on the causes of on-duty firefighter fatalities that occur in the United States. We conduct an annual analysis to identify specific problems so that we may direct efforts toward finding solutions that will reduce firefighter fatalities in the future. Acknowledgements This study of firefighter fatalities would not have been possible without members of individual fire departments chief fire officers fire service organizations the National Fire Protection Association and the National Fallen Firefighters Foundation.,CSV,,[firefighting],CC0,,,224,2091,0.265625,"Name, rank, and cause of death for all firefighters killed since 2000",Firefighter Fatalities in the United States,https://www.kaggle.com/fema/firefighter-fatalities,Thu Jan 26 2017
,UCI Machine Learning,"[ID, LIMIT_BAL, SEX, EDUCATION, MARRIAGE, AGE, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6, BILL_AMT1, BILL_AMT2, BILL_AMT3, BILL_AMT4, BILL_AMT5, BILL_AMT6, PAY_AMT1, PAY_AMT2, PAY_AMT3, PAY_AMT4, PAY_AMT5, PAY_AMT6, default.payment.next.month]","[numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric, numeric]",Dataset Information This dataset contains information on default payments demographic factors credit data history of payment and bill statements of credit card clients in Taiwan from April 2005 to September 2005.  Content There are 25 variables  ID ID of each client LIMIT_BAL Amount of given credit in NT dollars (includes individual and family/supplementary credit SEX Gender (1=male 2=female) EDUCATION (1=graduate school 2=university 3=high school 4=others 5=unknown 6=unknown) MARRIAGE Marital status (1=married 2=single 3=others) AGE Age in years PAY_0 Repayment status in September 2005 (-1=pay duly 1=payment delay for one month 2=payment delay for two months ... 8=payment delay for eight months 9=payment delay for nine months and above) PAY_2 Repayment status in August 2005 (scale same as above) PAY_3 Repayment status in July 2005 (scale same as above) PAY_4 Repayment status in June 2005 (scale same as above) PAY_5 Repayment status in May 2005 (scale same as above) PAY_6 Repayment status in April 2005 (scale same as above) BILL_AMT1 Amount of bill statement in September 2005 (NT dollar) BILL_AMT2 Amount of bill statement in August 2005 (NT dollar) BILL_AMT3 Amount of bill statement in July 2005 (NT dollar) BILL_AMT4 Amount of bill statement in June 2005 (NT dollar) BILL_AMT5 Amount of bill statement in May 2005 (NT dollar) BILL_AMT6 Amount of bill statement in April 2005 (NT dollar) PAY_AMT1 Amount of previous payment in September 2005 (NT dollar) PAY_AMT2 Amount of previous payment in August 2005 (NT dollar) PAY_AMT3 Amount of previous payment in July 2005 (NT dollar) PAY_AMT4 Amount of previous payment in June 2005 (NT dollar) PAY_AMT5 Amount of previous payment in May 2005 (NT dollar) PAY_AMT6 Amount of previous payment in April 2005 (NT dollar) default.payment.next.month Default payment (1=yes 0=no)  Inspiration Some ideas for exploration  How does the probability of default payment vary by categories of different demographic variables? Which variables are the strongest predictors of default payment?  Acknowledgements Any publications based on this dataset should acknowledge the following  Lichman M. (2013). UCI Machine Learning Repository [http//archive.ics.uci.edu/ml]. Irvine CA University of California School of Information and Computer Science. The original dataset can be found here at the UCI Machine Learning Repository.,CSV,,[finance],CC0,,,7491,75025,3,Default Payments of Credit Card Clients in Taiwan from 2005,Default of Credit Card Clients Dataset,https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset,Thu Nov 03 2016
,,,,,,,,,,,,,,,,,
,,,,,CSV,,"[food and drink, psychometrics]",Other,,,0,0,0.0029296875,Evaluate the effects of chopsticks length on food-serving performance,Beginner Projects - Ergonomic Study on Chopsticks,https://www.kaggle.com/priya2908/chopsticks-1992,Mon Jun 12 2017
